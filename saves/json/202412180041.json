[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.12094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v1",
                "updated": "2024-12-16T18:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v2",
                "updated": "2024-12-16T16:37:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    37,
                    7,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10382v1",
                "updated": "2024-11-28T10:40:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    40,
                    47,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T10:40:47Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    40,
                    47,
                    3,
                    333,
                    0
                ],
                "title": "Many Hands Make Light Work: Accelerating Edge Inference via Multi-Client\n  Collaborative Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many Hands Make Light Work: Accelerating Edge Inference via Multi-Client\n  Collaborative Caching"
                },
                "summary": "Edge inference is a technology that enables real-time data processing and\nanalysis on clients near the data source. To ensure compliance with the\nService-Level Objectives (SLOs), such as a 30% latency reduction target,\ncaching is usually adopted to reduce redundant computations in inference tasks\non stream data. Due to task and data correlations, sharing cache information\namong clients can improve the inference performance. However, the\nnon-independent and identically distributed (non-IID) nature of data across\ndifferent clients and the long-tail distributions, where some classes have\nsignificantly more samples than others, will reduce cache hit ratios and\nincrease latency. To address the aforementioned challenges, we propose an\nefficient inference framework, CoCa, which leverages a multi-client\ncollaborative caching mechanism to accelerate edge inference. On the client\nside, the model is pre-set with multiple cache layers to achieve a quick\ninference. During inference, the model performs sequential lookups at cache\nlayers activated by the edge server. On the server side, CoCa uses a\ntwo-dimensional global cache to periodically aggregate information from\nclients, mitigating the effects of non-IID data. For client cache allocation,\nCoCa first evaluates the importance of classes based on how frequently and\nrecently their samples have been accessed. CoCa then selects frequently\nrecurring classes to address long-tail distribution challenges. Finally, CoCa\ndynamically activates cache layers to balance lookup overhead and accuracy.\nExtensive experiments demonstrate that CoCa reduces inference latency by 23.0%\nto 45.2% on the VGG, ResNet and AST models with a slight loss of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge inference is a technology that enables real-time data processing and\nanalysis on clients near the data source. To ensure compliance with the\nService-Level Objectives (SLOs), such as a 30% latency reduction target,\ncaching is usually adopted to reduce redundant computations in inference tasks\non stream data. Due to task and data correlations, sharing cache information\namong clients can improve the inference performance. However, the\nnon-independent and identically distributed (non-IID) nature of data across\ndifferent clients and the long-tail distributions, where some classes have\nsignificantly more samples than others, will reduce cache hit ratios and\nincrease latency. To address the aforementioned challenges, we propose an\nefficient inference framework, CoCa, which leverages a multi-client\ncollaborative caching mechanism to accelerate edge inference. On the client\nside, the model is pre-set with multiple cache layers to achieve a quick\ninference. During inference, the model performs sequential lookups at cache\nlayers activated by the edge server. On the server side, CoCa uses a\ntwo-dimensional global cache to periodically aggregate information from\nclients, mitigating the effects of non-IID data. For client cache allocation,\nCoCa first evaluates the importance of classes based on how frequently and\nrecently their samples have been accessed. CoCa then selects frequently\nrecurring classes to address long-tail distribution challenges. Finally, CoCa\ndynamically activates cache layers to balance lookup overhead and accuracy.\nExtensive experiments demonstrate that CoCa reduces inference latency by 23.0%\nto 45.2% on the VGG, ResNet and AST models with a slight loss of accuracy."
                },
                "authors": [
                    {
                        "name": "Wenyi Liang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "arxiv_comment": "IEEE International Conference on Data Engineering (ICDE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08652v1",
                "updated": "2024-11-26T17:52:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    52,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:52:21Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    52,
                    21,
                    1,
                    331,
                    0
                ],
                "title": "Twenty-Year Review of Outdoor Air Quality in Utah, USA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twenty-Year Review of Outdoor Air Quality in Utah, USA"
                },
                "summary": "Air quality is a prevalent concern due to its imposing health risks. The\nstate of Utah, USA, has, at times over the last 20 years, experienced some of\nthe worst air quality in the nation. The propensity for Utah to experience\nelevated concentrations of particulate matter ($\\mathrm{PM_{2.5}}$) and ozone\n($\\mathrm{O_3}$) can, in part, be attributed to its unique geography, which\nfeatures dry, mountainous terrain. Valleys in Utah create ideal environments\nfor extended cold-pool events. In this review, we summarize air quality\nresearch conducted in Utah over the past 20 years (2002-2022) by dividing the\nstate into six regions: Utah Valley, Summit County, Southern Utah (regions\nsouth of Utah Valley), Cache Valley, Uinta Basin, and Salt Lake Valley. We\nreview the published literature chronologically and provide a summary for each\nregion, identifying areas where additional research is warranted. We found that\nresearch efforts are heavily weighted toward the Uinta Basin and Salt Lake\nValley, with the remaining regions collectively accounting for only 20% of\nstudies. We identified the need for more source apportionment studies,\nspeciated volatile organic compound (VOC) analyses, and ozone isopleths. Where\nozone isopleths cannot be created, measurements of glyoxal ($\\mathrm{CHOCHO}$)\nand formaldehyde ($\\mathrm{HCHO}$) concentrations could serve as cost-effective\nsurrogates to inform ozone mitigation policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air quality is a prevalent concern due to its imposing health risks. The\nstate of Utah, USA, has, at times over the last 20 years, experienced some of\nthe worst air quality in the nation. The propensity for Utah to experience\nelevated concentrations of particulate matter ($\\mathrm{PM_{2.5}}$) and ozone\n($\\mathrm{O_3}$) can, in part, be attributed to its unique geography, which\nfeatures dry, mountainous terrain. Valleys in Utah create ideal environments\nfor extended cold-pool events. In this review, we summarize air quality\nresearch conducted in Utah over the past 20 years (2002-2022) by dividing the\nstate into six regions: Utah Valley, Summit County, Southern Utah (regions\nsouth of Utah Valley), Cache Valley, Uinta Basin, and Salt Lake Valley. We\nreview the published literature chronologically and provide a summary for each\nregion, identifying areas where additional research is warranted. We found that\nresearch efforts are heavily weighted toward the Uinta Basin and Salt Lake\nValley, with the remaining regions collectively accounting for only 20% of\nstudies. We identified the need for more source apportionment studies,\nspeciated volatile organic compound (VOC) analyses, and ozone isopleths. Where\nozone isopleths cannot be created, measurements of glyoxal ($\\mathrm{CHOCHO}$)\nand formaldehyde ($\\mathrm{HCHO}$) concentrations could serve as cost-effective\nsurrogates to inform ozone mitigation policies."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14101496",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14101496",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1496",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.12096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12096v1",
                "updated": "2024-12-16T18:59:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    59,
                    45,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:59:45Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    59,
                    45,
                    0,
                    351,
                    0
                ],
                "title": "PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting"
                },
                "summary": "With the advent of portable 360{\\deg} cameras, panorama has gained\nsignificant attention in applications like virtual reality (VR), virtual tours,\nrobotics, and autonomous driving. As a result, wide-baseline panorama view\nsynthesis has emerged as a vital task, where high resolution, fast inference,\nand memory efficiency are essential. Nevertheless, existing methods are\ntypically constrained to lower resolutions (512 $\\times$ 1024) due to demanding\nmemory and computational requirements. In this paper, we present PanSplat, a\ngeneralizable, feed-forward approach that efficiently supports resolution up to\n4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian\npyramid with a Fibonacci lattice arrangement, enhancing image quality while\nreducing information redundancy. To accommodate the demands of high resolution,\nwe propose a pipeline that integrates a hierarchical spherical cost volume and\nGaussian heads with local operations, enabling two-step deferred\nbackpropagation for memory-efficient training on a single A100 GPU. Experiments\ndemonstrate that PanSplat achieves state-of-the-art results with superior\nefficiency and image quality across both synthetic and real-world datasets.\nCode will be available at \\url{https://github.com/chengzhag/PanSplat}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of portable 360{\\deg} cameras, panorama has gained\nsignificant attention in applications like virtual reality (VR), virtual tours,\nrobotics, and autonomous driving. As a result, wide-baseline panorama view\nsynthesis has emerged as a vital task, where high resolution, fast inference,\nand memory efficiency are essential. Nevertheless, existing methods are\ntypically constrained to lower resolutions (512 $\\times$ 1024) due to demanding\nmemory and computational requirements. In this paper, we present PanSplat, a\ngeneralizable, feed-forward approach that efficiently supports resolution up to\n4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian\npyramid with a Fibonacci lattice arrangement, enhancing image quality while\nreducing information redundancy. To accommodate the demands of high resolution,\nwe propose a pipeline that integrates a hierarchical spherical cost volume and\nGaussian heads with local operations, enabling two-step deferred\nbackpropagation for memory-efficient training on a single A100 GPU. Experiments\ndemonstrate that PanSplat achieves state-of-the-art results with superior\nefficiency and image quality across both synthetic and real-world datasets.\nCode will be available at \\url{https://github.com/chengzhag/PanSplat}."
                },
                "authors": [
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Haofei Xu"
                    },
                    {
                        "name": "Qianyi Wu"
                    },
                    {
                        "name": "Camilo Cruz Gambardella"
                    },
                    {
                        "name": "Dinh Phung"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Project Page: https://chengzhag.github.io/publication/pansplat/ Code:\n  https://github.com/chengzhag/PanSplat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v1",
                "updated": "2024-12-16T18:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12072v1",
                "updated": "2024-12-16T18:46:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    46,
                    12,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:46:12Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    46,
                    12,
                    0,
                    351,
                    0
                ],
                "title": "Making FETCH! Happen: Finding Emergent Dog Whistles Through Common\n  Habitats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making FETCH! Happen: Finding Emergent Dog Whistles Through Common\n  Habitats"
                },
                "summary": "WARNING: This paper contains content that maybe upsetting or offensive to\nsome readers. Dog whistles are coded expressions with dual meanings: one\nintended for the general public (outgroup) and another that conveys a specific\nmessage to an intended audience (ingroup). Often, these expressions are used to\nconvey controversial political opinions while maintaining plausible deniability\nand slip by content moderation filters. Identification of dog whistles relies\non curated lexicons, which have trouble keeping up to date. We introduce\n\\textbf{FETCH!}, a task for finding novel dog whistles in massive social media\ncorpora. We find that state-of-the-art systems fail to achieve meaningful\nresults across three distinct social media case studies. We present\n\\textbf{EarShot}, a novel system that combines the strengths of vector\ndatabases and Large Language Models (LLMs) to efficiently and effectively\nidentify new dog whistles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WARNING: This paper contains content that maybe upsetting or offensive to\nsome readers. Dog whistles are coded expressions with dual meanings: one\nintended for the general public (outgroup) and another that conveys a specific\nmessage to an intended audience (ingroup). Often, these expressions are used to\nconvey controversial political opinions while maintaining plausible deniability\nand slip by content moderation filters. Identification of dog whistles relies\non curated lexicons, which have trouble keeping up to date. We introduce\n\\textbf{FETCH!}, a task for finding novel dog whistles in massive social media\ncorpora. We find that state-of-the-art systems fail to achieve meaningful\nresults across three distinct social media case studies. We present\n\\textbf{EarShot}, a novel system that combines the strengths of vector\ndatabases and Large Language Models (LLMs) to efficiently and effectively\nidentify new dog whistles."
                },
                "authors": [
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Carlos Aguirre"
                    },
                    {
                        "name": "Isabel Cachola"
                    },
                    {
                        "name": "Sharon Levy"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10799v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10799v4",
                "updated": "2024-12-16T18:31:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    31,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-03-16T04:12:50Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    4,
                    12,
                    50,
                    5,
                    76,
                    0
                ],
                "title": "Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment"
                },
                "summary": "Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10799v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10799v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08039v2",
                "updated": "2024-12-16T18:28:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    28,
                    19,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-12T09:41:12Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    9,
                    41,
                    12,
                    2,
                    164,
                    0
                ],
                "title": "Differentially Private Prototypes for Imbalanced Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Prototypes for Imbalanced Transfer Learning"
                },
                "summary": "Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\n\\textit{pure DP}. We additionally show that privacy-utility trade-offs can be\nfurther improved when leveraging the public data beyond pre-training of the\nencoder: in particular, we can privately sample our DP prototypes from the\npublicly available data points used to train the encoder. Our experimental\nevaluation with four state-of-the-art encoders, four vision datasets, and under\ndifferent data and imbalancedness regimes demonstrate DPPL's high performance\nunder strong privacy guarantees in challenging private learning setups",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\n\\textit{pure DP}. We additionally show that privacy-utility trade-offs can be\nfurther improved when leveraging the public data beyond pre-training of the\nencoder: in particular, we can privately sample our DP prototypes from the\npublicly available data points used to train the encoder. Our experimental\nevaluation with four state-of-the-art encoders, four vision datasets, and under\ndifferent data and imbalancedness regimes demonstrate DPPL's high performance\nunder strong privacy guarantees in challenging private learning setups"
                },
                "authors": [
                    {
                        "name": "Dariush Wahdany"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Adam Dziedzic"
                    },
                    {
                        "name": "Franziska Boenisch"
                    }
                ],
                "author_detail": {
                    "name": "Franziska Boenisch"
                },
                "author": "Franziska Boenisch",
                "arxiv_comment": "To be published at the 39th Annual AAAI Conference on Artificial\n  Intelligence, Philadelphia, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11848v2",
                "updated": "2024-12-16T18:25:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    25,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-13T01:30:03Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    1,
                    30,
                    3,
                    1,
                    226,
                    0
                ],
                "title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MGH Radiology Llama: A Llama 3 70B Model for Radiology"
                },
                "summary": "In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs."
                },
                "authors": [
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "11 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08628v2",
                "updated": "2024-12-16T18:16:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    16,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-11T18:48:20Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    48,
                    20,
                    2,
                    346,
                    0
                ],
                "title": "EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation"
                },
                "summary": "Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatialaware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and\n11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times\nfaster than state-of-theart methods. Especially, equipped with ResNet50\nbackbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090\nGPU. Code is available at https://github.com/nhw649/EOV-Seg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatialaware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and\n11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times\nfaster than state-of-theart methods. Especially, equipped with ResNet50\nbackbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090\nGPU. Code is available at https://github.com/nhw649/EOV-Seg."
                },
                "authors": [
                    {
                        "name": "Hongwei Niu"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Jianghang Lin"
                    },
                    {
                        "name": "Guannan Jiang"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengchuan Zhang"
                },
                "author": "Shengchuan Zhang",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12039v1",
                "updated": "2024-12-16T18:08:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    8,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:08:14Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    8,
                    14,
                    0,
                    351,
                    0
                ],
                "title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability\n  Detection"
                },
                "summary": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively."
                },
                "authors": [
                    {
                        "name": "Ira Ceka"
                    },
                    {
                        "name": "Feitong Qiao"
                    },
                    {
                        "name": "Anik Dey"
                    },
                    {
                        "name": "Aastha Valechia"
                    },
                    {
                        "name": "Gail Kaiser"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12038v1",
                "updated": "2024-12-16T18:03:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    3,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:03:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    3,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "LLMs for Cold-Start Cutting Plane Separator Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Cold-Start Cutting Plane Separator Configuration"
                },
                "summary": "Mixed integer linear programming (MILP) solvers ship with a staggering number\nof parameters that are challenging to select a priori for all but expert\noptimization users, but can have an outsized impact on the performance of the\nMILP solver. Existing machine learning (ML) approaches to configure solvers\nrequire training ML models by solving thousands of related MILP instances,\ngeneralize poorly to new problem sizes, and often require implementing complex\nML pipelines and custom solver interfaces that can be difficult to integrate\ninto existing optimization workflows. In this paper, we introduce a new\nLLM-based framework to configure which cutting plane separators to use for a\ngiven MILP problem with little to no training data based on characteristics of\nthe instance, such as a natural language description of the problem and the\nassociated LaTeX formulation. We augment these LLMs with descriptions of\ncutting plane separators available in a given solver, grounded by summarizing\nthe existing research literature on separators. While individual solver\nconfigurations have a large variance in performance, we present a novel\nensembling strategy that clusters and aggregates configurations to create a\nsmall portfolio of high-performing configurations. Our LLM-based methodology\nrequires no custom solver interface, can find a high-performing configuration\nby solving only a small number of MILPs, and can generate the configuration\nwith simple API calls that run in under a second. Numerical results show our\napproach is competitive with existing configuration approaches on a suite of\nclassic combinatorial optimization problems and real-world datasets with only a\nfraction of the training data and computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed integer linear programming (MILP) solvers ship with a staggering number\nof parameters that are challenging to select a priori for all but expert\noptimization users, but can have an outsized impact on the performance of the\nMILP solver. Existing machine learning (ML) approaches to configure solvers\nrequire training ML models by solving thousands of related MILP instances,\ngeneralize poorly to new problem sizes, and often require implementing complex\nML pipelines and custom solver interfaces that can be difficult to integrate\ninto existing optimization workflows. In this paper, we introduce a new\nLLM-based framework to configure which cutting plane separators to use for a\ngiven MILP problem with little to no training data based on characteristics of\nthe instance, such as a natural language description of the problem and the\nassociated LaTeX formulation. We augment these LLMs with descriptions of\ncutting plane separators available in a given solver, grounded by summarizing\nthe existing research literature on separators. While individual solver\nconfigurations have a large variance in performance, we present a novel\nensembling strategy that clusters and aggregates configurations to create a\nsmall portfolio of high-performing configurations. Our LLM-based methodology\nrequires no custom solver interface, can find a high-performing configuration\nby solving only a small number of MILPs, and can generate the configuration\nwith simple API calls that run in under a second. Numerical results show our\napproach is competitive with existing configuration approaches on a suite of\nclassic combinatorial optimization problems and real-world datasets with only a\nfraction of the training data and computation time."
                },
                "authors": [
                    {
                        "name": "Connor Lawless"
                    },
                    {
                        "name": "Yingxi Li"
                    },
                    {
                        "name": "Anders Wikum"
                    },
                    {
                        "name": "Madeleine Udell"
                    },
                    {
                        "name": "Ellen Vitercik"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Vitercik"
                },
                "author": "Ellen Vitercik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12009v1",
                "updated": "2024-12-16T17:36:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    36,
                    2,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:36:02Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    36,
                    2,
                    0,
                    351,
                    0
                ],
                "title": "SpeechPrune: Context-aware Token Pruning for Speech Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechPrune: Context-aware Token Pruning for Speech Information\n  Retrieval"
                },
                "summary": "We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding."
                },
                "authors": [
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Project page and dataset is available at\n  https://speechprune.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14397v2",
                "updated": "2024-12-16T17:34:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    34,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-04-22T17:56:26Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    17,
                    56,
                    26,
                    0,
                    113,
                    0
                ],
                "title": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?"
                },
                "summary": "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Ishaan Watts"
                    },
                    {
                        "name": "Tua Wongsangaroonsri"
                    },
                    {
                        "name": "Minghui Zhang"
                    },
                    {
                        "name": "Noura Farra"
                    },
                    {
                        "name": "Nektar Ege Altntoprak"
                    },
                    {
                        "name": "Lena Baur"
                    },
                    {
                        "name": "Samantha Claudet"
                    },
                    {
                        "name": "Pavel Gajdusek"
                    },
                    {
                        "name": "Can Gren"
                    },
                    {
                        "name": "Qilong Gu"
                    },
                    {
                        "name": "Anna Kaminska"
                    },
                    {
                        "name": "Tomasz Kaminski"
                    },
                    {
                        "name": "Ruby Kuo"
                    },
                    {
                        "name": "Akiko Kyuba"
                    },
                    {
                        "name": "Jongho Lee"
                    },
                    {
                        "name": "Kartik Mathur"
                    },
                    {
                        "name": "Petter Merok"
                    },
                    {
                        "name": "Ivana Milovanovi"
                    },
                    {
                        "name": "Nani Paananen"
                    },
                    {
                        "name": "Vesa-Matti Paananen"
                    },
                    {
                        "name": "Anna Pavlenko"
                    },
                    {
                        "name": "Bruno Pereira Vidal"
                    },
                    {
                        "name": "Luciano Strika"
                    },
                    {
                        "name": "Yueh Tsao"
                    },
                    {
                        "name": "Davide Turcato"
                    },
                    {
                        "name": "Oleksandr Vakhno"
                    },
                    {
                        "name": "Judit Velcsov"
                    },
                    {
                        "name": "Anna Vickers"
                    },
                    {
                        "name": "Stphanie Visser"
                    },
                    {
                        "name": "Herdyan Widarmanto"
                    },
                    {
                        "name": "Andrey Zaikin"
                    },
                    {
                        "name": "Si-Qing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Si-Qing Chen"
                },
                "author": "Si-Qing Chen",
                "arxiv_comment": "AAAI 2025--camera ready + extended abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12004v1",
                "updated": "2024-12-16T17:32:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    32,
                    11,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:32:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    32,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "The Open Source Advantage in Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Source Advantage in Large Language Models (LLMs)"
                },
                "summary": "Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment."
                },
                "authors": [
                    {
                        "name": "Jiya Manchanda"
                    },
                    {
                        "name": "Laura Boettcher"
                    },
                    {
                        "name": "Matheus Westphalen"
                    },
                    {
                        "name": "Jasser Jasser"
                    }
                ],
                "author_detail": {
                    "name": "Jasser Jasser"
                },
                "author": "Jasser Jasser",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12001v1",
                "updated": "2024-12-16T17:29:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    29,
                    51,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:29:51Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    29,
                    51,
                    0,
                    351,
                    0
                ],
                "title": "LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts"
                },
                "summary": "Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem."
                },
                "authors": [
                    {
                        "name": "Zhuhao Wang"
                    },
                    {
                        "name": "Yihua Sun"
                    },
                    {
                        "name": "Zihan Li"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Fang Chen"
                    },
                    {
                        "name": "Hongen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Hongen Liao"
                },
                "author": "Hongen Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11995v1",
                "updated": "2024-12-16T17:22:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    22,
                    40,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:22:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    22,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Combining Large Language Models with Tutoring System Intelligence: A\n  Case Study in Caregiver Homework Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Large Language Models with Tutoring System Intelligence: A\n  Case Study in Caregiver Homework Support"
                },
                "summary": "Caregivers (i.e., parents and members of a child's caring community) are\nunderappreciated stakeholders in learning analytics. Although caregiver\ninvolvement can enhance student academic outcomes, many obstacles hinder\ninvolvement, most notably knowledge gaps with respect to modern school\ncurricula. An emerging topic of interest in learning analytics is hybrid\ntutoring, which includes instructional and motivational support. Caregivers\nassert similar roles in homework, yet it is unknown how learning analytics can\nsupport them. Our past work with caregivers suggested that conversational\nsupport is a promising method of providing caregivers with the guidance needed\nto effectively support student learning. We developed a system that provides\ninstructional support to caregivers through conversational recommendations\ngenerated by a Large Language Model (LLM). Addressing known instructional\nlimitations of LLMs, we use instructional intelligence from tutoring systems\nwhile conducting prompt engineering experiments with the open-source Llama 3\nLLM. This LLM generated message recommendations for caregivers supporting their\nchild's math practice via chat. Few-shot prompting and combining real-time\nproblem-solving context from tutoring systems with examples of tutoring\npractices yielded desirable message recommendations. These recommendations were\nevaluated with ten middle school caregivers, who valued recommendations\nfacilitating content-level support and student metacognition through\nself-explanation. We contribute insights into how tutoring systems can best be\nmerged with LLMs to support hybrid tutoring settings through conversational\nassistance, facilitating effective caregiver involvement in tutoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caregivers (i.e., parents and members of a child's caring community) are\nunderappreciated stakeholders in learning analytics. Although caregiver\ninvolvement can enhance student academic outcomes, many obstacles hinder\ninvolvement, most notably knowledge gaps with respect to modern school\ncurricula. An emerging topic of interest in learning analytics is hybrid\ntutoring, which includes instructional and motivational support. Caregivers\nassert similar roles in homework, yet it is unknown how learning analytics can\nsupport them. Our past work with caregivers suggested that conversational\nsupport is a promising method of providing caregivers with the guidance needed\nto effectively support student learning. We developed a system that provides\ninstructional support to caregivers through conversational recommendations\ngenerated by a Large Language Model (LLM). Addressing known instructional\nlimitations of LLMs, we use instructional intelligence from tutoring systems\nwhile conducting prompt engineering experiments with the open-source Llama 3\nLLM. This LLM generated message recommendations for caregivers supporting their\nchild's math practice via chat. Few-shot prompting and combining real-time\nproblem-solving context from tutoring systems with examples of tutoring\npractices yielded desirable message recommendations. These recommendations were\nevaluated with ten middle school caregivers, who valued recommendations\nfacilitating content-level support and student metacognition through\nself-explanation. We contribute insights into how tutoring systems can best be\nmerged with LLMs to support hybrid tutoring settings through conversational\nassistance, facilitating effective caregiver involvement in tutoring systems."
                },
                "authors": [
                    {
                        "name": "Devika Venugopalan"
                    },
                    {
                        "name": "Ziwen Yan"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Vincent Aleven"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Aleven"
                },
                "author": "Vincent Aleven",
                "arxiv_doi": "10.1145/3706468.3706516",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706468.3706516",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.11995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11990v1",
                "updated": "2024-12-16T17:14:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    14,
                    35,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    14,
                    35,
                    0,
                    351,
                    0
                ],
                "title": "ExecRepoBench: Multi-level Executable Code Completion Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExecRepoBench: Multi-level Executable Code Completion Evaluation"
                },
                "summary": "Code completion has become an essential tool for daily software development.\nExisting evaluation benchmarks often employ static methods that do not fully\ncapture the dynamic nature of real-world coding environments and face\nsignificant challenges, including limited context length, reliance on\nsuperficial evaluation metrics, and potential overfitting to training datasets.\nIn this work, we introduce a novel framework for enhancing code completion in\nsoftware development through the creation of a repository-level benchmark\nExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the\nfunctionality of open-source large language models (LLMs) in real-world coding\nscenarios that involve complex interdependencies across multiple files.\nExecRepoBench includes 1.2K samples from active Python repositories. Plus, we\npresent a multi-level grammar-based completion methodology conditioned on the\nabstract syntax tree to mask code fragments at various logical units (e.g.\nstatements, expressions, and functions). Then, we fine-tune the open-source LLM\nwith 7B parameters on Repo-Instruct to produce a strong code completion\nbaseline model Qwen2.5-Coder-Instruct-C based on the open-source model.\nQwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks,\nincluding MultiPL-E and ExecRepoBench, which consistently outperforms prior\nbaselines across all programming languages. The deployment of \\ourmethod{} can\nbe used as a high-performance, local service for programming\ndevelopment\\footnote{\\url{https://execrepobench.github.io/}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion has become an essential tool for daily software development.\nExisting evaluation benchmarks often employ static methods that do not fully\ncapture the dynamic nature of real-world coding environments and face\nsignificant challenges, including limited context length, reliance on\nsuperficial evaluation metrics, and potential overfitting to training datasets.\nIn this work, we introduce a novel framework for enhancing code completion in\nsoftware development through the creation of a repository-level benchmark\nExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the\nfunctionality of open-source large language models (LLMs) in real-world coding\nscenarios that involve complex interdependencies across multiple files.\nExecRepoBench includes 1.2K samples from active Python repositories. Plus, we\npresent a multi-level grammar-based completion methodology conditioned on the\nabstract syntax tree to mask code fragments at various logical units (e.g.\nstatements, expressions, and functions). Then, we fine-tune the open-source LLM\nwith 7B parameters on Repo-Instruct to produce a strong code completion\nbaseline model Qwen2.5-Coder-Instruct-C based on the open-source model.\nQwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks,\nincluding MultiPL-E and ExecRepoBench, which consistently outperforms prior\nbaselines across all programming languages. The deployment of \\ourmethod{} can\nbe used as a high-performance, local service for programming\ndevelopment\\footnote{\\url{https://execrepobench.github.io/}}."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11988v1",
                "updated": "2024-12-16T17:11:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    11,
                    48,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:11:48Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    11,
                    48,
                    0,
                    351,
                    0
                ],
                "title": "SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation"
                },
                "summary": "Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors."
                },
                "authors": [
                    {
                        "name": "Debarshi Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Debarshi Kundu"
                },
                "author": "Debarshi Kundu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11983v1",
                "updated": "2024-12-16T17:04:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    4,
                    40,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:04:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    4,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Cost-Effective Label-free Node Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective Label-free Node Classification with LLMs"
                },
                "summary": "Graph neural networks (GNNs) have emerged as go-to models for node\nclassification in graph data due to their powerful abilities in fusing graph\nstructures and attributes. However, such models strongly rely on adequate\nhigh-quality labeled data for training, which are expensive to acquire in\npractice. With the advent of large language models (LLMs), a promising way is\nto leverage their superb zero-shot capabilities and massive knowledge for node\nlabeling. Despite promising results reported, this methodology either demands\nconsiderable queries to LLMs, or suffers from compromised performance caused by\nnoisy labels produced by LLMs.\n  To remedy these issues, this work presents Cella, an active self-training\nframework that integrates LLMs into GNNs in a cost-effective manner. The design\nrecipe of Cella is to iteratively identify small sets of \"critical\" samples\nusing GNNs and extract informative pseudo-labels for them with both LLMs and\nGNNs as additional supervision signals to enhance model training. Particularly,\nCella includes three major components: (i) an effective active node selection\nstrategy for initial annotations; (ii) a judicious sample selection scheme to\nsift out the \"critical\" nodes based on label disharmonicity and entropy; and\n(iii) a label refinement module combining LLMs and GNNs with rewired topology.\nOur extensive experiments over five benchmark text-attributed graph datasets\ndemonstrate that Cella significantly outperforms the state of the arts under\nthe same query budget to LLMs in terms of label-free node classification. In\nparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an\n8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost\nof less than one cent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have emerged as go-to models for node\nclassification in graph data due to their powerful abilities in fusing graph\nstructures and attributes. However, such models strongly rely on adequate\nhigh-quality labeled data for training, which are expensive to acquire in\npractice. With the advent of large language models (LLMs), a promising way is\nto leverage their superb zero-shot capabilities and massive knowledge for node\nlabeling. Despite promising results reported, this methodology either demands\nconsiderable queries to LLMs, or suffers from compromised performance caused by\nnoisy labels produced by LLMs.\n  To remedy these issues, this work presents Cella, an active self-training\nframework that integrates LLMs into GNNs in a cost-effective manner. The design\nrecipe of Cella is to iteratively identify small sets of \"critical\" samples\nusing GNNs and extract informative pseudo-labels for them with both LLMs and\nGNNs as additional supervision signals to enhance model training. Particularly,\nCella includes three major components: (i) an effective active node selection\nstrategy for initial annotations; (ii) a judicious sample selection scheme to\nsift out the \"critical\" nodes based on label disharmonicity and entropy; and\n(iii) a label refinement module combining LLMs and GNNs with rewired topology.\nOur extensive experiments over five benchmark text-attributed graph datasets\ndemonstrate that Cella significantly outperforms the state of the arts under\nthe same query budget to LLMs in terms of label-free node classification. In\nparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an\n8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost\nof less than one cent."
                },
                "authors": [
                    {
                        "name": "Taiyan Zhang"
                    },
                    {
                        "name": "Renchi Yang"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Dongrui Fan"
                    },
                    {
                        "name": "Yurui Lai"
                    }
                ],
                "author_detail": {
                    "name": "Yurui Lai"
                },
                "author": "Yurui Lai",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11979v1",
                "updated": "2024-12-16T16:59:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    59,
                    55,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:59:55Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    59,
                    55,
                    0,
                    351,
                    0
                ],
                "title": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power\n  Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power\n  Laws"
                },
                "summary": "Neural scaling laws are observed in a range of domains, to date with no clear\nunderstanding of why they occur. Recent theories suggest that loss power laws\narise from Zipf's law, a power law observed in domains like natural language.\nOne theory suggests that language scaling laws emerge when Zipf-distributed\ntask quanta are learned in descending order of frequency. In this paper we\nexamine power-law scaling in AlphaZero, a reinforcement learning algorithm,\nusing a theory of language-model scaling. We find that game states in training\nand inference data scale with Zipf's law, which is known to arise from the tree\nstructure of the environment, and examine the correlation between scaling-law\nand Zipf's-law exponents. In agreement with quanta scaling theory, we find that\nagents optimize state loss in descending order of frequency, even though this\norder scales inversely with modelling complexity. We also find that inverse\nscaling, the failure of models to improve with size, is correlated with unusual\nZipf curves where end-game states are among the most frequent states. We show\nevidence that larger models shift their focus to these less-important states,\nsacrificing their understanding of important early-game states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural scaling laws are observed in a range of domains, to date with no clear\nunderstanding of why they occur. Recent theories suggest that loss power laws\narise from Zipf's law, a power law observed in domains like natural language.\nOne theory suggests that language scaling laws emerge when Zipf-distributed\ntask quanta are learned in descending order of frequency. In this paper we\nexamine power-law scaling in AlphaZero, a reinforcement learning algorithm,\nusing a theory of language-model scaling. We find that game states in training\nand inference data scale with Zipf's law, which is known to arise from the tree\nstructure of the environment, and examine the correlation between scaling-law\nand Zipf's-law exponents. In agreement with quanta scaling theory, we find that\nagents optimize state loss in descending order of frequency, even though this\norder scales inversely with modelling complexity. We also find that inverse\nscaling, the failure of models to improve with size, is correlated with unusual\nZipf curves where end-game states are among the most frequent states. We show\nevidence that larger models shift their focus to these less-important states,\nsacrificing their understanding of important early-game states."
                },
                "authors": [
                    {
                        "name": "Oren Neumann"
                    },
                    {
                        "name": "Claudius Gros"
                    }
                ],
                "author_detail": {
                    "name": "Claudius Gros"
                },
                "author": "Claudius Gros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11970v1",
                "updated": "2024-12-16T16:51:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    51,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:51:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    51,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners"
                },
                "summary": "Materials discovery and design aim to find components and structures with\ndesirable properties over highly complex and diverse search spaces. Traditional\nsolutions, such as high-throughput simulations and machine learning (ML), often\nrely on complex descriptors, which hinder generalizability and transferability\nacross tasks. Moreover, these descriptors may deviate from experimental data\ndue to inevitable defects and purity issues in the real world, which may reduce\ntheir effectiveness in practical applications. To address these challenges, we\npropose Darwin 1.5, an open-source large language model (LLM) tailored for\nmaterials science. By leveraging natural language as input, Darwin eliminates\nthe need for task-specific descriptors and enables a flexible, unified approach\nto material property prediction and discovery. We employ a two-stage training\nstrategy combining question-answering (QA) fine-tuning with multi-task learning\n(MTL) to inject domain-specific knowledge in various modalities and facilitate\ncross-task knowledge transfer. Through our strategic approach, we achieved a\nsignificant enhancement in the prediction accuracy of LLMs, with a maximum\nimprovement of 60\\% compared to LLaMA-7B base models. It further outperforms\ntraditional machine learning models on various tasks in material science,\nshowcasing the potential of LLMs to provide a more versatile and scalable\nfoundation model for materials discovery and design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and design aim to find components and structures with\ndesirable properties over highly complex and diverse search spaces. Traditional\nsolutions, such as high-throughput simulations and machine learning (ML), often\nrely on complex descriptors, which hinder generalizability and transferability\nacross tasks. Moreover, these descriptors may deviate from experimental data\ndue to inevitable defects and purity issues in the real world, which may reduce\ntheir effectiveness in practical applications. To address these challenges, we\npropose Darwin 1.5, an open-source large language model (LLM) tailored for\nmaterials science. By leveraging natural language as input, Darwin eliminates\nthe need for task-specific descriptors and enables a flexible, unified approach\nto material property prediction and discovery. We employ a two-stage training\nstrategy combining question-answering (QA) fine-tuning with multi-task learning\n(MTL) to inject domain-specific knowledge in various modalities and facilitate\ncross-task knowledge transfer. Through our strategic approach, we achieved a\nsignificant enhancement in the prediction accuracy of LLMs, with a maximum\nimprovement of 60\\% compared to LLaMA-7B base models. It further outperforms\ntraditional machine learning models on various tasks in material science,\nshowcasing the potential of LLMs to provide a more versatile and scalable\nfoundation model for materials discovery and design."
                },
                "authors": [
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuwei Wan"
                    },
                    {
                        "name": "Yixuan Liu"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Chunyu Kit"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Bram Hoex"
                    }
                ],
                "author_detail": {
                    "name": "Bram Hoex"
                },
                "author": "Bram Hoex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11965v1",
                "updated": "2024-12-16T16:45:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    45,
                    33,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:45:33Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    45,
                    33,
                    0,
                    351,
                    0
                ],
                "title": "Inferring Functionality of Attention Heads from their Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Functionality of Attention Heads from their Parameters"
                },
                "summary": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations."
                },
                "authors": [
                    {
                        "name": "Amit Elhelo"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11964v1",
                "updated": "2024-12-16T16:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    45,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    45,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "BetaExplainer: A Probabilistic Method to Explain Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BetaExplainer: A Probabilistic Method to Explain Graph Neural Networks"
                },
                "summary": "Graph neural networks (GNNs) are powerful tools for conducting inference on\ngraph data but are often seen as \"black boxes\" due to difficulty in extracting\nmeaningful subnetworks driving predictive performance. Many interpretable GNN\nmethods exist, but they cannot quantify uncertainty in edge weights and suffer\nin predictive accuracy when applied to challenging graph structures. In this\nwork, we proposed BetaExplainer which addresses these issues by using a\nsparsity-inducing prior to mask unimportant edges during model training. To\nevaluate our approach, we examine various simulated data sets with diverse\nreal-world characteristics. Not only does this implementation provide a notion\nof edge importance uncertainty, it also improves upon evaluation metrics for\nchallenging datasets compared to state-of-the art explainer methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are powerful tools for conducting inference on\ngraph data but are often seen as \"black boxes\" due to difficulty in extracting\nmeaningful subnetworks driving predictive performance. Many interpretable GNN\nmethods exist, but they cannot quantify uncertainty in edge weights and suffer\nin predictive accuracy when applied to challenging graph structures. In this\nwork, we proposed BetaExplainer which addresses these issues by using a\nsparsity-inducing prior to mask unimportant edges during model training. To\nevaluate our approach, we examine various simulated data sets with diverse\nreal-world characteristics. Not only does this implementation provide a notion\nof edge importance uncertainty, it also improves upon evaluation metrics for\nchallenging datasets compared to state-of-the art explainer methods."
                },
                "authors": [
                    {
                        "name": "Whitney Sloneker"
                    },
                    {
                        "name": "Shalin Patel"
                    },
                    {
                        "name": "Michael Wang"
                    },
                    {
                        "name": "Lorin Crawford"
                    },
                    {
                        "name": "Ritambhara Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ritambhara Singh"
                },
                "author": "Ritambhara Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12701v2",
                "updated": "2024-12-16T16:44:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    44,
                    52,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-19T18:11:36Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    11,
                    36,
                    1,
                    324,
                    0
                ],
                "title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations"
                },
                "summary": "Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability."
                },
                "authors": [
                    {
                        "name": "Huaizhi Ge"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00840v2",
                "updated": "2024-12-16T16:44:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    44,
                    34,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-01T18:00:07Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    0,
                    7,
                    3,
                    214,
                    0
                ],
                "title": "Merging White Dwarf Binaries Produce Type Ia Supernovae in Elliptical\n  Galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging White Dwarf Binaries Produce Type Ia Supernovae in Elliptical\n  Galaxies"
                },
                "summary": "I find that Type Ia supernovae (SNe Ia) with bimodal nebular emission\nprofiles occur almost exclusively in massive ($M_\\star \\gtrsim\n10^{11}~M_\\odot$) galaxies with low star-formation rates (SFR~$\\lesssim\n0.5~M_\\odot$/yr). The bimodal profiles are likely produced by two white dwarfs\nthat exploded during a merger or collision, supported by a correlation between\nthe peak-to-peak velocity separation ($v_{\\rm sep}$) and the SN Ia peak\nluminosity ($M_V$) which arises naturally from more massive white dwarf\nbinaries synthesizing more $^{56}$Ni during the explosion. The quiescent hosts\nare consistent with the long delay times required to form double white dwarf\nbinaries. The distributions of SNe Ia with and without bimodal nebular lines\ndiffer in host mass, SFR, and specific SFR with K-S test probabilities of\n$3.1\\%$, $0.03\\%$, and $0.02\\%$, respectively. Viewing angle effects can fully\nexplain the SNe Ia in quiescent hosts without bimodal emission profiles and the\ndearth of merger/collision driven SNe Ia in star-forming hosts requires at\nleast two distinct progenitor channels for normal SNe Ia. $30-40\\%$ of all SNe\nIa originate from mergers or collisions depending on how cleanly host\nenvironment distinguishes progenitor scenarios. The bimodal SNe Ia share some\ncharacteristics with the underluminous 91bg-like SNe Ia that also prefer older\npopulations, but there is no unambiguous connection between the two\nclassifications. This may suggest separate processes or multiple axes of ejecta\n(a)symmetry. Existing models for WD mergers and collisions broadly reproduce\nthe $v_{\\rm sep} - M_V$ correlation and future analyses may be able to infer\nthe masses/mass-ratios of merging white dwarfs in external galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I find that Type Ia supernovae (SNe Ia) with bimodal nebular emission\nprofiles occur almost exclusively in massive ($M_\\star \\gtrsim\n10^{11}~M_\\odot$) galaxies with low star-formation rates (SFR~$\\lesssim\n0.5~M_\\odot$/yr). The bimodal profiles are likely produced by two white dwarfs\nthat exploded during a merger or collision, supported by a correlation between\nthe peak-to-peak velocity separation ($v_{\\rm sep}$) and the SN Ia peak\nluminosity ($M_V$) which arises naturally from more massive white dwarf\nbinaries synthesizing more $^{56}$Ni during the explosion. The quiescent hosts\nare consistent with the long delay times required to form double white dwarf\nbinaries. The distributions of SNe Ia with and without bimodal nebular lines\ndiffer in host mass, SFR, and specific SFR with K-S test probabilities of\n$3.1\\%$, $0.03\\%$, and $0.02\\%$, respectively. Viewing angle effects can fully\nexplain the SNe Ia in quiescent hosts without bimodal emission profiles and the\ndearth of merger/collision driven SNe Ia in star-forming hosts requires at\nleast two distinct progenitor channels for normal SNe Ia. $30-40\\%$ of all SNe\nIa originate from mergers or collisions depending on how cleanly host\nenvironment distinguishes progenitor scenarios. The bimodal SNe Ia share some\ncharacteristics with the underluminous 91bg-like SNe Ia that also prefer older\npopulations, but there is no unambiguous connection between the two\nclassifications. This may suggest separate processes or multiple axes of ejecta\n(a)symmetry. Existing models for WD mergers and collisions broadly reproduce\nthe $v_{\\rm sep} - M_V$ correlation and future analyses may be able to infer\nthe masses/mass-ratios of merging white dwarfs in external galaxies."
                },
                "authors": [
                    {
                        "name": "Michael A. Tucker"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Tucker"
                },
                "author": "Michael A. Tucker",
                "arxiv_comment": "7 pages, 4 figures, 1 table. Accepted to MNRAS Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12792v2",
                "updated": "2024-12-16T16:39:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    39,
                    11,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-18T16:58:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    58,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Inference of entropy production for periodically driven systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of entropy production for periodically driven systems"
                },
                "summary": "The problem of estimating entropy production from incomplete information in\nstochastic thermodynamics is essential for theory and experiments. Whereas a\nconsiderable amount of work has been done on this topic, arguably, most of it\nis restricted to the case of nonequilibrium steady states driven by a fixed\nthermodynamic force. Based on a recent method that has been proposed for\nnonequilibrium steady states, we obtain an estimate of the entropy production\nbased on the statistics of visible transitions and their waiting times for the\ncase of periodically driven systems. The time-dependence of transition rates in\nperiodically driven systems produces several differences in relation to steady\nstates, which is reflected in the entropy production estimation. More\nspecifically, we propose an estimate that does depend on the time between\ntransitions but is independent of the specific time of the first transition,\nthus it does not require tracking the protocol. Formally, this elimination of\nthe time-dependence of the first transition leads to an extra term in the\ninequality that involves the rate of entropy production and its estimate. We\nanalyze a simple model of a molecular pump to understand the relation between\nthe performance of the method and physical quantities such as energies, energy\nbarriers, and thermodynamic affinity. Our results with this model indicate that\nthe emergence of net motion in the form of a probability current in the space\nof states is a necessary condition for a relevant estimate of the rate of\nentropy production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of estimating entropy production from incomplete information in\nstochastic thermodynamics is essential for theory and experiments. Whereas a\nconsiderable amount of work has been done on this topic, arguably, most of it\nis restricted to the case of nonequilibrium steady states driven by a fixed\nthermodynamic force. Based on a recent method that has been proposed for\nnonequilibrium steady states, we obtain an estimate of the entropy production\nbased on the statistics of visible transitions and their waiting times for the\ncase of periodically driven systems. The time-dependence of transition rates in\nperiodically driven systems produces several differences in relation to steady\nstates, which is reflected in the entropy production estimation. More\nspecifically, we propose an estimate that does depend on the time between\ntransitions but is independent of the specific time of the first transition,\nthus it does not require tracking the protocol. Formally, this elimination of\nthe time-dependence of the first transition leads to an extra term in the\ninequality that involves the rate of entropy production and its estimate. We\nanalyze a simple model of a molecular pump to understand the relation between\nthe performance of the method and physical quantities such as energies, energy\nbarriers, and thermodynamic affinity. Our results with this model indicate that\nthe emergence of net motion in the form of a probability current in the space\nof states is a necessary condition for a relevant estimate of the rate of\nentropy production."
                },
                "authors": [
                    {
                        "name": "Pedro E. Harunari"
                    },
                    {
                        "name": "Carlos E. Fiore"
                    },
                    {
                        "name": "Andre C. Barato"
                    }
                ],
                "author_detail": {
                    "name": "Andre C. Barato"
                },
                "author": "Andre C. Barato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v2",
                "updated": "2024-12-16T16:37:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    37,
                    7,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11951v1",
                "updated": "2024-12-16T16:35:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    35,
                    31,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:35:31Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    35,
                    31,
                    0,
                    351,
                    0
                ],
                "title": "The Impact of Generalization Techniques on the Interplay Among Privacy,\n  Utility, and Fairness in Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Generalization Techniques on the Interplay Among Privacy,\n  Utility, and Fairness in Image Classification"
                },
                "summary": "This study investigates the trade-offs between fairness, privacy, and utility\nin image classification using machine learning (ML). Recent research suggests\nthat generalization techniques can improve the balance between privacy and\nutility. One focus of this work is sharpness-aware training (SAT) and its\nintegration with differential privacy (DP-SAT) to further improve this balance.\nAdditionally, we examine fairness in both private and non-private learning\nmodels trained on datasets with synthetic and real-world biases. We also\nmeasure the privacy risks involved in these scenarios by performing membership\ninference attacks (MIAs) and explore the consequences of eliminating\nhigh-privacy risk samples, termed outliers. Moreover, we introduce a new\nmetric, named \\emph{harmonic score}, which combines accuracy, privacy, and\nfairness into a single measure.\n  Through empirical analysis using generalization techniques, we achieve an\naccuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\%\nreported by De et al. (2022). Moreover, our experiments show that memorization\nof training samples can begin before the overfitting point, and generalization\ntechniques do not guarantee the prevention of this memorization. Our analysis\nof synthetic biases shows that generalization techniques can amplify model bias\nin both private and non-private models. Additionally, our results indicate that\nincreased bias in training data leads to reduced accuracy, greater\nvulnerability to privacy attacks, and higher model bias. We validate these\nfindings with the CelebA dataset, demonstrating that similar trends persist\nwith real-world attribute imbalances. Finally, our experiments show that\nremoving outlier data decreases accuracy and further amplifies model bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the trade-offs between fairness, privacy, and utility\nin image classification using machine learning (ML). Recent research suggests\nthat generalization techniques can improve the balance between privacy and\nutility. One focus of this work is sharpness-aware training (SAT) and its\nintegration with differential privacy (DP-SAT) to further improve this balance.\nAdditionally, we examine fairness in both private and non-private learning\nmodels trained on datasets with synthetic and real-world biases. We also\nmeasure the privacy risks involved in these scenarios by performing membership\ninference attacks (MIAs) and explore the consequences of eliminating\nhigh-privacy risk samples, termed outliers. Moreover, we introduce a new\nmetric, named \\emph{harmonic score}, which combines accuracy, privacy, and\nfairness into a single measure.\n  Through empirical analysis using generalization techniques, we achieve an\naccuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\%\nreported by De et al. (2022). Moreover, our experiments show that memorization\nof training samples can begin before the overfitting point, and generalization\ntechniques do not guarantee the prevention of this memorization. Our analysis\nof synthetic biases shows that generalization techniques can amplify model bias\nin both private and non-private models. Additionally, our results indicate that\nincreased bias in training data leads to reduced accuracy, greater\nvulnerability to privacy attacks, and higher model bias. We validate these\nfindings with the CelebA dataset, demonstrating that similar trends persist\nwith real-world attribute imbalances. Finally, our experiments show that\nremoving outlier data decreases accuracy and further amplifies model bias."
                },
                "authors": [
                    {
                        "name": "Ahmad Hassanpour"
                    },
                    {
                        "name": "Amir Zarei"
                    },
                    {
                        "name": "Khawla Mallat"
                    },
                    {
                        "name": "Anderson Santana de Oliveira"
                    },
                    {
                        "name": "Bian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bian Yang"
                },
                "author": "Bian Yang",
                "arxiv_comment": "Published as a conference paper at the 25th Privacy Enhancing\n  Technologies Symposium (PETS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11948v1",
                "updated": "2024-12-16T16:31:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    31,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:31:00Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    31,
                    0,
                    0,
                    351,
                    0
                ],
                "title": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews"
                },
                "summary": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top ML conferences. Given a PDF paper submission\nand review template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces significantly more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top ML conferences. Given a PDF paper submission\nand review template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces significantly more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool."
                },
                "authors": [
                    {
                        "name": "Maximilian Idahl"
                    },
                    {
                        "name": "Zahra Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Zahra Ahmadi"
                },
                "author": "Zahra Ahmadi",
                "arxiv_comment": "Demo: https://huggingface.co/spaces/maxidl/openreviewer Model:\n  https://huggingface.co/maxidl/Llama-OpenReviewer-8B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11945v1",
                "updated": "2024-12-16T16:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    27,
                    33,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    27,
                    33,
                    0,
                    351,
                    0
                ],
                "title": "Euclid: Field-level inference of primordial non-Gaussianity and cosmic\n  initial conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid: Field-level inference of primordial non-Gaussianity and cosmic\n  initial conditions"
                },
                "summary": "A primary target of the \\Euclid space mission is to constrain early-universe\nphysics by searching for deviations from a primordial Gaussian random field. A\nsignificant detection of primordial non-Gaussianity would rule out the simplest\nmodels of cosmic inflation and transform our understanding of the origin of the\nUniverse. This paper forecasts how well field-level inference of galaxy\nredshift surveys can constrain the amplitude of local primordial\nnon-Gaussianity ($f_{NL}$), within a Bayesian hierarchical framework, in the\nupcoming \\Euclid data. We design and simulate mock data sets and perform Markov\nchain Monte Carlo analyses using a full-field forward modelling approach. By\nincluding the formation history of the cosmic matter field in the analysis, the\nmethod takes into account all available probes of primordial non-Gaussianity,\nand goes beyond statistical summary estimators of $f_{NL}$. Probes include, for\nexample, two-point and higher-order statistics, peculiar velocity fields, and\nscale-dependent galaxy biases. Furthermore, the method simultaneously handles\nsystematic survey effects, such as selection effects, survey geometries, and\ngalaxy biases. The forecast shows that the method can reach precision levels of\nup to $\\sigma (f_{NL}) = 2.3$ (68.3\\% CI, and at the grid resolution $\\Delta L\n= 62.5\\,h^{-1}$Mpc) with \\Euclid data. We also provide data products, including\nrealistic $N$-body simulations with nonzero values of $f_{NL}$ and maps of\nadiabatic curvature fluctuations. The results underscore the feasibility and\nadvantages of field-level inference to constrain $f_{NL}$ in galaxy redshift\nsurveys. Our approach consistently captures all the information available in\nthe large-scale structure to constrain $f_{NL}$, and resolves the degeneracy\nbetween early-universe physics and late-time gravitational effects, while\nmitigating the impact of systematic and observational effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primary target of the \\Euclid space mission is to constrain early-universe\nphysics by searching for deviations from a primordial Gaussian random field. A\nsignificant detection of primordial non-Gaussianity would rule out the simplest\nmodels of cosmic inflation and transform our understanding of the origin of the\nUniverse. This paper forecasts how well field-level inference of galaxy\nredshift surveys can constrain the amplitude of local primordial\nnon-Gaussianity ($f_{NL}$), within a Bayesian hierarchical framework, in the\nupcoming \\Euclid data. We design and simulate mock data sets and perform Markov\nchain Monte Carlo analyses using a full-field forward modelling approach. By\nincluding the formation history of the cosmic matter field in the analysis, the\nmethod takes into account all available probes of primordial non-Gaussianity,\nand goes beyond statistical summary estimators of $f_{NL}$. Probes include, for\nexample, two-point and higher-order statistics, peculiar velocity fields, and\nscale-dependent galaxy biases. Furthermore, the method simultaneously handles\nsystematic survey effects, such as selection effects, survey geometries, and\ngalaxy biases. The forecast shows that the method can reach precision levels of\nup to $\\sigma (f_{NL}) = 2.3$ (68.3\\% CI, and at the grid resolution $\\Delta L\n= 62.5\\,h^{-1}$Mpc) with \\Euclid data. We also provide data products, including\nrealistic $N$-body simulations with nonzero values of $f_{NL}$ and maps of\nadiabatic curvature fluctuations. The results underscore the feasibility and\nadvantages of field-level inference to constrain $f_{NL}$ in galaxy redshift\nsurveys. Our approach consistently captures all the information available in\nthe large-scale structure to constrain $f_{NL}$, and resolves the degeneracy\nbetween early-universe physics and late-time gravitational effects, while\nmitigating the impact of systematic and observational effects."
                },
                "authors": [
                    {
                        "name": "A. Andrews"
                    },
                    {
                        "name": "J. Jasche"
                    },
                    {
                        "name": "G. Lavaux"
                    },
                    {
                        "name": "F. Leclercq"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "N. Bartolo"
                    },
                    {
                        "name": "G. Caas-Herrera"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "B. R. Granett"
                    },
                    {
                        "name": "F. Pace"
                    },
                    {
                        "name": "D. Paoletti"
                    },
                    {
                        "name": "N. Porqueres"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "J. Dinis"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "C. A. J. Duncan"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "P. Gmez-Alvarez"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "S. Ili"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "J. W. Nightingale"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "A. G. Snchez"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "M. Viel"
                    }
                ],
                "author_detail": {
                    "name": "M. Viel"
                },
                "arxiv_affiliation": "ICSC - Centro Nazionale di Ricerca in High Performance Computing, Big Data e Quantum Computing, Via Magnanelli 2, Bologna, Italy",
                "author": "M. Viel",
                "arxiv_comment": "31 pages and 26 figures, 3 tables. Comments are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11937v1",
                "updated": "2024-12-16T16:22:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    22,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:22:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    22,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "Precise Length Control in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise Length Control in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in production systems,\npowering applications such as chatbots, summarization, and question answering.\nDespite their success, controlling the length of their response remains a\nsignificant challenge, particularly for tasks requiring structured outputs or\nspecific levels of detail. In this work, we propose a method to adapt\npre-trained decoder-only LLMs for precise control of response length. Our\napproach incorporates a secondary length-difference positional encoding (LDPE)\ninto the input embeddings, which counts down to a user-set response termination\nlength. Fine-tuning with LDPE allows the model to learn to terminate responses\ncoherently at the desired length, achieving mean token errors of less than 3\ntokens. We also introduce Max New Tokens++, an extension that enables flexible\nupper-bound length control, rather than an exact target. Experimental results\non tasks such as question answering and document summarization demonstrate that\nour method enables precise length control without compromising response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in production systems,\npowering applications such as chatbots, summarization, and question answering.\nDespite their success, controlling the length of their response remains a\nsignificant challenge, particularly for tasks requiring structured outputs or\nspecific levels of detail. In this work, we propose a method to adapt\npre-trained decoder-only LLMs for precise control of response length. Our\napproach incorporates a secondary length-difference positional encoding (LDPE)\ninto the input embeddings, which counts down to a user-set response termination\nlength. Fine-tuning with LDPE allows the model to learn to terminate responses\ncoherently at the desired length, achieving mean token errors of less than 3\ntokens. We also introduce Max New Tokens++, an extension that enables flexible\nupper-bound length control, rather than an exact target. Experimental results\non tasks such as question answering and document summarization demonstrate that\nour method enables precise length control without compromising response\nquality."
                },
                "authors": [
                    {
                        "name": "Bradley Butcher"
                    },
                    {
                        "name": "Michael O'Keefe"
                    },
                    {
                        "name": "James Titchener"
                    }
                ],
                "author_detail": {
                    "name": "James Titchener"
                },
                "author": "James Titchener",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11936v1",
                "updated": "2024-12-16T16:21:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    41,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:21:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges"
                },
                "summary": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Jiamin Su"
                    },
                    {
                        "name": "Jianxiang He"
                    },
                    {
                        "name": "Fangteng Fu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16658v2",
                "updated": "2024-12-16T16:21:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-10-22T03:19:16Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    19,
                    16,
                    1,
                    296,
                    0
                ],
                "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent"
                },
                "summary": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions."
                },
                "authors": [
                    {
                        "name": "Janghoon Ock"
                    },
                    {
                        "name": "Tirtha Vinchurkar"
                    },
                    {
                        "name": "Yayati Jadhav"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11934v1",
                "updated": "2024-12-16T16:20:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    20,
                    41,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:20:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    20,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "Stepwise Reasoning Error Disruption Attack of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Reasoning Error Disruption Attack of LLMs"
                },
                "summary": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications."
                },
                "authors": [
                    {
                        "name": "Jingyu Peng"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11927v1",
                "updated": "2024-12-16T16:13:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    13,
                    55,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:13:55Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    13,
                    55,
                    0,
                    351,
                    0
                ],
                "title": "Explainable Procedural Mistake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Procedural Mistake Detection"
                },
                "summary": "Automated task guidance has recently attracted attention from the AI research\ncommunity. Procedural mistake detection (PMD) is a challenging sub-problem of\nclassifying whether a human user (observed through egocentric video) has\nsuccessfully executed the task at hand (specified by a procedural text).\nDespite significant efforts in building resources and models for PMD, machine\nperformance remains nonviable, and the reasoning processes underlying this\nperformance are opaque. As such, we recast PMD to an explanatory self-dialog of\nquestions and answers, which serve as evidence for a decision. As this\nreformulation enables an unprecedented transparency, we leverage a fine-tuned\nnatural language inference (NLI) model to formulate two automated coherence\nmetrics for generated explanations. Our results show that while open-source\nVLMs struggle with this task off-the-shelf, their accuracy, coherence, and\ndialog efficiency can be vastly improved by incorporating these coherence\nmetrics into common inference and fine-tuning methods. Furthermore, our\nmulti-faceted metrics can visualize common outcomes at a glance, highlighting\nareas for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated task guidance has recently attracted attention from the AI research\ncommunity. Procedural mistake detection (PMD) is a challenging sub-problem of\nclassifying whether a human user (observed through egocentric video) has\nsuccessfully executed the task at hand (specified by a procedural text).\nDespite significant efforts in building resources and models for PMD, machine\nperformance remains nonviable, and the reasoning processes underlying this\nperformance are opaque. As such, we recast PMD to an explanatory self-dialog of\nquestions and answers, which serve as evidence for a decision. As this\nreformulation enables an unprecedented transparency, we leverage a fine-tuned\nnatural language inference (NLI) model to formulate two automated coherence\nmetrics for generated explanations. Our results show that while open-source\nVLMs struggle with this task off-the-shelf, their accuracy, coherence, and\ndialog efficiency can be vastly improved by incorporating these coherence\nmetrics into common inference and fine-tuning methods. Furthermore, our\nmulti-faceted metrics can visualize common outcomes at a glance, highlighting\nareas for improvement."
                },
                "authors": [
                    {
                        "name": "Shane Storks"
                    },
                    {
                        "name": "Itamar Bar-Yossef"
                    },
                    {
                        "name": "Yayuan Li"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Jason J. Corso"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02224v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02224v4",
                "updated": "2024-12-16T16:13:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    13,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-04T11:36:09Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    11,
                    36,
                    9,
                    1,
                    156,
                    0
                ],
                "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models"
                },
                "summary": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs."
                },
                "authors": [
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Guoqiang Ma"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Hanlin Gu"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02224v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02224v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11923v1",
                "updated": "2024-12-16T16:09:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    9,
                    35,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:09:35Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    9,
                    35,
                    0,
                    351,
                    0
                ],
                "title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection"
                },
                "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations."
                },
                "authors": [
                    {
                        "name": "Sepideh Mamooler"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Alexander Mathis"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11919v1",
                "updated": "2024-12-16T16:03:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    3,
                    25,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:03:25Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    3,
                    25,
                    0,
                    351,
                    0
                ],
                "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation"
                },
                "summary": "Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}."
                },
                "authors": [
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yongkang Wu"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Qi Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11917v1",
                "updated": "2024-12-16T16:01:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    1,
                    18,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    1,
                    18,
                    0,
                    351,
                    0
                ],
                "title": "Does VLM Classification Benefit from LLM Description Semantics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does VLM Classification Benefit from LLM Description Semantics?"
                },
                "summary": "Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs."
                },
                "authors": [
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Lennart Rietdorf"
                    },
                    {
                        "name": "Dmytro Kotovenko"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11913v1",
                "updated": "2024-12-16T15:56:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    56,
                    4,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:56:04Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    56,
                    4,
                    0,
                    351,
                    0
                ],
                "title": "Learning Human-Aware Robot Policies for Adaptive Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Human-Aware Robot Policies for Adaptive Assistance"
                },
                "summary": "Developing robots that can assist humans efficiently, safely, and adaptively\nis crucial for real-world applications such as healthcare. While previous work\noften assumes a centralized system for co-optimizing human-robot interactions,\nwe argue that real-world scenarios are much more complicated, as humans have\nindividual preferences regarding how tasks are performed. Robots typically lack\ndirect access to these implicit preferences. However, to provide effective\nassistance, robots must still be able to recognize and adapt to the individual\nneeds and preferences of different users. To address these challenges, we\npropose a novel framework in which robots infer human intentions and reason\nabout human utilities through interaction. Our approach features two critical\nmodules: the anticipation module is a motion predictor that captures the\nspatial-temporal relationship between the robot agent and user agent, which\ncontributes to predicting human behavior; the utility module infers the\nunderlying human utility functions through progressive task demonstration\nsampling. Extensive experiments across various robot types and assistive tasks\ndemonstrate that the proposed framework not only enhances task success and\nefficiency but also significantly improves user satisfaction, paving the way\nfor more personalized and adaptive assistive robotic systems. Code and demos\nare available at https://asonin.github.io/Human-Aware-Assistance/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing robots that can assist humans efficiently, safely, and adaptively\nis crucial for real-world applications such as healthcare. While previous work\noften assumes a centralized system for co-optimizing human-robot interactions,\nwe argue that real-world scenarios are much more complicated, as humans have\nindividual preferences regarding how tasks are performed. Robots typically lack\ndirect access to these implicit preferences. However, to provide effective\nassistance, robots must still be able to recognize and adapt to the individual\nneeds and preferences of different users. To address these challenges, we\npropose a novel framework in which robots infer human intentions and reason\nabout human utilities through interaction. Our approach features two critical\nmodules: the anticipation module is a motion predictor that captures the\nspatial-temporal relationship between the robot agent and user agent, which\ncontributes to predicting human behavior; the utility module infers the\nunderlying human utility functions through progressive task demonstration\nsampling. Extensive experiments across various robot types and assistive tasks\ndemonstrate that the proposed framework not only enhances task success and\nefficiency but also significantly improves user satisfaction, paving the way\nfor more personalized and adaptive assistive robotic systems. Code and demos\nare available at https://asonin.github.io/Human-Aware-Assistance/."
                },
                "authors": [
                    {
                        "name": "Jason Qin"
                    },
                    {
                        "name": "Shikun Ban"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Dimitris Samaras"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Samaras"
                },
                "author": "Dimitris Samaras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11912v1",
                "updated": "2024-12-16T15:55:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    55,
                    34,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:55:34Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    55,
                    34,
                    0,
                    351,
                    0
                ],
                "title": "CharacterBench: Benchmarking Character Customization of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CharacterBench: Benchmarking Character Customization of Large Language\n  Models"
                },
                "summary": "Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench."
                },
                "authors": [
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Yongkang Huang"
                    },
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Kuntian Tang"
                    },
                    {
                        "name": "Rongsheng Zhang"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Tangjie Lv"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11909v1",
                "updated": "2024-12-16T15:54:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    23,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:54:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "Playground of Lognormal Seminumerical Simulations of the Lyman $$\n  Forest: Thermal History of the Intergalactic Medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playground of Lognormal Seminumerical Simulations of the Lyman $$\n  Forest: Thermal History of the Intergalactic Medium"
                },
                "summary": "This study aims to test a potential application of lognormal seminumerical\nsimulations to recover the thermal parameters and Jeans length. This could be\nsuitable for generating large number of synthetic spectra with various input\ndata and parameters, and thus ideal for interpreting the high-quality data\nobtained from QSO absorption spectra surveys.\n  We use a seminumerical approach to simulate absorption spectra of quasars at\nredshifts $ 3 \\leq z \\leq 5$. These synthetic spectra are compared with the 1D\nflux power spectra and using the Markov Chain Monte Carlo analysis method we\ndetermine the temperature at mean density, slope of the temperature-density\nrelation and Jeans length. Our best-fit model is also compared with the\nevolution of the temperature of the intergalactic medium from various UVB\nmodels.\n  We show that the lognormal simulations can effectively recover thermal\nparameters and Jeans length. Besides, by comparing the synthetic flux power\nspectra with observations from Baryon Oscillation Spectroscopy Survey we found,\nthat such an approach can be also used for the cosmological parameter\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study aims to test a potential application of lognormal seminumerical\nsimulations to recover the thermal parameters and Jeans length. This could be\nsuitable for generating large number of synthetic spectra with various input\ndata and parameters, and thus ideal for interpreting the high-quality data\nobtained from QSO absorption spectra surveys.\n  We use a seminumerical approach to simulate absorption spectra of quasars at\nredshifts $ 3 \\leq z \\leq 5$. These synthetic spectra are compared with the 1D\nflux power spectra and using the Markov Chain Monte Carlo analysis method we\ndetermine the temperature at mean density, slope of the temperature-density\nrelation and Jeans length. Our best-fit model is also compared with the\nevolution of the temperature of the intergalactic medium from various UVB\nmodels.\n  We show that the lognormal simulations can effectively recover thermal\nparameters and Jeans length. Besides, by comparing the synthetic flux power\nspectra with observations from Baryon Oscillation Spectroscopy Survey we found,\nthat such an approach can be also used for the cosmological parameter\ninference."
                },
                "authors": [
                    {
                        "name": "Tomas Ondro"
                    },
                    {
                        "name": "Bhaskar Arya"
                    },
                    {
                        "name": "Rudolf Galis"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Galis"
                },
                "author": "Rudolf Galis",
                "arxiv_comment": "11 pages, 11 figures, submitted to A&A. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11908v1",
                "updated": "2024-12-16T15:54:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    6,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:54:06Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    6,
                    0,
                    351,
                    0
                ],
                "title": "Can Language Models Rival Mathematics Students? Evaluating Mathematical\n  Reasoning through Textual Manipulation and Human Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Rival Mathematics Students? Evaluating Mathematical\n  Reasoning through Textual Manipulation and Human Experiments"
                },
                "summary": "In this paper we look at the ability of recent large language models (LLMs)\nat solving mathematical problems in combinatorics. We compare models LLaMA-2,\nLLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and\nundergraduates with prior experience in mathematical olympiads. To facilitate\nthese comparisons we introduce the Combi-Puzzles dataset, which contains 125\nproblem variants based on 25 combinatorial reasoning problems. Each problem is\npresented in one of five distinct forms, created by systematically manipulating\nthe problem statements through adversarial additions, numeric parameter\nchanges, and linguistic obfuscation. Our variations preserve the mathematical\ncore and are designed to measure the generalisability of LLM problem-solving\nabilities, while also increasing confidence that problems are submitted to LLMs\nin forms that have not been seen as training instances. We found that a model\nbased on GPT-4 outperformed all other models in producing correct responses,\nand performed significantly better in the mathematical variation of the\nproblems than humans. We also found that modifications to problem statements\nsignificantly impact the LLM's performance, while human performance remains\nunaffected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we look at the ability of recent large language models (LLMs)\nat solving mathematical problems in combinatorics. We compare models LLaMA-2,\nLLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and\nundergraduates with prior experience in mathematical olympiads. To facilitate\nthese comparisons we introduce the Combi-Puzzles dataset, which contains 125\nproblem variants based on 25 combinatorial reasoning problems. Each problem is\npresented in one of five distinct forms, created by systematically manipulating\nthe problem statements through adversarial additions, numeric parameter\nchanges, and linguistic obfuscation. Our variations preserve the mathematical\ncore and are designed to measure the generalisability of LLM problem-solving\nabilities, while also increasing confidence that problems are submitted to LLMs\nin forms that have not been seen as training instances. We found that a model\nbased on GPT-4 outperformed all other models in producing correct responses,\nand performed significantly better in the mathematical variation of the\nproblems than humans. We also found that modifications to problem statements\nsignificantly impact the LLM's performance, while human performance remains\nunaffected."
                },
                "authors": [
                    {
                        "name": "Andrii Nikolaiev"
                    },
                    {
                        "name": "Yiannos Stathopoulos"
                    },
                    {
                        "name": "Simone Teufel"
                    }
                ],
                "author_detail": {
                    "name": "Simone Teufel"
                },
                "author": "Simone Teufel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16133v3",
                "updated": "2024-12-16T15:42:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    42,
                    38,
                    0,
                    351,
                    0
                ],
                "published": "2024-05-25T08:57:28Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    8,
                    57,
                    28,
                    5,
                    146,
                    0
                ],
                "title": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via\n  Code Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via\n  Code Rewriting"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating code. However, the misuse of LLM-generated (synthetic) code has\nraised concerns in both educational and industrial contexts, underscoring the\nurgent need for synthetic code detectors. Existing methods for detecting\nsynthetic content are primarily designed for general text and struggle with\ncode due to the unique grammatical structure of programming languages and the\npresence of numerous ''low-entropy'' tokens. Building on this, our work\nproposes a novel zero-shot synthetic code detector based on the similarity\nbetween the original code and its LLM-rewritten variants. Our method is based\non the observation that differences between LLM-rewritten and original code\ntend to be smaller when the original code is synthetic. We utilize\nself-supervised contrastive learning to train a code similarity model and\nevaluate our approach on two synthetic code detection benchmarks. Our results\ndemonstrate a significant improvement over existing SOTA synthetic content\ndetectors, with AUROC scores increasing by 20.5% on the APPS benchmark and\n29.1% on the MBPP benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating code. However, the misuse of LLM-generated (synthetic) code has\nraised concerns in both educational and industrial contexts, underscoring the\nurgent need for synthetic code detectors. Existing methods for detecting\nsynthetic content are primarily designed for general text and struggle with\ncode due to the unique grammatical structure of programming languages and the\npresence of numerous ''low-entropy'' tokens. Building on this, our work\nproposes a novel zero-shot synthetic code detector based on the similarity\nbetween the original code and its LLM-rewritten variants. Our method is based\non the observation that differences between LLM-rewritten and original code\ntend to be smaller when the original code is synthetic. We utilize\nself-supervised contrastive learning to train a code similarity model and\nevaluate our approach on two synthetic code detection benchmarks. Our results\ndemonstrate a significant improvement over existing SOTA synthetic content\ndetectors, with AUROC scores increasing by 20.5% on the APPS benchmark and\n29.1% on the MBPP benchmark."
                },
                "authors": [
                    {
                        "name": "Tong Ye"
                    },
                    {
                        "name": "Yangkai Du"
                    },
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Lingfei Wu"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Shouling Ji"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "Accepted by AAAI 2025; previously submitted to EMNLP 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11878v1",
                "updated": "2024-12-16T15:27:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    27,
                    37,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:27:37Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    27,
                    37,
                    0,
                    351,
                    0
                ],
                "title": "Using Instruction-Tuned Large Language Models to Identify Indicators of\n  Vulnerability in Police Incident Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Instruction-Tuned Large Language Models to Identify Indicators of\n  Vulnerability in Police Incident Narratives"
                },
                "summary": "Objectives: Compare qualitative coding of instruction tuned large language\nmodels (IT-LLMs) against human coders in classifying the presence or absence of\nvulnerability in routinely collected unstructured text that describes\npolice-public interactions. Evaluate potential bias in IT-LLM codings. Methods:\nAnalyzing publicly available text narratives of police-public interactions\nrecorded by Boston Police Department, we provide humans and IT-LLMs with\nqualitative labelling codebooks and compare labels generated by both, seeking\nto identify situations associated with (i) mental ill health; (ii) substance\nmisuse; (iii) alcohol dependence; and (iv) homelessness. We explore multiple\nprompting strategies and model sizes, and the variability of labels generated\nby repeated prompts. Additionally, to explore model bias, we utilize\ncounterfactual methods to assess the impact of two protected characteristics -\nrace and gender - on IT-LLM classification. Results: Results demonstrate that\nIT-LLMs can effectively support human qualitative coding of police incident\nnarratives. While there is some disagreement between LLM and human generated\nlabels, IT-LLMs are highly effective at screening narratives where no\nvulnerabilities are present, potentially vastly reducing the requirement for\nhuman coding. Counterfactual analyses demonstrate that manipulations to both\ngender and race of individuals described in narratives have very limited\neffects on IT-LLM classifications beyond those expected by chance. Conclusions:\nIT-LLMs offer effective means to augment human qualitative coding in a way that\nrequires much lower levels of resource to analyze large unstructured datasets.\nMoreover, they encourage specificity in qualitative coding, promote\ntransparency, and provide the opportunity for more standardized, replicable\napproaches to analyzing large free-text police data sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Compare qualitative coding of instruction tuned large language\nmodels (IT-LLMs) against human coders in classifying the presence or absence of\nvulnerability in routinely collected unstructured text that describes\npolice-public interactions. Evaluate potential bias in IT-LLM codings. Methods:\nAnalyzing publicly available text narratives of police-public interactions\nrecorded by Boston Police Department, we provide humans and IT-LLMs with\nqualitative labelling codebooks and compare labels generated by both, seeking\nto identify situations associated with (i) mental ill health; (ii) substance\nmisuse; (iii) alcohol dependence; and (iv) homelessness. We explore multiple\nprompting strategies and model sizes, and the variability of labels generated\nby repeated prompts. Additionally, to explore model bias, we utilize\ncounterfactual methods to assess the impact of two protected characteristics -\nrace and gender - on IT-LLM classification. Results: Results demonstrate that\nIT-LLMs can effectively support human qualitative coding of police incident\nnarratives. While there is some disagreement between LLM and human generated\nlabels, IT-LLMs are highly effective at screening narratives where no\nvulnerabilities are present, potentially vastly reducing the requirement for\nhuman coding. Counterfactual analyses demonstrate that manipulations to both\ngender and race of individuals described in narratives have very limited\neffects on IT-LLM classifications beyond those expected by chance. Conclusions:\nIT-LLMs offer effective means to augment human qualitative coding in a way that\nrequires much lower levels of resource to analyze large unstructured datasets.\nMoreover, they encourage specificity in qualitative coding, promote\ntransparency, and provide the opportunity for more standardized, replicable\napproaches to analyzing large free-text police data sources."
                },
                "authors": [
                    {
                        "name": "Sam Relins"
                    },
                    {
                        "name": "Daniel Birks"
                    },
                    {
                        "name": "Charlie Lloyd"
                    }
                ],
                "author_detail": {
                    "name": "Charlie Lloyd"
                },
                "author": "Charlie Lloyd",
                "arxiv_comment": "33 pages, 6 figures Submitted to Journal of Quantitative Criminology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11868v1",
                "updated": "2024-12-16T15:22:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    22,
                    10,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:22:10Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    22,
                    10,
                    0,
                    351,
                    0
                ],
                "title": "A Variable Occurrence-Centric Framework for Inconsistency Handling\n  (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Variable Occurrence-Centric Framework for Inconsistency Handling\n  (Extended Version)"
                },
                "summary": "In this paper, we introduce a syntactic framework for analyzing and handling\ninconsistencies in propositional bases. Our approach focuses on examining the\nrelationships between variable occurrences within conflicts. We propose two\ndual concepts: Minimal Inconsistency Relation (MIR) and Maximal Consistency\nRelation (MCR). Each MIR is a minimal equivalence relation on variable\noccurrences that results in inconsistency, while each MCR is a maximal\nequivalence relation designed to prevent inconsistency. Notably, MIRs capture\nconflicts overlooked by minimal inconsistent subsets. Using MCRs, we develop a\nseries of non-explosive inference relations. The main strategy involves\nrestoring consistency by modifying the propositional base according to each\nMCR, followed by employing the classical inference relation to derive\nconclusions. Additionally, we propose an unusual semantics that assigns truth\nvalues to variable occurrences instead of the variables themselves. The\nassociated inference relations are established through Boolean interpretations\ncompatible with the occurrence-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a syntactic framework for analyzing and handling\ninconsistencies in propositional bases. Our approach focuses on examining the\nrelationships between variable occurrences within conflicts. We propose two\ndual concepts: Minimal Inconsistency Relation (MIR) and Maximal Consistency\nRelation (MCR). Each MIR is a minimal equivalence relation on variable\noccurrences that results in inconsistency, while each MCR is a maximal\nequivalence relation designed to prevent inconsistency. Notably, MIRs capture\nconflicts overlooked by minimal inconsistent subsets. Using MCRs, we develop a\nseries of non-explosive inference relations. The main strategy involves\nrestoring consistency by modifying the propositional base according to each\nMCR, followed by employing the classical inference relation to derive\nconclusions. Additionally, we propose an unusual semantics that assigns truth\nvalues to variable occurrences instead of the variables themselves. The\nassociated inference relations are established through Boolean interpretations\ncompatible with the occurrence-based models."
                },
                "authors": [
                    {
                        "name": "Yakoub Salhi"
                    }
                ],
                "author_detail": {
                    "name": "Yakoub Salhi"
                },
                "author": "Yakoub Salhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11854v1",
                "updated": "2024-12-16T15:12:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    12,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:12:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    12,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "Towards Understanding Systems Trade-offs in Retrieval-Augmented\n  Generation Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Systems Trade-offs in Retrieval-Augmented\n  Generation Model Inference"
                },
                "summary": "The rapid increase in the number of parameters in large language models\n(LLMs) has significantly increased the cost involved in fine-tuning and\nretraining LLMs, a necessity for keeping models up to date and improving\naccuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to\nimproving the capabilities and accuracy of LLMs without the necessity of\nretraining. Although RAG eliminates the need for continuous retraining to\nupdate model data, it incurs a trade-off in the form of slower model inference\ntimes. Resultingly, the use of RAG in enhancing the accuracy and capabilities\nof LLMs often involves diverse performance implications and trade-offs based on\nits design. In an effort to begin tackling and mitigating the performance\npenalties associated with RAG from a systems perspective, this paper introduces\na detailed taxonomy and characterization of the different elements within the\nRAG ecosystem for LLMs that explore trade-offs within latency, throughput, and\nmemory. Our study reveals underlying inefficiencies in RAG for systems\ndeployment, that can result in TTFT latencies that are twice as long and\nunoptimized datastores that consume terabytes of storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the number of parameters in large language models\n(LLMs) has significantly increased the cost involved in fine-tuning and\nretraining LLMs, a necessity for keeping models up to date and improving\naccuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to\nimproving the capabilities and accuracy of LLMs without the necessity of\nretraining. Although RAG eliminates the need for continuous retraining to\nupdate model data, it incurs a trade-off in the form of slower model inference\ntimes. Resultingly, the use of RAG in enhancing the accuracy and capabilities\nof LLMs often involves diverse performance implications and trade-offs based on\nits design. In an effort to begin tackling and mitigating the performance\npenalties associated with RAG from a systems perspective, this paper introduces\na detailed taxonomy and characterization of the different elements within the\nRAG ecosystem for LLMs that explore trade-offs within latency, throughput, and\nmemory. Our study reveals underlying inefficiencies in RAG for systems\ndeployment, that can result in TTFT latencies that are twice as long and\nunoptimized datastores that consume terabytes of storage."
                },
                "authors": [
                    {
                        "name": "Michael Shen"
                    },
                    {
                        "name": "Muhammad Umar"
                    },
                    {
                        "name": "Kiwan Maeng"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11851v1",
                "updated": "2024-12-16T15:11:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    11,
                    3,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:11:03Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    11,
                    3,
                    0,
                    351,
                    0
                ],
                "title": "A Benchmark and Robustness Study of In-Context-Learning with Large\n  Language Models in Music Entity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark and Robustness Study of In-Context-Learning with Large\n  Language Models in Music Entity Detection"
                },
                "summary": "Detecting music entities such as song titles or artist names is a useful\napplication to help use cases like processing music search queries or analyzing\nmusic consumption on the web. Recent approaches incorporate smaller language\nmodels (SLMs) like BERT and achieve high results. However, further research\nindicates a high influence of entity exposure during pre-training on the\nperformance of the models. With the advent of large language models (LLMs),\nthese outperform SLMs in a variety of downstream tasks. However, researchers\nare still divided if this is applicable to tasks like entity detection in texts\ndue to issues like hallucination. In this paper, we provide a novel dataset of\nuser-generated metadata and conduct a benchmark and a robustness study using\nrecent LLMs with in-context-learning (ICL). Our results indicate that LLMs in\nthe ICL setting yield higher performance than SLMs. We further uncover the\nlarge impact of entity exposure on the best performing LLM in our study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting music entities such as song titles or artist names is a useful\napplication to help use cases like processing music search queries or analyzing\nmusic consumption on the web. Recent approaches incorporate smaller language\nmodels (SLMs) like BERT and achieve high results. However, further research\nindicates a high influence of entity exposure during pre-training on the\nperformance of the models. With the advent of large language models (LLMs),\nthese outperform SLMs in a variety of downstream tasks. However, researchers\nare still divided if this is applicable to tasks like entity detection in texts\ndue to issues like hallucination. In this paper, we provide a novel dataset of\nuser-generated metadata and conduct a benchmark and a robustness study using\nrecent LLMs with in-context-learning (ICL). Our results indicate that LLMs in\nthe ICL setting yield higher performance than SLMs. We further uncover the\nlarge impact of entity exposure on the best performing LLM in our study."
                },
                "authors": [
                    {
                        "name": "Simon Hachmeier"
                    },
                    {
                        "name": "Robert Jschke"
                    }
                ],
                "author_detail": {
                    "name": "Robert Jschke"
                },
                "author": "Robert Jschke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16502v2",
                "updated": "2024-12-16T15:10:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    10,
                    12,
                    0,
                    351,
                    0
                ],
                "published": "2024-07-23T14:18:26Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    14,
                    18,
                    26,
                    1,
                    205,
                    0
                ],
                "title": "Neural information field filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural information field filter"
                },
                "summary": "We introduce neural information field filter, a Bayesian state and parameter\nestimation method for high-dimensional nonlinear dynamical systems given large\nmeasurement datasets. Solving such a problem using traditional methods, such as\nKalman and particle filters, is computationally expensive. Information field\ntheory is a Bayesian approach that can efficiently reconstruct dynamical model\nstate paths and calibrate model parameters from noisy measurement data. To\napply the method, we parameterize the time evolution state path using the span\nof a finite linear basis. The existing method has to reparameterize the state\npath by initial states to satisfy the initial condition. Designing an\nexpressive yet simple linear basis before knowing the true state path is\ncrucial for inference accuracy but challenging. Moreover, reparameterizing the\nstate path using the initial state is easy to perform for a linear basis, but\nis nontrivial for more complex and expressive function parameterizations, such\nas neural networks. The objective of this paper is to simplify and enrich the\nclass of state path parameterizations using neural networks for the information\nfield theory approach. To this end, we propose a generalized physics-informed\nconditional prior using an auxiliary initial state. We show the existing\nreparameterization is a special case. We parameterize the state path using a\nresidual neural network that consists of a linear basis function and a Fourier\nencoding fully connected neural network residual function. The residual\nfunction aims to correct the error of the linear basis function. To sample from\nthe intractable posterior distribution, we develop an optimization algorithm,\nnested stochastic variational inference, and a sampling algorithm, nested\npreconditioned stochastic gradient Langevin dynamics. A series of numerical and\nexperimental examples verify and validate the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce neural information field filter, a Bayesian state and parameter\nestimation method for high-dimensional nonlinear dynamical systems given large\nmeasurement datasets. Solving such a problem using traditional methods, such as\nKalman and particle filters, is computationally expensive. Information field\ntheory is a Bayesian approach that can efficiently reconstruct dynamical model\nstate paths and calibrate model parameters from noisy measurement data. To\napply the method, we parameterize the time evolution state path using the span\nof a finite linear basis. The existing method has to reparameterize the state\npath by initial states to satisfy the initial condition. Designing an\nexpressive yet simple linear basis before knowing the true state path is\ncrucial for inference accuracy but challenging. Moreover, reparameterizing the\nstate path using the initial state is easy to perform for a linear basis, but\nis nontrivial for more complex and expressive function parameterizations, such\nas neural networks. The objective of this paper is to simplify and enrich the\nclass of state path parameterizations using neural networks for the information\nfield theory approach. To this end, we propose a generalized physics-informed\nconditional prior using an auxiliary initial state. We show the existing\nreparameterization is a special case. We parameterize the state path using a\nresidual neural network that consists of a linear basis function and a Fourier\nencoding fully connected neural network residual function. The residual\nfunction aims to correct the error of the linear basis function. To sample from\nthe intractable posterior distribution, we develop an optimization algorithm,\nnested stochastic variational inference, and a sampling algorithm, nested\npreconditioned stochastic gradient Langevin dynamics. A series of numerical and\nexperimental examples verify and validate the proposed method."
                },
                "authors": [
                    {
                        "name": "Kairui Hao"
                    },
                    {
                        "name": "Ilias Bilionis"
                    }
                ],
                "author_detail": {
                    "name": "Ilias Bilionis"
                },
                "author": "Ilias Bilionis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10497v2",
                "updated": "2024-12-16T15:03:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    3,
                    54,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-20T02:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    44,
                    45,
                    1,
                    233,
                    0
                ],
                "title": "QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory"
                },
                "summary": "Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases."
                },
                "authors": [
                    {
                        "name": "Yihang Wang"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Bowen Tian"
                    },
                    {
                        "name": "Yueyang Su"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Huaming Liao"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05668v2",
                "updated": "2024-12-16T15:02:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    2,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-02-08T13:42:50Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    13,
                    42,
                    50,
                    3,
                    39,
                    0
                ],
                "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Assessment of Jailbreak Attacks Against LLMs"
                },
                "summary": "Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers\nhave studied different jailbreak attacks in depth, they have done so in\nisolation -- either with unaligned experiment settings or comparing a limited\nrange of methods. To fill this gap, we present the first large-scale\nmeasurement of various jailbreak attack methods. We collect 17 cutting-edge\njailbreak methods, summarize their features, and establish a novel jailbreak\nattack taxonomy. Based on eight popular censored LLMs and 160 questions from 16\nviolation categories, we conduct a unified and impartial assessment of attack\neffectiveness as well as a comprehensive ablation study. Our extensive\nexperimental results demonstrate that all the jailbreak attacks have a powerful\neffect on the LLMs. This indicates that all LLMs fail to cover all the\nviolation categories, and they are susceptible to significant jailbreak risks,\nwith even the well-aligned Llama3 facing a maximum attack success rate of 0.88.\nAdditionally, we test jailbreak attacks under eight advanced external defenses\nand find none of the defenses could mitigate the jailbreak attacks entirely.\nOur study offers valuable insights for future research on jailbreak attacks and\ndefenses and serves as a benchmark tool for researchers and practitioners to\nevaluate them effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers\nhave studied different jailbreak attacks in depth, they have done so in\nisolation -- either with unaligned experiment settings or comparing a limited\nrange of methods. To fill this gap, we present the first large-scale\nmeasurement of various jailbreak attack methods. We collect 17 cutting-edge\njailbreak methods, summarize their features, and establish a novel jailbreak\nattack taxonomy. Based on eight popular censored LLMs and 160 questions from 16\nviolation categories, we conduct a unified and impartial assessment of attack\neffectiveness as well as a comprehensive ablation study. Our extensive\nexperimental results demonstrate that all the jailbreak attacks have a powerful\neffect on the LLMs. This indicates that all LLMs fail to cover all the\nviolation categories, and they are susceptible to significant jailbreak risks,\nwith even the well-aligned Llama3 facing a maximum attack success rate of 0.88.\nAdditionally, we test jailbreak attacks under eight advanced external defenses\nand find none of the defenses could mitigate the jailbreak attacks entirely.\nOur study offers valuable insights for future research on jailbreak attacks and\ndefenses and serves as a benchmark tool for researchers and practitioners to\nevaluate them effectively."
                },
                "authors": [
                    {
                        "name": "Junjie Chu"
                    },
                    {
                        "name": "Yugeng Liu"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Xinyue Shen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "22 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15594v2",
                "updated": "2024-12-16T15:00:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    0,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-23T16:03:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    16,
                    3,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "A Survey on LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-as-a-Judge"
                },
                "summary": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Jiawei Gu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Xuehao Zhai"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "arxiv_comment": "33 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2310.05470 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11832v1",
                "updated": "2024-12-16T14:55:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    55,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:55:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    55,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "A Distributed Collaborative Retrieval Framework Excelling in All Queries\n  and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed Collaborative Retrieval Framework Excelling in All Queries\n  and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation"
                },
                "summary": "Numerous retrieval models, including sparse, dense and llm-based methods,\nhave demonstrated remarkable performance in predicting the relevance between\nqueries and corpora. However, the preliminary effectiveness analysis\nexperiments indicate that these models fail to achieve satisfactory performance\non the majority of queries and corpora, revealing their effectiveness\nrestricted to specific scenarios. Thus, to tackle this problem, we propose a\nnovel Distributed Collaborative Retrieval Framework (DCRF), outperforming each\nsingle model across all queries and corpora. Specifically, the framework\nintegrates various retrieval models into a unified system and dynamically\nselects the optimal results for each user's query. It can easily aggregate any\nretrieval model and expand to any application scenarios, illustrating its\nflexibility and scalability.Moreover, to reduce maintenance and training costs,\nwe design four effective prompting strategies with large language models (LLMs)\nto evaluate the quality of ranks without reliance of labeled data. Extensive\nexperiments demonstrate that proposed framework, combined with 8 efficient\nretrieval models, can achieve performance comparable to effective listwise\nmethods like RankGPT and ListT5, while offering superior efficiency. Besides,\nDCRF surpasses all selected retrieval models on the most datasets, indicating\nthe effectiveness of our prompting strategies on rank-oriented automatic\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous retrieval models, including sparse, dense and llm-based methods,\nhave demonstrated remarkable performance in predicting the relevance between\nqueries and corpora. However, the preliminary effectiveness analysis\nexperiments indicate that these models fail to achieve satisfactory performance\non the majority of queries and corpora, revealing their effectiveness\nrestricted to specific scenarios. Thus, to tackle this problem, we propose a\nnovel Distributed Collaborative Retrieval Framework (DCRF), outperforming each\nsingle model across all queries and corpora. Specifically, the framework\nintegrates various retrieval models into a unified system and dynamically\nselects the optimal results for each user's query. It can easily aggregate any\nretrieval model and expand to any application scenarios, illustrating its\nflexibility and scalability.Moreover, to reduce maintenance and training costs,\nwe design four effective prompting strategies with large language models (LLMs)\nto evaluate the quality of ranks without reliance of labeled data. Extensive\nexperiments demonstrate that proposed framework, combined with 8 efficient\nretrieval models, can achieve performance comparable to effective listwise\nmethods like RankGPT and ListT5, while offering superior efficiency. Besides,\nDCRF surpasses all selected retrieval models on the most datasets, indicating\nthe effectiveness of our prompting strategies on rank-oriented automatic\nevaluation."
                },
                "authors": [
                    {
                        "name": "Tian-Yi Che"
                    },
                    {
                        "name": "Xian-Ling Mao"
                    },
                    {
                        "name": "Chun Xu"
                    },
                    {
                        "name": "Cheng-Xin Xin"
                    },
                    {
                        "name": "Heng-Da Xu"
                    },
                    {
                        "name": "Jin-Yu Liu"
                    },
                    {
                        "name": "Heyan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heyan Huang"
                },
                "author": "Heyan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10257v2",
                "updated": "2024-12-16T14:54:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    54,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-13T16:26:34Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    26,
                    34,
                    4,
                    348,
                    0
                ],
                "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models"
                },
                "summary": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015)."
                },
                "authors": [
                    {
                        "name": "Harry J. Davies"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "14 pages, 5 figures, 1 table. Fixing typo with the final weight\n  editing equation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04905v2",
                "updated": "2024-12-16T14:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    36,
                    19,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-06T10:01:38Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    1,
                    38,
                    4,
                    341,
                    0
                ],
                "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling"
                },
                "summary": "Large language models (LLMs) have made dialogue one of the central modes in\nhuman-machine interaction, leading to the vast amounts of conversation logs and\nincreasing demand for dialogue generation. The dialogue's life-cycle spans from\nthe $\\textit{Prelude}$ through the $\\textit{Interlocution}$ to the\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite the large\nvolumes of dialogue-related studies, there is a lack of benchmark that\nencompasses comprehensive dialogue elements, which hinders precise modeling,\ngeneration and systematic evaluation. To bridge this gap, in this paper, we\nintroduce a new research task $\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made dialogue one of the central modes in\nhuman-machine interaction, leading to the vast amounts of conversation logs and\nincreasing demand for dialogue generation. The dialogue's life-cycle spans from\nthe $\\textit{Prelude}$ through the $\\textit{Interlocution}$ to the\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite the large\nvolumes of dialogue-related studies, there is a lack of benchmark that\nencompasses comprehensive dialogue elements, which hinders precise modeling,\ngeneration and systematic evaluation. To bridge this gap, in this paper, we\nintroduce a new research task $\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks."
                },
                "authors": [
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Wenji Mao"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "We release the code and data at https://github.com/MozerWang/DEMO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11816v1",
                "updated": "2024-12-16T14:33:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    33,
                    33,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:33:33Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    33,
                    33,
                    0,
                    351,
                    0
                ],
                "title": "Stochastic Model for a Piezoelectric Energy Harvester Driven by\n  Broadband Vibrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Model for a Piezoelectric Energy Harvester Driven by\n  Broadband Vibrations"
                },
                "summary": "We present an experimental and numerical study of a piezoelectric energy\nharvester driven by broadband vibrations. This device can extract power from\nrandom fluctuations and can be described by a stochastic model, based on an\nunderdamped Langevin equation with white noise, which mimics the dynamics of\nthe piezoelectric material. A crucial point in the modelisation is represented\nby the appropriate description of the coupled load circuit that is necessary to\nharvest electrical energy. We consider a linear load (resistance) and a\nnonlinear load (diode bridge rectifier connected to the parallel of a\ncapacitance and a load resistance), and focus on the characteristic curve of\nthe extracted power as a function of the load resistance, in order to estimate\nthe optimal values of the parameters that maximise the collected energy. In\nboth cases, we find good agreement between the numerical simulations of the\ntheoretical model and the results obtained in experiments. In particular, we\nobserve a non-monotonic behaviour of the characteristic curve which signals the\npresence of an optimal value for the load resistance at which the extracted\npower is maximised. We also address a more theoretical issue, related to the\ninference of the non-equilibrium features of the system from data: we show that\nthe analysis of high-order correlation functions of the relevant variables,\nwhen in the presence of nonlinearities, can represent a simple and effective\ntool to check the irreversible dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an experimental and numerical study of a piezoelectric energy\nharvester driven by broadband vibrations. This device can extract power from\nrandom fluctuations and can be described by a stochastic model, based on an\nunderdamped Langevin equation with white noise, which mimics the dynamics of\nthe piezoelectric material. A crucial point in the modelisation is represented\nby the appropriate description of the coupled load circuit that is necessary to\nharvest electrical energy. We consider a linear load (resistance) and a\nnonlinear load (diode bridge rectifier connected to the parallel of a\ncapacitance and a load resistance), and focus on the characteristic curve of\nthe extracted power as a function of the load resistance, in order to estimate\nthe optimal values of the parameters that maximise the collected energy. In\nboth cases, we find good agreement between the numerical simulations of the\ntheoretical model and the results obtained in experiments. In particular, we\nobserve a non-monotonic behaviour of the characteristic curve which signals the\npresence of an optimal value for the load resistance at which the extracted\npower is maximised. We also address a more theoretical issue, related to the\ninference of the non-equilibrium features of the system from data: we show that\nthe analysis of high-order correlation functions of the relevant variables,\nwhen in the presence of nonlinearities, can represent a simple and effective\ntool to check the irreversible dynamics."
                },
                "authors": [
                    {
                        "name": "Angelo Sanfelice"
                    },
                    {
                        "name": "Luigi Costanzo"
                    },
                    {
                        "name": "Alessandro Lo Schiavo"
                    },
                    {
                        "name": "Alessandro Sarracino"
                    },
                    {
                        "name": "Massimo Vitelli"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Vitelli"
                },
                "author": "Massimo Vitelli",
                "arxiv_doi": "10.3390/e26121097",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/e26121097",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 13 figures",
                "arxiv_journal_ref": "Entropy 26(12), 1097 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11814v1",
                "updated": "2024-12-16T14:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:29:49Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    49,
                    0,
                    351,
                    0
                ],
                "title": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents"
                },
                "summary": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information."
                },
                "authors": [
                    {
                        "name": "Mengna Zhu"
                    },
                    {
                        "name": "Kaisheng Zeng"
                    },
                    {
                        "name": "Mao Wang"
                    },
                    {
                        "name": "Kaiming Xiao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Hongbin Huang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Extended version for paper accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11812v1",
                "updated": "2024-12-16T14:25:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    25,
                    52,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:25:52Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    25,
                    52,
                    0,
                    351,
                    0
                ],
                "title": "CLDA-YOLO: Visual Contrastive Learning Based Domain Adaptive YOLO\n  Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLDA-YOLO: Visual Contrastive Learning Based Domain Adaptive YOLO\n  Detector"
                },
                "summary": "Unsupervised domain adaptive (UDA) algorithms can markedly enhance the\nperformance of object detectors under conditions of domain shifts, thereby\nreducing the necessity for extensive labeling and retraining. Current domain\nadaptive object detection algorithms primarily cater to two-stage detectors,\nwhich tend to offer minimal improvements when directly applied to single-stage\ndetectors such as YOLO. Intending to benefit the YOLO detector from UDA, we\nbuild a comprehensive domain adaptive architecture using a teacher-student\ncooperative system for the YOLO detector. In this process, we propose\nuncertainty learning to cope with pseudo-labeling generated by the teacher\nmodel with extreme uncertainty and leverage dynamic data augmentation to\nasymptotically adapt the teacher-student system to the environment. To address\nthe inability of single-stage object detectors to align at multiple stages, we\nutilize a unified visual contrastive learning paradigm that aligns instance at\nbackbone and head respectively, which steadily improves the robustness of the\ndetectors in cross-domain tasks. In summary, we present an unsupervised domain\nadaptive YOLO detector based on visual contrastive learning (CLDA-YOLO), which\nachieves highly competitive results across multiple domain adaptive datasets\nwithout any reduction in inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised domain adaptive (UDA) algorithms can markedly enhance the\nperformance of object detectors under conditions of domain shifts, thereby\nreducing the necessity for extensive labeling and retraining. Current domain\nadaptive object detection algorithms primarily cater to two-stage detectors,\nwhich tend to offer minimal improvements when directly applied to single-stage\ndetectors such as YOLO. Intending to benefit the YOLO detector from UDA, we\nbuild a comprehensive domain adaptive architecture using a teacher-student\ncooperative system for the YOLO detector. In this process, we propose\nuncertainty learning to cope with pseudo-labeling generated by the teacher\nmodel with extreme uncertainty and leverage dynamic data augmentation to\nasymptotically adapt the teacher-student system to the environment. To address\nthe inability of single-stage object detectors to align at multiple stages, we\nutilize a unified visual contrastive learning paradigm that aligns instance at\nbackbone and head respectively, which steadily improves the robustness of the\ndetectors in cross-domain tasks. In summary, we present an unsupervised domain\nadaptive YOLO detector based on visual contrastive learning (CLDA-YOLO), which\nachieves highly competitive results across multiple domain adaptive datasets\nwithout any reduction in inference speed."
                },
                "authors": [
                    {
                        "name": "Tianheng Qiu"
                    },
                    {
                        "name": "Ka Lung Law"
                    },
                    {
                        "name": "Guanghua Pan"
                    },
                    {
                        "name": "Jufei Wang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Xuan Huang"
                    },
                    {
                        "name": "Hu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hu Wei"
                },
                "author": "Hu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11803v1",
                "updated": "2024-12-16T14:14:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    14,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:14:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    14,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on\n  Large Language Models"
                },
                "summary": "Despite demonstrating impressive capabilities, Large Language Models (LLMs)\nstill often struggle to accurately express the factual knowledge they possess,\nespecially in cases where the LLMs' knowledge boundaries are ambiguous. To\nimprove LLMs' factual expressions, we propose the UAlign framework, which\nleverages Uncertainty estimations to represent knowledge boundaries, and then\nexplicitly incorporates these representations as input features into prompts\nfor LLMs to Align with factual knowledge. First, we prepare the dataset on\nknowledge question-answering (QA) samples by calculating two uncertainty\nestimations, including confidence score and semantic entropy, to represent the\nknowledge boundaries for LLMs. Subsequently, using the prepared dataset, we\ntrain a reward model that incorporates uncertainty estimations and then employ\nthe Proximal Policy Optimization (PPO) algorithm for factuality alignment on\nLLMs. Experimental results indicate that, by integrating uncertainty\nrepresentations in LLM alignment, the proposed UAlign can significantly enhance\nthe LLMs' capacities to confidently answer known questions and refuse unknown\nquestions on both in-domain and out-of-domain tasks, showing reliability\nimprovements and good generalizability over various prompt- and training-based\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite demonstrating impressive capabilities, Large Language Models (LLMs)\nstill often struggle to accurately express the factual knowledge they possess,\nespecially in cases where the LLMs' knowledge boundaries are ambiguous. To\nimprove LLMs' factual expressions, we propose the UAlign framework, which\nleverages Uncertainty estimations to represent knowledge boundaries, and then\nexplicitly incorporates these representations as input features into prompts\nfor LLMs to Align with factual knowledge. First, we prepare the dataset on\nknowledge question-answering (QA) samples by calculating two uncertainty\nestimations, including confidence score and semantic entropy, to represent the\nknowledge boundaries for LLMs. Subsequently, using the prepared dataset, we\ntrain a reward model that incorporates uncertainty estimations and then employ\nthe Proximal Policy Optimization (PPO) algorithm for factuality alignment on\nLLMs. Experimental results indicate that, by integrating uncertainty\nrepresentations in LLM alignment, the proposed UAlign can significantly enhance\nthe LLMs' capacities to confidently answer known questions and refuse unknown\nquestions on both in-domain and out-of-domain tasks, showing reliability\nimprovements and good generalizability over various prompt- and training-based\nbaselines."
                },
                "authors": [
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v3",
                "updated": "2024-12-16T14:05:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    5,
                    3,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "GPTKB: Comprehensively Materializing Factual LLM Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB: Comprehensively Materializing Factual LLM Knowledge"
                },
                "summary": "LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "13 pages, 4 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11790v1",
                "updated": "2024-12-16T14:01:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    1,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:01:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    1,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "Variable importance measures for heterogeneous treatment effects with\n  survival outcome",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable importance measures for heterogeneous treatment effects with\n  survival outcome"
                },
                "summary": "Treatment effect heterogeneity plays an important role in many areas of\ncausal inference and within recent years, estimation of the conditional average\ntreatment effect (CATE) has received much attention in the statistical\ncommunity. While accurate estimation of the CATE-function through flexible\nmachine learning procedures provides a tool for prediction of the individual\ntreatment effect, it does not provide further insight into the driving features\nof potential treatment effect heterogeneity. Recent papers have addressed this\nproblem by providing variable importance measures for treatment effect\nheterogeneity. Most of the suggestions have been developed for continuous or\nbinary outcome, while little attention has been given to censored time-to-event\noutcome. In this paper, we extend the treatment effect variable importance\nmeasure (TE-VIM) proposed in Hines et al. (2022) to the survival setting with\ncensored outcome. We derive an estimator for the TE-VIM for two different CATE\nfunctions based on the survival function and RMST, respectively. Along with the\nTE-VIM, we propose a new measure of treatment effect heterogeneity based on the\nbest partially linear projection of the CATE and suggest accompanying\nestimators for that projection. All estimators are based on semiparametric\nefficiency theory, and we give conditions under which they are asymptotically\nlinear. The finite sample performance of the derived estimators are\ninvestigated in a simulation study. Finally, the estimators are applied and\ncontrasted in two real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment effect heterogeneity plays an important role in many areas of\ncausal inference and within recent years, estimation of the conditional average\ntreatment effect (CATE) has received much attention in the statistical\ncommunity. While accurate estimation of the CATE-function through flexible\nmachine learning procedures provides a tool for prediction of the individual\ntreatment effect, it does not provide further insight into the driving features\nof potential treatment effect heterogeneity. Recent papers have addressed this\nproblem by providing variable importance measures for treatment effect\nheterogeneity. Most of the suggestions have been developed for continuous or\nbinary outcome, while little attention has been given to censored time-to-event\noutcome. In this paper, we extend the treatment effect variable importance\nmeasure (TE-VIM) proposed in Hines et al. (2022) to the survival setting with\ncensored outcome. We derive an estimator for the TE-VIM for two different CATE\nfunctions based on the survival function and RMST, respectively. Along with the\nTE-VIM, we propose a new measure of treatment effect heterogeneity based on the\nbest partially linear projection of the CATE and suggest accompanying\nestimators for that projection. All estimators are based on semiparametric\nefficiency theory, and we give conditions under which they are asymptotically\nlinear. The finite sample performance of the derived estimators are\ninvestigated in a simulation study. Finally, the estimators are applied and\ncontrasted in two real data examples."
                },
                "authors": [
                    {
                        "name": "Simon Christoffer Ziersen"
                    },
                    {
                        "name": "Torben Martinussen"
                    }
                ],
                "author_detail": {
                    "name": "Torben Martinussen"
                },
                "author": "Torben Martinussen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12692v2",
                "updated": "2024-12-16T13:52:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    52,
                    51,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-18T15:06:06Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    15,
                    6,
                    6,
                    1,
                    170,
                    0
                ],
                "title": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL"
                },
                "summary": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation."
                },
                "authors": [
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Christian Poelitz"
                    },
                    {
                        "name": "Xinye Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xinye Tang"
                },
                "author": "Xinye Tang",
                "arxiv_comment": "Accepted at Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11779v1",
                "updated": "2024-12-16T13:49:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    49,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:49:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    49,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "Impact of Face Alignment on Face Image Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Face Alignment on Face Image Quality"
                },
                "summary": "Face alignment is a crucial step in preparing face images for feature\nextraction in facial analysis tasks. For applications such as face recognition,\nfacial expression recognition, and facial attribute classification, alignment\nis widely utilized during both training and inference to standardize the\npositions of key landmarks in the face. It is well known that the application\nand method of face alignment significantly affect the performance of facial\nanalysis models. However, the impact of alignment on face image quality has not\nbeen thoroughly investigated. Current FIQA studies often assume alignment as a\nprerequisite but do not explicitly evaluate how alignment affects quality\nmetrics, especially with the advent of modern deep learning-based detectors\nthat integrate detection and landmark localization. To address this need, our\nstudy examines the impact of face alignment on face image quality scores. We\nconducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN\nand RetinaFace models for face detection and alignment. To evaluate face image\nquality, we utilized several assessment methods, including SER-FIQ, FaceQAN,\nDifFIQA, and SDD-FIQA. Our analysis included examining quality score\ndistributions for the LFW and IJB-B datasets and analyzing average quality\nscores at varying distances in the SCFace dataset. Our findings reveal that\nface image quality assessment methods are sensitive to alignment. Moreover,\nthis sensitivity increases under challenging real-life conditions, highlighting\nthe importance of evaluating alignment's role in quality assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face alignment is a crucial step in preparing face images for feature\nextraction in facial analysis tasks. For applications such as face recognition,\nfacial expression recognition, and facial attribute classification, alignment\nis widely utilized during both training and inference to standardize the\npositions of key landmarks in the face. It is well known that the application\nand method of face alignment significantly affect the performance of facial\nanalysis models. However, the impact of alignment on face image quality has not\nbeen thoroughly investigated. Current FIQA studies often assume alignment as a\nprerequisite but do not explicitly evaluate how alignment affects quality\nmetrics, especially with the advent of modern deep learning-based detectors\nthat integrate detection and landmark localization. To address this need, our\nstudy examines the impact of face alignment on face image quality scores. We\nconducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN\nand RetinaFace models for face detection and alignment. To evaluate face image\nquality, we utilized several assessment methods, including SER-FIQ, FaceQAN,\nDifFIQA, and SDD-FIQA. Our analysis included examining quality score\ndistributions for the LFW and IJB-B datasets and analyzing average quality\nscores at varying distances in the SCFace dataset. Our findings reveal that\nface image quality assessment methods are sensitive to alignment. Moreover,\nthis sensitivity increases under challenging real-life conditions, highlighting\nthe importance of evaluating alignment's role in quality assessment."
                },
                "authors": [
                    {
                        "name": "Eren Onaran"
                    },
                    {
                        "name": "Erdi Sarta"
                    },
                    {
                        "name": "Hazm Kemal Ekenel"
                    }
                ],
                "author_detail": {
                    "name": "Hazm Kemal Ekenel"
                },
                "author": "Hazm Kemal Ekenel",
                "arxiv_comment": "Accepted at EAI ROSENET 2024 - 8th EAI International Conference on\n  Robotic Sensor Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04903v2",
                "updated": "2024-12-16T13:47:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    47,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-06T09:59:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    59,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation"
                },
                "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Liang Ma"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11768v1",
                "updated": "2024-12-16T13:41:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    41,
                    37,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:41:37Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    41,
                    37,
                    0,
                    351,
                    0
                ],
                "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No More Adam: Learning Rate Scaling at Initialization is All You Need"
                },
                "summary": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings."
                },
                "authors": [
                    {
                        "name": "Minghao Xu"
                    },
                    {
                        "name": "Lichuan Xiang"
                    },
                    {
                        "name": "Xu Cai"
                    },
                    {
                        "name": "Hongkai Wen"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Wen"
                },
                "author": "Hongkai Wen",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11763v1",
                "updated": "2024-12-16T13:28:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    28,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:28:29Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    28,
                    29,
                    0,
                    351,
                    0
                ],
                "title": "QUENCH: Measuring the gap between Indic and Non-Indic Contextual General\n  Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUENCH: Measuring the gap between Indic and Non-Indic Contextual General\n  Reasoning in LLMs"
                },
                "summary": "The rise of large language models (LLMs) has created a need for advanced\nbenchmarking systems beyond traditional setups. To this end, we introduce\nQUENCH, a novel text-based English Quizzing Benchmark manually curated and\ntranscribed from YouTube quiz videos. QUENCH possesses masked entities and\nrationales for the LLMs to predict via generation. At the intersection of\ngeographical context and common sense reasoning, QUENCH helps assess world\nknowledge and deduction capabilities of LLMs via a zero-shot, open-domain\nquizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics,\ninvestigating the influence of model size, prompting style, geographical\ncontext, and gold-labeled rationale generation. The benchmarking concludes with\nan error analysis to which the LLMs are prone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has created a need for advanced\nbenchmarking systems beyond traditional setups. To this end, we introduce\nQUENCH, a novel text-based English Quizzing Benchmark manually curated and\ntranscribed from YouTube quiz videos. QUENCH possesses masked entities and\nrationales for the LLMs to predict via generation. At the intersection of\ngeographical context and common sense reasoning, QUENCH helps assess world\nknowledge and deduction capabilities of LLMs via a zero-shot, open-domain\nquizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics,\ninvestigating the influence of model size, prompting style, geographical\ncontext, and gold-labeled rationale generation. The benchmarking concludes with\nan error analysis to which the LLMs are prone."
                },
                "authors": [
                    {
                        "name": "Mohammad Aflah Khan"
                    },
                    {
                        "name": "Neemesh Yadav"
                    },
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Md. Shad Akhtar"
                    }
                ],
                "author_detail": {
                    "name": "Md. Shad Akhtar"
                },
                "author": "Md. Shad Akhtar",
                "arxiv_comment": "17 Pages, 6 Figures, 8 Tables, COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11761v1",
                "updated": "2024-12-16T13:25:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    25,
                    42,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:25:42Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    25,
                    42,
                    0,
                    351,
                    0
                ],
                "title": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. A promising but largely under-explored area is their potential\nto facilitate human coordination with many agents. Such capabilities would be\nuseful in domains including disaster response, urban planning, and real-time\nstrategy scenarios. In this work, we introduce (1) a real-time strategy game\nbenchmark designed to evaluate these abilities and (2) a novel framework we\nterm HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000\nagents using natural language dialog with an LLM. We present promising results\non this multi-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. However, our\nfindings also highlight critical limitations of current models, including\ndifficulties in processing spatial visual information and challenges in\nformulating long-term strategic plans. This work sheds light on the potential\nand limitations of LLMs in human-swarm coordination, paving the way for future\nresearch in this area. The HIVE project page, which includes videos of the\nsystem in action, can be found here: hive.syrkis.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. A promising but largely under-explored area is their potential\nto facilitate human coordination with many agents. Such capabilities would be\nuseful in domains including disaster response, urban planning, and real-time\nstrategy scenarios. In this work, we introduce (1) a real-time strategy game\nbenchmark designed to evaluate these abilities and (2) a novel framework we\nterm HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000\nagents using natural language dialog with an LLM. We present promising results\non this multi-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. However, our\nfindings also highlight critical limitations of current models, including\ndifficulties in processing spatial visual information and challenges in\nformulating long-term strategic plans. This work sheds light on the potential\nand limitations of LLMs in human-swarm coordination, paving the way for future\nresearch in this area. The HIVE project page, which includes videos of the\nsystem in action, can be found here: hive.syrkis.com."
                },
                "authors": [
                    {
                        "name": "Timothe Anne"
                    },
                    {
                        "name": "Noah Syrkis"
                    },
                    {
                        "name": "Meriem Elhosni"
                    },
                    {
                        "name": "Florian Turati"
                    },
                    {
                        "name": "Franck Legendre"
                    },
                    {
                        "name": "Alain Jaquier"
                    },
                    {
                        "name": "Sebastian Risi"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Risi"
                },
                "author": "Sebastian Risi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.11251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.11251v2",
                "updated": "2024-12-16T13:15:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    15,
                    13,
                    0,
                    351,
                    0
                ],
                "published": "2023-06-20T03:05:28Z",
                "published_parsed": [
                    2023,
                    6,
                    20,
                    3,
                    5,
                    28,
                    1,
                    171,
                    0
                ],
                "title": "Lipschitz Singularities in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lipschitz Singularities in Diffusion Models"
                },
                "summary": "Diffusion models, which employ stochastic differential equations to sample\nimages through integrals, have emerged as a dominant class of generative\nmodels. However, the rationality of the diffusion process itself receives\nlimited attention, leaving the question of whether the problem is well-posed\nand well-conditioned. In this paper, we explore a perplexing tendency of\ndiffusion models: they often display the infinite Lipschitz property of the\nnetwork with respect to time variable near the zero point. We provide\ntheoretical proofs to illustrate the presence of infinite Lipschitz constants\nand empirical results to confirm it. The Lipschitz singularities pose a threat\nto the stability and accuracy during both the training and inference processes\nof diffusion models. Therefore, the mitigation of Lipschitz singularities holds\ngreat potential for enhancing the performance of diffusion models. To address\nthis challenge, we propose a novel approach, dubbed E-TSDM, which alleviates\nthe Lipschitz singularities of the diffusion model near the zero point of\ntimesteps. Remarkably, our technique yields a substantial improvement in\nperformance. Moreover, as a byproduct of our method, we achieve a dramatic\nreduction in the Fr\\'echet Inception Distance of acceleration methods relying\non network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive\nexperiments on diverse datasets validate our theory and method. Our work may\nadvance the understanding of the general diffusion process, and also provide\ninsights for the design of diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models, which employ stochastic differential equations to sample\nimages through integrals, have emerged as a dominant class of generative\nmodels. However, the rationality of the diffusion process itself receives\nlimited attention, leaving the question of whether the problem is well-posed\nand well-conditioned. In this paper, we explore a perplexing tendency of\ndiffusion models: they often display the infinite Lipschitz property of the\nnetwork with respect to time variable near the zero point. We provide\ntheoretical proofs to illustrate the presence of infinite Lipschitz constants\nand empirical results to confirm it. The Lipschitz singularities pose a threat\nto the stability and accuracy during both the training and inference processes\nof diffusion models. Therefore, the mitigation of Lipschitz singularities holds\ngreat potential for enhancing the performance of diffusion models. To address\nthis challenge, we propose a novel approach, dubbed E-TSDM, which alleviates\nthe Lipschitz singularities of the diffusion model near the zero point of\ntimesteps. Remarkably, our technique yields a substantial improvement in\nperformance. Moreover, as a byproduct of our method, we achieve a dramatic\nreduction in the Fr\\'echet Inception Distance of acceleration methods relying\non network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive\nexperiments on diverse datasets validate our theory and method. Our work may\nadvance the understanding of the general diffusion process, and also provide\ninsights for the design of diffusion models."
                },
                "authors": [
                    {
                        "name": "Zhantao Yang"
                    },
                    {
                        "name": "Ruili Feng"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Kai Zhu"
                    },
                    {
                        "name": "Lianghua Huang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Fan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Fan Cheng"
                },
                "author": "Fan Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.11251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.11251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11753v1",
                "updated": "2024-12-16T13:12:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    12,
                    11,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:12:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    12,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "DriveGazen: Event-Based Driving Status Recognition using Conventional\n  Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveGazen: Event-Based Driving Status Recognition using Conventional\n  Camera"
                },
                "summary": "We introduce a wearable driving status recognition device and our open-source\ndataset, along with a new real-time method robust to changes in lighting\nconditions for identifying driving status from eye observations of drivers. The\ncore of our method is generating event frames from conventional intensity\nframes, and the other is a newly designed Attention Driving State Network\n(ADSN). Compared to event cameras, conventional cameras offer complete\ninformation and lower hardware costs, enabling captured frames to encode rich\nspatial information. However, these textures lack temporal information, posing\nchallenges in effectively identifying driving status. DriveGazen addresses this\nissue from three perspectives. First, we utilize video frames to generate\nrealistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a\nspiking neural network to decode pertinent temporal information. Lastly, ADSN\nextracts crucial spatial cues from corresponding intensity frames and conveys\nspatial attention to convolutional spiking layers during both training and\ninference through a novel guide attention module to guide the feature learning\nand feature enhancement of the event frame. We specifically collected the\nDriving Status (DriveGaze) dataset to demonstrate the effectiveness of our\napproach. Additionally, we validate the superiority of the DriveGazen on the\nSingle-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our\nmethod is the first to utilize guide attention spiking neural networks and\neye-based event frames generated from conventional cameras for driving status\nrecognition. Please refer to our project page for more details:\nhttps://github.com/TooyoungALEX/AAAI25-DriveGazen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a wearable driving status recognition device and our open-source\ndataset, along with a new real-time method robust to changes in lighting\nconditions for identifying driving status from eye observations of drivers. The\ncore of our method is generating event frames from conventional intensity\nframes, and the other is a newly designed Attention Driving State Network\n(ADSN). Compared to event cameras, conventional cameras offer complete\ninformation and lower hardware costs, enabling captured frames to encode rich\nspatial information. However, these textures lack temporal information, posing\nchallenges in effectively identifying driving status. DriveGazen addresses this\nissue from three perspectives. First, we utilize video frames to generate\nrealistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a\nspiking neural network to decode pertinent temporal information. Lastly, ADSN\nextracts crucial spatial cues from corresponding intensity frames and conveys\nspatial attention to convolutional spiking layers during both training and\ninference through a novel guide attention module to guide the feature learning\nand feature enhancement of the event frame. We specifically collected the\nDriving Status (DriveGaze) dataset to demonstrate the effectiveness of our\napproach. Additionally, we validate the superiority of the DriveGazen on the\nSingle-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our\nmethod is the first to utilize guide attention spiking neural networks and\neye-based event frames generated from conventional cameras for driving status\nrecognition. Please refer to our project page for more details:\nhttps://github.com/TooyoungALEX/AAAI25-DriveGazen."
                },
                "authors": [
                    {
                        "name": "Xiaoyin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyin Yang"
                },
                "author": "Xiaoyin Yang",
                "arxiv_comment": "9 pages, 4 figures, (AAAI25)The 39th Annual AAAI Conference on\n  Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11749v1",
                "updated": "2024-12-16T13:06:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    6,
                    44,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:06:44Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    6,
                    44,
                    0,
                    351,
                    0
                ],
                "title": "Inferring additional physics through unmodelled signal reconstructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring additional physics through unmodelled signal reconstructions"
                },
                "summary": "Parameter estimation of gravitational wave data is often computationally\nexpensive, requiring simplifying assumptions such as circularisation of binary\norbits. Although, if included, the sub-dominant effects like orbital\neccentricity may provide crucial insights into the formation channels of\ncompact binary mergers. To address these challenges, we present a pipeline\nstrategy leveraging minimally modelled waveform reconstruction to identify the\npresence of eccentricity in real time. Using injected signals, we demonstrate\nthat ignoring eccentricity ($e_{\\rm 20Hz} \\gtrsim 0.1$) leads to significant\nbiases in parameter recovery, including chirp mass estimates falling outside\nthe 90% credible interval. Waveform reconstruction shows inconsistencies\nincrease with eccentricity, and this behaviour is consistent for different mass\nratios. Our method enables low-latency inferences of binary properties\nsupporting targeted follow-up analyses and can be applied to identify any\nphysical effect of measurable strength.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation of gravitational wave data is often computationally\nexpensive, requiring simplifying assumptions such as circularisation of binary\norbits. Although, if included, the sub-dominant effects like orbital\neccentricity may provide crucial insights into the formation channels of\ncompact binary mergers. To address these challenges, we present a pipeline\nstrategy leveraging minimally modelled waveform reconstruction to identify the\npresence of eccentricity in real time. Using injected signals, we demonstrate\nthat ignoring eccentricity ($e_{\\rm 20Hz} \\gtrsim 0.1$) leads to significant\nbiases in parameter recovery, including chirp mass estimates falling outside\nthe 90% credible interval. Waveform reconstruction shows inconsistencies\nincrease with eccentricity, and this behaviour is consistent for different mass\nratios. Our method enables low-latency inferences of binary properties\nsupporting targeted follow-up analyses and can be applied to identify any\nphysical effect of measurable strength."
                },
                "authors": [
                    {
                        "name": "Rimo Das"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "Sijil Jose"
                    },
                    {
                        "name": "Imre Bartos"
                    },
                    {
                        "name": "Sergey Klimenko"
                    },
                    {
                        "name": "Chandra Kant Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Kant Mishra"
                },
                "author": "Chandra Kant Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11743v1",
                "updated": "2024-12-16T13:02:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    2,
                    17,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:02:17Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    2,
                    17,
                    0,
                    351,
                    0
                ],
                "title": "Generalized Bayesian deep reinforcement learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Bayesian deep reinforcement learning"
                },
                "summary": "Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. Similar to other model-based RL approaches, it involves\ntwo key components: (1) Inferring the posterior distribution of the data\ngenerating process (DGP) modeling the true environment and (2) policy learning\nusing the learned posterior. We propose to model the dynamics of the unknown\nenvironment through deep generative models assuming Markov dependence. In\nabsence of likelihood functions for these models we train them by learning a\ngeneralized predictive-sequential (or prequential) scoring rule (SR) posterior.\nWe use sequential Monte Carlo (SMC) samplers to draw samples from this\ngeneralized Bayesian posterior distribution. In conjunction, to achieve\nscalability in the high dimensional parameter space of the neural networks, we\nuse the gradient based Markov chain Monte Carlo (MCMC) kernels within SMC. To\njustify the use of the prequential scoring rule posterior we prove a\nBernstein-von Misses type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximizing the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies assuming discrete action and\nstate-space. Finally we successfully extend our setup for a challenging problem\nwith continuous action space without theoretical guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian reinforcement learning (BRL) is a method that merges principles from\nBayesian statistics and reinforcement learning to make optimal decisions in\nuncertain environments. Similar to other model-based RL approaches, it involves\ntwo key components: (1) Inferring the posterior distribution of the data\ngenerating process (DGP) modeling the true environment and (2) policy learning\nusing the learned posterior. We propose to model the dynamics of the unknown\nenvironment through deep generative models assuming Markov dependence. In\nabsence of likelihood functions for these models we train them by learning a\ngeneralized predictive-sequential (or prequential) scoring rule (SR) posterior.\nWe use sequential Monte Carlo (SMC) samplers to draw samples from this\ngeneralized Bayesian posterior distribution. In conjunction, to achieve\nscalability in the high dimensional parameter space of the neural networks, we\nuse the gradient based Markov chain Monte Carlo (MCMC) kernels within SMC. To\njustify the use of the prequential scoring rule posterior we prove a\nBernstein-von Misses type theorem. For policy learning, we propose expected\nThompson sampling (ETS) to learn the optimal policy by maximizing the expected\nvalue function with respect to the posterior distribution. This improves upon\ntraditional Thompson sampling (TS) and its extensions which utilize only one\nsample drawn from the posterior distribution. This improvement is studied both\ntheoretically and using simulation studies assuming discrete action and\nstate-space. Finally we successfully extend our setup for a challenging problem\nwith continuous action space without theoretical guarantees."
                },
                "authors": [
                    {
                        "name": "Shreya Sinha Roy"
                    },
                    {
                        "name": "Richard G. Everitt"
                    },
                    {
                        "name": "Christian P. Robert"
                    },
                    {
                        "name": "Ritabrata Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Ritabrata Dutta"
                },
                "author": "Ritabrata Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03769v2",
                "updated": "2024-12-16T12:57:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    23,
                    0,
                    351,
                    0
                ],
                "published": "2024-10-02T16:34:48Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    48,
                    2,
                    276,
                    0
                ],
                "title": "SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large\n  Language Models in Scientific Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large\n  Language Models in Scientific Tasks"
                },
                "summary": "Large language models (LLMs) have a transformative impact on a variety of\nscientific tasks across disciplines including biology, chemistry, medicine, and\nphysics. However, ensuring the safety alignment of these models in scientific\nresearch remains an underexplored area, with existing benchmarks primarily\nfocusing on textual content and overlooking key scientific representations such\nas molecular, protein, and genomic languages. Moreover, the safety mechanisms\nof LLMs in scientific tasks are insufficiently studied. To address these\nlimitations, we introduce SciSafeEval, a comprehensive benchmark designed to\nevaluate the safety alignment of LLMs across a range of scientific tasks.\nSciSafeEval spans multiple scientific languages-including textual, molecular,\nprotein, and genomic-and covers a wide range of scientific domains. We evaluate\nLLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a\n\"jailbreak\" enhancement feature that challenges LLMs equipped with safety\nguardrails, rigorously testing their defenses against malicious intention. Our\nbenchmark surpasses existing safety datasets in both scale and scope, providing\na robust platform for assessing the safety and performance of LLMs in\nscientific contexts. This work aims to facilitate the responsible development\nand deployment of LLMs, promoting alignment with safety and ethical standards\nin scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have a transformative impact on a variety of\nscientific tasks across disciplines including biology, chemistry, medicine, and\nphysics. However, ensuring the safety alignment of these models in scientific\nresearch remains an underexplored area, with existing benchmarks primarily\nfocusing on textual content and overlooking key scientific representations such\nas molecular, protein, and genomic languages. Moreover, the safety mechanisms\nof LLMs in scientific tasks are insufficiently studied. To address these\nlimitations, we introduce SciSafeEval, a comprehensive benchmark designed to\nevaluate the safety alignment of LLMs across a range of scientific tasks.\nSciSafeEval spans multiple scientific languages-including textual, molecular,\nprotein, and genomic-and covers a wide range of scientific domains. We evaluate\nLLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a\n\"jailbreak\" enhancement feature that challenges LLMs equipped with safety\nguardrails, rigorously testing their defenses against malicious intention. Our\nbenchmark surpasses existing safety datasets in both scale and scope, providing\na robust platform for assessing the safety and performance of LLMs in\nscientific contexts. This work aims to facilitate the responsible development\nand deployment of LLMs, promoting alignment with safety and ethical standards\nin scientific research."
                },
                "authors": [
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Jingyu Lu"
                    },
                    {
                        "name": "Chuangxin Chu"
                    },
                    {
                        "name": "Tianyu Zeng"
                    },
                    {
                        "name": "Yujia Zheng"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Haotian Huang"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Zuoxian Liu"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Xuejing Yuan"
                    },
                    {
                        "name": "Xingkai Wang"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Qiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Zhang"
                },
                "author": "Qiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11736v1",
                "updated": "2024-12-16T12:57:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    19,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:57:19Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    19,
                    0,
                    351,
                    0
                ],
                "title": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users"
                },
                "summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLM, but overlooked the diversity of questioners.\nIn this work, we propose a new form of questioner-aware LLM personalization,\ngenerating different responses even for the same query from different\nquestioners. We design a dual-tower model architecture with a cross-questioner\ngeneral encoder and a questioner-specific encoder. We further apply contrastive\nlearning with multi-view augmentation, pulling close the dialogue\nrepresentations of the same questioner, while pulling apart those of different\nquestioners. To mitigate the impact of question diversity on\nquestioner-contrastive learning, we cluster the dialogues based on question\nsimilarity and restrict the scope of contrastive learning within each cluster.\nWe also build a multi-questioner dataset from English and Chinese scripts and\nWeChat records, called MQDialog, containing 173 questioners and 12 responders.\nExtensive evaluation with different metrics shows a significant improvement in\nthe quality of personalized response generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLM, but overlooked the diversity of questioners.\nIn this work, we propose a new form of questioner-aware LLM personalization,\ngenerating different responses even for the same query from different\nquestioners. We design a dual-tower model architecture with a cross-questioner\ngeneral encoder and a questioner-specific encoder. We further apply contrastive\nlearning with multi-view augmentation, pulling close the dialogue\nrepresentations of the same questioner, while pulling apart those of different\nquestioners. To mitigate the impact of question diversity on\nquestioner-contrastive learning, we cluster the dialogues based on question\nsimilarity and restrict the scope of contrastive learning within each cluster.\nWe also build a multi-questioner dataset from English and Chinese scripts and\nWeChat records, called MQDialog, containing 173 questioners and 12 responders.\nExtensive evaluation with different metrics shows a significant improvement in\nthe quality of personalized response generation."
                },
                "authors": [
                    {
                        "name": "Hang Zeng"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13925v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13925v3",
                "updated": "2024-12-16T12:51:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    51,
                    46,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-20T01:45:44Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    45,
                    44,
                    3,
                    172,
                    0
                ],
                "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Yuxiang Xiao"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "James Foulds"
                    },
                    {
                        "name": "Shimei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shimei Pan"
                },
                "author": "Shimei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13925v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13925v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05813v2",
                "updated": "2024-12-16T12:44:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    44,
                    7,
                    0,
                    351,
                    0
                ],
                "published": "2024-02-08T16:50:01Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    16,
                    50,
                    1,
                    3,
                    39,
                    0
                ],
                "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and\n  Evaluation in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Forgetting: Advancing Machine Unlearning Techniques and\n  Evaluation in Language Models"
                },
                "summary": "This paper explores Machine Unlearning (MU), an emerging field that is\ngaining increased attention due to concerns about neural models unintentionally\nremembering personal or sensitive information. We present SeUL, a novel method\nthat enables selective and fine-grained unlearning for language models. Unlike\nprevious work that employs a fully reversed training objective in unlearning,\nSeUL minimizes the negative impact on the capability of language models,\nparticularly in terms of generation. Furthermore, we introduce two innovative\nevaluation metrics, sensitive extraction likelihood (S-EL) and sensitive\nmemorization accuracy (S-MA), specifically designed to assess the effectiveness\nof forgetting sensitive information. In support of the unlearning framework, we\npropose efficient automatic online and offline sensitive span annotation\nmethods. The online selection method, based on language probability scores,\nensures computational efficiency, while the offline annotation involves a\ntwo-stage LLM-based process for robust verification. In summary, this paper\ncontributes a novel selective unlearning method (SeUL), introduces specialized\nevaluation metrics (S-EL and S-MA) for assessing sensitive information\nforgetting, and proposes automatic online and offline sensitive span annotation\nmethods to support the overall unlearning framework and evaluation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores Machine Unlearning (MU), an emerging field that is\ngaining increased attention due to concerns about neural models unintentionally\nremembering personal or sensitive information. We present SeUL, a novel method\nthat enables selective and fine-grained unlearning for language models. Unlike\nprevious work that employs a fully reversed training objective in unlearning,\nSeUL minimizes the negative impact on the capability of language models,\nparticularly in terms of generation. Furthermore, we introduce two innovative\nevaluation metrics, sensitive extraction likelihood (S-EL) and sensitive\nmemorization accuracy (S-MA), specifically designed to assess the effectiveness\nof forgetting sensitive information. In support of the unlearning framework, we\npropose efficient automatic online and offline sensitive span annotation\nmethods. The online selection method, based on language probability scores,\nensures computational efficiency, while the offline annotation involves a\ntwo-stage LLM-based process for robust verification. In summary, this paper\ncontributes a novel selective unlearning method (SeUL), introduces specialized\nevaluation metrics (S-EL and S-MA) for assessing sensitive information\nforgetting, and proposes automatic online and offline sensitive span annotation\nmethods to support the overall unlearning framework and evaluation process."
                },
                "authors": [
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Jinsong Guo"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Georg Gottlob"
                    }
                ],
                "author_detail": {
                    "name": "Georg Gottlob"
                },
                "author": "Georg Gottlob",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11716v1",
                "updated": "2024-12-16T12:36:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    36,
                    47,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:36:47Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    36,
                    47,
                    0,
                    351,
                    0
                ],
                "title": "LLMs Can Simulate Standardized Patients via Agent Coevolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Simulate Standardized Patients via Agent Coevolution"
                },
                "summary": "Training medical personnel using standardized patients (SPs) remains a\ncomplex challenge, requiring extensive domain expertise and role-specific\npractice. Most research on Large Language Model (LLM)-based simulated patients\nfocuses on improving data retrieval accuracy or adjusting prompts through human\nfeedback. However, this focus has overlooked the critical need for patient\nagents to learn a standardized presentation pattern that transforms data into\nhuman-like patient responses through unsupervised simulations. To address this\ngap, we propose EvoPatient, a novel simulated patient framework in which a\npatient agent and doctor agents simulate the diagnostic process through\nmulti-turn dialogues, simultaneously gathering experience to improve the\nquality of both questions and answers, ultimately enabling human doctor\ntraining. Extensive experiments on various cases demonstrate that, by providing\nonly overall SP requirements, our framework improves over existing reasoning\nmethods by more than 10% in requirement alignment and better human preference,\nwhile achieving an optimal balance of resource consumption after evolving over\n200 cases for 10 hours, with excellent generalizability. The code will be\navailable at https://github.com/ZJUMAI/EvoPatient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training medical personnel using standardized patients (SPs) remains a\ncomplex challenge, requiring extensive domain expertise and role-specific\npractice. Most research on Large Language Model (LLM)-based simulated patients\nfocuses on improving data retrieval accuracy or adjusting prompts through human\nfeedback. However, this focus has overlooked the critical need for patient\nagents to learn a standardized presentation pattern that transforms data into\nhuman-like patient responses through unsupervised simulations. To address this\ngap, we propose EvoPatient, a novel simulated patient framework in which a\npatient agent and doctor agents simulate the diagnostic process through\nmulti-turn dialogues, simultaneously gathering experience to improve the\nquality of both questions and answers, ultimately enabling human doctor\ntraining. Extensive experiments on various cases demonstrate that, by providing\nonly overall SP requirements, our framework improves over existing reasoning\nmethods by more than 10% in requirement alignment and better human preference,\nwhile achieving an optimal balance of resource consumption after evolving over\n200 cases for 10 hours, with excellent generalizability. The code will be\navailable at https://github.com/ZJUMAI/EvoPatient."
                },
                "authors": [
                    {
                        "name": "Zhuoyun Du"
                    },
                    {
                        "name": "Lujie Zheng"
                    },
                    {
                        "name": "Renjun Hu"
                    },
                    {
                        "name": "Yuyang Xu"
                    },
                    {
                        "name": "Xiawei Li"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Haolei Cai"
                    },
                    {
                        "name": "Haohao Ying"
                    }
                ],
                "author_detail": {
                    "name": "Haohao Ying"
                },
                "author": "Haohao Ying",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11713v1",
                "updated": "2024-12-16T12:35:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    35,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:35:29Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    35,
                    29,
                    0,
                    351,
                    0
                ],
                "title": "Seeker: Towards Exception Safety Code Generation with Intermediate\n  Language Agents Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeker: Towards Exception Safety Code Generation with Intermediate\n  Language Agents Framework"
                },
                "summary": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open-source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nBlock, and Distorted Handling Solution. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a\nmulti-agent framework inspired by expert developer strategies for exception\nhandling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler\nto assist LLMs in detecting, capturing, and resolving exceptions more\neffectively. Our work is the first systematic study on leveraging LLMs to\nenhance exception handling practices in real development scenarios, providing\nvaluable insights for future improvements in code reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open-source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nBlock, and Distorted Handling Solution. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a\nmulti-agent framework inspired by expert developer strategies for exception\nhandling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler\nto assist LLMs in detecting, capturing, and resolving exceptions more\neffectively. Our work is the first systematic study on leveraging LLMs to\nenhance exception handling practices in real development scenarios, providing\nvaluable insights for future improvements in code reliability."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yiming Zheng"
                    },
                    {
                        "name": "Zhexin Zhang"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "30 pages, 9 figures, submitted to ARR Dec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11711v1",
                "updated": "2024-12-16T12:33:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    33,
                    12,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:33:12Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    33,
                    12,
                    0,
                    351,
                    0
                ],
                "title": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for\n  Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for\n  Table Reasoning"
                },
                "summary": "Extensive research has been conducted to explore the capability of Large\nLanguage Models (LLMs) for table reasoning and has significantly improved the\nperformance on existing benchmarks. However, tables and user questions in\nreal-world applications are more complex and diverse, presenting an unignorable\ngap compared to the existing benchmarks. To fill the gap, we propose a\n\\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta\n\\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable.\nSpecifically, MiMoTable incorporates two key features. First, the tables in\nMiMoTable are all spreadsheets used in real-world scenarios, which cover seven\ndomains and contain different types. Second, we define a new criterion with six\ncategories of meta operations for measuring the difficulty of each question in\nMiMoTable, simultaneously as a new perspective for measuring the difficulty of\nthe existing benchmarks. Experimental results show that Claude-3.5-Sonnet\nachieves the best performance with 77.4\\% accuracy, indicating that there is\nstill significant room to improve for LLMs on MiMoTable. Furthermore, we grade\nthe difficulty of existing benchmarks according to our new criteria.\nExperiments have shown that the performance of LLMs decreases as the difficulty\nof benchmarks increases, thereby proving the effectiveness of our proposed new\ncriterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive research has been conducted to explore the capability of Large\nLanguage Models (LLMs) for table reasoning and has significantly improved the\nperformance on existing benchmarks. However, tables and user questions in\nreal-world applications are more complex and diverse, presenting an unignorable\ngap compared to the existing benchmarks. To fill the gap, we propose a\n\\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta\n\\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable.\nSpecifically, MiMoTable incorporates two key features. First, the tables in\nMiMoTable are all spreadsheets used in real-world scenarios, which cover seven\ndomains and contain different types. Second, we define a new criterion with six\ncategories of meta operations for measuring the difficulty of each question in\nMiMoTable, simultaneously as a new perspective for measuring the difficulty of\nthe existing benchmarks. Experimental results show that Claude-3.5-Sonnet\nachieves the best performance with 77.4\\% accuracy, indicating that there is\nstill significant room to improve for LLMs on MiMoTable. Furthermore, we grade\nthe difficulty of existing benchmarks according to our new criteria.\nExperiments have shown that the performance of LLMs decreases as the difficulty\nof benchmarks increases, thereby proving the effectiveness of our proposed new\ncriterion."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Mingyang Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingyang Song"
                },
                "author": "Mingyang Song",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11702v1",
                "updated": "2024-12-16T12:25:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    25,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:25:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    25,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "Flex-PE: Flexible and SIMD Multi-Precision Processing Element for AI\n  Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flex-PE: Flexible and SIMD Multi-Precision Processing Element for AI\n  Workloads"
                },
                "summary": "The rapid adaptation of data driven AI models, such as deep learning\ninference, training, Vision Transformers (ViTs), and other HPC applications,\ndrives a strong need for runtime precision configurable different non linear\nactivation functions (AF) hardware support. Existing solutions support diverse\nprecision or runtime AF reconfigurability but fail to address both\nsimultaneously. This work proposes a flexible and SIMD multiprecision\nprocessing element (FlexPE), which supports diverse runtime configurable AFs,\nincluding sigmoid, tanh, ReLU and softmax, and MAC operation. The proposed\ndesign achieves an improved throughput of up to 16X FxP4, 8X FxP8, 4X FxP16 and\n1X FxP32 in pipeline mode with 100% time multiplexed hardware. This work\nproposes an area efficient multiprecision iterative mode in the SIMD systolic\narrays for edge AI use cases. The design delivers superior performance with up\nto 62X and 371X reductions in DMA reads for input feature maps and weight\nfilters in VGG16, with an energy efficiency of 8.42 GOPS / W within the\naccuracy loss of 2%. The proposed architecture supports emerging 4-bit\ncomputations for DL inference while enhancing throughput in FxP8/16 modes for\ntransformers and other HPC applications. The proposed approach enables future\nenergy-efficient AI accelerators in edge and cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adaptation of data driven AI models, such as deep learning\ninference, training, Vision Transformers (ViTs), and other HPC applications,\ndrives a strong need for runtime precision configurable different non linear\nactivation functions (AF) hardware support. Existing solutions support diverse\nprecision or runtime AF reconfigurability but fail to address both\nsimultaneously. This work proposes a flexible and SIMD multiprecision\nprocessing element (FlexPE), which supports diverse runtime configurable AFs,\nincluding sigmoid, tanh, ReLU and softmax, and MAC operation. The proposed\ndesign achieves an improved throughput of up to 16X FxP4, 8X FxP8, 4X FxP16 and\n1X FxP32 in pipeline mode with 100% time multiplexed hardware. This work\nproposes an area efficient multiprecision iterative mode in the SIMD systolic\narrays for edge AI use cases. The design delivers superior performance with up\nto 62X and 371X reductions in DMA reads for input feature maps and weight\nfilters in VGG16, with an energy efficiency of 8.42 GOPS / W within the\naccuracy loss of 2%. The proposed architecture supports emerging 4-bit\ncomputations for DL inference while enhancing throughput in FxP8/16 modes for\ntransformers and other HPC applications. The proposed approach enables future\nenergy-efficient AI accelerators in edge and cloud environments."
                },
                "authors": [
                    {
                        "name": "Mukul Lokhande"
                    },
                    {
                        "name": "Gopal Raut"
                    },
                    {
                        "name": "Santosh Kumar Vishvakarma"
                    }
                ],
                "author_detail": {
                    "name": "Santosh Kumar Vishvakarma"
                },
                "author": "Santosh Kumar Vishvakarma",
                "arxiv_comment": "10 pages, 5 figures, Preprint, Submitted to TVLSI Regular papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11699v1",
                "updated": "2024-12-16T12:21:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    11,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:21:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "CoinMath: Harnessing the Power of Coding Instruction for Math LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoinMath: Harnessing the Power of Coding Instruction for Math LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown strong performance in solving\nmathematical problems, with code-based solutions proving particularly\neffective. However, the best practice to leverage coding instruction data to\nenhance mathematical reasoning remains underexplored. This study investigates\nthree key questions: (1) How do different coding styles of mathematical\ncode-based rationales impact LLMs' learning performance? (2) Can general-domain\ncoding instructions improve performance? (3) How does integrating textual\nrationales with code-based ones during training enhance mathematical reasoning\nabilities? Our findings reveal that code-based rationales with concise\ncomments, descriptive naming, and hardcoded solutions are beneficial, while\nimprovements from general-domain coding instructions and textual rationales are\nrelatively minor. Based on these insights, we propose CoinMath, a learning\nstrategy designed to enhance mathematical reasoning by diversifying the coding\nstyles of code-based rationales. CoinMath generates a variety of code-based\nrationales incorporating concise comments, descriptive naming conventions, and\nhardcoded solutions. Experimental results demonstrate that CoinMath\nsignificantly outperforms its baseline model, MAmmoTH, one of the SOTA math\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong performance in solving\nmathematical problems, with code-based solutions proving particularly\neffective. However, the best practice to leverage coding instruction data to\nenhance mathematical reasoning remains underexplored. This study investigates\nthree key questions: (1) How do different coding styles of mathematical\ncode-based rationales impact LLMs' learning performance? (2) Can general-domain\ncoding instructions improve performance? (3) How does integrating textual\nrationales with code-based ones during training enhance mathematical reasoning\nabilities? Our findings reveal that code-based rationales with concise\ncomments, descriptive naming, and hardcoded solutions are beneficial, while\nimprovements from general-domain coding instructions and textual rationales are\nrelatively minor. Based on these insights, we propose CoinMath, a learning\nstrategy designed to enhance mathematical reasoning by diversifying the coding\nstyles of code-based rationales. CoinMath generates a variety of code-based\nrationales incorporating concise comments, descriptive naming conventions, and\nhardcoded solutions. Experimental results demonstrate that CoinMath\nsignificantly outperforms its baseline model, MAmmoTH, one of the SOTA math\nLLMs."
                },
                "authors": [
                    {
                        "name": "Chengwei Wei"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Jung-jae Kim"
                    },
                    {
                        "name": "Guimei Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11698v1",
                "updated": "2024-12-16T12:21:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    5,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?"
                },
                "summary": "Context. The security of critical infrastructure has been a fundamental\nconcern since the advent of computers, and this concern has only intensified in\ntoday's cyber warfare landscape. Protecting mission-critical systems (MCSs),\nincluding essential assets like healthcare, telecommunications, and military\ncoordination, is vital for national security. These systems require prompt and\ncomprehensive governance to ensure their resilience, yet recent events have\nshown that meeting these demands is increasingly challenging. Aim. Building on\nprior research that demonstrated the potential of GAI, particularly Large\nLanguage Models (LLMs), in improving risk analysis tasks, we aim to explore\npractitioners' perspectives, specifically developers and security personnel, on\nusing generative AI (GAI) in the governance of IT MCSs seeking to provide\ninsights and recommendations for various stakeholders, including researchers,\npractitioners, and policymakers. Method. We designed a survey to collect\npractical experiences, concerns, and expectations of practitioners who develop\nand implement security solutions in the context of MCSs. Analyzing this data\nwill help identify key trends, challenges, and opportunities for introducing\nGAIs in this niche domain. Conclusions and Future Works. Our findings highlight\nthat the safe use of LLMs in MCS governance requires interdisciplinary\ncollaboration. Researchers should focus on designing regulation-oriented models\nand focus on accountability; practitioners emphasize data protection and\ntransparency, while policymakers must establish a unified AI framework with\nglobal benchmarks to ensure ethical and secure LLMs-based MCS governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The security of critical infrastructure has been a fundamental\nconcern since the advent of computers, and this concern has only intensified in\ntoday's cyber warfare landscape. Protecting mission-critical systems (MCSs),\nincluding essential assets like healthcare, telecommunications, and military\ncoordination, is vital for national security. These systems require prompt and\ncomprehensive governance to ensure their resilience, yet recent events have\nshown that meeting these demands is increasingly challenging. Aim. Building on\nprior research that demonstrated the potential of GAI, particularly Large\nLanguage Models (LLMs), in improving risk analysis tasks, we aim to explore\npractitioners' perspectives, specifically developers and security personnel, on\nusing generative AI (GAI) in the governance of IT MCSs seeking to provide\ninsights and recommendations for various stakeholders, including researchers,\npractitioners, and policymakers. Method. We designed a survey to collect\npractical experiences, concerns, and expectations of practitioners who develop\nand implement security solutions in the context of MCSs. Analyzing this data\nwill help identify key trends, challenges, and opportunities for introducing\nGAIs in this niche domain. Conclusions and Future Works. Our findings highlight\nthat the safe use of LLMs in MCS governance requires interdisciplinary\ncollaboration. Researchers should focus on designing regulation-oriented models\nand focus on accountability; practitioners emphasize data protection and\ntransparency, while policymakers must establish a unified AI framework with\nglobal benchmarks to ensure ethical and secure LLMs-based MCS governance."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Francesco Palagiano"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    },
                    {
                        "name": "Davide Taibi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Taibi"
                },
                "author": "Davide Taibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12196v2",
                "updated": "2024-12-16T12:13:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    13,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-19T03:29:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    29,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs"
                },
                "summary": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability."
                },
                "authors": [
                    {
                        "name": "Zixin Liu"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiran Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Ding"
                },
                "author": "Yiran Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05644v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05644v3",
                "updated": "2024-12-16T12:12:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    12,
                    19,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-07T13:15:22Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    15,
                    22,
                    5,
                    342,
                    0
                ],
                "title": "Mixture of Hidden-Dimensions Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Hidden-Dimensions Transformer"
                },
                "summary": "Transformer models encounter challenges in scaling hidden dimensions\nefficiently, as uniformly increasing them inflates computational and memory\ncosts while failing to emphasize the most relevant features for each token. For\nfurther understanding, we study hidden dimension sparsity and observe that\ntrained Transformers utilize only a small fraction of token dimensions,\nrevealing an \"activation flow\" pattern. Notably, there are shared\nsub-dimensions with sustained activation across multiple consecutive tokens and\nspecialized sub-dimensions uniquely activated for each token. To better model\ntoken-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions),\na sparse conditional activation architecture. Particularly, MoHD employs shared\nsub-dimensions for common token features and a routing mechanism to dynamically\nactivate specialized sub-dimensions. To mitigate potential information loss\nfrom sparsity, we design activation scaling and group fusion mechanisms to\npreserve activation flow. In this way, MoHD expands hidden dimensions with\nnegligible increases in computation or parameters, efficient training and\ninference while maintaining performance. Evaluations across 10 NLP tasks show\nthat MoHD surpasses Vanilla Transformers in parameter efficiency and task\nperformance. It achieves 1.7% higher performance with 50% fewer activation\nparameters and 3.7% higher performance with a 3x parameter expansion at\nconstant activation cost. MOHD offers a new perspective for scaling the model,\nshowcasing the potential of hidden dimension sparsity to boost efficiency",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models encounter challenges in scaling hidden dimensions\nefficiently, as uniformly increasing them inflates computational and memory\ncosts while failing to emphasize the most relevant features for each token. For\nfurther understanding, we study hidden dimension sparsity and observe that\ntrained Transformers utilize only a small fraction of token dimensions,\nrevealing an \"activation flow\" pattern. Notably, there are shared\nsub-dimensions with sustained activation across multiple consecutive tokens and\nspecialized sub-dimensions uniquely activated for each token. To better model\ntoken-relevant sub-dimensions, we propose MoHD (Mixture of Hidden Dimensions),\na sparse conditional activation architecture. Particularly, MoHD employs shared\nsub-dimensions for common token features and a routing mechanism to dynamically\nactivate specialized sub-dimensions. To mitigate potential information loss\nfrom sparsity, we design activation scaling and group fusion mechanisms to\npreserve activation flow. In this way, MoHD expands hidden dimensions with\nnegligible increases in computation or parameters, efficient training and\ninference while maintaining performance. Evaluations across 10 NLP tasks show\nthat MoHD surpasses Vanilla Transformers in parameter efficiency and task\nperformance. It achieves 1.7% higher performance with 50% fewer activation\nparameters and 3.7% higher performance with a 3x parameter expansion at\nconstant activation cost. MOHD offers a new perspective for scaling the model,\nshowcasing the potential of hidden dimension sparsity to boost efficiency"
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "16 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05644v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05644v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11692v1",
                "updated": "2024-12-16T12:10:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    10,
                    23,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:10:23Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    10,
                    23,
                    0,
                    351,
                    0
                ],
                "title": "A partial likelihood approach to tree-based density modeling and its\n  application in Bayesian inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A partial likelihood approach to tree-based density modeling and its\n  application in Bayesian inference"
                },
                "summary": "Tree-based models for probability distributions are usually specified using a\npredetermined, data-independent collection of candidate recursive partitions of\nthe sample space. To characterize an unknown target density in detail over the\nentire sample space, candidate partitions must have the capacity to expand\ndeeply into all areas of the sample space with potential non-zero sampling\nprobability. Such an expansive system of partitions often incurs prohibitive\ncomputational costs and makes inference prone to overfitting, especially in\nregions with little probability mass. Existing models typically make a\ncompromise and rely on relatively shallow trees. This hampers one of the most\ndesirable features of trees, their ability to characterize local features, and\nresults in reduced statistical efficiency. Traditional wisdom suggests that\nthis compromise is inevitable to ensure coherent likelihood-based reasoning, as\na data-dependent partition system that allows deeper expansion only in regions\nwith more observations would induce double dipping of the data and thus lead to\ninconsistent inference. We propose a simple strategy to restore coherency while\nallowing the candidate partitions to be data-dependent, using Cox's partial\nlikelihood. This strategy parametrizes the tree-based sampling model according\nto the allocation of probability mass based on the observed data, and yet under\nappropriate specification, the resulting inference remains valid. Our partial\nlikelihood approach is broadly applicable to existing likelihood-based methods\nand in particular to Bayesian inference on tree-based models. We give examples\nin density estimation in which the partial likelihood is endowed with existing\npriors on tree-based models and compare with the standard, full-likelihood\napproach. The results show substantial gains in estimation accuracy and\ncomputational efficiency from using the partial likelihood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based models for probability distributions are usually specified using a\npredetermined, data-independent collection of candidate recursive partitions of\nthe sample space. To characterize an unknown target density in detail over the\nentire sample space, candidate partitions must have the capacity to expand\ndeeply into all areas of the sample space with potential non-zero sampling\nprobability. Such an expansive system of partitions often incurs prohibitive\ncomputational costs and makes inference prone to overfitting, especially in\nregions with little probability mass. Existing models typically make a\ncompromise and rely on relatively shallow trees. This hampers one of the most\ndesirable features of trees, their ability to characterize local features, and\nresults in reduced statistical efficiency. Traditional wisdom suggests that\nthis compromise is inevitable to ensure coherent likelihood-based reasoning, as\na data-dependent partition system that allows deeper expansion only in regions\nwith more observations would induce double dipping of the data and thus lead to\ninconsistent inference. We propose a simple strategy to restore coherency while\nallowing the candidate partitions to be data-dependent, using Cox's partial\nlikelihood. This strategy parametrizes the tree-based sampling model according\nto the allocation of probability mass based on the observed data, and yet under\nappropriate specification, the resulting inference remains valid. Our partial\nlikelihood approach is broadly applicable to existing likelihood-based methods\nand in particular to Bayesian inference on tree-based models. We give examples\nin density estimation in which the partial likelihood is endowed with existing\npriors on tree-based models and compare with the standard, full-likelihood\napproach. The results show substantial gains in estimation accuracy and\ncomputational efficiency from using the partial likelihood."
                },
                "authors": [
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Benedetta Bruni"
                    }
                ],
                "author_detail": {
                    "name": "Benedetta Bruni"
                },
                "author": "Benedetta Bruni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04902v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04902v5",
                "updated": "2024-12-16T12:06:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    6,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-02-07T14:35:05Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    14,
                    35,
                    5,
                    2,
                    38,
                    0
                ],
                "title": "L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models"
                },
                "summary": "Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning."
                },
                "authors": [
                    {
                        "name": "Hyesung Jeon"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-joon Kim"
                },
                "author": "Jae-joon Kim",
                "arxiv_comment": "8 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04902v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04902v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v2",
                "updated": "2024-12-16T12:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    6,
                    25,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10272v2",
                "updated": "2024-12-16T12:00:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    0,
                    34,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-15T15:28:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    28,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning"
                },
                "summary": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Yanling Wang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11749v2",
                "updated": "2024-12-16T11:53:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    53,
                    9,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-21T16:16:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    16,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks"
                },
                "summary": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks."
                },
                "authors": [
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Russa Biswas"
                    },
                    {
                        "name": "Heather Lent"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "arxiv_comment": "11 pages, 4 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11683v1",
                "updated": "2024-12-16T11:50:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    50,
                    30,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:50:30Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    50,
                    30,
                    0,
                    351,
                    0
                ],
                "title": "Multimodal LLM for Intelligent Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLM for Intelligent Transportation Systems"
                },
                "summary": "In the evolving landscape of transportation systems, integrating Large\nLanguage Models (LLMs) offers a promising frontier for advancing intelligent\ndecision-making across various applications. This paper introduces a novel\n3-dimensional framework that encapsulates the intersection of applications,\nmachine learning methodologies, and hardware devices, particularly emphasizing\nthe role of LLMs. Instead of using multiple machine learning algorithms, our\nframework uses a single, data-centric LLM architecture that can analyze time\nseries, images, and videos. We explore how LLMs can enhance data interpretation\nand decision-making in transportation. We apply this LLM framework to different\nsensor datasets, including time-series data and visual data from sources like\nOxford Radar RobotCar, D-Behavior (D-Set), nuScenes by Motional, and Comma2k19.\nThe goal is to streamline data processing workflows, reduce the complexity of\ndeploying multiple models, and make intelligent transportation systems more\nefficient and accurate. The study was conducted using state-of-the-art\nhardware, leveraging the computational power of AMD RTX 3060 GPUs and Intel\ni9-12900 processors. The experimental results demonstrate that our framework\nachieves an average accuracy of 91.33\\% across these datasets, with the highest\naccuracy observed in time-series data (92.7\\%), showcasing the model's\nproficiency in handling sequential information essential for tasks such as\nmotion planning and predictive maintenance. Through our exploration, we\ndemonstrate the versatility and efficacy of LLMs in handling multimodal data\nwithin the transportation sector, ultimately providing insights into their\napplication in real-world scenarios. Our findings align with the broader\nconference themes, highlighting the transformative potential of LLMs in\nadvancing transportation technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of transportation systems, integrating Large\nLanguage Models (LLMs) offers a promising frontier for advancing intelligent\ndecision-making across various applications. This paper introduces a novel\n3-dimensional framework that encapsulates the intersection of applications,\nmachine learning methodologies, and hardware devices, particularly emphasizing\nthe role of LLMs. Instead of using multiple machine learning algorithms, our\nframework uses a single, data-centric LLM architecture that can analyze time\nseries, images, and videos. We explore how LLMs can enhance data interpretation\nand decision-making in transportation. We apply this LLM framework to different\nsensor datasets, including time-series data and visual data from sources like\nOxford Radar RobotCar, D-Behavior (D-Set), nuScenes by Motional, and Comma2k19.\nThe goal is to streamline data processing workflows, reduce the complexity of\ndeploying multiple models, and make intelligent transportation systems more\nefficient and accurate. The study was conducted using state-of-the-art\nhardware, leveraging the computational power of AMD RTX 3060 GPUs and Intel\ni9-12900 processors. The experimental results demonstrate that our framework\nachieves an average accuracy of 91.33\\% across these datasets, with the highest\naccuracy observed in time-series data (92.7\\%), showcasing the model's\nproficiency in handling sequential information essential for tasks such as\nmotion planning and predictive maintenance. Through our exploration, we\ndemonstrate the versatility and efficacy of LLMs in handling multimodal data\nwithin the transportation sector, ultimately providing insights into their\napplication in real-world scenarios. Our findings align with the broader\nconference themes, highlighting the transformative potential of LLMs in\nadvancing transportation technologies."
                },
                "authors": [
                    {
                        "name": "Dexter Le"
                    },
                    {
                        "name": "Aybars Yunusoglu"
                    },
                    {
                        "name": "Karn Tiwari"
                    },
                    {
                        "name": "Murat Isik"
                    },
                    {
                        "name": "I. Can Dikmen"
                    }
                ],
                "author_detail": {
                    "name": "I. Can Dikmen"
                },
                "author": "I. Can Dikmen",
                "arxiv_comment": "Accepted at IEEE Symposium Series on Computational Intelligence\n  (SSCI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00356v2",
                "updated": "2024-12-16T11:27:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    27,
                    6,
                    0,
                    351,
                    0
                ],
                "published": "2023-09-30T12:18:56Z",
                "published_parsed": [
                    2023,
                    9,
                    30,
                    12,
                    18,
                    56,
                    5,
                    273,
                    0
                ],
                "title": "Functional conditional volatility modeling with missing data: inference\n  and application to energy commodities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional conditional volatility modeling with missing data: inference\n  and application to energy commodities"
                },
                "summary": "This paper explores the nonparametric estimation of the volatility component\nin a heteroscedastic scalar-on-function regression model, where the underlying\ndiscrete-time process is ergodic and subject to a missing-at-random mechanism.\nWe first propose a simplified estimator for the regression and volatility\noperators, constructed solely from the observed data. The asymptotic properties\nof these estimators, including the almost sure uniform consistency rate and\nasymptotic distribution, are rigorously analyzed. Subsequently, the simplified\nestimators are employed to impute the missing data in the original process,\nenhancing the estimation of the regression and volatility components. The\nasymptotic behavior of these imputed estimators is also thoroughly\ninvestigated. A numerical comparison of the simplified and imputed estimators\nis presented using simulated data. Finally, the methodology is applied to\nreal-world data to model the volatility of daily natural gas returns, utilizing\nintraday EU/USD exchange rate return curves sampled at a 1-hour frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the nonparametric estimation of the volatility component\nin a heteroscedastic scalar-on-function regression model, where the underlying\ndiscrete-time process is ergodic and subject to a missing-at-random mechanism.\nWe first propose a simplified estimator for the regression and volatility\noperators, constructed solely from the observed data. The asymptotic properties\nof these estimators, including the almost sure uniform consistency rate and\nasymptotic distribution, are rigorously analyzed. Subsequently, the simplified\nestimators are employed to impute the missing data in the original process,\nenhancing the estimation of the regression and volatility components. The\nasymptotic behavior of these imputed estimators is also thoroughly\ninvestigated. A numerical comparison of the simplified and imputed estimators\nis presented using simulated data. Finally, the methodology is applied to\nreal-world data to model the volatility of daily natural gas returns, utilizing\nintraday EU/USD exchange rate return curves sampled at a 1-hour frequency."
                },
                "authors": [
                    {
                        "name": "Abdelbasset Djeniah"
                    },
                    {
                        "name": "Mohamed Chaouch"
                    },
                    {
                        "name": "Amina Angelika Bouchentouf"
                    }
                ],
                "author_detail": {
                    "name": "Amina Angelika Bouchentouf"
                },
                "author": "Amina Angelika Bouchentouf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11847v2",
                "updated": "2024-12-16T11:26:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    26,
                    21,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-12T15:19:59Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    19,
                    59,
                    0,
                    225,
                    0
                ],
                "title": "Prompto: An open source library for asynchronous querying of LLM\n  endpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompto: An open source library for asynchronous querying of LLM\n  endpoints"
                },
                "summary": "Recent surge in Large Language Model (LLM) availability has opened exciting\navenues for research. However, efficiently interacting with these models\npresents a significant hurdle since LLMs often reside on proprietary or\nself-hosted API endpoints, each requiring custom code for interaction.\nConducting comparative studies between different models can therefore be\ntime-consuming and necessitate significant engineering effort, hindering\nresearch efficiency and reproducibility. To address these challenges, we\npresent prompto, an open source Python library which facilitates asynchronous\nquerying of LLM endpoints enabling researchers to interact with multiple LLMs\nconcurrently, while maximising efficiency and utilising individual rate limits.\nOur library empowers researchers and developers to interact with LLMs more\neffectively and allowing faster experimentation, data generation and\nevaluation. prompto is released with an introductory video\n(https://youtu.be/lWN9hXBOLyQ) under MIT License and is available via GitHub\n(https://github.com/alan-turing-institute/prompto).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent surge in Large Language Model (LLM) availability has opened exciting\navenues for research. However, efficiently interacting with these models\npresents a significant hurdle since LLMs often reside on proprietary or\nself-hosted API endpoints, each requiring custom code for interaction.\nConducting comparative studies between different models can therefore be\ntime-consuming and necessitate significant engineering effort, hindering\nresearch efficiency and reproducibility. To address these challenges, we\npresent prompto, an open source Python library which facilitates asynchronous\nquerying of LLM endpoints enabling researchers to interact with multiple LLMs\nconcurrently, while maximising efficiency and utilising individual rate limits.\nOur library empowers researchers and developers to interact with LLMs more\neffectively and allowing faster experimentation, data generation and\nevaluation. prompto is released with an introductory video\n(https://youtu.be/lWN9hXBOLyQ) under MIT License and is available via GitHub\n(https://github.com/alan-turing-institute/prompto)."
                },
                "authors": [
                    {
                        "name": "Ryan Sze-Yin Chan"
                    },
                    {
                        "name": "Federico Nanni"
                    },
                    {
                        "name": "Angus R. Williams"
                    },
                    {
                        "name": "Edwin Brown"
                    },
                    {
                        "name": "Liam Burke-Moore"
                    },
                    {
                        "name": "Ed Chapman"
                    },
                    {
                        "name": "Kate Onslow"
                    },
                    {
                        "name": "Tvesha Sippy"
                    },
                    {
                        "name": "Jonathan Bright"
                    },
                    {
                        "name": "Evelina Gabasova"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Gabasova"
                },
                "author": "Evelina Gabasova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11672v1",
                "updated": "2024-12-16T11:25:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    25,
                    56,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:25:56Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    25,
                    56,
                    0,
                    351,
                    0
                ],
                "title": "LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User\n  Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User\n  Requests"
                },
                "summary": "We propose LLM-DaaS, a novel Drone-as-a-Service (DaaS) framework that\nleverages Large Language Models (LLMs) to transform free-text user requests\ninto structured, actionable DaaS operation tasks. Our approach addresses the\nkey challenge of interpreting and structuring natural language input to\nautomate drone service operations under uncertain conditions. The system is\ncomposed of three main components: free-text request processing, structured\nrequest generation, and dynamic DaaS selection and composition. First, we\nfine-tune different LLM models such as Phi-3.5, LLaMA-3.2 7b and Gemma 2b on a\ndataset of text user requests mapped to structured DaaS requests. Users\ninteract with our model in a free conversational style, discussing package\ndelivery requests, while the fine-tuned LLM extracts DaaS metadata such as\ndelivery time, source and destination locations, and package weight. The DaaS\nservice selection model is designed to select the best available drone capable\nof delivering the requested package from the delivery point to the nearest\noptimal destination. Additionally, the DaaS composition model composes a\nservice from a set of the best available drones to deliver the package from the\nsource to the final destination. Second, the system integrates real-time\nweather data to optimize drone route planning and scheduling, ensuring safe and\nefficient operations. Simulations demonstrate the system's ability to\nsignificantly improve task accuracy, operational efficiency, and establish\nLLM-DaaS as a robust solution for DaaS operations in uncertain environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LLM-DaaS, a novel Drone-as-a-Service (DaaS) framework that\nleverages Large Language Models (LLMs) to transform free-text user requests\ninto structured, actionable DaaS operation tasks. Our approach addresses the\nkey challenge of interpreting and structuring natural language input to\nautomate drone service operations under uncertain conditions. The system is\ncomposed of three main components: free-text request processing, structured\nrequest generation, and dynamic DaaS selection and composition. First, we\nfine-tune different LLM models such as Phi-3.5, LLaMA-3.2 7b and Gemma 2b on a\ndataset of text user requests mapped to structured DaaS requests. Users\ninteract with our model in a free conversational style, discussing package\ndelivery requests, while the fine-tuned LLM extracts DaaS metadata such as\ndelivery time, source and destination locations, and package weight. The DaaS\nservice selection model is designed to select the best available drone capable\nof delivering the requested package from the delivery point to the nearest\noptimal destination. Additionally, the DaaS composition model composes a\nservice from a set of the best available drones to deliver the package from the\nsource to the final destination. Second, the system integrates real-time\nweather data to optimize drone route planning and scheduling, ensuring safe and\nefficient operations. Simulations demonstrate the system's ability to\nsignificantly improve task accuracy, operational efficiency, and establish\nLLM-DaaS as a robust solution for DaaS operations in uncertain environments."
                },
                "authors": [
                    {
                        "name": "Lillian Wassim"
                    },
                    {
                        "name": "Kamal Mohamed"
                    },
                    {
                        "name": "Ali Hamdi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Hamdi"
                },
                "author": "Ali Hamdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13578v2",
                "updated": "2024-12-16T11:23:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    23,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-07-18T15:20:18Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    15,
                    20,
                    18,
                    3,
                    200,
                    0
                ],
                "title": "How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and\n  Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and\n  Consistency"
                },
                "summary": "Large Language Models (LLMs) are increasingly explored as knowledge bases\n(KBs), yet current evaluation methods focus too narrowly on knowledge\nretention, overlooking other crucial criteria for reliable performance. In this\nwork, we rethink the requirements for evaluating reliable LLM-as-KB usage and\nhighlight two essential factors: factuality, ensuring accurate responses to\nseen and unseen knowledge, and consistency, maintaining stable answers to\nquestions about the same knowledge. We introduce UnseenQA, a dataset designed\nto assess LLM performance on unseen knowledge, and propose new criteria and\nmetrics to quantify factuality and consistency, leading to a final reliability\nscore. Our experiments on 26 LLMs reveal several challenges regarding their use\nas KBs, underscoring the need for more principled and comprehensive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly explored as knowledge bases\n(KBs), yet current evaluation methods focus too narrowly on knowledge\nretention, overlooking other crucial criteria for reliable performance. In this\nwork, we rethink the requirements for evaluating reliable LLM-as-KB usage and\nhighlight two essential factors: factuality, ensuring accurate responses to\nseen and unseen knowledge, and consistency, maintaining stable answers to\nquestions about the same knowledge. We introduce UnseenQA, a dataset designed\nto assess LLM performance on unseen knowledge, and propose new criteria and\nmetrics to quantify factuality and consistency, leading to a final reliability\nscore. Our experiments on 26 LLMs reveal several challenges regarding their use\nas KBs, underscoring the need for more principled and comprehensive evaluation."
                },
                "authors": [
                    {
                        "name": "Danna Zheng"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08707v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08707v7",
                "updated": "2024-12-16T11:21:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    21,
                    30,
                    0,
                    351,
                    0
                ],
                "published": "2024-04-11T17:44:56Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    44,
                    56,
                    3,
                    102,
                    0
                ],
                "title": "CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes"
                },
                "summary": "As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25."
                },
                "authors": [
                    {
                        "name": "Haokun Zhao"
                    },
                    {
                        "name": "Haixia Han"
                    },
                    {
                        "name": "Jie Shi"
                    },
                    {
                        "name": "Chengyu Du"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08707v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08707v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11664v1",
                "updated": "2024-12-16T11:12:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    12,
                    45,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:12:45Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    12,
                    45,
                    0,
                    351,
                    0
                ],
                "title": "C3oT: Generating Shorter Chain-of-Thought without Compromising\n  Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C3oT: Generating Shorter Chain-of-Thought without Compromising\n  Effectiveness"
                },
                "summary": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness."
                },
                "authors": [
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Xianghui Sun"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Wei Zou"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zou"
                },
                "author": "Wei Zou",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19377v2",
                "updated": "2024-12-16T11:09:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    9,
                    58,
                    0,
                    351,
                    0
                ],
                "published": "2024-09-28T15:03:49Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    49,
                    5,
                    272,
                    0
                ],
                "title": "Interpretable, multi-dimensional Evaluation Framework for Causal\n  Discovery from observational i.i.d. Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable, multi-dimensional Evaluation Framework for Causal\n  Discovery from observational i.i.d. Data"
                },
                "summary": "Nonlinear causal discovery from observational data imposes strict\nidentifiability assumptions on the formulation of structural equations utilized\nin the data generating process. The evaluation of structure learning methods\nunder assumption violations requires a rigorous and interpretable approach,\nwhich quantifies both the structural similarity of the estimation with the\nground truth and the capacity of the discovered graphs to be used for causal\ninference. Motivated by the lack of unified performance assessment framework,\nwe introduce an interpretable, six-dimensional evaluation metric, i.e.,\ndistance to optimal solution (DOS), which is specifically tailored to the field\nof causal discovery. Furthermore, this is the first research to assess the\nperformance of structure learning algorithms from seven different families on\nincreasing percentage of non-identifiable, nonlinear causal patterns, inspired\nby real-world processes. Our large-scale simulation study, which incorporates\nseven experimental factors, shows that besides causal order-based methods,\namortized causal discovery delivers results with comparatively high proximity\nto the optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear causal discovery from observational data imposes strict\nidentifiability assumptions on the formulation of structural equations utilized\nin the data generating process. The evaluation of structure learning methods\nunder assumption violations requires a rigorous and interpretable approach,\nwhich quantifies both the structural similarity of the estimation with the\nground truth and the capacity of the discovered graphs to be used for causal\ninference. Motivated by the lack of unified performance assessment framework,\nwe introduce an interpretable, six-dimensional evaluation metric, i.e.,\ndistance to optimal solution (DOS), which is specifically tailored to the field\nof causal discovery. Furthermore, this is the first research to assess the\nperformance of structure learning algorithms from seven different families on\nincreasing percentage of non-identifiable, nonlinear causal patterns, inspired\nby real-world processes. Our large-scale simulation study, which incorporates\nseven experimental factors, shows that besides causal order-based methods,\namortized causal discovery delivers results with comparatively high proximity\nto the optimal solution."
                },
                "authors": [
                    {
                        "name": "Georg Velev"
                    },
                    {
                        "name": "Stefan Lessmann"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Lessmann"
                },
                "author": "Stefan Lessmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01704v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01704v3",
                "updated": "2024-12-16T11:03:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    3,
                    31,
                    0,
                    351,
                    0
                ],
                "published": "2024-01-24T22:22:00Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    22,
                    22,
                    0,
                    2,
                    24,
                    0
                ],
                "title": "Steering Language Models with Game-Theoretic Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models with Game-Theoretic Solvers"
                },
                "summary": "Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research."
                },
                "authors": [
                    {
                        "name": "Ian Gemp"
                    },
                    {
                        "name": "Roma Patel"
                    },
                    {
                        "name": "Yoram Bachrach"
                    },
                    {
                        "name": "Marc Lanctot"
                    },
                    {
                        "name": "Vibhavari Dasagi"
                    },
                    {
                        "name": "Luke Marris"
                    },
                    {
                        "name": "Georgios Piliouras"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Karl Tuyls"
                    }
                ],
                "author_detail": {
                    "name": "Karl Tuyls"
                },
                "author": "Karl Tuyls",
                "arxiv_comment": "Code available @\n  https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01704v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11656v1",
                "updated": "2024-12-16T10:59:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    59,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:59:49Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    59,
                    49,
                    0,
                    351,
                    0
                ],
                "title": "Private Yet Social: How LLM Chatbots Support and Challenge Eating\n  Disorder Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Yet Social: How LLM Chatbots Support and Challenge Eating\n  Disorder Recovery"
                },
                "summary": "Eating disorders (ED) are complex mental health conditions that require\nlong-term management and support. Recent advancements in large language model\n(LLM)-based chatbots offer the potential to assist individuals in receiving\nimmediate support. Yet, concerns remain about their reliability and safety in\nsensitive contexts such as ED. We explore the opportunities and potential harms\nof using LLM-based chatbots for ED recovery. We observe the interactions\nbetween 26 participants with ED and an LLM-based chatbot, WellnessBot, designed\nto support ED recovery, over 10 days. We discovered that our participants have\nfelt empowered in recovery by discussing ED-related stories with the chatbot,\nwhich served as a personal yet social avenue. However, we also identified\nharmful chatbot responses, especially concerning individuals with ED, that went\nunnoticed partly due to participants' unquestioning trust in the chatbot's\nreliability. Based on these findings, we provide design implications for safe\nand effective LLM-based interventions in ED management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eating disorders (ED) are complex mental health conditions that require\nlong-term management and support. Recent advancements in large language model\n(LLM)-based chatbots offer the potential to assist individuals in receiving\nimmediate support. Yet, concerns remain about their reliability and safety in\nsensitive contexts such as ED. We explore the opportunities and potential harms\nof using LLM-based chatbots for ED recovery. We observe the interactions\nbetween 26 participants with ED and an LLM-based chatbot, WellnessBot, designed\nto support ED recovery, over 10 days. We discovered that our participants have\nfelt empowered in recovery by discussing ED-related stories with the chatbot,\nwhich served as a personal yet social avenue. However, we also identified\nharmful chatbot responses, especially concerning individuals with ED, that went\nunnoticed partly due to participants' unquestioning trust in the chatbot's\nreliability. Based on these findings, we provide design implications for safe\nand effective LLM-based interventions in ED management."
                },
                "authors": [
                    {
                        "name": "Ryuhaerang Choi"
                    },
                    {
                        "name": "Taehan Kim"
                    },
                    {
                        "name": "Subin Park"
                    },
                    {
                        "name": "Jennifer G Kim"
                    },
                    {
                        "name": "Sung-Ju Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sung-Ju Lee"
                },
                "author": "Sung-Ju Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10666v3",
                "updated": "2024-12-16T10:48:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    48,
                    28,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-16T02:02:49Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    2,
                    2,
                    49,
                    5,
                    321,
                    0
                ],
                "title": "SAM Decoding: Speculative Decoding via Suffix Automaton",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM Decoding: Speculative Decoding via Suffix Automaton"
                },
                "summary": "Speculative decoding (SD) has been demonstrated as an effective technique for\nlossless LLM inference acceleration. Retrieval-based SD methods, one kind of\nmodel-free method, have yielded promising speedup, but they often rely on\nincomplete retrieval resources, inefficient retrieval methods, and are\nconstrained to certain domains. This paper presents a novel retrieval-based\nspeculative decoding method that adapts suffix automaton (SAM) for efficient\nand accurate draft generation by utilizing common text corpus and dynamic text\nsequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the\nexact longest suffix match, achieving an average time complexity of O(1) per\ngeneration step of SAM update and suffix retrieval. It can also integrate with\nexisting methods, adaptively selecting a draft generation strategy based on\nmatch length to generalize to broader domains. Extensive experiments on\nSpec-Bench show that our method is $18\\%+$ faster than other retrieval-based SD\nmethods. Additionally, when combined with advanced EAGLE-2, it provides an\nadditional speedup of $3.28\\%$ -- $11.13\\%$ across various-sized LLM backbones.\nOur code is available at our\n\\href{https://github.com/hyx1999/SAM-Decoding}{repository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been demonstrated as an effective technique for\nlossless LLM inference acceleration. Retrieval-based SD methods, one kind of\nmodel-free method, have yielded promising speedup, but they often rely on\nincomplete retrieval resources, inefficient retrieval methods, and are\nconstrained to certain domains. This paper presents a novel retrieval-based\nspeculative decoding method that adapts suffix automaton (SAM) for efficient\nand accurate draft generation by utilizing common text corpus and dynamic text\nsequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the\nexact longest suffix match, achieving an average time complexity of O(1) per\ngeneration step of SAM update and suffix retrieval. It can also integrate with\nexisting methods, adaptively selecting a draft generation strategy based on\nmatch length to generalize to broader domains. Extensive experiments on\nSpec-Bench show that our method is $18\\%+$ faster than other retrieval-based SD\nmethods. Additionally, when combined with advanced EAGLE-2, it provides an\nadditional speedup of $3.28\\%$ -- $11.13\\%$ across various-sized LLM backbones.\nOur code is available at our\n\\href{https://github.com/hyx1999/SAM-Decoding}{repository}."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Fanjin Zhang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "16 pages, 9 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11640v1",
                "updated": "2024-12-16T10:37:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    37,
                    30,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:37:30Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    37,
                    30,
                    0,
                    351,
                    0
                ],
                "title": "SeSeMI: Secure Serverless Model Inference on Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeSeMI: Secure Serverless Model Inference on Sensitive Data"
                },
                "summary": "Model inference systems are essential for implementing end-to-end data\nanalytics pipelines that deliver the benefits of machine learning models to\nusers. Existing cloud-based model inference systems are costly, not easy to\nscale, and must be trusted in handling the models and user request data.\nServerless computing presents a new opportunity, as it provides elasticity and\nfine-grained pricing. Our goal is to design a serverless model inference system\nthat protects models and user request data from untrusted cloud providers. It\noffers high performance and low cost, while requiring no intrusive changes to\nthe current serverless platforms. To realize our goal, we leverage trusted\nhardware. We identify and address three challenges in using trusted hardware\nfor serverless model inference. These challenges arise from the high-level\nabstraction of serverless computing, the performance overhead of trusted\nhardware, and the characteristics of model inference workloads. We present\nSeSeMI, a secure, efficient, and cost-effective serverless model inference\nsystem. It adds three novel features non-intrusively to the existing serverless\ninfrastructure and nothing else.The first feature is a key service that\nestablishes secure channels between the user and the serverless instances,\nwhich also provides access control to models and users' data. The second is an\nenclave runtime that allows one enclave to process multiple concurrent\nrequests. The final feature is a model packer that allows multiple models to be\nexecuted by one serverless instance. We build SeSeMI on top of Apache\nOpenWhisk, and conduct extensive experiments with three popular machine\nlearning models. The results show that SeSeMI achieves low latency and low cost\nat scale for realistic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model inference systems are essential for implementing end-to-end data\nanalytics pipelines that deliver the benefits of machine learning models to\nusers. Existing cloud-based model inference systems are costly, not easy to\nscale, and must be trusted in handling the models and user request data.\nServerless computing presents a new opportunity, as it provides elasticity and\nfine-grained pricing. Our goal is to design a serverless model inference system\nthat protects models and user request data from untrusted cloud providers. It\noffers high performance and low cost, while requiring no intrusive changes to\nthe current serverless platforms. To realize our goal, we leverage trusted\nhardware. We identify and address three challenges in using trusted hardware\nfor serverless model inference. These challenges arise from the high-level\nabstraction of serverless computing, the performance overhead of trusted\nhardware, and the characteristics of model inference workloads. We present\nSeSeMI, a secure, efficient, and cost-effective serverless model inference\nsystem. It adds three novel features non-intrusively to the existing serverless\ninfrastructure and nothing else.The first feature is a key service that\nestablishes secure channels between the user and the serverless instances,\nwhich also provides access control to models and users' data. The second is an\nenclave runtime that allows one enclave to process multiple concurrent\nrequests. The final feature is a model packer that allows multiple models to be\nexecuted by one serverless instance. We build SeSeMI on top of Apache\nOpenWhisk, and conduct extensive experiments with three popular machine\nlearning models. The results show that SeSeMI achieves low latency and low cost\nat scale for realistic workloads."
                },
                "authors": [
                    {
                        "name": "Guoyu Hu"
                    },
                    {
                        "name": "Yuncheng Wu"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Tien Tuan Anh Dinh"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.12094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v1",
                "updated": "2024-12-16T18:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12084v1",
                "updated": "2024-12-16T18:55:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    55,
                    12,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:55:12Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    55,
                    12,
                    0,
                    351,
                    0
                ],
                "title": "DarkNESS: developing a skipper-CCD instrument to search for Dark Matter\n  from Low Earth Orbit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DarkNESS: developing a skipper-CCD instrument to search for Dark Matter\n  from Low Earth Orbit"
                },
                "summary": "The DarkNESS (Dark Matter Nano-satellite Equipped with Skipper Sensors)\nmission aims to deploy a skipper-CCD CubeSat Observatory to search for dark\nmatter (DM) from Low Earth Orbit. This mission will employ novel skipper-CCDs\nto investigate O(keV) X-rays from decaying DM, as well as electron recoils from\nstrongly-interacting sub-GeV DM. The DarkNESS mission will be the first space\ndeployment of skipper-CCDs, and the DarkNESS team is developing a skipper-CCD\ninstrument that is compatible with the CubeSat platform. DarkNESS has recently\nprogressed from laboratory validation to a Critical Design Review (CDR) phase,\nwith a launch opportunity anticipated in late 2025. The implementation of the\nDarkNESS skipper-CCD payload on the CubeSat platform will pave the way for\nfuture demonstrators of space-based imagers for X-ray and single-electron\ncounting applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DarkNESS (Dark Matter Nano-satellite Equipped with Skipper Sensors)\nmission aims to deploy a skipper-CCD CubeSat Observatory to search for dark\nmatter (DM) from Low Earth Orbit. This mission will employ novel skipper-CCDs\nto investigate O(keV) X-rays from decaying DM, as well as electron recoils from\nstrongly-interacting sub-GeV DM. The DarkNESS mission will be the first space\ndeployment of skipper-CCDs, and the DarkNESS team is developing a skipper-CCD\ninstrument that is compatible with the CubeSat platform. DarkNESS has recently\nprogressed from laboratory validation to a Critical Design Review (CDR) phase,\nwith a launch opportunity anticipated in late 2025. The implementation of the\nDarkNESS skipper-CCD payload on the CubeSat platform will pave the way for\nfuture demonstrators of space-based imagers for X-ray and single-electron\ncounting applications."
                },
                "authors": [
                    {
                        "name": "Phoenix Alpine"
                    },
                    {
                        "name": "Samriddhi Bhatia"
                    },
                    {
                        "name": "Fernando Chierchie"
                    },
                    {
                        "name": "Alex Drlica-Wagner"
                    },
                    {
                        "name": "Rouven Essig"
                    },
                    {
                        "name": "Juan Estrada"
                    },
                    {
                        "name": "Erez Etzion"
                    },
                    {
                        "name": "Roni Harnik"
                    },
                    {
                        "name": "Michael Lembeck"
                    },
                    {
                        "name": "Nathan Saffold"
                    },
                    {
                        "name": "Sho Uemura"
                    }
                ],
                "author_detail": {
                    "name": "Sho Uemura"
                },
                "author": "Sho Uemura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12072v1",
                "updated": "2024-12-16T18:46:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    46,
                    12,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:46:12Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    46,
                    12,
                    0,
                    351,
                    0
                ],
                "title": "Making FETCH! Happen: Finding Emergent Dog Whistles Through Common\n  Habitats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making FETCH! Happen: Finding Emergent Dog Whistles Through Common\n  Habitats"
                },
                "summary": "WARNING: This paper contains content that maybe upsetting or offensive to\nsome readers. Dog whistles are coded expressions with dual meanings: one\nintended for the general public (outgroup) and another that conveys a specific\nmessage to an intended audience (ingroup). Often, these expressions are used to\nconvey controversial political opinions while maintaining plausible deniability\nand slip by content moderation filters. Identification of dog whistles relies\non curated lexicons, which have trouble keeping up to date. We introduce\n\\textbf{FETCH!}, a task for finding novel dog whistles in massive social media\ncorpora. We find that state-of-the-art systems fail to achieve meaningful\nresults across three distinct social media case studies. We present\n\\textbf{EarShot}, a novel system that combines the strengths of vector\ndatabases and Large Language Models (LLMs) to efficiently and effectively\nidentify new dog whistles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WARNING: This paper contains content that maybe upsetting or offensive to\nsome readers. Dog whistles are coded expressions with dual meanings: one\nintended for the general public (outgroup) and another that conveys a specific\nmessage to an intended audience (ingroup). Often, these expressions are used to\nconvey controversial political opinions while maintaining plausible deniability\nand slip by content moderation filters. Identification of dog whistles relies\non curated lexicons, which have trouble keeping up to date. We introduce\n\\textbf{FETCH!}, a task for finding novel dog whistles in massive social media\ncorpora. We find that state-of-the-art systems fail to achieve meaningful\nresults across three distinct social media case studies. We present\n\\textbf{EarShot}, a novel system that combines the strengths of vector\ndatabases and Large Language Models (LLMs) to efficiently and effectively\nidentify new dog whistles."
                },
                "authors": [
                    {
                        "name": "Kuleen Sasse"
                    },
                    {
                        "name": "Carlos Aguirre"
                    },
                    {
                        "name": "Isabel Cachola"
                    },
                    {
                        "name": "Sharon Levy"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04215v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04215v2",
                "updated": "2024-12-16T18:39:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    39,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-08T04:49:24Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    4,
                    49,
                    24,
                    3,
                    221,
                    0
                ],
                "title": "Comp-LTL: Temporal Logic Planning via Zero-Shot Policy Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comp-LTL: Temporal Logic Planning via Zero-Shot Policy Composition"
                },
                "summary": "This work develops a zero-shot mechanism, Comp-LTL, for an agent to satisfy a\nLinear Temporal Logic (LTL) specification given existing task primitives\ntrained via reinforcement learning (RL). Autonomous robots often need to\nsatisfy spatial and temporal goals that are unknown until run time. Prior work\nfocuses on learning policies for executing a task specified using LTL, but they\nincorporate the specification into the learning process. Any change to the\nspecification requires retraining the policy, either via fine-tuning or from\nscratch. We present a more flexible approach -- to learn a set of composable\ntask primitive policies that can be used to satisfy arbitrary LTL\nspecifications without retraining or fine-tuning. Task primitives can be\nlearned offline using RL and combined using Boolean composition at deployment.\nThis work focuses on creating and pruning a transition system (TS)\nrepresentation of the environment in order to solve for deterministic,\nnon-ambiguous, and feasible solutions to LTL specifications given an\nenvironment and a set of task primitive policies. We show that our pruned TS is\ndeterministic, contains no unrealizable transitions, and is sound. We verify\nour approach via simulation and compare it to other state of the art\napproaches, showing that Comp-LTL is safer and more adaptable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work develops a zero-shot mechanism, Comp-LTL, for an agent to satisfy a\nLinear Temporal Logic (LTL) specification given existing task primitives\ntrained via reinforcement learning (RL). Autonomous robots often need to\nsatisfy spatial and temporal goals that are unknown until run time. Prior work\nfocuses on learning policies for executing a task specified using LTL, but they\nincorporate the specification into the learning process. Any change to the\nspecification requires retraining the policy, either via fine-tuning or from\nscratch. We present a more flexible approach -- to learn a set of composable\ntask primitive policies that can be used to satisfy arbitrary LTL\nspecifications without retraining or fine-tuning. Task primitives can be\nlearned offline using RL and combined using Boolean composition at deployment.\nThis work focuses on creating and pruning a transition system (TS)\nrepresentation of the environment in order to solve for deterministic,\nnon-ambiguous, and feasible solutions to LTL specifications given an\nenvironment and a set of task primitive policies. We show that our pruned TS is\ndeterministic, contains no unrealizable transitions, and is sound. We verify\nour approach via simulation and compare it to other state of the art\napproaches, showing that Comp-LTL is safer and more adaptable."
                },
                "authors": [
                    {
                        "name": "Taylor Bergeron"
                    },
                    {
                        "name": "Zachary Serlin"
                    },
                    {
                        "name": "Kevin Leahy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Leahy"
                },
                "author": "Kevin Leahy",
                "arxiv_comment": "16 pages, 11 figures. Updated to reflect additional results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04215v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04215v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10799v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10799v4",
                "updated": "2024-12-16T18:31:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    31,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-03-16T04:12:50Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    4,
                    12,
                    50,
                    5,
                    76,
                    0
                ],
                "title": "Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Adaptive Large Language Models Structured Pruning via\n  Hybrid-grained Weight Importance Assessment"
                },
                "summary": "Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured pruning for large language models (LLMs) has garnered significant\nacademic interest due to its ability to efficiently compress and accelerate\nLLMs by eliminating redundant weight groups at a coarse-grained granularity.\nCurrent structured pruning methods for LLMs typically depend on a singular\ngranularity for assessing weight importance, resulting in notable performance\ndegradation in downstream tasks. Intriguingly, our empirical investigations\nreveal that utilizing unstructured pruning, which achieves better performance\nretention by pruning weights at a finer granularity, \\emph{i.e.}, individual\nweights, yields significantly varied sparse LLM structures when juxtaposed to\nstructured pruning. This suggests that evaluating both holistic and individual\nassessment for weight importance is essential for LLM pruning. Building on this\ninsight, we introduce the Hybrid-grained Weight Importance Assessment (HyWIA),\na novel method that merges fine-grained and coarse-grained evaluations of\nweight importance for the pruning of LLMs. Leveraging an attention mechanism,\nHyWIA adaptively determines the optimal blend of granularity in weight\nimportance assessments in an end-to-end pruning manner. Extensive experiments\non LLaMA-V1/V2, Vicuna, Baichuan, and Bloom across various benchmarks\ndemonstrate the effectiveness of HyWIA in pruning LLMs. For example, HyWIA\nsurpasses the cutting-edge LLM-Pruner by an average margin of 2.82\\% in\naccuracy across seven downstream tasks when pruning LLaMA-7B by 50\\%."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10799v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10799v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11848v2",
                "updated": "2024-12-16T18:25:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    25,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-13T01:30:03Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    1,
                    30,
                    3,
                    1,
                    226,
                    0
                ],
                "title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MGH Radiology Llama: A Llama 3 70B Model for Radiology"
                },
                "summary": "In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs."
                },
                "authors": [
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Peng Shu"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "11 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12039v1",
                "updated": "2024-12-16T18:08:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    8,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:08:14Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    8,
                    14,
                    0,
                    351,
                    0
                ],
                "title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability\n  Detection"
                },
                "summary": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively."
                },
                "authors": [
                    {
                        "name": "Ira Ceka"
                    },
                    {
                        "name": "Feitong Qiao"
                    },
                    {
                        "name": "Anik Dey"
                    },
                    {
                        "name": "Aastha Valechia"
                    },
                    {
                        "name": "Gail Kaiser"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12038v1",
                "updated": "2024-12-16T18:03:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    3,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T18:03:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    3,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "LLMs for Cold-Start Cutting Plane Separator Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Cold-Start Cutting Plane Separator Configuration"
                },
                "summary": "Mixed integer linear programming (MILP) solvers ship with a staggering number\nof parameters that are challenging to select a priori for all but expert\noptimization users, but can have an outsized impact on the performance of the\nMILP solver. Existing machine learning (ML) approaches to configure solvers\nrequire training ML models by solving thousands of related MILP instances,\ngeneralize poorly to new problem sizes, and often require implementing complex\nML pipelines and custom solver interfaces that can be difficult to integrate\ninto existing optimization workflows. In this paper, we introduce a new\nLLM-based framework to configure which cutting plane separators to use for a\ngiven MILP problem with little to no training data based on characteristics of\nthe instance, such as a natural language description of the problem and the\nassociated LaTeX formulation. We augment these LLMs with descriptions of\ncutting plane separators available in a given solver, grounded by summarizing\nthe existing research literature on separators. While individual solver\nconfigurations have a large variance in performance, we present a novel\nensembling strategy that clusters and aggregates configurations to create a\nsmall portfolio of high-performing configurations. Our LLM-based methodology\nrequires no custom solver interface, can find a high-performing configuration\nby solving only a small number of MILPs, and can generate the configuration\nwith simple API calls that run in under a second. Numerical results show our\napproach is competitive with existing configuration approaches on a suite of\nclassic combinatorial optimization problems and real-world datasets with only a\nfraction of the training data and computation time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed integer linear programming (MILP) solvers ship with a staggering number\nof parameters that are challenging to select a priori for all but expert\noptimization users, but can have an outsized impact on the performance of the\nMILP solver. Existing machine learning (ML) approaches to configure solvers\nrequire training ML models by solving thousands of related MILP instances,\ngeneralize poorly to new problem sizes, and often require implementing complex\nML pipelines and custom solver interfaces that can be difficult to integrate\ninto existing optimization workflows. In this paper, we introduce a new\nLLM-based framework to configure which cutting plane separators to use for a\ngiven MILP problem with little to no training data based on characteristics of\nthe instance, such as a natural language description of the problem and the\nassociated LaTeX formulation. We augment these LLMs with descriptions of\ncutting plane separators available in a given solver, grounded by summarizing\nthe existing research literature on separators. While individual solver\nconfigurations have a large variance in performance, we present a novel\nensembling strategy that clusters and aggregates configurations to create a\nsmall portfolio of high-performing configurations. Our LLM-based methodology\nrequires no custom solver interface, can find a high-performing configuration\nby solving only a small number of MILPs, and can generate the configuration\nwith simple API calls that run in under a second. Numerical results show our\napproach is competitive with existing configuration approaches on a suite of\nclassic combinatorial optimization problems and real-world datasets with only a\nfraction of the training data and computation time."
                },
                "authors": [
                    {
                        "name": "Connor Lawless"
                    },
                    {
                        "name": "Yingxi Li"
                    },
                    {
                        "name": "Anders Wikum"
                    },
                    {
                        "name": "Madeleine Udell"
                    },
                    {
                        "name": "Ellen Vitercik"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Vitercik"
                },
                "author": "Ellen Vitercik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12009v1",
                "updated": "2024-12-16T17:36:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    36,
                    2,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:36:02Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    36,
                    2,
                    0,
                    351,
                    0
                ],
                "title": "SpeechPrune: Context-aware Token Pruning for Speech Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechPrune: Context-aware Token Pruning for Speech Information\n  Retrieval"
                },
                "summary": "We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding."
                },
                "authors": [
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Project page and dataset is available at\n  https://speechprune.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14397v2",
                "updated": "2024-12-16T17:34:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    34,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-04-22T17:56:26Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    17,
                    56,
                    26,
                    0,
                    113,
                    0
                ],
                "title": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?"
                },
                "summary": "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment."
                },
                "authors": [
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Ishaan Watts"
                    },
                    {
                        "name": "Tua Wongsangaroonsri"
                    },
                    {
                        "name": "Minghui Zhang"
                    },
                    {
                        "name": "Noura Farra"
                    },
                    {
                        "name": "Nektar Ege Altntoprak"
                    },
                    {
                        "name": "Lena Baur"
                    },
                    {
                        "name": "Samantha Claudet"
                    },
                    {
                        "name": "Pavel Gajdusek"
                    },
                    {
                        "name": "Can Gren"
                    },
                    {
                        "name": "Qilong Gu"
                    },
                    {
                        "name": "Anna Kaminska"
                    },
                    {
                        "name": "Tomasz Kaminski"
                    },
                    {
                        "name": "Ruby Kuo"
                    },
                    {
                        "name": "Akiko Kyuba"
                    },
                    {
                        "name": "Jongho Lee"
                    },
                    {
                        "name": "Kartik Mathur"
                    },
                    {
                        "name": "Petter Merok"
                    },
                    {
                        "name": "Ivana Milovanovi"
                    },
                    {
                        "name": "Nani Paananen"
                    },
                    {
                        "name": "Vesa-Matti Paananen"
                    },
                    {
                        "name": "Anna Pavlenko"
                    },
                    {
                        "name": "Bruno Pereira Vidal"
                    },
                    {
                        "name": "Luciano Strika"
                    },
                    {
                        "name": "Yueh Tsao"
                    },
                    {
                        "name": "Davide Turcato"
                    },
                    {
                        "name": "Oleksandr Vakhno"
                    },
                    {
                        "name": "Judit Velcsov"
                    },
                    {
                        "name": "Anna Vickers"
                    },
                    {
                        "name": "Stphanie Visser"
                    },
                    {
                        "name": "Herdyan Widarmanto"
                    },
                    {
                        "name": "Andrey Zaikin"
                    },
                    {
                        "name": "Si-Qing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Si-Qing Chen"
                },
                "author": "Si-Qing Chen",
                "arxiv_comment": "AAAI 2025--camera ready + extended abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12004v1",
                "updated": "2024-12-16T17:32:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    32,
                    11,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:32:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    32,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "The Open Source Advantage in Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Source Advantage in Large Language Models (LLMs)"
                },
                "summary": "Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) mark a key shift in natural language processing\n(NLP), having advanced text generation, translation, and domain-specific\nreasoning. Closed-source models like GPT-4, powered by proprietary datasets and\nextensive computational resources, lead with state-of-the-art performance\ntoday. However, they face criticism for their \"black box\" nature and for\nlimiting accessibility in a manner that hinders reproducibility and equitable\nAI development. By contrast, open-source initiatives like LLaMA and BLOOM\nprioritize democratization through community-driven development and\ncomputational efficiency. These models have significantly reduced performance\ngaps, particularly in linguistic diversity and domain-specific applications,\nwhile providing accessible tools for global researchers and developers.\nNotably, both paradigms rely on foundational architectural innovations, such as\nthe Transformer framework by Vaswani et al. (2017). Closed-source models excel\nby scaling effectively, while open-source models adapt to real-world\napplications in underrepresented languages and domains. Techniques like\nLow-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source\nmodels to achieve competitive results despite limited resources. To be sure,\nthe tension between closed-source and open-source approaches underscores a\nbroader debate on transparency versus proprietary control in AI. Ethical\nconsiderations further highlight this divide. Closed-source systems restrict\nexternal scrutiny, while open-source models promote reproducibility and\ncollaboration but lack standardized auditing documentation frameworks to\nmitigate biases. Hybrid approaches that leverage the strengths of both\nparadigms are likely to shape the future of LLM innovation, ensuring\naccessibility, competitive technical performance, and ethical deployment."
                },
                "authors": [
                    {
                        "name": "Jiya Manchanda"
                    },
                    {
                        "name": "Laura Boettcher"
                    },
                    {
                        "name": "Matheus Westphalen"
                    },
                    {
                        "name": "Jasser Jasser"
                    }
                ],
                "author_detail": {
                    "name": "Jasser Jasser"
                },
                "author": "Jasser Jasser",
                "arxiv_comment": "7 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12001v1",
                "updated": "2024-12-16T17:29:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    29,
                    51,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:29:51Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    29,
                    51,
                    0,
                    351,
                    0
                ],
                "title": "LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts"
                },
                "summary": "Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem."
                },
                "authors": [
                    {
                        "name": "Zhuhao Wang"
                    },
                    {
                        "name": "Yihua Sun"
                    },
                    {
                        "name": "Zihan Li"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Fang Chen"
                    },
                    {
                        "name": "Hongen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Hongen Liao"
                },
                "author": "Hongen Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11995v1",
                "updated": "2024-12-16T17:22:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    22,
                    40,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:22:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    22,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Combining Large Language Models with Tutoring System Intelligence: A\n  Case Study in Caregiver Homework Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Large Language Models with Tutoring System Intelligence: A\n  Case Study in Caregiver Homework Support"
                },
                "summary": "Caregivers (i.e., parents and members of a child's caring community) are\nunderappreciated stakeholders in learning analytics. Although caregiver\ninvolvement can enhance student academic outcomes, many obstacles hinder\ninvolvement, most notably knowledge gaps with respect to modern school\ncurricula. An emerging topic of interest in learning analytics is hybrid\ntutoring, which includes instructional and motivational support. Caregivers\nassert similar roles in homework, yet it is unknown how learning analytics can\nsupport them. Our past work with caregivers suggested that conversational\nsupport is a promising method of providing caregivers with the guidance needed\nto effectively support student learning. We developed a system that provides\ninstructional support to caregivers through conversational recommendations\ngenerated by a Large Language Model (LLM). Addressing known instructional\nlimitations of LLMs, we use instructional intelligence from tutoring systems\nwhile conducting prompt engineering experiments with the open-source Llama 3\nLLM. This LLM generated message recommendations for caregivers supporting their\nchild's math practice via chat. Few-shot prompting and combining real-time\nproblem-solving context from tutoring systems with examples of tutoring\npractices yielded desirable message recommendations. These recommendations were\nevaluated with ten middle school caregivers, who valued recommendations\nfacilitating content-level support and student metacognition through\nself-explanation. We contribute insights into how tutoring systems can best be\nmerged with LLMs to support hybrid tutoring settings through conversational\nassistance, facilitating effective caregiver involvement in tutoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caregivers (i.e., parents and members of a child's caring community) are\nunderappreciated stakeholders in learning analytics. Although caregiver\ninvolvement can enhance student academic outcomes, many obstacles hinder\ninvolvement, most notably knowledge gaps with respect to modern school\ncurricula. An emerging topic of interest in learning analytics is hybrid\ntutoring, which includes instructional and motivational support. Caregivers\nassert similar roles in homework, yet it is unknown how learning analytics can\nsupport them. Our past work with caregivers suggested that conversational\nsupport is a promising method of providing caregivers with the guidance needed\nto effectively support student learning. We developed a system that provides\ninstructional support to caregivers through conversational recommendations\ngenerated by a Large Language Model (LLM). Addressing known instructional\nlimitations of LLMs, we use instructional intelligence from tutoring systems\nwhile conducting prompt engineering experiments with the open-source Llama 3\nLLM. This LLM generated message recommendations for caregivers supporting their\nchild's math practice via chat. Few-shot prompting and combining real-time\nproblem-solving context from tutoring systems with examples of tutoring\npractices yielded desirable message recommendations. These recommendations were\nevaluated with ten middle school caregivers, who valued recommendations\nfacilitating content-level support and student metacognition through\nself-explanation. We contribute insights into how tutoring systems can best be\nmerged with LLMs to support hybrid tutoring settings through conversational\nassistance, facilitating effective caregiver involvement in tutoring systems."
                },
                "authors": [
                    {
                        "name": "Devika Venugopalan"
                    },
                    {
                        "name": "Ziwen Yan"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Vincent Aleven"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Aleven"
                },
                "author": "Vincent Aleven",
                "arxiv_doi": "10.1145/3706468.3706516",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706468.3706516",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.11995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11990v1",
                "updated": "2024-12-16T17:14:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    14,
                    35,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    14,
                    35,
                    0,
                    351,
                    0
                ],
                "title": "ExecRepoBench: Multi-level Executable Code Completion Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExecRepoBench: Multi-level Executable Code Completion Evaluation"
                },
                "summary": "Code completion has become an essential tool for daily software development.\nExisting evaluation benchmarks often employ static methods that do not fully\ncapture the dynamic nature of real-world coding environments and face\nsignificant challenges, including limited context length, reliance on\nsuperficial evaluation metrics, and potential overfitting to training datasets.\nIn this work, we introduce a novel framework for enhancing code completion in\nsoftware development through the creation of a repository-level benchmark\nExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the\nfunctionality of open-source large language models (LLMs) in real-world coding\nscenarios that involve complex interdependencies across multiple files.\nExecRepoBench includes 1.2K samples from active Python repositories. Plus, we\npresent a multi-level grammar-based completion methodology conditioned on the\nabstract syntax tree to mask code fragments at various logical units (e.g.\nstatements, expressions, and functions). Then, we fine-tune the open-source LLM\nwith 7B parameters on Repo-Instruct to produce a strong code completion\nbaseline model Qwen2.5-Coder-Instruct-C based on the open-source model.\nQwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks,\nincluding MultiPL-E and ExecRepoBench, which consistently outperforms prior\nbaselines across all programming languages. The deployment of \\ourmethod{} can\nbe used as a high-performance, local service for programming\ndevelopment\\footnote{\\url{https://execrepobench.github.io/}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion has become an essential tool for daily software development.\nExisting evaluation benchmarks often employ static methods that do not fully\ncapture the dynamic nature of real-world coding environments and face\nsignificant challenges, including limited context length, reliance on\nsuperficial evaluation metrics, and potential overfitting to training datasets.\nIn this work, we introduce a novel framework for enhancing code completion in\nsoftware development through the creation of a repository-level benchmark\nExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the\nfunctionality of open-source large language models (LLMs) in real-world coding\nscenarios that involve complex interdependencies across multiple files.\nExecRepoBench includes 1.2K samples from active Python repositories. Plus, we\npresent a multi-level grammar-based completion methodology conditioned on the\nabstract syntax tree to mask code fragments at various logical units (e.g.\nstatements, expressions, and functions). Then, we fine-tune the open-source LLM\nwith 7B parameters on Repo-Instruct to produce a strong code completion\nbaseline model Qwen2.5-Coder-Instruct-C based on the open-source model.\nQwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks,\nincluding MultiPL-E and ExecRepoBench, which consistently outperforms prior\nbaselines across all programming languages. The deployment of \\ourmethod{} can\nbe used as a high-performance, local service for programming\ndevelopment\\footnote{\\url{https://execrepobench.github.io/}}."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Ke Jin"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11988v1",
                "updated": "2024-12-16T17:11:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    11,
                    48,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:11:48Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    11,
                    48,
                    0,
                    351,
                    0
                ],
                "title": "SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with\n  a GAN-Inspired Approach to Synthetic Dataset Generation"
                },
                "summary": "Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider the problem: ``If one man and one woman can produce one child in one\nyear, how many children will be produced by one woman and three men in 0.5\nyears?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview,\nand Gemini Flash frequently answer \"0.5,\" which does not make sense. While\nthese models sometimes acknowledge the unrealistic nature of the question, in\nmany cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5\nchild.\" Additionally, temporal variation has been observed: if an LLM answers\ncorrectly once (by recognizing the faulty nature of the question), subsequent\nresponses are more likely to also reflect this understanding. However, this is\ninconsistent.\n  These types of questions have motivated us to develop a dataset of science\nquestions, SciFaultyQA, where the questions themselves are intentionally\nfaulty. We observed that LLMs often proceed to answer these flawed questions\nwithout recognizing their inherent issues, producing results that are logically\nor scientifically invalid. By analyzing such patterns, we developed a novel\nmethod for generating synthetic datasets to evaluate and benchmark the\nperformance of various LLMs in identifying these flawed questions. We have also\ndeveloped novel approaches to reduce the errors."
                },
                "authors": [
                    {
                        "name": "Debarshi Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Debarshi Kundu"
                },
                "author": "Debarshi Kundu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11983v1",
                "updated": "2024-12-16T17:04:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    4,
                    40,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T17:04:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    4,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Cost-Effective Label-free Node Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective Label-free Node Classification with LLMs"
                },
                "summary": "Graph neural networks (GNNs) have emerged as go-to models for node\nclassification in graph data due to their powerful abilities in fusing graph\nstructures and attributes. However, such models strongly rely on adequate\nhigh-quality labeled data for training, which are expensive to acquire in\npractice. With the advent of large language models (LLMs), a promising way is\nto leverage their superb zero-shot capabilities and massive knowledge for node\nlabeling. Despite promising results reported, this methodology either demands\nconsiderable queries to LLMs, or suffers from compromised performance caused by\nnoisy labels produced by LLMs.\n  To remedy these issues, this work presents Cella, an active self-training\nframework that integrates LLMs into GNNs in a cost-effective manner. The design\nrecipe of Cella is to iteratively identify small sets of \"critical\" samples\nusing GNNs and extract informative pseudo-labels for them with both LLMs and\nGNNs as additional supervision signals to enhance model training. Particularly,\nCella includes three major components: (i) an effective active node selection\nstrategy for initial annotations; (ii) a judicious sample selection scheme to\nsift out the \"critical\" nodes based on label disharmonicity and entropy; and\n(iii) a label refinement module combining LLMs and GNNs with rewired topology.\nOur extensive experiments over five benchmark text-attributed graph datasets\ndemonstrate that Cella significantly outperforms the state of the arts under\nthe same query budget to LLMs in terms of label-free node classification. In\nparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an\n8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost\nof less than one cent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have emerged as go-to models for node\nclassification in graph data due to their powerful abilities in fusing graph\nstructures and attributes. However, such models strongly rely on adequate\nhigh-quality labeled data for training, which are expensive to acquire in\npractice. With the advent of large language models (LLMs), a promising way is\nto leverage their superb zero-shot capabilities and massive knowledge for node\nlabeling. Despite promising results reported, this methodology either demands\nconsiderable queries to LLMs, or suffers from compromised performance caused by\nnoisy labels produced by LLMs.\n  To remedy these issues, this work presents Cella, an active self-training\nframework that integrates LLMs into GNNs in a cost-effective manner. The design\nrecipe of Cella is to iteratively identify small sets of \"critical\" samples\nusing GNNs and extract informative pseudo-labels for them with both LLMs and\nGNNs as additional supervision signals to enhance model training. Particularly,\nCella includes three major components: (i) an effective active node selection\nstrategy for initial annotations; (ii) a judicious sample selection scheme to\nsift out the \"critical\" nodes based on label disharmonicity and entropy; and\n(iii) a label refinement module combining LLMs and GNNs with rewired topology.\nOur extensive experiments over five benchmark text-attributed graph datasets\ndemonstrate that Cella significantly outperforms the state of the arts under\nthe same query budget to LLMs in terms of label-free node classification. In\nparticular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an\n8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost\nof less than one cent."
                },
                "authors": [
                    {
                        "name": "Taiyan Zhang"
                    },
                    {
                        "name": "Renchi Yang"
                    },
                    {
                        "name": "Mingyu Yan"
                    },
                    {
                        "name": "Xiaochun Ye"
                    },
                    {
                        "name": "Dongrui Fan"
                    },
                    {
                        "name": "Yurui Lai"
                    }
                ],
                "author_detail": {
                    "name": "Yurui Lai"
                },
                "author": "Yurui Lai",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11970v1",
                "updated": "2024-12-16T16:51:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    51,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:51:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    51,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARWIN 1.5: Large Language Models as Materials Science Adapted Learners"
                },
                "summary": "Materials discovery and design aim to find components and structures with\ndesirable properties over highly complex and diverse search spaces. Traditional\nsolutions, such as high-throughput simulations and machine learning (ML), often\nrely on complex descriptors, which hinder generalizability and transferability\nacross tasks. Moreover, these descriptors may deviate from experimental data\ndue to inevitable defects and purity issues in the real world, which may reduce\ntheir effectiveness in practical applications. To address these challenges, we\npropose Darwin 1.5, an open-source large language model (LLM) tailored for\nmaterials science. By leveraging natural language as input, Darwin eliminates\nthe need for task-specific descriptors and enables a flexible, unified approach\nto material property prediction and discovery. We employ a two-stage training\nstrategy combining question-answering (QA) fine-tuning with multi-task learning\n(MTL) to inject domain-specific knowledge in various modalities and facilitate\ncross-task knowledge transfer. Through our strategic approach, we achieved a\nsignificant enhancement in the prediction accuracy of LLMs, with a maximum\nimprovement of 60\\% compared to LLaMA-7B base models. It further outperforms\ntraditional machine learning models on various tasks in material science,\nshowcasing the potential of LLMs to provide a more versatile and scalable\nfoundation model for materials discovery and design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Materials discovery and design aim to find components and structures with\ndesirable properties over highly complex and diverse search spaces. Traditional\nsolutions, such as high-throughput simulations and machine learning (ML), often\nrely on complex descriptors, which hinder generalizability and transferability\nacross tasks. Moreover, these descriptors may deviate from experimental data\ndue to inevitable defects and purity issues in the real world, which may reduce\ntheir effectiveness in practical applications. To address these challenges, we\npropose Darwin 1.5, an open-source large language model (LLM) tailored for\nmaterials science. By leveraging natural language as input, Darwin eliminates\nthe need for task-specific descriptors and enables a flexible, unified approach\nto material property prediction and discovery. We employ a two-stage training\nstrategy combining question-answering (QA) fine-tuning with multi-task learning\n(MTL) to inject domain-specific knowledge in various modalities and facilitate\ncross-task knowledge transfer. Through our strategic approach, we achieved a\nsignificant enhancement in the prediction accuracy of LLMs, with a maximum\nimprovement of 60\\% compared to LLaMA-7B base models. It further outperforms\ntraditional machine learning models on various tasks in material science,\nshowcasing the potential of LLMs to provide a more versatile and scalable\nfoundation model for materials discovery and design."
                },
                "authors": [
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Yuwei Wan"
                    },
                    {
                        "name": "Yixuan Liu"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Chunyu Kit"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Bram Hoex"
                    }
                ],
                "author_detail": {
                    "name": "Bram Hoex"
                },
                "author": "Bram Hoex",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11965v1",
                "updated": "2024-12-16T16:45:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    45,
                    33,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:45:33Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    45,
                    33,
                    0,
                    351,
                    0
                ],
                "title": "Inferring Functionality of Attention Heads from their Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Functionality of Attention Heads from their Parameters"
                },
                "summary": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations."
                },
                "authors": [
                    {
                        "name": "Amit Elhelo"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12701v2",
                "updated": "2024-12-16T16:44:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    44,
                    52,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-19T18:11:36Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    11,
                    36,
                    1,
                    324,
                    0
                ],
                "title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through\n  Model-Generated Explanations"
                },
                "summary": "Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be vulnerable to backdoor attacks,\nwhere triggers embedded in poisoned samples can maliciously alter LLMs'\nbehaviors. In this paper, we move beyond attacking LLMs and instead examine\nbackdoor attacks through the novel lens of natural language explanations.\nSpecifically, we leverage LLMs' generative capabilities to produce\nhuman-readable explanations for their decisions, enabling direct comparisons\nbetween explanations for clean and poisoned samples. Our results show that\nbackdoored models produce coherent explanations for clean inputs but diverse\nand logically flawed explanations for poisoned data, a pattern consistent\nacross classification and generation tasks for different backdoor attacks.\nFurther analysis reveals key insights into the explanation generation process.\nAt the token level, explanation tokens associated with poisoned samples only\nappear in the final few transformer layers. At the sentence level, attention\ndynamics indicate that poisoned inputs shift attention away from the original\ninput context during explanation generation. These findings enhance our\nunderstanding of backdoor mechanisms in LLMs and present a promising framework\nfor detecting vulnerabilities through explainability."
                },
                "authors": [
                    {
                        "name": "Huaizhi Ge"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v2",
                "updated": "2024-12-16T16:37:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    37,
                    7,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11948v1",
                "updated": "2024-12-16T16:31:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    31,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:31:00Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    31,
                    0,
                    0,
                    351,
                    0
                ],
                "title": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReviewer: A Specialized Large Language Model for Generating Critical\n  Scientific Paper Reviews"
                },
                "summary": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top ML conferences. Given a PDF paper submission\nand review template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces significantly more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top ML conferences. Given a PDF paper submission\nand review template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces significantly more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool."
                },
                "authors": [
                    {
                        "name": "Maximilian Idahl"
                    },
                    {
                        "name": "Zahra Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Zahra Ahmadi"
                },
                "author": "Zahra Ahmadi",
                "arxiv_comment": "Demo: https://huggingface.co/spaces/maxidl/openreviewer Model:\n  https://huggingface.co/maxidl/Llama-OpenReviewer-8B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11937v1",
                "updated": "2024-12-16T16:22:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    22,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:22:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    22,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "Precise Length Control in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise Length Control in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in production systems,\npowering applications such as chatbots, summarization, and question answering.\nDespite their success, controlling the length of their response remains a\nsignificant challenge, particularly for tasks requiring structured outputs or\nspecific levels of detail. In this work, we propose a method to adapt\npre-trained decoder-only LLMs for precise control of response length. Our\napproach incorporates a secondary length-difference positional encoding (LDPE)\ninto the input embeddings, which counts down to a user-set response termination\nlength. Fine-tuning with LDPE allows the model to learn to terminate responses\ncoherently at the desired length, achieving mean token errors of less than 3\ntokens. We also introduce Max New Tokens++, an extension that enables flexible\nupper-bound length control, rather than an exact target. Experimental results\non tasks such as question answering and document summarization demonstrate that\nour method enables precise length control without compromising response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in production systems,\npowering applications such as chatbots, summarization, and question answering.\nDespite their success, controlling the length of their response remains a\nsignificant challenge, particularly for tasks requiring structured outputs or\nspecific levels of detail. In this work, we propose a method to adapt\npre-trained decoder-only LLMs for precise control of response length. Our\napproach incorporates a secondary length-difference positional encoding (LDPE)\ninto the input embeddings, which counts down to a user-set response termination\nlength. Fine-tuning with LDPE allows the model to learn to terminate responses\ncoherently at the desired length, achieving mean token errors of less than 3\ntokens. We also introduce Max New Tokens++, an extension that enables flexible\nupper-bound length control, rather than an exact target. Experimental results\non tasks such as question answering and document summarization demonstrate that\nour method enables precise length control without compromising response\nquality."
                },
                "authors": [
                    {
                        "name": "Bradley Butcher"
                    },
                    {
                        "name": "Michael O'Keefe"
                    },
                    {
                        "name": "James Titchener"
                    }
                ],
                "author_detail": {
                    "name": "James Titchener"
                },
                "author": "James Titchener",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11936v1",
                "updated": "2024-12-16T16:21:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    41,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:21:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mathematical Reasoning in the Era of Multimodal Large\n  Language Model: Benchmark, Method & Challenges"
                },
                "summary": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning, a core aspect of human cognition, is vital across\nmany domains, from educational problem-solving to scientific advancements. As\nartificial general intelligence (AGI) progresses, integrating large language\nmodels (LLMs) with mathematical reasoning tasks is becoming increasingly\nsignificant. This survey provides the first comprehensive analysis of\nmathematical reasoning in the era of multimodal large language models (MLLMs).\nWe review over 200 studies published since 2021, and examine the\nstate-of-the-art developments in Math-LLMs, with a focus on multimodal\nsettings. We categorize the field into three dimensions: benchmarks,\nmethodologies, and challenges. In particular, we explore multimodal\nmathematical reasoning pipeline, as well as the role of (M)LLMs and the\nassociated methodologies. Finally, we identify five major challenges hindering\nthe realization of AGI in this domain, offering insights into the future\ndirection for enhancing multimodal reasoning capabilities. This survey serves\nas a critical resource for the research community in advancing the capabilities\nof LLMs to tackle complex multimodal reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Jiamin Su"
                    },
                    {
                        "name": "Jianxiang He"
                    },
                    {
                        "name": "Fangteng Fu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16658v2",
                "updated": "2024-12-16T16:21:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    21,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-10-22T03:19:16Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    19,
                    16,
                    1,
                    296,
                    0
                ],
                "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorb-Agent: Autonomous Identification of Stable Adsorption\n  Configurations via Large Language Model Agent"
                },
                "summary": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adsorption energy is a key reactivity descriptor in catalysis, enabling\nefficient screening for optimal catalysts. However, determining adsorption\nenergy typically requires evaluating numerous adsorbate-catalyst\nconfigurations. Current algorithmic approaches rely on exhaustive enumeration\nof adsorption sites and configurations, which makes the process computationally\nintensive and does not inherently guarantee the identification of the global\nminimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model\n(LLM) agent designed to efficiently identify system-specific stable adsorption\nconfigurations corresponding to the global minimum adsorption energy.\nAdsorb-Agent leverages its built-in knowledge and emergent reasoning\ncapabilities to strategically explore adsorption configurations likely to hold\nadsorption energy. By reducing the reliance on exhaustive sampling, it\nsignificantly decreases the number of initial configurations required while\nimproving the accuracy of adsorption energy predictions. We evaluate\nAdsorb-Agent's performance across twenty representative systems encompassing a\nrange of complexities. The Adsorb-Agent successfully identifies comparable\nadsorption energies for 83.7% of the systems and achieves lower energies,\ncloser to the actual global minimum, for 35% of the systems, while requiring\nsignificantly fewer initial configurations than conventional methods. Its\ncapability is particularly evident in complex systems, where it identifies\nlower adsorption energies for 46.7% of systems involving intermetallic surfaces\nand 66.7% of systems with large adsorbate molecules. These results demonstrate\nthe potential of Adsorb-Agent to accelerate catalyst discovery by reducing\ncomputational costs and improving the reliability of adsorption energy\npredictions."
                },
                "authors": [
                    {
                        "name": "Janghoon Ock"
                    },
                    {
                        "name": "Tirtha Vinchurkar"
                    },
                    {
                        "name": "Yayati Jadhav"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11934v1",
                "updated": "2024-12-16T16:20:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    20,
                    41,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:20:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    20,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "Stepwise Reasoning Error Disruption Attack of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepwise Reasoning Error Disruption Attack of LLMs"
                },
                "summary": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications."
                },
                "authors": [
                    {
                        "name": "Jingyu Peng"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02224v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02224v4",
                "updated": "2024-12-16T16:13:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    13,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-04T11:36:09Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    11,
                    36,
                    9,
                    1,
                    156,
                    0
                ],
                "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models"
                },
                "summary": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs."
                },
                "authors": [
                    {
                        "name": "Tao Fan"
                    },
                    {
                        "name": "Guoqiang Ma"
                    },
                    {
                        "name": "Yan Kang"
                    },
                    {
                        "name": "Hanlin Gu"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02224v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02224v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11923v1",
                "updated": "2024-12-16T16:09:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    9,
                    35,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:09:35Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    9,
                    35,
                    0,
                    351,
                    0
                ],
                "title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named\n  Entity Detection"
                },
                "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations."
                },
                "authors": [
                    {
                        "name": "Sepideh Mamooler"
                    },
                    {
                        "name": "Syrielle Montariol"
                    },
                    {
                        "name": "Alexander Mathis"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11919v1",
                "updated": "2024-12-16T16:03:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    3,
                    25,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:03:25Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    3,
                    25,
                    0,
                    351,
                    0
                ],
                "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained\n  Evidence within Generation"
                },
                "summary": "Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}."
                },
                "authors": [
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yongkang Wu"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Qi Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11917v1",
                "updated": "2024-12-16T16:01:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    1,
                    18,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T16:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    16,
                    1,
                    18,
                    0,
                    351,
                    0
                ],
                "title": "Does VLM Classification Benefit from LLM Description Semantics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does VLM Classification Benefit from LLM Description Semantics?"
                },
                "summary": "Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs."
                },
                "authors": [
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Lennart Rietdorf"
                    },
                    {
                        "name": "Dmytro Kotovenko"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11912v1",
                "updated": "2024-12-16T15:55:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    55,
                    34,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:55:34Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    55,
                    34,
                    0,
                    351,
                    0
                ],
                "title": "CharacterBench: Benchmarking Character Customization of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CharacterBench: Benchmarking Character Customization of Large Language\n  Models"
                },
                "summary": "Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench."
                },
                "authors": [
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Yongkang Huang"
                    },
                    {
                        "name": "Bosi Wen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Pei Ke"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Kuntian Tang"
                    },
                    {
                        "name": "Rongsheng Zhang"
                    },
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Tangjie Lv"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11908v1",
                "updated": "2024-12-16T15:54:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    6,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:54:06Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    54,
                    6,
                    0,
                    351,
                    0
                ],
                "title": "Can Language Models Rival Mathematics Students? Evaluating Mathematical\n  Reasoning through Textual Manipulation and Human Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Rival Mathematics Students? Evaluating Mathematical\n  Reasoning through Textual Manipulation and Human Experiments"
                },
                "summary": "In this paper we look at the ability of recent large language models (LLMs)\nat solving mathematical problems in combinatorics. We compare models LLaMA-2,\nLLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and\nundergraduates with prior experience in mathematical olympiads. To facilitate\nthese comparisons we introduce the Combi-Puzzles dataset, which contains 125\nproblem variants based on 25 combinatorial reasoning problems. Each problem is\npresented in one of five distinct forms, created by systematically manipulating\nthe problem statements through adversarial additions, numeric parameter\nchanges, and linguistic obfuscation. Our variations preserve the mathematical\ncore and are designed to measure the generalisability of LLM problem-solving\nabilities, while also increasing confidence that problems are submitted to LLMs\nin forms that have not been seen as training instances. We found that a model\nbased on GPT-4 outperformed all other models in producing correct responses,\nand performed significantly better in the mathematical variation of the\nproblems than humans. We also found that modifications to problem statements\nsignificantly impact the LLM's performance, while human performance remains\nunaffected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we look at the ability of recent large language models (LLMs)\nat solving mathematical problems in combinatorics. We compare models LLaMA-2,\nLLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and\nundergraduates with prior experience in mathematical olympiads. To facilitate\nthese comparisons we introduce the Combi-Puzzles dataset, which contains 125\nproblem variants based on 25 combinatorial reasoning problems. Each problem is\npresented in one of five distinct forms, created by systematically manipulating\nthe problem statements through adversarial additions, numeric parameter\nchanges, and linguistic obfuscation. Our variations preserve the mathematical\ncore and are designed to measure the generalisability of LLM problem-solving\nabilities, while also increasing confidence that problems are submitted to LLMs\nin forms that have not been seen as training instances. We found that a model\nbased on GPT-4 outperformed all other models in producing correct responses,\nand performed significantly better in the mathematical variation of the\nproblems than humans. We also found that modifications to problem statements\nsignificantly impact the LLM's performance, while human performance remains\nunaffected."
                },
                "authors": [
                    {
                        "name": "Andrii Nikolaiev"
                    },
                    {
                        "name": "Yiannos Stathopoulos"
                    },
                    {
                        "name": "Simone Teufel"
                    }
                ],
                "author_detail": {
                    "name": "Simone Teufel"
                },
                "author": "Simone Teufel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16133v3",
                "updated": "2024-12-16T15:42:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    42,
                    38,
                    0,
                    351,
                    0
                ],
                "published": "2024-05-25T08:57:28Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    8,
                    57,
                    28,
                    5,
                    146,
                    0
                ],
                "title": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via\n  Code Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via\n  Code Rewriting"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating code. However, the misuse of LLM-generated (synthetic) code has\nraised concerns in both educational and industrial contexts, underscoring the\nurgent need for synthetic code detectors. Existing methods for detecting\nsynthetic content are primarily designed for general text and struggle with\ncode due to the unique grammatical structure of programming languages and the\npresence of numerous ''low-entropy'' tokens. Building on this, our work\nproposes a novel zero-shot synthetic code detector based on the similarity\nbetween the original code and its LLM-rewritten variants. Our method is based\non the observation that differences between LLM-rewritten and original code\ntend to be smaller when the original code is synthetic. We utilize\nself-supervised contrastive learning to train a code similarity model and\nevaluate our approach on two synthetic code detection benchmarks. Our results\ndemonstrate a significant improvement over existing SOTA synthetic content\ndetectors, with AUROC scores increasing by 20.5% on the APPS benchmark and\n29.1% on the MBPP benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating code. However, the misuse of LLM-generated (synthetic) code has\nraised concerns in both educational and industrial contexts, underscoring the\nurgent need for synthetic code detectors. Existing methods for detecting\nsynthetic content are primarily designed for general text and struggle with\ncode due to the unique grammatical structure of programming languages and the\npresence of numerous ''low-entropy'' tokens. Building on this, our work\nproposes a novel zero-shot synthetic code detector based on the similarity\nbetween the original code and its LLM-rewritten variants. Our method is based\non the observation that differences between LLM-rewritten and original code\ntend to be smaller when the original code is synthetic. We utilize\nself-supervised contrastive learning to train a code similarity model and\nevaluate our approach on two synthetic code detection benchmarks. Our results\ndemonstrate a significant improvement over existing SOTA synthetic content\ndetectors, with AUROC scores increasing by 20.5% on the APPS benchmark and\n29.1% on the MBPP benchmark."
                },
                "authors": [
                    {
                        "name": "Tong Ye"
                    },
                    {
                        "name": "Yangkai Du"
                    },
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Lingfei Wu"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Shouling Ji"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "Accepted by AAAI 2025; previously submitted to EMNLP 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11882v1",
                "updated": "2024-12-16T15:31:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    31,
                    15,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:31:15Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    31,
                    15,
                    0,
                    351,
                    0
                ],
                "title": "Hardware-in-the-loop Simulation Testbed for Geomagnetic Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-in-the-loop Simulation Testbed for Geomagnetic Navigation"
                },
                "summary": "Geomagnetic navigation leverages the ubiquitous Earth's magnetic signals to\nnavigate missions, without dependence on GPS services or pre-stored geographic\nmaps. It has drawn increasing attention and is promising particularly for\nlong-range navigation into unexplored areas. Current geomagnetic navigation\nstudies are still in the early stages with simulations and computational\nvalidations, without concrete efforts to develop cost-friendly test platforms\nthat can empower deployment and experimental analysis of the developed\napproaches. This paper presents a hardware-in-the-loop simulation testbed to\nsupport geomagnetic navigation experimentation. Our testbed is dedicated to\nsynthesizing geomagnetic field environment for the navigation. We develop the\nsoftware in the testbed to simulate the dynamics of the navigation environment,\nand we build the hardware to generate the physical magnetic field, which\nfollows and aligns with the simulated environment. The testbed aims to provide\ncontrollable magnetic field that can be used to experiment with geomagnetic\nnavigation in labs, thus avoiding real and expensive navigation experiments,\ne.g., in the ocean, for validating navigation prototypes. We build the testbed\nwith off-the-shelf hardware in an unshielded environment to reduce cost. We\nalso develop the field generation control and hardware parameter optimization\nfor quality magnetic field generation. We conduct a detailed performance\nanalysis to show the quality of the field generation by the testbed, and we\nreport the experimental results on performance indicators, including accuracy,\nuniformity, stability, and convergence of the generated field towards the\ntarget geomagnetic environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetic navigation leverages the ubiquitous Earth's magnetic signals to\nnavigate missions, without dependence on GPS services or pre-stored geographic\nmaps. It has drawn increasing attention and is promising particularly for\nlong-range navigation into unexplored areas. Current geomagnetic navigation\nstudies are still in the early stages with simulations and computational\nvalidations, without concrete efforts to develop cost-friendly test platforms\nthat can empower deployment and experimental analysis of the developed\napproaches. This paper presents a hardware-in-the-loop simulation testbed to\nsupport geomagnetic navigation experimentation. Our testbed is dedicated to\nsynthesizing geomagnetic field environment for the navigation. We develop the\nsoftware in the testbed to simulate the dynamics of the navigation environment,\nand we build the hardware to generate the physical magnetic field, which\nfollows and aligns with the simulated environment. The testbed aims to provide\ncontrollable magnetic field that can be used to experiment with geomagnetic\nnavigation in labs, thus avoiding real and expensive navigation experiments,\ne.g., in the ocean, for validating navigation prototypes. We build the testbed\nwith off-the-shelf hardware in an unshielded environment to reduce cost. We\nalso develop the field generation control and hardware parameter optimization\nfor quality magnetic field generation. We conduct a detailed performance\nanalysis to show the quality of the field generation by the testbed, and we\nreport the experimental results on performance indicators, including accuracy,\nuniformity, stability, and convergence of the generated field towards the\ntarget geomagnetic environment."
                },
                "authors": [
                    {
                        "name": "Songnan Yang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qianyun Zhang"
                    },
                    {
                        "name": "Xiaohui Zhang"
                    },
                    {
                        "name": "Xuehui Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xuehui Ma"
                },
                "author": "Xuehui Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11878v1",
                "updated": "2024-12-16T15:27:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    27,
                    37,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:27:37Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    27,
                    37,
                    0,
                    351,
                    0
                ],
                "title": "Using Instruction-Tuned Large Language Models to Identify Indicators of\n  Vulnerability in Police Incident Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Instruction-Tuned Large Language Models to Identify Indicators of\n  Vulnerability in Police Incident Narratives"
                },
                "summary": "Objectives: Compare qualitative coding of instruction tuned large language\nmodels (IT-LLMs) against human coders in classifying the presence or absence of\nvulnerability in routinely collected unstructured text that describes\npolice-public interactions. Evaluate potential bias in IT-LLM codings. Methods:\nAnalyzing publicly available text narratives of police-public interactions\nrecorded by Boston Police Department, we provide humans and IT-LLMs with\nqualitative labelling codebooks and compare labels generated by both, seeking\nto identify situations associated with (i) mental ill health; (ii) substance\nmisuse; (iii) alcohol dependence; and (iv) homelessness. We explore multiple\nprompting strategies and model sizes, and the variability of labels generated\nby repeated prompts. Additionally, to explore model bias, we utilize\ncounterfactual methods to assess the impact of two protected characteristics -\nrace and gender - on IT-LLM classification. Results: Results demonstrate that\nIT-LLMs can effectively support human qualitative coding of police incident\nnarratives. While there is some disagreement between LLM and human generated\nlabels, IT-LLMs are highly effective at screening narratives where no\nvulnerabilities are present, potentially vastly reducing the requirement for\nhuman coding. Counterfactual analyses demonstrate that manipulations to both\ngender and race of individuals described in narratives have very limited\neffects on IT-LLM classifications beyond those expected by chance. Conclusions:\nIT-LLMs offer effective means to augment human qualitative coding in a way that\nrequires much lower levels of resource to analyze large unstructured datasets.\nMoreover, they encourage specificity in qualitative coding, promote\ntransparency, and provide the opportunity for more standardized, replicable\napproaches to analyzing large free-text police data sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objectives: Compare qualitative coding of instruction tuned large language\nmodels (IT-LLMs) against human coders in classifying the presence or absence of\nvulnerability in routinely collected unstructured text that describes\npolice-public interactions. Evaluate potential bias in IT-LLM codings. Methods:\nAnalyzing publicly available text narratives of police-public interactions\nrecorded by Boston Police Department, we provide humans and IT-LLMs with\nqualitative labelling codebooks and compare labels generated by both, seeking\nto identify situations associated with (i) mental ill health; (ii) substance\nmisuse; (iii) alcohol dependence; and (iv) homelessness. We explore multiple\nprompting strategies and model sizes, and the variability of labels generated\nby repeated prompts. Additionally, to explore model bias, we utilize\ncounterfactual methods to assess the impact of two protected characteristics -\nrace and gender - on IT-LLM classification. Results: Results demonstrate that\nIT-LLMs can effectively support human qualitative coding of police incident\nnarratives. While there is some disagreement between LLM and human generated\nlabels, IT-LLMs are highly effective at screening narratives where no\nvulnerabilities are present, potentially vastly reducing the requirement for\nhuman coding. Counterfactual analyses demonstrate that manipulations to both\ngender and race of individuals described in narratives have very limited\neffects on IT-LLM classifications beyond those expected by chance. Conclusions:\nIT-LLMs offer effective means to augment human qualitative coding in a way that\nrequires much lower levels of resource to analyze large unstructured datasets.\nMoreover, they encourage specificity in qualitative coding, promote\ntransparency, and provide the opportunity for more standardized, replicable\napproaches to analyzing large free-text police data sources."
                },
                "authors": [
                    {
                        "name": "Sam Relins"
                    },
                    {
                        "name": "Daniel Birks"
                    },
                    {
                        "name": "Charlie Lloyd"
                    }
                ],
                "author_detail": {
                    "name": "Charlie Lloyd"
                },
                "author": "Charlie Lloyd",
                "arxiv_comment": "33 pages, 6 figures Submitted to Journal of Quantitative Criminology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11854v1",
                "updated": "2024-12-16T15:12:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    12,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:12:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    12,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "Towards Understanding Systems Trade-offs in Retrieval-Augmented\n  Generation Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Systems Trade-offs in Retrieval-Augmented\n  Generation Model Inference"
                },
                "summary": "The rapid increase in the number of parameters in large language models\n(LLMs) has significantly increased the cost involved in fine-tuning and\nretraining LLMs, a necessity for keeping models up to date and improving\naccuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to\nimproving the capabilities and accuracy of LLMs without the necessity of\nretraining. Although RAG eliminates the need for continuous retraining to\nupdate model data, it incurs a trade-off in the form of slower model inference\ntimes. Resultingly, the use of RAG in enhancing the accuracy and capabilities\nof LLMs often involves diverse performance implications and trade-offs based on\nits design. In an effort to begin tackling and mitigating the performance\npenalties associated with RAG from a systems perspective, this paper introduces\na detailed taxonomy and characterization of the different elements within the\nRAG ecosystem for LLMs that explore trade-offs within latency, throughput, and\nmemory. Our study reveals underlying inefficiencies in RAG for systems\ndeployment, that can result in TTFT latencies that are twice as long and\nunoptimized datastores that consume terabytes of storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in the number of parameters in large language models\n(LLMs) has significantly increased the cost involved in fine-tuning and\nretraining LLMs, a necessity for keeping models up to date and improving\naccuracy. Retrieval-Augmented Generation (RAG) offers a promising approach to\nimproving the capabilities and accuracy of LLMs without the necessity of\nretraining. Although RAG eliminates the need for continuous retraining to\nupdate model data, it incurs a trade-off in the form of slower model inference\ntimes. Resultingly, the use of RAG in enhancing the accuracy and capabilities\nof LLMs often involves diverse performance implications and trade-offs based on\nits design. In an effort to begin tackling and mitigating the performance\npenalties associated with RAG from a systems perspective, this paper introduces\na detailed taxonomy and characterization of the different elements within the\nRAG ecosystem for LLMs that explore trade-offs within latency, throughput, and\nmemory. Our study reveals underlying inefficiencies in RAG for systems\ndeployment, that can result in TTFT latencies that are twice as long and\nunoptimized datastores that consume terabytes of storage."
                },
                "authors": [
                    {
                        "name": "Michael Shen"
                    },
                    {
                        "name": "Muhammad Umar"
                    },
                    {
                        "name": "Kiwan Maeng"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11851v1",
                "updated": "2024-12-16T15:11:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    11,
                    3,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T15:11:03Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    11,
                    3,
                    0,
                    351,
                    0
                ],
                "title": "A Benchmark and Robustness Study of In-Context-Learning with Large\n  Language Models in Music Entity Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark and Robustness Study of In-Context-Learning with Large\n  Language Models in Music Entity Detection"
                },
                "summary": "Detecting music entities such as song titles or artist names is a useful\napplication to help use cases like processing music search queries or analyzing\nmusic consumption on the web. Recent approaches incorporate smaller language\nmodels (SLMs) like BERT and achieve high results. However, further research\nindicates a high influence of entity exposure during pre-training on the\nperformance of the models. With the advent of large language models (LLMs),\nthese outperform SLMs in a variety of downstream tasks. However, researchers\nare still divided if this is applicable to tasks like entity detection in texts\ndue to issues like hallucination. In this paper, we provide a novel dataset of\nuser-generated metadata and conduct a benchmark and a robustness study using\nrecent LLMs with in-context-learning (ICL). Our results indicate that LLMs in\nthe ICL setting yield higher performance than SLMs. We further uncover the\nlarge impact of entity exposure on the best performing LLM in our study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting music entities such as song titles or artist names is a useful\napplication to help use cases like processing music search queries or analyzing\nmusic consumption on the web. Recent approaches incorporate smaller language\nmodels (SLMs) like BERT and achieve high results. However, further research\nindicates a high influence of entity exposure during pre-training on the\nperformance of the models. With the advent of large language models (LLMs),\nthese outperform SLMs in a variety of downstream tasks. However, researchers\nare still divided if this is applicable to tasks like entity detection in texts\ndue to issues like hallucination. In this paper, we provide a novel dataset of\nuser-generated metadata and conduct a benchmark and a robustness study using\nrecent LLMs with in-context-learning (ICL). Our results indicate that LLMs in\nthe ICL setting yield higher performance than SLMs. We further uncover the\nlarge impact of entity exposure on the best performing LLM in our study."
                },
                "authors": [
                    {
                        "name": "Simon Hachmeier"
                    },
                    {
                        "name": "Robert Jschke"
                    }
                ],
                "author_detail": {
                    "name": "Robert Jschke"
                },
                "author": "Robert Jschke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10497v2",
                "updated": "2024-12-16T15:03:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    3,
                    54,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-20T02:44:45Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    2,
                    44,
                    45,
                    1,
                    233,
                    0
                ],
                "title": "QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory"
                },
                "summary": "Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases."
                },
                "authors": [
                    {
                        "name": "Yihang Wang"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Bowen Tian"
                    },
                    {
                        "name": "Yueyang Su"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Huaming Liao"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05668v2",
                "updated": "2024-12-16T15:02:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    2,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-02-08T13:42:50Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    13,
                    42,
                    50,
                    3,
                    39,
                    0
                ],
                "title": "Comprehensive Assessment of Jailbreak Attacks Against LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Assessment of Jailbreak Attacks Against LLMs"
                },
                "summary": "Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers\nhave studied different jailbreak attacks in depth, they have done so in\nisolation -- either with unaligned experiment settings or comparing a limited\nrange of methods. To fill this gap, we present the first large-scale\nmeasurement of various jailbreak attack methods. We collect 17 cutting-edge\njailbreak methods, summarize their features, and establish a novel jailbreak\nattack taxonomy. Based on eight popular censored LLMs and 160 questions from 16\nviolation categories, we conduct a unified and impartial assessment of attack\neffectiveness as well as a comprehensive ablation study. Our extensive\nexperimental results demonstrate that all the jailbreak attacks have a powerful\neffect on the LLMs. This indicates that all LLMs fail to cover all the\nviolation categories, and they are susceptible to significant jailbreak risks,\nwith even the well-aligned Llama3 facing a maximum attack success rate of 0.88.\nAdditionally, we test jailbreak attacks under eight advanced external defenses\nand find none of the defenses could mitigate the jailbreak attacks entirely.\nOur study offers valuable insights for future research on jailbreak attacks and\ndefenses and serves as a benchmark tool for researchers and practitioners to\nevaluate them effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks aim to bypass the safeguards of LLMs. While researchers\nhave studied different jailbreak attacks in depth, they have done so in\nisolation -- either with unaligned experiment settings or comparing a limited\nrange of methods. To fill this gap, we present the first large-scale\nmeasurement of various jailbreak attack methods. We collect 17 cutting-edge\njailbreak methods, summarize their features, and establish a novel jailbreak\nattack taxonomy. Based on eight popular censored LLMs and 160 questions from 16\nviolation categories, we conduct a unified and impartial assessment of attack\neffectiveness as well as a comprehensive ablation study. Our extensive\nexperimental results demonstrate that all the jailbreak attacks have a powerful\neffect on the LLMs. This indicates that all LLMs fail to cover all the\nviolation categories, and they are susceptible to significant jailbreak risks,\nwith even the well-aligned Llama3 facing a maximum attack success rate of 0.88.\nAdditionally, we test jailbreak attacks under eight advanced external defenses\nand find none of the defenses could mitigate the jailbreak attacks entirely.\nOur study offers valuable insights for future research on jailbreak attacks and\ndefenses and serves as a benchmark tool for researchers and practitioners to\nevaluate them effectively."
                },
                "authors": [
                    {
                        "name": "Junjie Chu"
                    },
                    {
                        "name": "Yugeng Liu"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Xinyue Shen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "22 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15594v2",
                "updated": "2024-12-16T15:00:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    0,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-23T16:03:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    16,
                    3,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "A Survey on LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-as-a-Judge"
                },
                "summary": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Jiawei Gu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Xuehao Zhai"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "arxiv_comment": "33 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2310.05470 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11832v1",
                "updated": "2024-12-16T14:55:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    55,
                    57,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:55:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    55,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "A Distributed Collaborative Retrieval Framework Excelling in All Queries\n  and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed Collaborative Retrieval Framework Excelling in All Queries\n  and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation"
                },
                "summary": "Numerous retrieval models, including sparse, dense and llm-based methods,\nhave demonstrated remarkable performance in predicting the relevance between\nqueries and corpora. However, the preliminary effectiveness analysis\nexperiments indicate that these models fail to achieve satisfactory performance\non the majority of queries and corpora, revealing their effectiveness\nrestricted to specific scenarios. Thus, to tackle this problem, we propose a\nnovel Distributed Collaborative Retrieval Framework (DCRF), outperforming each\nsingle model across all queries and corpora. Specifically, the framework\nintegrates various retrieval models into a unified system and dynamically\nselects the optimal results for each user's query. It can easily aggregate any\nretrieval model and expand to any application scenarios, illustrating its\nflexibility and scalability.Moreover, to reduce maintenance and training costs,\nwe design four effective prompting strategies with large language models (LLMs)\nto evaluate the quality of ranks without reliance of labeled data. Extensive\nexperiments demonstrate that proposed framework, combined with 8 efficient\nretrieval models, can achieve performance comparable to effective listwise\nmethods like RankGPT and ListT5, while offering superior efficiency. Besides,\nDCRF surpasses all selected retrieval models on the most datasets, indicating\nthe effectiveness of our prompting strategies on rank-oriented automatic\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous retrieval models, including sparse, dense and llm-based methods,\nhave demonstrated remarkable performance in predicting the relevance between\nqueries and corpora. However, the preliminary effectiveness analysis\nexperiments indicate that these models fail to achieve satisfactory performance\non the majority of queries and corpora, revealing their effectiveness\nrestricted to specific scenarios. Thus, to tackle this problem, we propose a\nnovel Distributed Collaborative Retrieval Framework (DCRF), outperforming each\nsingle model across all queries and corpora. Specifically, the framework\nintegrates various retrieval models into a unified system and dynamically\nselects the optimal results for each user's query. It can easily aggregate any\nretrieval model and expand to any application scenarios, illustrating its\nflexibility and scalability.Moreover, to reduce maintenance and training costs,\nwe design four effective prompting strategies with large language models (LLMs)\nto evaluate the quality of ranks without reliance of labeled data. Extensive\nexperiments demonstrate that proposed framework, combined with 8 efficient\nretrieval models, can achieve performance comparable to effective listwise\nmethods like RankGPT and ListT5, while offering superior efficiency. Besides,\nDCRF surpasses all selected retrieval models on the most datasets, indicating\nthe effectiveness of our prompting strategies on rank-oriented automatic\nevaluation."
                },
                "authors": [
                    {
                        "name": "Tian-Yi Che"
                    },
                    {
                        "name": "Xian-Ling Mao"
                    },
                    {
                        "name": "Chun Xu"
                    },
                    {
                        "name": "Cheng-Xin Xin"
                    },
                    {
                        "name": "Heng-Da Xu"
                    },
                    {
                        "name": "Jin-Yu Liu"
                    },
                    {
                        "name": "Heyan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heyan Huang"
                },
                "author": "Heyan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10257v2",
                "updated": "2024-12-16T14:54:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    54,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-13T16:26:34Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    26,
                    34,
                    4,
                    348,
                    0
                ],
                "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models"
                },
                "summary": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015)."
                },
                "authors": [
                    {
                        "name": "Harry J. Davies"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "14 pages, 5 figures, 1 table. Fixing typo with the final weight\n  editing equation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04905v2",
                "updated": "2024-12-16T14:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    36,
                    19,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-06T10:01:38Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    10,
                    1,
                    38,
                    4,
                    341,
                    0
                ],
                "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling"
                },
                "summary": "Large language models (LLMs) have made dialogue one of the central modes in\nhuman-machine interaction, leading to the vast amounts of conversation logs and\nincreasing demand for dialogue generation. The dialogue's life-cycle spans from\nthe $\\textit{Prelude}$ through the $\\textit{Interlocution}$ to the\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite the large\nvolumes of dialogue-related studies, there is a lack of benchmark that\nencompasses comprehensive dialogue elements, which hinders precise modeling,\ngeneration and systematic evaluation. To bridge this gap, in this paper, we\nintroduce a new research task $\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made dialogue one of the central modes in\nhuman-machine interaction, leading to the vast amounts of conversation logs and\nincreasing demand for dialogue generation. The dialogue's life-cycle spans from\nthe $\\textit{Prelude}$ through the $\\textit{Interlocution}$ to the\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite the large\nvolumes of dialogue-related studies, there is a lack of benchmark that\nencompasses comprehensive dialogue elements, which hinders precise modeling,\ngeneration and systematic evaluation. To bridge this gap, in this paper, we\nintroduce a new research task $\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks."
                },
                "authors": [
                    {
                        "name": "Minzheng Wang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Wenji Mao"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "We release the code and data at https://github.com/MozerWang/DEMO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11814v1",
                "updated": "2024-12-16T14:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:29:49Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    49,
                    0,
                    351,
                    0
                ],
                "title": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents"
                },
                "summary": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information."
                },
                "authors": [
                    {
                        "name": "Mengna Zhu"
                    },
                    {
                        "name": "Kaisheng Zeng"
                    },
                    {
                        "name": "Mao Wang"
                    },
                    {
                        "name": "Kaiming Xiao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Hongbin Huang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Extended version for paper accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11813v1",
                "updated": "2024-12-16T14:29:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    31,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:29:31Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    31,
                    0,
                    351,
                    0
                ],
                "title": "Designing Semi-Structured Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Semi-Structured Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition"
                },
                "summary": "Deep neural networks (DNNs) are nowadays witnessing a major success in\nsolving many pattern recognition tasks including skeleton-based classification.\nThe deployment of DNNs on edge-devices, endowed with limited time and memory\nresources, requires designing lightweight and efficient variants of these\nnetworks. Pruning is one of the lightweight network design techniques that\noperate by removing unnecessary network parts, in a structured or an\nunstructured manner, including individual weights, neurons or even entire\nchannels. Nonetheless, structured and unstructured pruning methods, when\napplied separately, may either be inefficient or ineffective. In this paper, we\ndevise a novel semi-structured method that discards the downsides of structured\nand unstructured pruning while gathering their upsides to some extent. The\nproposed solution is based on a differentiable cascaded parametrization which\ncombines (i) a band-stop mechanism that prunes weights depending on their\nmagnitudes, (ii) a weight-sharing parametrization that prunes connections\neither individually or group-wise, and (iii) a gating mechanism which\narbitrates between different group-wise and entry-wise pruning. All these\ncascaded parametrizations are built upon a common latent tensor which is\ntrained end-to-end by minimizing a classification loss and a surrogate tensor\nrank regularizer. Extensive experiments, conducted on the challenging tasks of\naction and hand-gesture recognition, show the clear advantage of our proposed\nsemi-structured pruning approach against both structured and unstructured\npruning, when taken separately, as well as the related work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are nowadays witnessing a major success in\nsolving many pattern recognition tasks including skeleton-based classification.\nThe deployment of DNNs on edge-devices, endowed with limited time and memory\nresources, requires designing lightweight and efficient variants of these\nnetworks. Pruning is one of the lightweight network design techniques that\noperate by removing unnecessary network parts, in a structured or an\nunstructured manner, including individual weights, neurons or even entire\nchannels. Nonetheless, structured and unstructured pruning methods, when\napplied separately, may either be inefficient or ineffective. In this paper, we\ndevise a novel semi-structured method that discards the downsides of structured\nand unstructured pruning while gathering their upsides to some extent. The\nproposed solution is based on a differentiable cascaded parametrization which\ncombines (i) a band-stop mechanism that prunes weights depending on their\nmagnitudes, (ii) a weight-sharing parametrization that prunes connections\neither individually or group-wise, and (iii) a gating mechanism which\narbitrates between different group-wise and entry-wise pruning. All these\ncascaded parametrizations are built upon a common latent tensor which is\ntrained end-to-end by minimizing a classification loss and a surrogate tensor\nrank regularizer. Extensive experiments, conducted on the challenging tasks of\naction and hand-gesture recognition, show the clear advantage of our proposed\nsemi-structured pruning approach against both structured and unstructured\npruning, when taken separately, as well as the related work."
                },
                "authors": [
                    {
                        "name": "Hichem Sahbi"
                    }
                ],
                "author_detail": {
                    "name": "Hichem Sahbi"
                },
                "author": "Hichem Sahbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11803v1",
                "updated": "2024-12-16T14:14:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    14,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:14:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    14,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on\n  Large Language Models"
                },
                "summary": "Despite demonstrating impressive capabilities, Large Language Models (LLMs)\nstill often struggle to accurately express the factual knowledge they possess,\nespecially in cases where the LLMs' knowledge boundaries are ambiguous. To\nimprove LLMs' factual expressions, we propose the UAlign framework, which\nleverages Uncertainty estimations to represent knowledge boundaries, and then\nexplicitly incorporates these representations as input features into prompts\nfor LLMs to Align with factual knowledge. First, we prepare the dataset on\nknowledge question-answering (QA) samples by calculating two uncertainty\nestimations, including confidence score and semantic entropy, to represent the\nknowledge boundaries for LLMs. Subsequently, using the prepared dataset, we\ntrain a reward model that incorporates uncertainty estimations and then employ\nthe Proximal Policy Optimization (PPO) algorithm for factuality alignment on\nLLMs. Experimental results indicate that, by integrating uncertainty\nrepresentations in LLM alignment, the proposed UAlign can significantly enhance\nthe LLMs' capacities to confidently answer known questions and refuse unknown\nquestions on both in-domain and out-of-domain tasks, showing reliability\nimprovements and good generalizability over various prompt- and training-based\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite demonstrating impressive capabilities, Large Language Models (LLMs)\nstill often struggle to accurately express the factual knowledge they possess,\nespecially in cases where the LLMs' knowledge boundaries are ambiguous. To\nimprove LLMs' factual expressions, we propose the UAlign framework, which\nleverages Uncertainty estimations to represent knowledge boundaries, and then\nexplicitly incorporates these representations as input features into prompts\nfor LLMs to Align with factual knowledge. First, we prepare the dataset on\nknowledge question-answering (QA) samples by calculating two uncertainty\nestimations, including confidence score and semantic entropy, to represent the\nknowledge boundaries for LLMs. Subsequently, using the prepared dataset, we\ntrain a reward model that incorporates uncertainty estimations and then employ\nthe Proximal Policy Optimization (PPO) algorithm for factuality alignment on\nLLMs. Experimental results indicate that, by integrating uncertainty\nrepresentations in LLM alignment, the proposed UAlign can significantly enhance\nthe LLMs' capacities to confidently answer known questions and refuse unknown\nquestions on both in-domain and out-of-domain tasks, showing reliability\nimprovements and good generalizability over various prompt- and training-based\nbaselines."
                },
                "authors": [
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11800v1",
                "updated": "2024-12-16T14:11:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    11,
                    28,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:11:28Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    11,
                    28,
                    0,
                    351,
                    0
                ],
                "title": "Scalable Temporal Anomaly Causality Discovery in Large Systems:\n  Achieving Computational Efficiency with Binary Anomaly Flag Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Temporal Anomaly Causality Discovery in Large Systems:\n  Achieving Computational Efficiency with Binary Anomaly Flag Data"
                },
                "summary": "Extracting anomaly causality facilitates diagnostics once monitoring systems\ndetect system faults. Identifying anomaly causes in large systems involves\ninvestigating a more extensive set of monitoring variables across multiple\nsubsystems. However, learning causal graphs comes with a significant\ncomputational burden that restrains the applicability of most existing methods\nin real-time and large-scale deployments. In addition, modern monitoring\napplications for large systems often generate large amounts of binary alarm\nflags, and the distinct characteristics of binary anomaly data -- the meaning\nof state transition and data sparsity -- challenge existing causality learning\nmechanisms. This study proposes an anomaly causal discovery approach\n(AnomalyCD), addressing the accuracy and computational challenges of generating\ncausal graphs from binary flag data sets. The AnomalyCD framework presents\nseveral strategies, such as anomaly flag characteristics incorporating\ncausality testing, sparse data and link compression, and edge pruning\nadjustment approaches. We validate the performance of this framework on two\ndatasets: monitoring sensor data of the readout-box system of the Compact Muon\nSolenoid experiment at CERN, and a public data set for information technology\nmonitoring. The results demonstrate the considerable reduction of the\ncomputation overhead and moderate enhancement of the accuracy of temporal\ncausal discovery on binary anomaly data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting anomaly causality facilitates diagnostics once monitoring systems\ndetect system faults. Identifying anomaly causes in large systems involves\ninvestigating a more extensive set of monitoring variables across multiple\nsubsystems. However, learning causal graphs comes with a significant\ncomputational burden that restrains the applicability of most existing methods\nin real-time and large-scale deployments. In addition, modern monitoring\napplications for large systems often generate large amounts of binary alarm\nflags, and the distinct characteristics of binary anomaly data -- the meaning\nof state transition and data sparsity -- challenge existing causality learning\nmechanisms. This study proposes an anomaly causal discovery approach\n(AnomalyCD), addressing the accuracy and computational challenges of generating\ncausal graphs from binary flag data sets. The AnomalyCD framework presents\nseveral strategies, such as anomaly flag characteristics incorporating\ncausality testing, sparse data and link compression, and edge pruning\nadjustment approaches. We validate the performance of this framework on two\ndatasets: monitoring sensor data of the readout-box system of the Compact Muon\nSolenoid experiment at CERN, and a public data set for information technology\nmonitoring. The results demonstrate the considerable reduction of the\ncomputation overhead and moderate enhancement of the accuracy of temporal\ncausal discovery on binary anomaly data sets."
                },
                "authors": [
                    {
                        "name": "Mulugeta Weldezgina Asres"
                    },
                    {
                        "name": "Christian Walter Omlin"
                    },
                    {
                        "name": "The CMS-HCAL Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The CMS-HCAL Collaboration"
                },
                "author": "The CMS-HCAL Collaboration",
                "arxiv_comment": "30 pages, 17 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04920v3",
                "updated": "2024-12-16T14:05:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    5,
                    3,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-07T17:57:03Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    57,
                    3,
                    3,
                    312,
                    0
                ],
                "title": "GPTKB: Comprehensively Materializing Factual LLM Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTKB: Comprehensively Materializing Factual LLM Knowledge"
                },
                "summary": "LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have majorly advanced NLP and AI, and next to their ability to perform a\nwide range of procedural tasks, a major success factor is their internalized\nfactual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has\ngained attention. However, most approaches investigate one question at a time\nvia modest-sized pre-defined samples, introducing an availability bias (Tversky\nand Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of\nLLMs beyond the experimenter's predisposition.\n  To address this challenge, we propose a novel methodology to comprehensively\nmaterializing an LLM's factual knowledge through recursive querying and result\nconsolidation.\n  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale\nknowledge base (KB) comprising 105 million triples for over 2.9 million\nentities - achieved at 1% of the cost of previous KB projects. This work marks\na milestone in two areas: For LLM research, for the first time, it provides\nconstructive insights into the scope and structure of LLMs' knowledge (or\nbeliefs). For KB construction, it pioneers new pathways for the long-standing\nchallenge of general-domain KB construction. GPTKB is accessible at\nhttps://gptkb.org."
                },
                "authors": [
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "arxiv_comment": "13 pages, 4 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05892v2",
                "updated": "2024-12-16T13:56:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    56,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-08T11:14:16Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    14,
                    16,
                    6,
                    343,
                    0
                ],
                "title": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack\n  for Toxicity Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack\n  for Toxicity Maximization"
                },
                "summary": "Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to\njailbreak attacks is essential for their responsible real-world deployment.\nMost previous work requires access to model gradients, or is based on human\nknowledge (prompt engineering) to complete jailbreak, and they hardly consider\nthe interaction of images and text, resulting in inability to jailbreak in\nblack box scenarios or poor performance. To overcome these limitations, we\npropose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\ntoxicity maximization, referred to as PBI-Attack. Our method begins by\nextracting malicious features from a harmful corpus using an alternative LVLM\nand embedding these features into a benign image as prior information.\nSubsequently, we enhance these features through bidirectional cross-modal\ninteraction optimization, which iteratively optimizes the bimodal perturbations\nin an alternating manner through greedy search, aiming to maximize the toxicity\nof the generated response. The toxicity level is quantified using a\nwell-trained evaluation model. Experiments demonstrate that PBI-Attack\noutperforms previous state-of-the-art jailbreak methods, achieving an average\nattack success rate of 92.5% across three open-source LVLMs and around 67.3% on\nthree closed-source LVLMs. Disclaimer: This paper contains potentially\ndisturbing and offensive content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to\njailbreak attacks is essential for their responsible real-world deployment.\nMost previous work requires access to model gradients, or is based on human\nknowledge (prompt engineering) to complete jailbreak, and they hardly consider\nthe interaction of images and text, resulting in inability to jailbreak in\nblack box scenarios or poor performance. To overcome these limitations, we\npropose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\ntoxicity maximization, referred to as PBI-Attack. Our method begins by\nextracting malicious features from a harmful corpus using an alternative LVLM\nand embedding these features into a benign image as prior information.\nSubsequently, we enhance these features through bidirectional cross-modal\ninteraction optimization, which iteratively optimizes the bimodal perturbations\nin an alternating manner through greedy search, aiming to maximize the toxicity\nof the generated response. The toxicity level is quantified using a\nwell-trained evaluation model. Experiments demonstrate that PBI-Attack\noutperforms previous state-of-the-art jailbreak methods, achieving an average\nattack success rate of 92.5% across three open-source LVLMs and around 67.3% on\nthree closed-source LVLMs. Disclaimer: This paper contains potentially\ndisturbing and offensive content."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Yizhong Ding"
                    },
                    {
                        "name": "Shuirong Cao"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Xiaoshuang Jia"
                    },
                    {
                        "name": "Shaowei Yuan"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Jia"
                },
                "author": "Xiaojun Jia",
                "arxiv_comment": "Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\n  Toxicity Maximization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12692v2",
                "updated": "2024-12-16T13:52:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    52,
                    51,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-18T15:06:06Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    15,
                    6,
                    6,
                    1,
                    170,
                    0
                ],
                "title": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL"
                },
                "summary": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhance the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. We make all agent interactions publicly available to\nthe research community, to foster further research in this area, offering a\nsynthetic dataset for future explorations into automatic self-correction\nguideline generation."
                },
                "authors": [
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Christian Poelitz"
                    },
                    {
                        "name": "Xinye Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xinye Tang"
                },
                "author": "Xinye Tang",
                "arxiv_comment": "Accepted at Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11777v1",
                "updated": "2024-12-16T13:48:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    48,
                    40,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:48:40Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    48,
                    40,
                    0,
                    351,
                    0
                ],
                "title": "Fast and Slow Gradient Approximation for Binary Neural Network\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Slow Gradient Approximation for Binary Neural Network\n  Optimization"
                },
                "summary": "Binary Neural Networks (BNNs) have garnered significant attention due to\ntheir immense potential for deployment on edge devices. However, the\nnon-differentiability of the quantization function poses a challenge for the\noptimization of BNNs, as its derivative cannot be backpropagated. To address\nthis issue, hypernetwork based methods, which utilize neural networks to learn\nthe gradients of non-differentiable quantization functions, have emerged as a\npromising approach due to their adaptive learning capabilities to reduce\nestimation errors. However, existing hypernetwork based methods typically rely\nsolely on current gradient information, neglecting the influence of historical\ngradients. This oversight can lead to accumulated gradient errors when\ncalculating gradient momentum during optimization. To incorporate historical\ngradient information, we design a Historical Gradient Storage (HGS) module,\nwhich models the historical gradient sequence to generate the first-order\nmomentum required for optimization. To further enhance gradient generation in\nhypernetworks, we propose a Fast and Slow Gradient Generation (FSG) method.\nAdditionally, to produce more precise gradients, we introduce Layer Recognition\nEmbeddings (LRE) into the hypernetwork, facilitating the generation of\nlayer-specific fine gradients. Extensive comparative experiments on the\nCIFAR-10 and CIFAR-100 datasets demonstrate that our method achieves faster\nconvergence and lower loss values, outperforming existing baselines.Code is\navailable at http://github.com/two-tiger/FSG .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Networks (BNNs) have garnered significant attention due to\ntheir immense potential for deployment on edge devices. However, the\nnon-differentiability of the quantization function poses a challenge for the\noptimization of BNNs, as its derivative cannot be backpropagated. To address\nthis issue, hypernetwork based methods, which utilize neural networks to learn\nthe gradients of non-differentiable quantization functions, have emerged as a\npromising approach due to their adaptive learning capabilities to reduce\nestimation errors. However, existing hypernetwork based methods typically rely\nsolely on current gradient information, neglecting the influence of historical\ngradients. This oversight can lead to accumulated gradient errors when\ncalculating gradient momentum during optimization. To incorporate historical\ngradient information, we design a Historical Gradient Storage (HGS) module,\nwhich models the historical gradient sequence to generate the first-order\nmomentum required for optimization. To further enhance gradient generation in\nhypernetworks, we propose a Fast and Slow Gradient Generation (FSG) method.\nAdditionally, to produce more precise gradients, we introduce Layer Recognition\nEmbeddings (LRE) into the hypernetwork, facilitating the generation of\nlayer-specific fine gradients. Extensive comparative experiments on the\nCIFAR-10 and CIFAR-100 datasets demonstrate that our method achieves faster\nconvergence and lower loss values, outperforming existing baselines.Code is\navailable at http://github.com/two-tiger/FSG ."
                },
                "authors": [
                    {
                        "name": "Xinquan Chen"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Yiang Luo"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Pengfei Li"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Li"
                },
                "author": "Pengfei Li",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04903v2",
                "updated": "2024-12-16T13:47:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    47,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-06T09:59:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    9,
                    59,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation"
                },
                "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs."
                },
                "authors": [
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Liang Ma"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Yuhao Cheng"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11768v1",
                "updated": "2024-12-16T13:41:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    41,
                    37,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:41:37Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    41,
                    37,
                    0,
                    351,
                    0
                ],
                "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No More Adam: Learning Rate Scaling at Initialization is All You Need"
                },
                "summary": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings."
                },
                "authors": [
                    {
                        "name": "Minghao Xu"
                    },
                    {
                        "name": "Lichuan Xiang"
                    },
                    {
                        "name": "Xu Cai"
                    },
                    {
                        "name": "Hongkai Wen"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Wen"
                },
                "author": "Hongkai Wen",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11764v1",
                "updated": "2024-12-16T13:31:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    31,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:31:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    31,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study"
                },
                "summary": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/."
                },
                "authors": [
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yuqing Xie"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yinuo Chen"
                    },
                    {
                        "name": "Shu'ang Yu"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Shilong Ji"
                    },
                    {
                        "name": "Mo Mu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "The first two authors contribute equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11763v1",
                "updated": "2024-12-16T13:28:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    28,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:28:29Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    28,
                    29,
                    0,
                    351,
                    0
                ],
                "title": "QUENCH: Measuring the gap between Indic and Non-Indic Contextual General\n  Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUENCH: Measuring the gap between Indic and Non-Indic Contextual General\n  Reasoning in LLMs"
                },
                "summary": "The rise of large language models (LLMs) has created a need for advanced\nbenchmarking systems beyond traditional setups. To this end, we introduce\nQUENCH, a novel text-based English Quizzing Benchmark manually curated and\ntranscribed from YouTube quiz videos. QUENCH possesses masked entities and\nrationales for the LLMs to predict via generation. At the intersection of\ngeographical context and common sense reasoning, QUENCH helps assess world\nknowledge and deduction capabilities of LLMs via a zero-shot, open-domain\nquizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics,\ninvestigating the influence of model size, prompting style, geographical\ncontext, and gold-labeled rationale generation. The benchmarking concludes with\nan error analysis to which the LLMs are prone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has created a need for advanced\nbenchmarking systems beyond traditional setups. To this end, we introduce\nQUENCH, a novel text-based English Quizzing Benchmark manually curated and\ntranscribed from YouTube quiz videos. QUENCH possesses masked entities and\nrationales for the LLMs to predict via generation. At the intersection of\ngeographical context and common sense reasoning, QUENCH helps assess world\nknowledge and deduction capabilities of LLMs via a zero-shot, open-domain\nquizzing setup. We perform an extensive evaluation on 7 LLMs and 4 metrics,\ninvestigating the influence of model size, prompting style, geographical\ncontext, and gold-labeled rationale generation. The benchmarking concludes with\nan error analysis to which the LLMs are prone."
                },
                "authors": [
                    {
                        "name": "Mohammad Aflah Khan"
                    },
                    {
                        "name": "Neemesh Yadav"
                    },
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Md. Shad Akhtar"
                    }
                ],
                "author_detail": {
                    "name": "Md. Shad Akhtar"
                },
                "author": "Md. Shad Akhtar",
                "arxiv_comment": "17 Pages, 6 Figures, 8 Tables, COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11761v1",
                "updated": "2024-12-16T13:25:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    25,
                    42,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:25:42Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    25,
                    42,
                    0,
                    351,
                    0
                ],
                "title": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Language for Coordination: A Framework and Benchmark for\n  LLM-Driven Multi-Agent Control"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. A promising but largely under-explored area is their potential\nto facilitate human coordination with many agents. Such capabilities would be\nuseful in domains including disaster response, urban planning, and real-time\nstrategy scenarios. In this work, we introduce (1) a real-time strategy game\nbenchmark designed to evaluate these abilities and (2) a novel framework we\nterm HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000\nagents using natural language dialog with an LLM. We present promising results\non this multi-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. However, our\nfindings also highlight critical limitations of current models, including\ndifficulties in processing spatial visual information and challenges in\nformulating long-term strategic plans. This work sheds light on the potential\nand limitations of LLMs in human-swarm coordination, paving the way for future\nresearch in this area. The HIVE project page, which includes videos of the\nsystem in action, can be found here: hive.syrkis.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. A promising but largely under-explored area is their potential\nto facilitate human coordination with many agents. Such capabilities would be\nuseful in domains including disaster response, urban planning, and real-time\nstrategy scenarios. In this work, we introduce (1) a real-time strategy game\nbenchmark designed to evaluate these abilities and (2) a novel framework we\nterm HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000\nagents using natural language dialog with an LLM. We present promising results\non this multi-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. However, our\nfindings also highlight critical limitations of current models, including\ndifficulties in processing spatial visual information and challenges in\nformulating long-term strategic plans. This work sheds light on the potential\nand limitations of LLMs in human-swarm coordination, paving the way for future\nresearch in this area. The HIVE project page, which includes videos of the\nsystem in action, can be found here: hive.syrkis.com."
                },
                "authors": [
                    {
                        "name": "Timothe Anne"
                    },
                    {
                        "name": "Noah Syrkis"
                    },
                    {
                        "name": "Meriem Elhosni"
                    },
                    {
                        "name": "Florian Turati"
                    },
                    {
                        "name": "Franck Legendre"
                    },
                    {
                        "name": "Alain Jaquier"
                    },
                    {
                        "name": "Sebastian Risi"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Risi"
                },
                "author": "Sebastian Risi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03769v2",
                "updated": "2024-12-16T12:57:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    23,
                    0,
                    351,
                    0
                ],
                "published": "2024-10-02T16:34:48Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    48,
                    2,
                    276,
                    0
                ],
                "title": "SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large\n  Language Models in Scientific Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large\n  Language Models in Scientific Tasks"
                },
                "summary": "Large language models (LLMs) have a transformative impact on a variety of\nscientific tasks across disciplines including biology, chemistry, medicine, and\nphysics. However, ensuring the safety alignment of these models in scientific\nresearch remains an underexplored area, with existing benchmarks primarily\nfocusing on textual content and overlooking key scientific representations such\nas molecular, protein, and genomic languages. Moreover, the safety mechanisms\nof LLMs in scientific tasks are insufficiently studied. To address these\nlimitations, we introduce SciSafeEval, a comprehensive benchmark designed to\nevaluate the safety alignment of LLMs across a range of scientific tasks.\nSciSafeEval spans multiple scientific languages-including textual, molecular,\nprotein, and genomic-and covers a wide range of scientific domains. We evaluate\nLLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a\n\"jailbreak\" enhancement feature that challenges LLMs equipped with safety\nguardrails, rigorously testing their defenses against malicious intention. Our\nbenchmark surpasses existing safety datasets in both scale and scope, providing\na robust platform for assessing the safety and performance of LLMs in\nscientific contexts. This work aims to facilitate the responsible development\nand deployment of LLMs, promoting alignment with safety and ethical standards\nin scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have a transformative impact on a variety of\nscientific tasks across disciplines including biology, chemistry, medicine, and\nphysics. However, ensuring the safety alignment of these models in scientific\nresearch remains an underexplored area, with existing benchmarks primarily\nfocusing on textual content and overlooking key scientific representations such\nas molecular, protein, and genomic languages. Moreover, the safety mechanisms\nof LLMs in scientific tasks are insufficiently studied. To address these\nlimitations, we introduce SciSafeEval, a comprehensive benchmark designed to\nevaluate the safety alignment of LLMs across a range of scientific tasks.\nSciSafeEval spans multiple scientific languages-including textual, molecular,\nprotein, and genomic-and covers a wide range of scientific domains. We evaluate\nLLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a\n\"jailbreak\" enhancement feature that challenges LLMs equipped with safety\nguardrails, rigorously testing their defenses against malicious intention. Our\nbenchmark surpasses existing safety datasets in both scale and scope, providing\na robust platform for assessing the safety and performance of LLMs in\nscientific contexts. This work aims to facilitate the responsible development\nand deployment of LLMs, promoting alignment with safety and ethical standards\nin scientific research."
                },
                "authors": [
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Jingyu Lu"
                    },
                    {
                        "name": "Chuangxin Chu"
                    },
                    {
                        "name": "Tianyu Zeng"
                    },
                    {
                        "name": "Yujia Zheng"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Haotian Huang"
                    },
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Zuoxian Liu"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Xuejing Yuan"
                    },
                    {
                        "name": "Xingkai Wang"
                    },
                    {
                        "name": "Keyan Ding"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Qiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Zhang"
                },
                "author": "Qiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11736v1",
                "updated": "2024-12-16T12:57:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    19,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:57:19Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    57,
                    19,
                    0,
                    351,
                    0
                ],
                "title": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized LLM for Generating Customized Responses to the Same Query\n  from Different Users"
                },
                "summary": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLM, but overlooked the diversity of questioners.\nIn this work, we propose a new form of questioner-aware LLM personalization,\ngenerating different responses even for the same query from different\nquestioners. We design a dual-tower model architecture with a cross-questioner\ngeneral encoder and a questioner-specific encoder. We further apply contrastive\nlearning with multi-view augmentation, pulling close the dialogue\nrepresentations of the same questioner, while pulling apart those of different\nquestioners. To mitigate the impact of question diversity on\nquestioner-contrastive learning, we cluster the dialogues based on question\nsimilarity and restrict the scope of contrastive learning within each cluster.\nWe also build a multi-questioner dataset from English and Chinese scripts and\nWeChat records, called MQDialog, containing 173 questioners and 12 responders.\nExtensive evaluation with different metrics shows a significant improvement in\nthe quality of personalized response generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on large language model (LLM) personalization assigned\ndifferent responding roles to LLM, but overlooked the diversity of questioners.\nIn this work, we propose a new form of questioner-aware LLM personalization,\ngenerating different responses even for the same query from different\nquestioners. We design a dual-tower model architecture with a cross-questioner\ngeneral encoder and a questioner-specific encoder. We further apply contrastive\nlearning with multi-view augmentation, pulling close the dialogue\nrepresentations of the same questioner, while pulling apart those of different\nquestioners. To mitigate the impact of question diversity on\nquestioner-contrastive learning, we cluster the dialogues based on question\nsimilarity and restrict the scope of contrastive learning within each cluster.\nWe also build a multi-questioner dataset from English and Chinese scripts and\nWeChat records, called MQDialog, containing 173 questioners and 12 responders.\nExtensive evaluation with different metrics shows a significant improvement in\nthe quality of personalized response generation."
                },
                "authors": [
                    {
                        "name": "Hang Zeng"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Chengfei Lv"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13925v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13925v3",
                "updated": "2024-12-16T12:51:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    51,
                    46,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-20T01:45:44Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    45,
                    44,
                    3,
                    172,
                    0
                ],
                "title": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating content that exhibits\ngender biases, raising significant ethical concerns. Alignment, the process of\nfine-tuning LLMs to better align with desired behaviors, is recognized as an\neffective approach to mitigate gender biases. Although proprietary LLMs have\nmade significant strides in mitigating gender bias, their alignment datasets\nare not publicly available. The commonly used and publicly available alignment\ndataset, HH-RLHF, still exhibits gender bias to some extent. There is a lack of\npublicly available alignment datasets specifically designed to address gender\nbias. Hence, we developed a new dataset named GenderAlign, aiming at mitigating\na comprehensive set of gender biases in LLMs. This dataset comprises 8k\nsingle-turn dialogues, each paired with a \"chosen\" and a \"rejected\" response.\nCompared to the \"rejected\" responses, the \"chosen\" responses demonstrate lower\nlevels of gender bias and higher quality. Furthermore, we categorized the\ngender biases in the \"rejected\" responses of GenderAlign into 4 principal\ncategories. The experimental results show the effectiveness of GenderAlign in\nreducing gender bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Yuxiang Xiao"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    },
                    {
                        "name": "James Foulds"
                    },
                    {
                        "name": "Shimei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Shimei Pan"
                },
                "author": "Shimei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13925v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13925v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05813v2",
                "updated": "2024-12-16T12:44:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    44,
                    7,
                    0,
                    351,
                    0
                ],
                "published": "2024-02-08T16:50:01Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    16,
                    50,
                    1,
                    3,
                    39,
                    0
                ],
                "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and\n  Evaluation in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Forgetting: Advancing Machine Unlearning Techniques and\n  Evaluation in Language Models"
                },
                "summary": "This paper explores Machine Unlearning (MU), an emerging field that is\ngaining increased attention due to concerns about neural models unintentionally\nremembering personal or sensitive information. We present SeUL, a novel method\nthat enables selective and fine-grained unlearning for language models. Unlike\nprevious work that employs a fully reversed training objective in unlearning,\nSeUL minimizes the negative impact on the capability of language models,\nparticularly in terms of generation. Furthermore, we introduce two innovative\nevaluation metrics, sensitive extraction likelihood (S-EL) and sensitive\nmemorization accuracy (S-MA), specifically designed to assess the effectiveness\nof forgetting sensitive information. In support of the unlearning framework, we\npropose efficient automatic online and offline sensitive span annotation\nmethods. The online selection method, based on language probability scores,\nensures computational efficiency, while the offline annotation involves a\ntwo-stage LLM-based process for robust verification. In summary, this paper\ncontributes a novel selective unlearning method (SeUL), introduces specialized\nevaluation metrics (S-EL and S-MA) for assessing sensitive information\nforgetting, and proposes automatic online and offline sensitive span annotation\nmethods to support the overall unlearning framework and evaluation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores Machine Unlearning (MU), an emerging field that is\ngaining increased attention due to concerns about neural models unintentionally\nremembering personal or sensitive information. We present SeUL, a novel method\nthat enables selective and fine-grained unlearning for language models. Unlike\nprevious work that employs a fully reversed training objective in unlearning,\nSeUL minimizes the negative impact on the capability of language models,\nparticularly in terms of generation. Furthermore, we introduce two innovative\nevaluation metrics, sensitive extraction likelihood (S-EL) and sensitive\nmemorization accuracy (S-MA), specifically designed to assess the effectiveness\nof forgetting sensitive information. In support of the unlearning framework, we\npropose efficient automatic online and offline sensitive span annotation\nmethods. The online selection method, based on language probability scores,\nensures computational efficiency, while the offline annotation involves a\ntwo-stage LLM-based process for robust verification. In summary, this paper\ncontributes a novel selective unlearning method (SeUL), introduces specialized\nevaluation metrics (S-EL and S-MA) for assessing sensitive information\nforgetting, and proposes automatic online and offline sensitive span annotation\nmethods to support the overall unlearning framework and evaluation process."
                },
                "authors": [
                    {
                        "name": "Lingzhi Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Jinsong Guo"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Georg Gottlob"
                    }
                ],
                "author_detail": {
                    "name": "Georg Gottlob"
                },
                "author": "Georg Gottlob",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11716v1",
                "updated": "2024-12-16T12:36:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    36,
                    47,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:36:47Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    36,
                    47,
                    0,
                    351,
                    0
                ],
                "title": "LLMs Can Simulate Standardized Patients via Agent Coevolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Simulate Standardized Patients via Agent Coevolution"
                },
                "summary": "Training medical personnel using standardized patients (SPs) remains a\ncomplex challenge, requiring extensive domain expertise and role-specific\npractice. Most research on Large Language Model (LLM)-based simulated patients\nfocuses on improving data retrieval accuracy or adjusting prompts through human\nfeedback. However, this focus has overlooked the critical need for patient\nagents to learn a standardized presentation pattern that transforms data into\nhuman-like patient responses through unsupervised simulations. To address this\ngap, we propose EvoPatient, a novel simulated patient framework in which a\npatient agent and doctor agents simulate the diagnostic process through\nmulti-turn dialogues, simultaneously gathering experience to improve the\nquality of both questions and answers, ultimately enabling human doctor\ntraining. Extensive experiments on various cases demonstrate that, by providing\nonly overall SP requirements, our framework improves over existing reasoning\nmethods by more than 10% in requirement alignment and better human preference,\nwhile achieving an optimal balance of resource consumption after evolving over\n200 cases for 10 hours, with excellent generalizability. The code will be\navailable at https://github.com/ZJUMAI/EvoPatient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training medical personnel using standardized patients (SPs) remains a\ncomplex challenge, requiring extensive domain expertise and role-specific\npractice. Most research on Large Language Model (LLM)-based simulated patients\nfocuses on improving data retrieval accuracy or adjusting prompts through human\nfeedback. However, this focus has overlooked the critical need for patient\nagents to learn a standardized presentation pattern that transforms data into\nhuman-like patient responses through unsupervised simulations. To address this\ngap, we propose EvoPatient, a novel simulated patient framework in which a\npatient agent and doctor agents simulate the diagnostic process through\nmulti-turn dialogues, simultaneously gathering experience to improve the\nquality of both questions and answers, ultimately enabling human doctor\ntraining. Extensive experiments on various cases demonstrate that, by providing\nonly overall SP requirements, our framework improves over existing reasoning\nmethods by more than 10% in requirement alignment and better human preference,\nwhile achieving an optimal balance of resource consumption after evolving over\n200 cases for 10 hours, with excellent generalizability. The code will be\navailable at https://github.com/ZJUMAI/EvoPatient."
                },
                "authors": [
                    {
                        "name": "Zhuoyun Du"
                    },
                    {
                        "name": "Lujie Zheng"
                    },
                    {
                        "name": "Renjun Hu"
                    },
                    {
                        "name": "Yuyang Xu"
                    },
                    {
                        "name": "Xiawei Li"
                    },
                    {
                        "name": "Ying Sun"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Haolei Cai"
                    },
                    {
                        "name": "Haohao Ying"
                    }
                ],
                "author_detail": {
                    "name": "Haohao Ying"
                },
                "author": "Haohao Ying",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11713v1",
                "updated": "2024-12-16T12:35:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    35,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:35:29Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    35,
                    29,
                    0,
                    351,
                    0
                ],
                "title": "Seeker: Towards Exception Safety Code Generation with Intermediate\n  Language Agents Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeker: Towards Exception Safety Code Generation with Intermediate\n  Language Agents Framework"
                },
                "summary": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open-source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nBlock, and Distorted Handling Solution. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a\nmulti-agent framework inspired by expert developer strategies for exception\nhandling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler\nto assist LLMs in detecting, capturing, and resolving exceptions more\neffectively. Our work is the first systematic study on leveraging LLMs to\nenhance exception handling practices in real development scenarios, providing\nvaluable insights for future improvements in code reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real world software development, improper or missing exception handling\ncan severely impact the robustness and reliability of code. Exception handling\nmechanisms require developers to detect, capture, and manage exceptions\naccording to high standards, but many developers struggle with these tasks,\nleading to fragile code. This problem is particularly evident in open-source\nprojects and impacts the overall quality of the software ecosystem. To address\nthis challenge, we explore the use of large language models (LLMs) to improve\nexception handling in code. Through extensive analysis, we identify three key\nissues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception\nBlock, and Distorted Handling Solution. These problems are widespread across\nreal world repositories, suggesting that robust exception handling practices\nare often overlooked or mishandled. In response, we propose Seeker, a\nmulti-agent framework inspired by expert developer strategies for exception\nhandling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler\nto assist LLMs in detecting, capturing, and resolving exceptions more\neffectively. Our work is the first systematic study on leveraging LLMs to\nenhance exception handling practices in real development scenarios, providing\nvaluable insights for future improvements in code reliability."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yiming Zheng"
                    },
                    {
                        "name": "Zhexin Zhang"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "30 pages, 9 figures, submitted to ARR Dec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11711v1",
                "updated": "2024-12-16T12:33:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    33,
                    12,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:33:12Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    33,
                    12,
                    0,
                    351,
                    0
                ],
                "title": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for\n  Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiMoTable: A Multi-scale Spreadsheet Benchmark with Meta Operations for\n  Table Reasoning"
                },
                "summary": "Extensive research has been conducted to explore the capability of Large\nLanguage Models (LLMs) for table reasoning and has significantly improved the\nperformance on existing benchmarks. However, tables and user questions in\nreal-world applications are more complex and diverse, presenting an unignorable\ngap compared to the existing benchmarks. To fill the gap, we propose a\n\\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta\n\\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable.\nSpecifically, MiMoTable incorporates two key features. First, the tables in\nMiMoTable are all spreadsheets used in real-world scenarios, which cover seven\ndomains and contain different types. Second, we define a new criterion with six\ncategories of meta operations for measuring the difficulty of each question in\nMiMoTable, simultaneously as a new perspective for measuring the difficulty of\nthe existing benchmarks. Experimental results show that Claude-3.5-Sonnet\nachieves the best performance with 77.4\\% accuracy, indicating that there is\nstill significant room to improve for LLMs on MiMoTable. Furthermore, we grade\nthe difficulty of existing benchmarks according to our new criteria.\nExperiments have shown that the performance of LLMs decreases as the difficulty\nof benchmarks increases, thereby proving the effectiveness of our proposed new\ncriterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extensive research has been conducted to explore the capability of Large\nLanguage Models (LLMs) for table reasoning and has significantly improved the\nperformance on existing benchmarks. However, tables and user questions in\nreal-world applications are more complex and diverse, presenting an unignorable\ngap compared to the existing benchmarks. To fill the gap, we propose a\n\\textbf{M}ult\\textbf{i}-scale spreadsheet benchmark with \\textbf{M}eta\n\\textbf{o}perations for \\textbf{Table} reasoning, named as MiMoTable.\nSpecifically, MiMoTable incorporates two key features. First, the tables in\nMiMoTable are all spreadsheets used in real-world scenarios, which cover seven\ndomains and contain different types. Second, we define a new criterion with six\ncategories of meta operations for measuring the difficulty of each question in\nMiMoTable, simultaneously as a new perspective for measuring the difficulty of\nthe existing benchmarks. Experimental results show that Claude-3.5-Sonnet\nachieves the best performance with 77.4\\% accuracy, indicating that there is\nstill significant room to improve for LLMs on MiMoTable. Furthermore, we grade\nthe difficulty of existing benchmarks according to our new criteria.\nExperiments have shown that the performance of LLMs decreases as the difficulty\nof benchmarks increases, thereby proving the effectiveness of our proposed new\ncriterion."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Mingyang Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingyang Song"
                },
                "author": "Mingyang Song",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11699v1",
                "updated": "2024-12-16T12:21:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    11,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:21:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "CoinMath: Harnessing the Power of Coding Instruction for Math LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoinMath: Harnessing the Power of Coding Instruction for Math LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown strong performance in solving\nmathematical problems, with code-based solutions proving particularly\neffective. However, the best practice to leverage coding instruction data to\nenhance mathematical reasoning remains underexplored. This study investigates\nthree key questions: (1) How do different coding styles of mathematical\ncode-based rationales impact LLMs' learning performance? (2) Can general-domain\ncoding instructions improve performance? (3) How does integrating textual\nrationales with code-based ones during training enhance mathematical reasoning\nabilities? Our findings reveal that code-based rationales with concise\ncomments, descriptive naming, and hardcoded solutions are beneficial, while\nimprovements from general-domain coding instructions and textual rationales are\nrelatively minor. Based on these insights, we propose CoinMath, a learning\nstrategy designed to enhance mathematical reasoning by diversifying the coding\nstyles of code-based rationales. CoinMath generates a variety of code-based\nrationales incorporating concise comments, descriptive naming conventions, and\nhardcoded solutions. Experimental results demonstrate that CoinMath\nsignificantly outperforms its baseline model, MAmmoTH, one of the SOTA math\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong performance in solving\nmathematical problems, with code-based solutions proving particularly\neffective. However, the best practice to leverage coding instruction data to\nenhance mathematical reasoning remains underexplored. This study investigates\nthree key questions: (1) How do different coding styles of mathematical\ncode-based rationales impact LLMs' learning performance? (2) Can general-domain\ncoding instructions improve performance? (3) How does integrating textual\nrationales with code-based ones during training enhance mathematical reasoning\nabilities? Our findings reveal that code-based rationales with concise\ncomments, descriptive naming, and hardcoded solutions are beneficial, while\nimprovements from general-domain coding instructions and textual rationales are\nrelatively minor. Based on these insights, we propose CoinMath, a learning\nstrategy designed to enhance mathematical reasoning by diversifying the coding\nstyles of code-based rationales. CoinMath generates a variety of code-based\nrationales incorporating concise comments, descriptive naming conventions, and\nhardcoded solutions. Experimental results demonstrate that CoinMath\nsignificantly outperforms its baseline model, MAmmoTH, one of the SOTA math\nLLMs."
                },
                "authors": [
                    {
                        "name": "Chengwei Wei"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Jung-jae Kim"
                    },
                    {
                        "name": "Guimei Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11698v1",
                "updated": "2024-12-16T12:21:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    5,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:21:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    21,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Large Language Models in Mission-Critical IT Governance: Are We Ready\n  Yet?"
                },
                "summary": "Context. The security of critical infrastructure has been a fundamental\nconcern since the advent of computers, and this concern has only intensified in\ntoday's cyber warfare landscape. Protecting mission-critical systems (MCSs),\nincluding essential assets like healthcare, telecommunications, and military\ncoordination, is vital for national security. These systems require prompt and\ncomprehensive governance to ensure their resilience, yet recent events have\nshown that meeting these demands is increasingly challenging. Aim. Building on\nprior research that demonstrated the potential of GAI, particularly Large\nLanguage Models (LLMs), in improving risk analysis tasks, we aim to explore\npractitioners' perspectives, specifically developers and security personnel, on\nusing generative AI (GAI) in the governance of IT MCSs seeking to provide\ninsights and recommendations for various stakeholders, including researchers,\npractitioners, and policymakers. Method. We designed a survey to collect\npractical experiences, concerns, and expectations of practitioners who develop\nand implement security solutions in the context of MCSs. Analyzing this data\nwill help identify key trends, challenges, and opportunities for introducing\nGAIs in this niche domain. Conclusions and Future Works. Our findings highlight\nthat the safe use of LLMs in MCS governance requires interdisciplinary\ncollaboration. Researchers should focus on designing regulation-oriented models\nand focus on accountability; practitioners emphasize data protection and\ntransparency, while policymakers must establish a unified AI framework with\nglobal benchmarks to ensure ethical and secure LLMs-based MCS governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The security of critical infrastructure has been a fundamental\nconcern since the advent of computers, and this concern has only intensified in\ntoday's cyber warfare landscape. Protecting mission-critical systems (MCSs),\nincluding essential assets like healthcare, telecommunications, and military\ncoordination, is vital for national security. These systems require prompt and\ncomprehensive governance to ensure their resilience, yet recent events have\nshown that meeting these demands is increasingly challenging. Aim. Building on\nprior research that demonstrated the potential of GAI, particularly Large\nLanguage Models (LLMs), in improving risk analysis tasks, we aim to explore\npractitioners' perspectives, specifically developers and security personnel, on\nusing generative AI (GAI) in the governance of IT MCSs seeking to provide\ninsights and recommendations for various stakeholders, including researchers,\npractitioners, and policymakers. Method. We designed a survey to collect\npractical experiences, concerns, and expectations of practitioners who develop\nand implement security solutions in the context of MCSs. Analyzing this data\nwill help identify key trends, challenges, and opportunities for introducing\nGAIs in this niche domain. Conclusions and Future Works. Our findings highlight\nthat the safe use of LLMs in MCS governance requires interdisciplinary\ncollaboration. Researchers should focus on designing regulation-oriented models\nand focus on accountability; practitioners emphasize data protection and\ntransparency, while policymakers must establish a unified AI framework with\nglobal benchmarks to ensure ethical and secure LLMs-based MCS governance."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Francesco Palagiano"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    },
                    {
                        "name": "Davide Taibi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Taibi"
                },
                "author": "Davide Taibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12196v2",
                "updated": "2024-12-16T12:13:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    13,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-19T03:29:17Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    3,
                    29,
                    17,
                    1,
                    324,
                    0
                ],
                "title": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A More Advanced Group Polarization Measurement Approach Based on\n  LLM-Based Agents and Graphs"
                },
                "summary": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group polarization is an important research direction in social media content\nanalysis, attracting many researchers to explore this field. Therefore, how to\neffectively measure group polarization has become a critical topic. Measuring\ngroup polarization on social media presents several challenges that have not\nyet been addressed by existing solutions. First, social media group\npolarization measurement involves processing vast amounts of text, which poses\na significant challenge for information extraction. Second, social media texts\noften contain hard-to-understand content, including sarcasm, memes, and\ninternet slang. Additionally, group polarization research focuses on holistic\nanalysis, while texts is typically fragmented. To address these challenges, we\ndesigned a solution based on a multi-agent system and used a graph-structured\nCommunity Sentiment Network (CSN) to represent polarization states.\nFurthermore, we developed a metric called Community Opposition Index (COI)\nbased on the CSN to quantify polarization. Finally, we tested our multi-agent\nsystem through a zero-shot stance detection task and achieved outstanding\nresults. In summary, the proposed approach has significant value in terms of\nusability, accuracy, and interpretability."
                },
                "authors": [
                    {
                        "name": "Zixin Liu"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiran Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Ding"
                },
                "author": "Yiran Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04902v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04902v5",
                "updated": "2024-12-16T12:06:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    6,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-02-07T14:35:05Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    14,
                    35,
                    5,
                    2,
                    38,
                    0
                ],
                "title": "L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models"
                },
                "summary": "Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically apply post-training quantization\n(PTQ) to pre-trained LLMs, followed by PEFT to recover accuracy loss.\nMeanwhile, this approach has limitations in recovering the accuracy loss. In\nthis paper, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA. By employing a memory-optimized layer design, L4Q\nsignificantly reduces QAT's memory overhead, making its training cost\ncomparable to LoRA, while preserving the advantage of QAT in producing fully\nquantized LLMs with high accuracy. Our experiments demonstrate that this\ncombined approach to quantization and fine-tuning achieves superior accuracy\ncompared to decoupled fine-tuning schemes, particularly in 4-bit and 3-bit\nquantization, positioning L4Q as an efficient QAT solution. Using the LLaMA and\nMistral models with instructional datasets, we showcase L4Q's capabilities in\nlanguage tasks and few-shot learning."
                },
                "authors": [
                    {
                        "name": "Hyesung Jeon"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-joon Kim"
                },
                "author": "Jae-joon Kim",
                "arxiv_comment": "8 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04902v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04902v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v2",
                "updated": "2024-12-16T12:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    6,
                    25,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10272v2",
                "updated": "2024-12-16T12:00:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    0,
                    34,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-15T15:28:42Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    15,
                    28,
                    42,
                    4,
                    320,
                    0
                ],
                "title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning"
                },
                "summary": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Yanling Wang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11749v2",
                "updated": "2024-12-16T11:53:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    53,
                    9,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-21T16:16:34Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    16,
                    16,
                    34,
                    2,
                    234,
                    0
                ],
                "title": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Against All Odds: Overcoming Typology, Script, and Language Confusion in\n  Multilingual Embedding Inversion Attacks"
                },
                "summary": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to malicious influence by cyber\nattackers through intrusions such as adversarial, backdoor, and embedding\ninversion attacks. In response, the burgeoning field of LLM Security aims to\nstudy and defend against such threats. Thus far, the majority of works in this\narea have focused on monolingual English models, however, emerging research\nsuggests that multilingual LLMs may be more vulnerable to various attacks than\ntheir monolingual counterparts. While previous work has investigated embedding\ninversion over a small subset of European languages, it is challenging to\nextrapolate these findings to languages from different linguistic families and\nwith differing scripts. To this end, we explore the security of multilingual\nLLMs in the context of embedding inversion attacks and investigate\ncross-lingual and cross-script inversion across 20 languages, spanning over 8\nlanguage families and 12 scripts. Our findings indicate that languages written\nin Arabic script and Cyrillic script are particularly vulnerable to embedding\ninversion, as are languages within the Indo-Aryan language family. We further\nobserve that inversion models tend to suffer from language confusion, sometimes\ngreatly reducing the efficacy of an attack. Accordingly, we systematically\nexplore this bottleneck for inversion models, uncovering predictable patterns\nwhich could be leveraged by attackers. Ultimately, this study aims to further\nthe field's understanding of the outstanding security vulnerabilities facing\nmultilingual LLMs and raise awareness for the languages most at risk of\nnegative impact from these attacks."
                },
                "authors": [
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Russa Biswas"
                    },
                    {
                        "name": "Heather Lent"
                    },
                    {
                        "name": "Johannes Bjerva"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Bjerva"
                },
                "author": "Johannes Bjerva",
                "arxiv_comment": "11 pages, 4 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11683v1",
                "updated": "2024-12-16T11:50:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    50,
                    30,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:50:30Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    50,
                    30,
                    0,
                    351,
                    0
                ],
                "title": "Multimodal LLM for Intelligent Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLM for Intelligent Transportation Systems"
                },
                "summary": "In the evolving landscape of transportation systems, integrating Large\nLanguage Models (LLMs) offers a promising frontier for advancing intelligent\ndecision-making across various applications. This paper introduces a novel\n3-dimensional framework that encapsulates the intersection of applications,\nmachine learning methodologies, and hardware devices, particularly emphasizing\nthe role of LLMs. Instead of using multiple machine learning algorithms, our\nframework uses a single, data-centric LLM architecture that can analyze time\nseries, images, and videos. We explore how LLMs can enhance data interpretation\nand decision-making in transportation. We apply this LLM framework to different\nsensor datasets, including time-series data and visual data from sources like\nOxford Radar RobotCar, D-Behavior (D-Set), nuScenes by Motional, and Comma2k19.\nThe goal is to streamline data processing workflows, reduce the complexity of\ndeploying multiple models, and make intelligent transportation systems more\nefficient and accurate. The study was conducted using state-of-the-art\nhardware, leveraging the computational power of AMD RTX 3060 GPUs and Intel\ni9-12900 processors. The experimental results demonstrate that our framework\nachieves an average accuracy of 91.33\\% across these datasets, with the highest\naccuracy observed in time-series data (92.7\\%), showcasing the model's\nproficiency in handling sequential information essential for tasks such as\nmotion planning and predictive maintenance. Through our exploration, we\ndemonstrate the versatility and efficacy of LLMs in handling multimodal data\nwithin the transportation sector, ultimately providing insights into their\napplication in real-world scenarios. Our findings align with the broader\nconference themes, highlighting the transformative potential of LLMs in\nadvancing transportation technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of transportation systems, integrating Large\nLanguage Models (LLMs) offers a promising frontier for advancing intelligent\ndecision-making across various applications. This paper introduces a novel\n3-dimensional framework that encapsulates the intersection of applications,\nmachine learning methodologies, and hardware devices, particularly emphasizing\nthe role of LLMs. Instead of using multiple machine learning algorithms, our\nframework uses a single, data-centric LLM architecture that can analyze time\nseries, images, and videos. We explore how LLMs can enhance data interpretation\nand decision-making in transportation. We apply this LLM framework to different\nsensor datasets, including time-series data and visual data from sources like\nOxford Radar RobotCar, D-Behavior (D-Set), nuScenes by Motional, and Comma2k19.\nThe goal is to streamline data processing workflows, reduce the complexity of\ndeploying multiple models, and make intelligent transportation systems more\nefficient and accurate. The study was conducted using state-of-the-art\nhardware, leveraging the computational power of AMD RTX 3060 GPUs and Intel\ni9-12900 processors. The experimental results demonstrate that our framework\nachieves an average accuracy of 91.33\\% across these datasets, with the highest\naccuracy observed in time-series data (92.7\\%), showcasing the model's\nproficiency in handling sequential information essential for tasks such as\nmotion planning and predictive maintenance. Through our exploration, we\ndemonstrate the versatility and efficacy of LLMs in handling multimodal data\nwithin the transportation sector, ultimately providing insights into their\napplication in real-world scenarios. Our findings align with the broader\nconference themes, highlighting the transformative potential of LLMs in\nadvancing transportation technologies."
                },
                "authors": [
                    {
                        "name": "Dexter Le"
                    },
                    {
                        "name": "Aybars Yunusoglu"
                    },
                    {
                        "name": "Karn Tiwari"
                    },
                    {
                        "name": "Murat Isik"
                    },
                    {
                        "name": "I. Can Dikmen"
                    }
                ],
                "author_detail": {
                    "name": "I. Can Dikmen"
                },
                "author": "I. Can Dikmen",
                "arxiv_comment": "Accepted at IEEE Symposium Series on Computational Intelligence\n  (SSCI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11847v2",
                "updated": "2024-12-16T11:26:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    26,
                    21,
                    0,
                    351,
                    0
                ],
                "published": "2024-08-12T15:19:59Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    19,
                    59,
                    0,
                    225,
                    0
                ],
                "title": "Prompto: An open source library for asynchronous querying of LLM\n  endpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompto: An open source library for asynchronous querying of LLM\n  endpoints"
                },
                "summary": "Recent surge in Large Language Model (LLM) availability has opened exciting\navenues for research. However, efficiently interacting with these models\npresents a significant hurdle since LLMs often reside on proprietary or\nself-hosted API endpoints, each requiring custom code for interaction.\nConducting comparative studies between different models can therefore be\ntime-consuming and necessitate significant engineering effort, hindering\nresearch efficiency and reproducibility. To address these challenges, we\npresent prompto, an open source Python library which facilitates asynchronous\nquerying of LLM endpoints enabling researchers to interact with multiple LLMs\nconcurrently, while maximising efficiency and utilising individual rate limits.\nOur library empowers researchers and developers to interact with LLMs more\neffectively and allowing faster experimentation, data generation and\nevaluation. prompto is released with an introductory video\n(https://youtu.be/lWN9hXBOLyQ) under MIT License and is available via GitHub\n(https://github.com/alan-turing-institute/prompto).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent surge in Large Language Model (LLM) availability has opened exciting\navenues for research. However, efficiently interacting with these models\npresents a significant hurdle since LLMs often reside on proprietary or\nself-hosted API endpoints, each requiring custom code for interaction.\nConducting comparative studies between different models can therefore be\ntime-consuming and necessitate significant engineering effort, hindering\nresearch efficiency and reproducibility. To address these challenges, we\npresent prompto, an open source Python library which facilitates asynchronous\nquerying of LLM endpoints enabling researchers to interact with multiple LLMs\nconcurrently, while maximising efficiency and utilising individual rate limits.\nOur library empowers researchers and developers to interact with LLMs more\neffectively and allowing faster experimentation, data generation and\nevaluation. prompto is released with an introductory video\n(https://youtu.be/lWN9hXBOLyQ) under MIT License and is available via GitHub\n(https://github.com/alan-turing-institute/prompto)."
                },
                "authors": [
                    {
                        "name": "Ryan Sze-Yin Chan"
                    },
                    {
                        "name": "Federico Nanni"
                    },
                    {
                        "name": "Angus R. Williams"
                    },
                    {
                        "name": "Edwin Brown"
                    },
                    {
                        "name": "Liam Burke-Moore"
                    },
                    {
                        "name": "Ed Chapman"
                    },
                    {
                        "name": "Kate Onslow"
                    },
                    {
                        "name": "Tvesha Sippy"
                    },
                    {
                        "name": "Jonathan Bright"
                    },
                    {
                        "name": "Evelina Gabasova"
                    }
                ],
                "author_detail": {
                    "name": "Evelina Gabasova"
                },
                "author": "Evelina Gabasova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11672v1",
                "updated": "2024-12-16T11:25:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    25,
                    56,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:25:56Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    25,
                    56,
                    0,
                    351,
                    0
                ],
                "title": "LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User\n  Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User\n  Requests"
                },
                "summary": "We propose LLM-DaaS, a novel Drone-as-a-Service (DaaS) framework that\nleverages Large Language Models (LLMs) to transform free-text user requests\ninto structured, actionable DaaS operation tasks. Our approach addresses the\nkey challenge of interpreting and structuring natural language input to\nautomate drone service operations under uncertain conditions. The system is\ncomposed of three main components: free-text request processing, structured\nrequest generation, and dynamic DaaS selection and composition. First, we\nfine-tune different LLM models such as Phi-3.5, LLaMA-3.2 7b and Gemma 2b on a\ndataset of text user requests mapped to structured DaaS requests. Users\ninteract with our model in a free conversational style, discussing package\ndelivery requests, while the fine-tuned LLM extracts DaaS metadata such as\ndelivery time, source and destination locations, and package weight. The DaaS\nservice selection model is designed to select the best available drone capable\nof delivering the requested package from the delivery point to the nearest\noptimal destination. Additionally, the DaaS composition model composes a\nservice from a set of the best available drones to deliver the package from the\nsource to the final destination. Second, the system integrates real-time\nweather data to optimize drone route planning and scheduling, ensuring safe and\nefficient operations. Simulations demonstrate the system's ability to\nsignificantly improve task accuracy, operational efficiency, and establish\nLLM-DaaS as a robust solution for DaaS operations in uncertain environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LLM-DaaS, a novel Drone-as-a-Service (DaaS) framework that\nleverages Large Language Models (LLMs) to transform free-text user requests\ninto structured, actionable DaaS operation tasks. Our approach addresses the\nkey challenge of interpreting and structuring natural language input to\nautomate drone service operations under uncertain conditions. The system is\ncomposed of three main components: free-text request processing, structured\nrequest generation, and dynamic DaaS selection and composition. First, we\nfine-tune different LLM models such as Phi-3.5, LLaMA-3.2 7b and Gemma 2b on a\ndataset of text user requests mapped to structured DaaS requests. Users\ninteract with our model in a free conversational style, discussing package\ndelivery requests, while the fine-tuned LLM extracts DaaS metadata such as\ndelivery time, source and destination locations, and package weight. The DaaS\nservice selection model is designed to select the best available drone capable\nof delivering the requested package from the delivery point to the nearest\noptimal destination. Additionally, the DaaS composition model composes a\nservice from a set of the best available drones to deliver the package from the\nsource to the final destination. Second, the system integrates real-time\nweather data to optimize drone route planning and scheduling, ensuring safe and\nefficient operations. Simulations demonstrate the system's ability to\nsignificantly improve task accuracy, operational efficiency, and establish\nLLM-DaaS as a robust solution for DaaS operations in uncertain environments."
                },
                "authors": [
                    {
                        "name": "Lillian Wassim"
                    },
                    {
                        "name": "Kamal Mohamed"
                    },
                    {
                        "name": "Ali Hamdi"
                    }
                ],
                "author_detail": {
                    "name": "Ali Hamdi"
                },
                "author": "Ali Hamdi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13578v2",
                "updated": "2024-12-16T11:23:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    23,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-07-18T15:20:18Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    15,
                    20,
                    18,
                    3,
                    200,
                    0
                ],
                "title": "How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and\n  Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Reliable are LLMs as Knowledge Bases? Re-thinking Facutality and\n  Consistency"
                },
                "summary": "Large Language Models (LLMs) are increasingly explored as knowledge bases\n(KBs), yet current evaluation methods focus too narrowly on knowledge\nretention, overlooking other crucial criteria for reliable performance. In this\nwork, we rethink the requirements for evaluating reliable LLM-as-KB usage and\nhighlight two essential factors: factuality, ensuring accurate responses to\nseen and unseen knowledge, and consistency, maintaining stable answers to\nquestions about the same knowledge. We introduce UnseenQA, a dataset designed\nto assess LLM performance on unseen knowledge, and propose new criteria and\nmetrics to quantify factuality and consistency, leading to a final reliability\nscore. Our experiments on 26 LLMs reveal several challenges regarding their use\nas KBs, underscoring the need for more principled and comprehensive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly explored as knowledge bases\n(KBs), yet current evaluation methods focus too narrowly on knowledge\nretention, overlooking other crucial criteria for reliable performance. In this\nwork, we rethink the requirements for evaluating reliable LLM-as-KB usage and\nhighlight two essential factors: factuality, ensuring accurate responses to\nseen and unseen knowledge, and consistency, maintaining stable answers to\nquestions about the same knowledge. We introduce UnseenQA, a dataset designed\nto assess LLM performance on unseen knowledge, and propose new criteria and\nmetrics to quantify factuality and consistency, leading to a final reliability\nscore. Our experiments on 26 LLMs reveal several challenges regarding their use\nas KBs, underscoring the need for more principled and comprehensive evaluation."
                },
                "authors": [
                    {
                        "name": "Danna Zheng"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08707v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08707v7",
                "updated": "2024-12-16T11:21:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    21,
                    30,
                    0,
                    351,
                    0
                ],
                "published": "2024-04-11T17:44:56Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    44,
                    56,
                    3,
                    102,
                    0
                ],
                "title": "CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CEM: A Data-Efficient Method for Large Language Models to Continue\n  Evolving From Mistakes"
                },
                "summary": "As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As world knowledge advances and new task schemas emerge, Continual Learning\n(CL) becomes essential for keeping Large Language Models (LLMs) current and\naddressing their shortcomings. This process typically involves continual\ninstruction tuning (CIT) and continual pre-training (CPT) to enable these\nmodels to adapt to novel tasks and acquire critical knowledge. However,\ncollecting sufficient CPT data and efficiently bridging knowledge gaps remain\nsignificant challenges. Inspired by the 'summarizing mistakes' strategy, we\npropose the Continue Evolving from Mistakes (CEM) method, a data-efficient\napproach aiming to collect CPT data and continually improve LLMs' performance\nthrough iterative evaluation and supplementation with mistake-relevant\nknowledge. To further optimize data usage and mitigate forgetting, we introduce\na novel training paradigm that combines CIT and CPT. Experiments show that CEM\nsubstantially enhances multiple models' performance on both in-domain and\nout-of-domain QA tasks, achieving gains of up to 29.63%. Code and datasets are\navailable on https://anonymous.4open.science/r/cem-BB25."
                },
                "authors": [
                    {
                        "name": "Haokun Zhao"
                    },
                    {
                        "name": "Haixia Han"
                    },
                    {
                        "name": "Jie Shi"
                    },
                    {
                        "name": "Chengyu Du"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08707v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08707v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11664v1",
                "updated": "2024-12-16T11:12:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    12,
                    45,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:12:45Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    12,
                    45,
                    0,
                    351,
                    0
                ],
                "title": "C3oT: Generating Shorter Chain-of-Thought without Compromising\n  Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C3oT: Generating Shorter Chain-of-Thought without Compromising\n  Effectiveness"
                },
                "summary": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness."
                },
                "authors": [
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Xianghui Sun"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Wei Zou"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zou"
                },
                "author": "Wei Zou",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01704v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01704v3",
                "updated": "2024-12-16T11:03:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    3,
                    31,
                    0,
                    351,
                    0
                ],
                "published": "2024-01-24T22:22:00Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    22,
                    22,
                    0,
                    2,
                    24,
                    0
                ],
                "title": "Steering Language Models with Game-Theoretic Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Language Models with Game-Theoretic Solvers"
                },
                "summary": "Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research."
                },
                "authors": [
                    {
                        "name": "Ian Gemp"
                    },
                    {
                        "name": "Roma Patel"
                    },
                    {
                        "name": "Yoram Bachrach"
                    },
                    {
                        "name": "Marc Lanctot"
                    },
                    {
                        "name": "Vibhavari Dasagi"
                    },
                    {
                        "name": "Luke Marris"
                    },
                    {
                        "name": "Georgios Piliouras"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Karl Tuyls"
                    }
                ],
                "author_detail": {
                    "name": "Karl Tuyls"
                },
                "author": "Karl Tuyls",
                "arxiv_comment": "Code available @\n  https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01704v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11656v1",
                "updated": "2024-12-16T10:59:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    59,
                    49,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:59:49Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    59,
                    49,
                    0,
                    351,
                    0
                ],
                "title": "Private Yet Social: How LLM Chatbots Support and Challenge Eating\n  Disorder Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Yet Social: How LLM Chatbots Support and Challenge Eating\n  Disorder Recovery"
                },
                "summary": "Eating disorders (ED) are complex mental health conditions that require\nlong-term management and support. Recent advancements in large language model\n(LLM)-based chatbots offer the potential to assist individuals in receiving\nimmediate support. Yet, concerns remain about their reliability and safety in\nsensitive contexts such as ED. We explore the opportunities and potential harms\nof using LLM-based chatbots for ED recovery. We observe the interactions\nbetween 26 participants with ED and an LLM-based chatbot, WellnessBot, designed\nto support ED recovery, over 10 days. We discovered that our participants have\nfelt empowered in recovery by discussing ED-related stories with the chatbot,\nwhich served as a personal yet social avenue. However, we also identified\nharmful chatbot responses, especially concerning individuals with ED, that went\nunnoticed partly due to participants' unquestioning trust in the chatbot's\nreliability. Based on these findings, we provide design implications for safe\nand effective LLM-based interventions in ED management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eating disorders (ED) are complex mental health conditions that require\nlong-term management and support. Recent advancements in large language model\n(LLM)-based chatbots offer the potential to assist individuals in receiving\nimmediate support. Yet, concerns remain about their reliability and safety in\nsensitive contexts such as ED. We explore the opportunities and potential harms\nof using LLM-based chatbots for ED recovery. We observe the interactions\nbetween 26 participants with ED and an LLM-based chatbot, WellnessBot, designed\nto support ED recovery, over 10 days. We discovered that our participants have\nfelt empowered in recovery by discussing ED-related stories with the chatbot,\nwhich served as a personal yet social avenue. However, we also identified\nharmful chatbot responses, especially concerning individuals with ED, that went\nunnoticed partly due to participants' unquestioning trust in the chatbot's\nreliability. Based on these findings, we provide design implications for safe\nand effective LLM-based interventions in ED management."
                },
                "authors": [
                    {
                        "name": "Ryuhaerang Choi"
                    },
                    {
                        "name": "Taehan Kim"
                    },
                    {
                        "name": "Subin Park"
                    },
                    {
                        "name": "Jennifer G Kim"
                    },
                    {
                        "name": "Sung-Ju Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sung-Ju Lee"
                },
                "author": "Sung-Ju Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10666v3",
                "updated": "2024-12-16T10:48:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    48,
                    28,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-16T02:02:49Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    2,
                    2,
                    49,
                    5,
                    321,
                    0
                ],
                "title": "SAM Decoding: Speculative Decoding via Suffix Automaton",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM Decoding: Speculative Decoding via Suffix Automaton"
                },
                "summary": "Speculative decoding (SD) has been demonstrated as an effective technique for\nlossless LLM inference acceleration. Retrieval-based SD methods, one kind of\nmodel-free method, have yielded promising speedup, but they often rely on\nincomplete retrieval resources, inefficient retrieval methods, and are\nconstrained to certain domains. This paper presents a novel retrieval-based\nspeculative decoding method that adapts suffix automaton (SAM) for efficient\nand accurate draft generation by utilizing common text corpus and dynamic text\nsequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the\nexact longest suffix match, achieving an average time complexity of O(1) per\ngeneration step of SAM update and suffix retrieval. It can also integrate with\nexisting methods, adaptively selecting a draft generation strategy based on\nmatch length to generalize to broader domains. Extensive experiments on\nSpec-Bench show that our method is $18\\%+$ faster than other retrieval-based SD\nmethods. Additionally, when combined with advanced EAGLE-2, it provides an\nadditional speedup of $3.28\\%$ -- $11.13\\%$ across various-sized LLM backbones.\nOur code is available at our\n\\href{https://github.com/hyx1999/SAM-Decoding}{repository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been demonstrated as an effective technique for\nlossless LLM inference acceleration. Retrieval-based SD methods, one kind of\nmodel-free method, have yielded promising speedup, but they often rely on\nincomplete retrieval resources, inefficient retrieval methods, and are\nconstrained to certain domains. This paper presents a novel retrieval-based\nspeculative decoding method that adapts suffix automaton (SAM) for efficient\nand accurate draft generation by utilizing common text corpus and dynamic text\nsequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the\nexact longest suffix match, achieving an average time complexity of O(1) per\ngeneration step of SAM update and suffix retrieval. It can also integrate with\nexisting methods, adaptively selecting a draft generation strategy based on\nmatch length to generalize to broader domains. Extensive experiments on\nSpec-Bench show that our method is $18\\%+$ faster than other retrieval-based SD\nmethods. Additionally, when combined with advanced EAGLE-2, it provides an\nadditional speedup of $3.28\\%$ -- $11.13\\%$ across various-sized LLM backbones.\nOur code is available at our\n\\href{https://github.com/hyx1999/SAM-Decoding}{repository}."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Fanjin Zhang"
                    },
                    {
                        "name": "Cuiping Li"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "arxiv_comment": "16 pages, 9 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18541v2",
                "updated": "2024-12-16T10:33:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    33,
                    44,
                    0,
                    351,
                    0
                ],
                "published": "2024-09-27T08:20:59Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    20,
                    59,
                    4,
                    271,
                    0
                ],
                "title": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference\n  Alignment for Multi-modal Instruction Curation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align$^2$LLaVA: Cascaded Human and Large Language Model Preference\n  Alignment for Multi-modal Instruction Curation"
                },
                "summary": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the training instructions from 158k to\n14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Multi-modal Large Language Models (MLLMs), such as\nLLaVA-series models, are driven by massive machine-generated\ninstruction-following data tuning. Such automatic instruction collection\npipelines, however, inadvertently introduce significant variability in data\nquality. This paper introduces a novel instruction curation algorithm, derived\nfrom two unique perspectives, human and LLM preference alignment, to compress\nthis vast corpus of machine-generated multimodal instructions to a compact and\nhigh-quality form: (i) For human preference alignment, we have collected a\nmachine-generated multimodal instruction dataset and established a\ncomprehensive set of both subjective and objective criteria to guide the data\nquality assessment critically from human experts. By doing so, a reward model\nwas trained on the annotated dataset to internalize the nuanced human\nunderstanding of instruction alignment. (ii) For LLM preference alignment,\ngiven the instruction selected by the reward model, we propose leveraging the\ninner LLM used in MLLM to align the writing style of visual instructions with\nthat of the inner LLM itself, resulting in LLM-aligned instruction improvement.\nExtensive experiments demonstrate that we can maintain or even improve model\nperformance by compressing synthetic multimodal instructions by up to 90%.\nImpressively, by aggressively reducing the training instructions from 158k to\n14k (9$\\times$ smaller), our model consistently outperforms its full-size\ndataset counterpart across various MLLM benchmarks. Our project is available at\nhttps://github.com/DCDmllm/Align2LLaVA."
                },
                "authors": [
                    {
                        "name": "Hongzhe Huang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Zhewen Yu"
                    },
                    {
                        "name": "Li Cai"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11629v1",
                "updated": "2024-12-16T10:14:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    14,
                    1,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:14:01Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    14,
                    1,
                    0,
                    351,
                    0
                ],
                "title": "QPruner: Probabilistic Decision Quantization for Structured Pruning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPruner: Probabilistic Decision Quantization for Structured Pruning in\n  Large Language Models"
                },
                "summary": "The rise of large language models (LLMs) has significantly advanced various\nnatural language processing (NLP) tasks. However, the resource demands of these\nmodels pose substantial challenges. Structured pruning is an effective approach\nto reducing model size, but it often results in significant accuracy\ndegradation, necessitating parameter updates to adapt. Unfortunately, such\nfine-tuning requires substantial memory, which limits its applicability. To\naddress these challenges, we introduce quantization into the structured pruning\nframework to reduce memory consumption during both fine-tuning and inference.\nHowever, the combined errors from pruning and quantization increase the\ndifficulty of fine-tuning, requiring a more refined quantization scheme. To\nthis end, we propose QPruner, a novel framework that employs structured pruning\nto reduce model size, followed by a layer-wise mixed-precision quantization\nscheme. Quantization precisions are assigned to each layer based on their\nimportance to the target task, and Bayesian optimization is employed to refine\nprecision allocation strategies, ensuring a balance between model accuracy and\nmemory efficiency. Extensive experiments on benchmark datasets demonstrate that\nQPruner significantly outperforms existing methods in memory savings while\nmaintaining or improving model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has significantly advanced various\nnatural language processing (NLP) tasks. However, the resource demands of these\nmodels pose substantial challenges. Structured pruning is an effective approach\nto reducing model size, but it often results in significant accuracy\ndegradation, necessitating parameter updates to adapt. Unfortunately, such\nfine-tuning requires substantial memory, which limits its applicability. To\naddress these challenges, we introduce quantization into the structured pruning\nframework to reduce memory consumption during both fine-tuning and inference.\nHowever, the combined errors from pruning and quantization increase the\ndifficulty of fine-tuning, requiring a more refined quantization scheme. To\nthis end, we propose QPruner, a novel framework that employs structured pruning\nto reduce model size, followed by a layer-wise mixed-precision quantization\nscheme. Quantization precisions are assigned to each layer based on their\nimportance to the target task, and Bayesian optimization is employed to refine\nprecision allocation strategies, ensuring a balance between model accuracy and\nmemory efficiency. Extensive experiments on benchmark datasets demonstrate that\nQPruner significantly outperforms existing methods in memory savings while\nmaintaining or improving model performance."
                },
                "authors": [
                    {
                        "name": "Changhai Zhou"
                    },
                    {
                        "name": "Yuhua Zhou"
                    },
                    {
                        "name": "Shijie Han"
                    },
                    {
                        "name": "Qian Qiao"
                    },
                    {
                        "name": "Hongguang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Li"
                },
                "author": "Hongguang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11625v1",
                "updated": "2024-12-16T10:10:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    10,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:10:27Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    10,
                    27,
                    0,
                    351,
                    0
                ],
                "title": "Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fool Me, Fool Me: User Attitudes Toward LLM Falsehoods"
                },
                "summary": "While Large Language Models (LLMs) have become central tools in various\nfields, they often provide inaccurate or false information. This study examines\nuser preferences regarding falsehood responses from LLMs. Specifically, we\nevaluate preferences for LLM responses where false statements are explicitly\nmarked versus unmarked responses and preferences for confident falsehoods\ncompared to LLM disclaimers acknowledging a lack of knowledge. Additionally, we\ninvestigate how requiring users to assess the truthfulness of statements\ninfluences these preferences.\n  Surprisingly, 61\\% of users prefer unmarked falsehood responses over marked\nones, and 69\\% prefer confident falsehoods over LLMs admitting lack of\nknowledge. In all our experiments, a total of 300 users participated,\ncontributing valuable data to our analysis and conclusions. When users are\nrequired to evaluate the truthfulness of statements, preferences for unmarked\nand falsehood responses decrease slightly but remain high. These findings\nsuggest that user preferences, which influence LLM training via feedback\nmechanisms, may inadvertently encourage the generation of falsehoods. Future\nresearch should address the ethical and practical implications of aligning LLM\nbehavior with such preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have become central tools in various\nfields, they often provide inaccurate or false information. This study examines\nuser preferences regarding falsehood responses from LLMs. Specifically, we\nevaluate preferences for LLM responses where false statements are explicitly\nmarked versus unmarked responses and preferences for confident falsehoods\ncompared to LLM disclaimers acknowledging a lack of knowledge. Additionally, we\ninvestigate how requiring users to assess the truthfulness of statements\ninfluences these preferences.\n  Surprisingly, 61\\% of users prefer unmarked falsehood responses over marked\nones, and 69\\% prefer confident falsehoods over LLMs admitting lack of\nknowledge. In all our experiments, a total of 300 users participated,\ncontributing valuable data to our analysis and conclusions. When users are\nrequired to evaluate the truthfulness of statements, preferences for unmarked\nand falsehood responses decrease slightly but remain high. These findings\nsuggest that user preferences, which influence LLM training via feedback\nmechanisms, may inadvertently encourage the generation of falsehoods. Future\nresearch should address the ethical and practical implications of aligning LLM\nbehavior with such preferences."
                },
                "authors": [
                    {
                        "name": "Diana Bar-Or Nirman"
                    },
                    {
                        "name": "Ariel Weizman"
                    },
                    {
                        "name": "Amos Azaria"
                    }
                ],
                "author_detail": {
                    "name": "Amos Azaria"
                },
                "author": "Amos Azaria",
                "arxiv_comment": "11 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12060v2",
                "updated": "2024-12-16T10:09:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    9,
                    23,
                    0,
                    351,
                    0
                ],
                "published": "2024-09-18T15:33:48Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    33,
                    48,
                    2,
                    262,
                    0
                ],
                "title": "PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase\n  Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase\n  Detection Models"
                },
                "summary": "The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,\nbenchmarking and selection of paraphrase detection models. We find that\nparaphrase detection models under our fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\nFurthermore, PARAPHRASUS allows prompt calibration for different use cases,\ntailoring LLM models to specific strictness levels. PARAPHRASUS includes 3\nchallenges spanning over 10 datasets, including 8 repurposed and 2 newly\nannotated; we release it along with a benchmarking library at\nhttps://github.com/impresso/paraphrasus",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of determining whether two texts are paraphrases has long been a\nchallenge in NLP. However, the prevailing notion of paraphrase is often quite\nsimplistic, offering only a limited view of the vast spectrum of paraphrase\nphenomena. Indeed, we find that evaluating models in a paraphrase dataset can\nleave uncertainty about their true semantic understanding. To alleviate this,\nwe create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,\nbenchmarking and selection of paraphrase detection models. We find that\nparaphrase detection models under our fine-grained evaluation lens exhibit\ntrade-offs that cannot be captured through a single classification dataset.\nFurthermore, PARAPHRASUS allows prompt calibration for different use cases,\ntailoring LLM models to specific strictness levels. PARAPHRASUS includes 3\nchallenges spanning over 10 datasets, including 8 repurposed and 2 newly\nannotated; we release it along with a benchmarking library at\nhttps://github.com/impresso/paraphrasus"
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Juri Opitz"
                    }
                ],
                "author_detail": {
                    "name": "Juri Opitz"
                },
                "author": "Juri Opitz",
                "arxiv_comment": "to appear at COLING2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11621v1",
                "updated": "2024-12-16T10:08:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    8,
                    38,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:08:38Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    8,
                    38,
                    0,
                    351,
                    0
                ],
                "title": "VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video\n  Prompting"
                },
                "summary": "Large Language Model (LLM)-based agents have shown promise in procedural\ntasks, but the potential of multimodal instructions augmented by texts and\nvideos to assist users remains under-explored. To address this gap, we propose\nthe Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel\nLLM-empowered Multimodal Procedural Planning (MPP) framework. It generates\ncohesive text and video procedural plans given a specified high-level\nobjective. The main challenges are achieving textual and visual\ninformativeness, temporal coherence, and accuracy in procedural plans. VG-TVP\nleverages the zero-shot reasoning capability of LLMs, the video-to-text\ngeneration ability of the video captioning models, and the text-to-video\ngeneration ability of diffusion models. VG-TVP improves the interaction between\nmodalities by proposing a novel Fusion of Captioning (FoC) method and using\nText-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs\nto guide the generation of visually-grounded text plans and textual-grounded\nvideo plans. To address the scarcity of datasets suitable for MPP, we have\ncurated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We\nconduct comprehensive experiments and benchmarks to evaluate human preferences\n(regarding textual and visual informativeness, temporal coherence, and plan\naccuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have shown promise in procedural\ntasks, but the potential of multimodal instructions augmented by texts and\nvideos to assist users remains under-explored. To address this gap, we propose\nthe Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel\nLLM-empowered Multimodal Procedural Planning (MPP) framework. It generates\ncohesive text and video procedural plans given a specified high-level\nobjective. The main challenges are achieving textual and visual\ninformativeness, temporal coherence, and accuracy in procedural plans. VG-TVP\nleverages the zero-shot reasoning capability of LLMs, the video-to-text\ngeneration ability of the video captioning models, and the text-to-video\ngeneration ability of diffusion models. VG-TVP improves the interaction between\nmodalities by proposing a novel Fusion of Captioning (FoC) method and using\nText-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs\nto guide the generation of visually-grounded text plans and textual-grounded\nvideo plans. To address the scarcity of datasets suitable for MPP, we have\ncurated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We\nconduct comprehensive experiments and benchmarks to evaluate human preferences\n(regarding textual and visual informativeness, temporal coherence, and plan\naccuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP\ndataset."
                },
                "authors": [
                    {
                        "name": "Muhammet Furkan Ilaslan"
                    },
                    {
                        "name": "Ali Koksal"
                    },
                    {
                        "name": "Kevin Qinhong Lin"
                    },
                    {
                        "name": "Burak Satar"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Qianli Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Xu"
                },
                "author": "Qianli Xu",
                "arxiv_comment": "Accepted for The 39th Annual AAAI Conference on Artificial\n  Intelligence 2025 in Main Track, 19 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11618v1",
                "updated": "2024-12-16T10:01:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    1,
                    33,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T10:01:33Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    1,
                    33,
                    0,
                    351,
                    0
                ],
                "title": "EvoLlama: Enhancing LLMs' Understanding of Proteins via Multimodal\n  Structure and Sequence Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoLlama: Enhancing LLMs' Understanding of Proteins via Multimodal\n  Structure and Sequence Representations"
                },
                "summary": "Current Large Language Models (LLMs) for understanding proteins primarily\ntreats amino acid sequences as a text modality. Meanwhile, Protein Language\nModels (PLMs), such as ESM-2, have learned massive sequential evolutionary\nknowledge from the universe of natural protein sequences. Furthermore,\nstructure-based encoders like ProteinMPNN learn the structural information of\nproteins through Graph Neural Networks. However, whether the incorporation of\nprotein encoders can enhance the protein understanding of LLMs has not been\nexplored. To bridge this gap, we propose EvoLlama, a multimodal framework that\nconnects a structure-based encoder, a sequence-based protein encoder and an LLM\nfor protein understanding. EvoLlama consists of a ProteinMPNN structure\nencoder, an ESM-2 protein sequence encoder, a multimodal projector to align\nprotein and text representations and a Llama-3 text decoder. To train EvoLlama,\nwe fine-tune it on protein-oriented instructions and protein property\nprediction datasets verbalized via natural language instruction templates. Our\nexperiments show that EvoLlama's protein understanding capabilities have been\nsignificantly enhanced, outperforming other fine-tuned protein-oriented LLMs in\nzero-shot settings by an average of 1%-8% and surpassing the state-of-the-art\nbaseline with supervised fine-tuning by an average of 6%. On protein property\nprediction datasets, our approach achieves promising results that are\ncompetitive with state-of-the-art task-specific baselines. We will release our\ncode in a future version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) for understanding proteins primarily\ntreats amino acid sequences as a text modality. Meanwhile, Protein Language\nModels (PLMs), such as ESM-2, have learned massive sequential evolutionary\nknowledge from the universe of natural protein sequences. Furthermore,\nstructure-based encoders like ProteinMPNN learn the structural information of\nproteins through Graph Neural Networks. However, whether the incorporation of\nprotein encoders can enhance the protein understanding of LLMs has not been\nexplored. To bridge this gap, we propose EvoLlama, a multimodal framework that\nconnects a structure-based encoder, a sequence-based protein encoder and an LLM\nfor protein understanding. EvoLlama consists of a ProteinMPNN structure\nencoder, an ESM-2 protein sequence encoder, a multimodal projector to align\nprotein and text representations and a Llama-3 text decoder. To train EvoLlama,\nwe fine-tune it on protein-oriented instructions and protein property\nprediction datasets verbalized via natural language instruction templates. Our\nexperiments show that EvoLlama's protein understanding capabilities have been\nsignificantly enhanced, outperforming other fine-tuned protein-oriented LLMs in\nzero-shot settings by an average of 1%-8% and surpassing the state-of-the-art\nbaseline with supervised fine-tuning by an average of 6%. On protein property\nprediction datasets, our approach achieves promising results that are\ncompetitive with state-of-the-art task-specific baselines. We will release our\ncode in a future version."
                },
                "authors": [
                    {
                        "name": "Nuowei Liu"
                    },
                    {
                        "name": "Changzhi Sun"
                    },
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Junfeng Tian"
                    },
                    {
                        "name": "Jianxin Tang"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Man Lan"
                    }
                ],
                "author_detail": {
                    "name": "Man Lan"
                },
                "author": "Man Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11615v1",
                "updated": "2024-12-16T09:57:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    57,
                    28,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T09:57:28Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    57,
                    28,
                    0,
                    351,
                    0
                ],
                "title": "MT-LENS: An all-in-one Toolkit for Better Machine Translation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-LENS: An all-in-one Toolkit for Better Machine Translation Evaluation"
                },
                "summary": "We introduce MT-LENS, a framework designed to evaluate Machine Translation\n(MT) systems across a variety of tasks, including translation quality, gender\nbias detection, added toxicity, and robustness to misspellings. While several\ntoolkits have become very popular for benchmarking the capabilities of Large\nLanguage Models (LLMs), existing evaluation tools often lack the ability to\nthoroughly assess the diverse aspects of MT performance. MT-LENS addresses\nthese limitations by extending the capabilities of LM-eval-harness for MT,\nsupporting state-of-the-art datasets and a wide range of evaluation metrics. It\nalso offers a user-friendly platform to compare systems and analyze\ntranslations with interactive visualizations. MT-LENS aims to broaden access to\nevaluation strategies that go beyond traditional translation quality\nevaluation, enabling researchers and engineers to better understand the\nperformance of a NMT model and also easily measure system's biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MT-LENS, a framework designed to evaluate Machine Translation\n(MT) systems across a variety of tasks, including translation quality, gender\nbias detection, added toxicity, and robustness to misspellings. While several\ntoolkits have become very popular for benchmarking the capabilities of Large\nLanguage Models (LLMs), existing evaluation tools often lack the ability to\nthoroughly assess the diverse aspects of MT performance. MT-LENS addresses\nthese limitations by extending the capabilities of LM-eval-harness for MT,\nsupporting state-of-the-art datasets and a wide range of evaluation metrics. It\nalso offers a user-friendly platform to compare systems and analyze\ntranslations with interactive visualizations. MT-LENS aims to broaden access to\nevaluation strategies that go beyond traditional translation quality\nevaluation, enabling researchers and engineers to better understand the\nperformance of a NMT model and also easily measure system's biases."
                },
                "authors": [
                    {
                        "name": "Javier Garca Gilabert"
                    },
                    {
                        "name": "Carlos Escolano"
                    },
                    {
                        "name": "Audrey Mash"
                    },
                    {
                        "name": "Xixian Liao"
                    },
                    {
                        "name": "Maite Melero"
                    }
                ],
                "author_detail": {
                    "name": "Maite Melero"
                },
                "author": "Maite Melero",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06618v2",
                "updated": "2024-12-16T09:52:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    52,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-10-09T07:14:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    14,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval"
                },
                "summary": "Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity."
                },
                "authors": [
                    {
                        "name": "Jian Xiao"
                    },
                    {
                        "name": "Zhenzhen Hu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Richang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Richang Hong"
                },
                "author": "Richang Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11605v1",
                "updated": "2024-12-16T09:47:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    47,
                    43,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T09:47:43Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    47,
                    43,
                    0,
                    351,
                    0
                ],
                "title": "SPaR: Self-Play with Tree-Search Refinement to Improve\n  Instruction-Following in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPaR: Self-Play with Tree-Search Refinement to Improve\n  Instruction-Following in Large Language Models"
                },
                "summary": "Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability and transferability,\ngreatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how\ninference scaling in tree search would impact model performance. Our code and\ndata are publicly available at https://github.com/thu-coai/SPaR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability and transferability,\ngreatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how\ninference scaling in tree search would impact model performance. Our code and\ndata are publicly available at https://github.com/thu-coai/SPaR."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Yida Lu"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11576v1",
                "updated": "2024-12-16T09:04:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    4,
                    58,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T09:04:58Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    9,
                    4,
                    58,
                    0,
                    351,
                    0
                ],
                "title": "Aligning Visual and Semantic Interpretability through Visually Grounded\n  Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Visual and Semantic Interpretability through Visually Grounded\n  Concept Bottleneck Models"
                },
                "summary": "The performance of neural networks increases steadily, but our understanding\nof their decision-making lags behind. Concept Bottleneck Models (CBMs) address\nthis issue by incorporating human-understandable concepts into the prediction\nprocess, thereby enhancing transparency and interpretability. Since existing\napproaches often rely on large language models (LLMs) to infer concepts, their\nresults may contain inaccurate or incomplete mappings, especially in complex\nvisual domains. We introduce visually Grounded Concept Bottleneck Models\n(GCBM), which derive concepts on the image level using segmentation and\ndetection foundation models. Our method generates inherently interpretable\nconcepts, which can be grounded in the input image using attribution methods,\nallowing interpretations to be traced back to the image plane. We show that\nGCBM concepts are meaningful interpretability vehicles, which aid our\nunderstanding of model embedding spaces. GCBMs allow users to control the\ngranularity, number, and naming of concepts, providing flexibility and are\neasily adaptable to new datasets without pre-training or additional data\nneeded. Prediction accuracy is within 0.3-6% of the linear probe and GCBMs\nperform especially well for fine-grained classification interpretability on\nCUB, due to their dataset specificity. Our code is available on\nhttps://github.com/KathPra/GCBM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of neural networks increases steadily, but our understanding\nof their decision-making lags behind. Concept Bottleneck Models (CBMs) address\nthis issue by incorporating human-understandable concepts into the prediction\nprocess, thereby enhancing transparency and interpretability. Since existing\napproaches often rely on large language models (LLMs) to infer concepts, their\nresults may contain inaccurate or incomplete mappings, especially in complex\nvisual domains. We introduce visually Grounded Concept Bottleneck Models\n(GCBM), which derive concepts on the image level using segmentation and\ndetection foundation models. Our method generates inherently interpretable\nconcepts, which can be grounded in the input image using attribution methods,\nallowing interpretations to be traced back to the image plane. We show that\nGCBM concepts are meaningful interpretability vehicles, which aid our\nunderstanding of model embedding spaces. GCBMs allow users to control the\ngranularity, number, and naming of concepts, providing flexibility and are\neasily adaptable to new datasets without pre-training or additional data\nneeded. Prediction accuracy is within 0.3-6% of the linear probe and GCBMs\nperform especially well for fine-grained classification interpretability on\nCUB, due to their dataset specificity. Our code is available on\nhttps://github.com/KathPra/GCBM."
                },
                "authors": [
                    {
                        "name": "Patrick Knab"
                    },
                    {
                        "name": "Katharina Prasse"
                    },
                    {
                        "name": "Sascha Marton"
                    },
                    {
                        "name": "Christian Bartelt"
                    },
                    {
                        "name": "Margret Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Margret Keuper"
                },
                "author": "Margret Keuper",
                "arxiv_comment": "*Equal contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15395v2",
                "updated": "2024-12-16T08:57:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    57,
                    29,
                    0,
                    351,
                    0
                ],
                "published": "2023-12-24T03:37:11Z",
                "published_parsed": [
                    2023,
                    12,
                    24,
                    3,
                    37,
                    11,
                    6,
                    358,
                    0
                ],
                "title": "Prompt Valuation Based on Shapley Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Valuation Based on Shapley Values"
                },
                "summary": "Large language models (LLMs) excel on new tasks without additional training,\nsimply by providing natural language prompts that demonstrate how the task\nshould be performed. Prompt ensemble methods comprehensively harness the\nknowledge of LLMs while mitigating individual biases and errors and further\nenhancing performance. However, more prompts do not necessarily lead to better\nresults, and not all prompts are beneficial. A small number of high-quality\nprompts often outperform many low-quality prompts. Currently, there is a lack\nof a suitable method for evaluating the impact of prompts on the results. In\nthis paper, we utilize the Shapley value to fairly quantify the contributions\nof prompts, helping to identify beneficial or detrimental prompts, and\npotentially guiding prompt valuation in data markets. Through extensive\nexperiments employing various ensemble methods and utility functions on diverse\ntasks, we validate the effectiveness of using the Shapley value method for\nprompts as it effectively distinguishes and quantifies the contributions of\neach prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel on new tasks without additional training,\nsimply by providing natural language prompts that demonstrate how the task\nshould be performed. Prompt ensemble methods comprehensively harness the\nknowledge of LLMs while mitigating individual biases and errors and further\nenhancing performance. However, more prompts do not necessarily lead to better\nresults, and not all prompts are beneficial. A small number of high-quality\nprompts often outperform many low-quality prompts. Currently, there is a lack\nof a suitable method for evaluating the impact of prompts on the results. In\nthis paper, we utilize the Shapley value to fairly quantify the contributions\nof prompts, helping to identify beneficial or detrimental prompts, and\npotentially guiding prompt valuation in data markets. Through extensive\nexperiments employing various ensemble methods and utility functions on diverse\ntasks, we validate the effectiveness of using the Shapley value method for\nprompts as it effectively distinguishes and quantifies the contributions of\neach prompt."
                },
                "authors": [
                    {
                        "name": "Hanxi Liu"
                    },
                    {
                        "name": "Xiaokai Mao"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jian Lou"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11560v1",
                "updated": "2024-12-16T08:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    46,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T08:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    46,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "The Role of Natural Language Processing Tasks in Automatic Literary\n  Character Network Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Natural Language Processing Tasks in Automatic Literary\n  Character Network Construction"
                },
                "summary": "The automatic extraction of character networks from literary texts is\ngenerally carried out using natural language processing (NLP) cascading\npipelines. While this approach is widespread, no study exists on the impact of\nlow-level NLP tasks on their performance. In this article, we conduct such a\nstudy on a literary dataset, focusing on the role of named entity recognition\n(NER) and coreference resolution when extracting co-occurrence networks. To\nhighlight the impact of these tasks' performance, we start with gold-standard\nannotations, progressively add uniformly distributed errors, and observe their\nimpact in terms of character network quality. We demonstrate that NER\nperformance depends on the tested novel and strongly affects character\ndetection. We also show that NER-detected mentions alone miss a lot of\ncharacter co-occurrences, and that coreference resolution is needed to prevent\nthis. Finally, we present comparison points with 2 methods based on large\nlanguage models (LLMs), including a fully end-to-end one, and show that these\nmodels are outperformed by traditional NLP pipelines in terms of recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic extraction of character networks from literary texts is\ngenerally carried out using natural language processing (NLP) cascading\npipelines. While this approach is widespread, no study exists on the impact of\nlow-level NLP tasks on their performance. In this article, we conduct such a\nstudy on a literary dataset, focusing on the role of named entity recognition\n(NER) and coreference resolution when extracting co-occurrence networks. To\nhighlight the impact of these tasks' performance, we start with gold-standard\nannotations, progressively add uniformly distributed errors, and observe their\nimpact in terms of character network quality. We demonstrate that NER\nperformance depends on the tested novel and strongly affects character\ndetection. We also show that NER-detected mentions alone miss a lot of\ncharacter co-occurrences, and that coreference resolution is needed to prevent\nthis. Finally, we present comparison points with 2 methods based on large\nlanguage models (LLMs), including a fully end-to-end one, and show that these\nmodels are outperformed by traditional NLP pipelines in terms of recall."
                },
                "authors": [
                    {
                        "name": "Arthur Amalvy"
                    },
                    {
                        "name": "Vincent Labatut"
                    },
                    {
                        "name": "Richard Dufour"
                    }
                ],
                "author_detail": {
                    "name": "Richard Dufour"
                },
                "arxiv_affiliation": "LS2N - quipe TALN",
                "author": "Richard Dufour",
                "arxiv_journal_ref": "31st International Conference on Computational Linguistics, Jan\n  2025, Abu Dhabi, France",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06561v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06561v4",
                "updated": "2024-12-16T08:43:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    43,
                    24,
                    0,
                    351,
                    0
                ],
                "published": "2024-01-12T13:15:05Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    13,
                    15,
                    5,
                    4,
                    12,
                    0
                ],
                "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intention Analysis Makes LLMs A Good Jailbreak Defender"
                },
                "summary": "Aligning large language models (LLMs) with human values, particularly when\nfacing complex and stealthy jailbreak attacks, presents a formidable challenge.\nUnfortunately, existing methods often overlook this intrinsic nature of\njailbreaks, which limits their effectiveness in such complex scenarios. In this\nstudy, we present a simple yet highly effective defense strategy, i.e.,\nIntention Analysis ($\\mathbb{IA}$). $\\mathbb{IA}$ works by triggering LLMs'\ninherent self-correct and improve ability through a two-stage process: 1)\nanalyzing the essential intention of the user input, and 2) providing final\npolicy-aligned responses based on the first round conversation. Notably,\n$\\mathbb{IA}$ is an inference-only method, thus could enhance LLM safety\nwithout compromising their helpfulness. Extensive experiments on varying\njailbreak benchmarks across a wide range of LLMs show that $\\mathbb{IA}$ could\nconsistently and significantly reduce the harmfulness in responses (averagely\n-48.2% attack success rate). Encouragingly, with our $\\mathbb{IA}$, Vicuna-7B\neven outperforms GPT-3.5 regarding attack success rate. We empirically\ndemonstrate that, to some extent, $\\mathbb{IA}$ is robust to errors in\ngenerated intentions. Further analyses reveal the underlying principle of\n$\\mathbb{IA}$: suppressing LLM's tendency to follow jailbreak prompts, thereby\nenhancing safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human values, particularly when\nfacing complex and stealthy jailbreak attacks, presents a formidable challenge.\nUnfortunately, existing methods often overlook this intrinsic nature of\njailbreaks, which limits their effectiveness in such complex scenarios. In this\nstudy, we present a simple yet highly effective defense strategy, i.e.,\nIntention Analysis ($\\mathbb{IA}$). $\\mathbb{IA}$ works by triggering LLMs'\ninherent self-correct and improve ability through a two-stage process: 1)\nanalyzing the essential intention of the user input, and 2) providing final\npolicy-aligned responses based on the first round conversation. Notably,\n$\\mathbb{IA}$ is an inference-only method, thus could enhance LLM safety\nwithout compromising their helpfulness. Extensive experiments on varying\njailbreak benchmarks across a wide range of LLMs show that $\\mathbb{IA}$ could\nconsistently and significantly reduce the harmfulness in responses (averagely\n-48.2% attack success rate). Encouragingly, with our $\\mathbb{IA}$, Vicuna-7B\neven outperforms GPT-3.5 regarding attack success rate. We empirically\ndemonstrate that, to some extent, $\\mathbb{IA}$ is robust to errors in\ngenerated intentions. Further analyses reveal the underlying principle of\n$\\mathbb{IA}$: suppressing LLM's tendency to follow jailbreak prompts, thereby\nenhancing safety."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06561v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06561v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11557v1",
                "updated": "2024-12-16T08:42:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    42,
                    43,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T08:42:43Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    42,
                    43,
                    0,
                    351,
                    0
                ],
                "title": "Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based\n  MOE Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based\n  MOE Architecture"
                },
                "summary": "With the increasing availability of multimodal data, many fields urgently\nrequire advanced architectures capable of effectively integrating these diverse\ndata sources to address specific problems. This study proposes a hybrid\nrecommendation model that combines the Mixture of Experts (MOE) framework with\nlarge language models to enhance the performance of recommendation systems in\nthe healthcare domain. We built a small dataset for recommending healthy food\nbased on patient descriptions and evaluated the model's performance on several\nkey metrics, including Precision, Recall, NDCG, and MAP@5. The experimental\nresults show that the hybrid model outperforms the baseline models, which use\nMOE or large language models individually, in terms of both accuracy and\npersonalized recommendation effectiveness. The paper finds image data provided\nrelatively limited improvement in the performance of the personalized\nrecommendation system, particularly in addressing the cold start problem. Then,\nthe issue of reclassification of images also affected the recommendation\nresults, especially when dealing with low-quality images or changes in the\nappearance of items, leading to suboptimal performance. The findings provide\nvaluable insights into the development of powerful, scalable, and\nhigh-performance recommendation systems, advancing the application of\npersonalized recommendation technologies in real-world domains such as\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing availability of multimodal data, many fields urgently\nrequire advanced architectures capable of effectively integrating these diverse\ndata sources to address specific problems. This study proposes a hybrid\nrecommendation model that combines the Mixture of Experts (MOE) framework with\nlarge language models to enhance the performance of recommendation systems in\nthe healthcare domain. We built a small dataset for recommending healthy food\nbased on patient descriptions and evaluated the model's performance on several\nkey metrics, including Precision, Recall, NDCG, and MAP@5. The experimental\nresults show that the hybrid model outperforms the baseline models, which use\nMOE or large language models individually, in terms of both accuracy and\npersonalized recommendation effectiveness. The paper finds image data provided\nrelatively limited improvement in the performance of the personalized\nrecommendation system, particularly in addressing the cold start problem. Then,\nthe issue of reclassification of images also affected the recommendation\nresults, especially when dealing with low-quality images or changes in the\nappearance of items, leading to suboptimal performance. The findings provide\nvaluable insights into the development of powerful, scalable, and\nhigh-performance recommendation systems, advancing the application of\npersonalized recommendation technologies in real-world domains such as\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Jingyu Xu"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "10 page, accpted by Conf-SMPL conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11556v1",
                "updated": "2024-12-16T08:42:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    42,
                    0,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T08:42:00Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    42,
                    0,
                    0,
                    351,
                    0
                ],
                "title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence\n  Embeddings from LLMs"
                },
                "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost."
                },
                "authors": [
                    {
                        "name": "Yuchen Fu"
                    },
                    {
                        "name": "Zifeng Cheng"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Zhonghui Wang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Zhengliang Li"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15734v2",
                "updated": "2024-12-16T08:19:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    19,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-06-22T04:52:58Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    4,
                    52,
                    58,
                    5,
                    174,
                    0
                ],
                "title": "RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning\n  Pruned LLMs via Performance Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning\n  Pruned LLMs via Performance Model"
                },
                "summary": "The efficient compression of large language models (LLMs) has become\nincreasingly popular. However, recovering the performance of compressed LLMs\nremains a major challenge. The current practice in LLM compression entails the\nimplementation of structural pruning, complemented by a recovery phase that\nleverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning's uneven\nmodification of model architecture, coupled with standard LoRA's fixed\nconfiguration allocation across layers in an online pipeline, leads to\nsuboptimal performance in various downstream tasks for pruned models. To\naddress this challenge, we introduce RankAdaptor, a hierarchical rank\nallocation method that enables efficient fine-tuning of pruned LLMs according\nto layerwise specific recovery requirements. We employ a performance model that\nconducts offline meta-learning and online incremental learning to explore\noptimal rank values for each layer. Comprehensive experiments on popular\nbenchmarks show that RankAdaptor consistently outperforms state-of-the-art\nmethods across a variety of pruning settings and LLM architectures, with\nimprovements ranging from 0.7\\% to 5.5\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient compression of large language models (LLMs) has become\nincreasingly popular. However, recovering the performance of compressed LLMs\nremains a major challenge. The current practice in LLM compression entails the\nimplementation of structural pruning, complemented by a recovery phase that\nleverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning's uneven\nmodification of model architecture, coupled with standard LoRA's fixed\nconfiguration allocation across layers in an online pipeline, leads to\nsuboptimal performance in various downstream tasks for pruned models. To\naddress this challenge, we introduce RankAdaptor, a hierarchical rank\nallocation method that enables efficient fine-tuning of pruned LLMs according\nto layerwise specific recovery requirements. We employ a performance model that\nconducts offline meta-learning and online incremental learning to explore\noptimal rank values for each layer. Comprehensive experiments on popular\nbenchmarks show that RankAdaptor consistently outperforms state-of-the-art\nmethods across a variety of pruning settings and LLM architectures, with\nimprovements ranging from 0.7\\% to 5.5\\%."
                },
                "authors": [
                    {
                        "name": "Changhai Zhou"
                    },
                    {
                        "name": "Shijie Han"
                    },
                    {
                        "name": "Lining Yang"
                    },
                    {
                        "name": "Yuhua Zhou"
                    },
                    {
                        "name": "Xu Cheng"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Hongguang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Li"
                },
                "author": "Hongguang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05299v2",
                "updated": "2024-12-16T08:17:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    17,
                    9,
                    0,
                    351,
                    0
                ],
                "published": "2024-11-25T07:48:31Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    7,
                    48,
                    31,
                    0,
                    330,
                    0
                ],
                "title": "Specifications: The missing link to making the development of LLM\n  systems an engineering discipline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specifications: The missing link to making the development of LLM\n  systems an engineering discipline"
                },
                "summary": "Despite the significant strides made by generative AI in just a few short\nyears, its future progress is constrained by the challenge of building modular\nand robust systems. This capability has been a cornerstone of past\ntechnological revolutions, which relied on combining components to create\nincreasingly sophisticated and reliable systems. Cars, airplanes, computers,\nand software consist of components-such as engines, wheels, CPUs, and\nlibraries-that can be assembled, debugged, and replaced. A key tool for\nbuilding such reliable and modular systems is specification: the precise\ndescription of the expected behavior, inputs, and outputs of each component.\nHowever, the generality of LLMs and the inherent ambiguity of natural language\nmake defining specifications for LLM-based components (e.g., agents) both a\nchallenging and urgent problem. In this paper, we discuss the progress the\nfield has made so far-through advances like structured outputs, process\nsupervision, and test-time compute-and outline several future directions for\nresearch to enable the development of modular and reliable LLM-based systems\nthrough improved specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant strides made by generative AI in just a few short\nyears, its future progress is constrained by the challenge of building modular\nand robust systems. This capability has been a cornerstone of past\ntechnological revolutions, which relied on combining components to create\nincreasingly sophisticated and reliable systems. Cars, airplanes, computers,\nand software consist of components-such as engines, wheels, CPUs, and\nlibraries-that can be assembled, debugged, and replaced. A key tool for\nbuilding such reliable and modular systems is specification: the precise\ndescription of the expected behavior, inputs, and outputs of each component.\nHowever, the generality of LLMs and the inherent ambiguity of natural language\nmake defining specifications for LLM-based components (e.g., agents) both a\nchallenging and urgent problem. In this paper, we discuss the progress the\nfield has made so far-through advances like structured outputs, process\nsupervision, and test-time compute-and outline several future directions for\nresearch to enable the development of modular and reliable LLM-based systems\nthrough improved specifications."
                },
                "authors": [
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ken Goldberg"
                    },
                    {
                        "name": "Koushik Sen"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Anastasios Angelopoulos"
                    },
                    {
                        "name": "Shishir G. Patil"
                    },
                    {
                        "name": "Lingjiao Chen"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Jared Q. Davis"
                    }
                ],
                "author_detail": {
                    "name": "Jared Q. Davis"
                },
                "author": "Jared Q. Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11536v1",
                "updated": "2024-12-16T08:13:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    13,
                    14,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T08:13:14Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    13,
                    14,
                    0,
                    351,
                    0
                ],
                "title": "Let your LLM generate a few tokens and you will reduce the need for\n  retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let your LLM generate a few tokens and you will reduce the need for\n  retrieval"
                },
                "summary": "In this paper, we investigate how efficiently large language models (LLM) can\nbe trained to check whether an answer is already stored in their parametric\nmemory. We distill an LLM-as-a-judge to compute the IK (I Know) score. We found\nthat this method is particularly beneficial in the context of\nretrieval-assisted augmented generation (RAG), with a respectable accuracy of\n80%. It enables a significant reduction (more than 50%) in the number of search\nand reranking steps required for certain data sets. We have also introduced the\nIK score, which serves as a useful tool for characterising datasets by\nfacilitating the classification task. Interestingly, through the inclusion of\nresponse tokens as input, our results suggest that only about 20,000 training\nsamples are required to achieve good performance. The central element of this\nwork is the use of a teacher model - the LLM as a judge - to generate training\ndata. We also assess the robustness of the IK classifier by evaluating it with\nvarious types of teachers, including both string-based methods and LLMs, with\nthe latter providing better results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate how efficiently large language models (LLM) can\nbe trained to check whether an answer is already stored in their parametric\nmemory. We distill an LLM-as-a-judge to compute the IK (I Know) score. We found\nthat this method is particularly beneficial in the context of\nretrieval-assisted augmented generation (RAG), with a respectable accuracy of\n80%. It enables a significant reduction (more than 50%) in the number of search\nand reranking steps required for certain data sets. We have also introduced the\nIK score, which serves as a useful tool for characterising datasets by\nfacilitating the classification task. Interestingly, through the inclusion of\nresponse tokens as input, our results suggest that only about 20,000 training\nsamples are required to achieve good performance. The central element of this\nwork is the use of a teacher model - the LLM as a judge - to generate training\ndata. We also assess the robustness of the IK classifier by evaluating it with\nvarious types of teachers, including both string-based methods and LLMs, with\nthe latter providing better results."
                },
                "authors": [
                    {
                        "name": "Herv Djean"
                    }
                ],
                "author_detail": {
                    "name": "Herv Djean"
                },
                "author": "Herv Djean",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14335v2",
                "updated": "2024-12-16T08:08:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    8,
                    51,
                    0,
                    351,
                    0
                ],
                "published": "2024-09-22T06:43:40Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    6,
                    43,
                    40,
                    6,
                    266,
                    0
                ],
                "title": "MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic\n  Post-Editing in LLM Translation Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic\n  Post-Editing in LLM Translation Evaluators"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential as judges for\nMachine Translation (MT) quality assessment, providing both scores and\nfine-grained feedback. Although approaches such as GEMBA-MQM have shown\nstate-of-the-art performance on reference-free evaluation, the predicted errors\ndo not align well with those annotated by human, limiting their\ninterpretability as feedback signals. To enhance the quality of error\nannotations predicted by LLM evaluators, we introduce a universal and\ntraining-free framework, $\\textbf{MQM-APE}$, based on the idea of filtering out\nnon-impactful errors by Automatically Post-Editing (APE) the original\ntranslation based on each error, leaving only those errors that contribute to\nquality improvement. Specifically, we prompt the LLM to act as 1)\n$\\textit{evaluator}$ to provide error annotations, 2) $\\textit{post-editor}$ to\ndetermine whether errors impact quality improvement and 3) $\\textit{pairwise\nquality verifier}$ as the error filter. Experiments show that our approach\nconsistently improves both the reliability and quality of error spans against\nGEMBA-MQM, across eight LLMs in both high- and low-resource languages.\nOrthogonal to trained approaches, MQM-APE complements translation-specific\nevaluators such as Tower, highlighting its broad applicability. Further\nanalysis confirms the effectiveness of each module and offers valuable insights\ninto evaluator design and LLMs selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential as judges for\nMachine Translation (MT) quality assessment, providing both scores and\nfine-grained feedback. Although approaches such as GEMBA-MQM have shown\nstate-of-the-art performance on reference-free evaluation, the predicted errors\ndo not align well with those annotated by human, limiting their\ninterpretability as feedback signals. To enhance the quality of error\nannotations predicted by LLM evaluators, we introduce a universal and\ntraining-free framework, $\\textbf{MQM-APE}$, based on the idea of filtering out\nnon-impactful errors by Automatically Post-Editing (APE) the original\ntranslation based on each error, leaving only those errors that contribute to\nquality improvement. Specifically, we prompt the LLM to act as 1)\n$\\textit{evaluator}$ to provide error annotations, 2) $\\textit{post-editor}$ to\ndetermine whether errors impact quality improvement and 3) $\\textit{pairwise\nquality verifier}$ as the error filter. Experiments show that our approach\nconsistently improves both the reliability and quality of error spans against\nGEMBA-MQM, across eight LLMs in both high- and low-resource languages.\nOrthogonal to trained approaches, MQM-APE complements translation-specific\nevaluators such as Tower, highlighting its broad applicability. Further\nanalysis confirms the effectiveness of each module and offers valuable insights\ninto evaluator design and LLMs selection."
                },
                "authors": [
                    {
                        "name": "Qingyu Lu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Kanjian Zhang"
                    },
                    {
                        "name": "Jinxia Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03258v3",
                "updated": "2024-12-16T08:06:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    8,
                    6,
                    27,
                    0,
                    351,
                    0
                ],
                "published": "2024-09-05T05:34:16Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    34,
                    16,
                    3,
                    249,
                    0
                ],
                "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Shuo Han"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zezhong Ding"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]