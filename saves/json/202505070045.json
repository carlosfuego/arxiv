[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02082v1",
                "updated": "2025-05-04T12:21:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    12,
                    21,
                    16,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T12:21:16Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    12,
                    21,
                    16,
                    6,
                    124,
                    0
                ],
                "title": "Performance Characterization of Containers in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization of Containers in Edge Computing"
                },
                "summary": "This paper presents an empirical evaluation of container-based virtualization\non embedded operating systems commonly used in Internet of Things (IoT)\ndeployments. Focusing on platforms like the Raspberry Pi, we investigate the\nfeasibility and performance implications of deploying Docker containers in\nresource-constrained edge environments. Our study employs both microbenchmarks\n(CPU, memory, and network profiling) and macrobenchmarks (AI-driven inference,\nsensor IO workloads) to capture a comprehensive view of system behavior. The\nanalysis is conducted on a custom-built physical testbed comprising Raspberry\nPi devices equipped with environmental sensors and camera modules, enabling\nreal-time deployment and measurement of representative IoT workloads. Through\nquantitative analysis across a diverse suite of IoT tasks and real-time\napplication services, we identify key overheads introduced by containerization\nand characterize challenges specific to embedded IoT contexts, including\nlimited hardware resources, cold-start delays, and suboptimal IO handling.\nPerformance metrics include CPU utilization, memory faults, cache misses,\nnetwork throughput, and latency. Our findings highlight trade-offs between\nisolation and efficiency and offer insights for optimizing container\nconfigurations to meet the real-time and reliability requirements of edge\ncomputing applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an empirical evaluation of container-based virtualization\non embedded operating systems commonly used in Internet of Things (IoT)\ndeployments. Focusing on platforms like the Raspberry Pi, we investigate the\nfeasibility and performance implications of deploying Docker containers in\nresource-constrained edge environments. Our study employs both microbenchmarks\n(CPU, memory, and network profiling) and macrobenchmarks (AI-driven inference,\nsensor IO workloads) to capture a comprehensive view of system behavior. The\nanalysis is conducted on a custom-built physical testbed comprising Raspberry\nPi devices equipped with environmental sensors and camera modules, enabling\nreal-time deployment and measurement of representative IoT workloads. Through\nquantitative analysis across a diverse suite of IoT tasks and real-time\napplication services, we identify key overheads introduced by containerization\nand characterize challenges specific to embedded IoT contexts, including\nlimited hardware resources, cold-start delays, and suboptimal IO handling.\nPerformance metrics include CPU utilization, memory faults, cache misses,\nnetwork throughput, and latency. Our findings highlight trade-offs between\nisolation and efficiency and offer insights for optimizing container\nconfigurations to meet the real-time and reliability requirements of edge\ncomputing applications."
                },
                "authors": [
                    {
                        "name": "Ragini Gupta"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v2",
                "updated": "2025-05-03T06:55:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    6,
                    55,
                    56,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v1",
                "updated": "2025-05-03T02:47:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.02836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02836v1",
                "updated": "2025-05-05T17:59:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    58,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:59:58Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    58,
                    0,
                    125,
                    0
                ],
                "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation"
                },
                "summary": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Lu Ling"
                    },
                    {
                        "name": "Chen-Hsuan Lin"
                    },
                    {
                        "name": "Tsung-Yi Lin"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Yu Zeng"
                    },
                    {
                        "name": "Yichen Sheng"
                    },
                    {
                        "name": "Yunhao Ge"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Aniket Bera"
                    },
                    {
                        "name": "Zhaoshuo Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoshuo Li"
                },
                "author": "Zhaoshuo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02835v1",
                "updated": "2025-05-05T17:59:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    50,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:59:50Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    50,
                    0,
                    125,
                    0
                ],
                "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning"
                },
                "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Tianke Zhang"
                    },
                    {
                        "name": "Changyi Liu"
                    },
                    {
                        "name": "Kaiyu Jiang"
                    },
                    {
                        "name": "Kaibing Chen"
                    },
                    {
                        "name": "Kaiyu Tang"
                    },
                    {
                        "name": "Haojie Ding"
                    },
                    {
                        "name": "Jiankang Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "arxiv_comment": "Home page: https://github.com/yfzhang114/r1_reward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10807v2",
                "updated": "2025-05-05T17:58:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    58,
                    28,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-14T17:59:24Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    24,
                    0,
                    288,
                    0
                ],
                "title": "Hard-Constrained Neural Networks with Universal Approximation Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard-Constrained Neural Networks with Universal Approximation Guarantees"
                },
                "summary": "Incorporating prior knowledge or specifications of input-output relationships\ninto machine learning models has gained significant attention, as it enhances\ngeneralization from limited data and leads to conforming outputs. However, most\nexisting approaches use soft constraints by penalizing violations through\nregularization, which offers no guarantee of constraint satisfaction--an\nessential requirement in safety-critical applications. On the other hand,\nimposing hard constraints on neural networks may hinder their representational\npower, adversely affecting performance. To address this, we propose HardNet, a\npractical framework for constructing neural networks that inherently satisfy\nhard constraints without sacrificing model capacity. Unlike approaches that\nmodify outputs only at inference time, HardNet enables end-to-end training with\nhard constraint guarantees, leading to improved performance. To the best of our\nknowledge, HardNet is the first method with an efficient forward pass to\nenforce more than one input-dependent inequality constraint. It allows\nunconstrained optimization of the network parameters using standard algorithms\nby appending a differentiable closed-form enforcement layer to the network's\noutput. Furthermore, we show that HardNet retains the universal approximation\ncapabilities of neural networks. We demonstrate the versatility and\neffectiveness of HardNet across various applications: learning with piecewise\nconstraints, learning optimization solvers, optimizing control policies in\nsafety-critical systems, and learning safe decision logic for aircraft systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating prior knowledge or specifications of input-output relationships\ninto machine learning models has gained significant attention, as it enhances\ngeneralization from limited data and leads to conforming outputs. However, most\nexisting approaches use soft constraints by penalizing violations through\nregularization, which offers no guarantee of constraint satisfaction--an\nessential requirement in safety-critical applications. On the other hand,\nimposing hard constraints on neural networks may hinder their representational\npower, adversely affecting performance. To address this, we propose HardNet, a\npractical framework for constructing neural networks that inherently satisfy\nhard constraints without sacrificing model capacity. Unlike approaches that\nmodify outputs only at inference time, HardNet enables end-to-end training with\nhard constraint guarantees, leading to improved performance. To the best of our\nknowledge, HardNet is the first method with an efficient forward pass to\nenforce more than one input-dependent inequality constraint. It allows\nunconstrained optimization of the network parameters using standard algorithms\nby appending a differentiable closed-form enforcement layer to the network's\noutput. Furthermore, we show that HardNet retains the universal approximation\ncapabilities of neural networks. We demonstrate the versatility and\neffectiveness of HardNet across various applications: learning with piecewise\nconstraints, learning optimization solvers, optimizing control policies in\nsafety-critical systems, and learning safe decision logic for aircraft systems."
                },
                "authors": [
                    {
                        "name": "Youngjae Min"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02825v2",
                "updated": "2025-05-06T10:17:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    17,
                    58,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T17:51:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    51,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology"
                },
                "summary": "Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows."
                },
                "authors": [
                    {
                        "name": "Alex Hoi Hang Chan"
                    },
                    {
                        "name": "Otto Brookes"
                    },
                    {
                        "name": "Urs Waldmann"
                    },
                    {
                        "name": "Hemal Naik"
                    },
                    {
                        "name": "Iain D. Couzin"
                    },
                    {
                        "name": "Majid Mirmehdi"
                    },
                    {
                        "name": "Noël Adiko Houa"
                    },
                    {
                        "name": "Emmanuelle Normand"
                    },
                    {
                        "name": "Christophe Boesch"
                    },
                    {
                        "name": "Lukas Boesch"
                    },
                    {
                        "name": "Mimi Arandjelovic"
                    },
                    {
                        "name": "Hjalmar Kühl"
                    },
                    {
                        "name": "Tilo Burghardt"
                    },
                    {
                        "name": "Fumihiro Kano"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiro Kano"
                },
                "author": "Fumihiro Kano",
                "arxiv_comment": "Accepted at CVPR Workshop, CV4Animals 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02820v1",
                "updated": "2025-05-05T17:47:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLibra: Agent Metric Induction from Open-Ended Feedback"
                },
                "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."
                },
                "authors": [
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Phil Cuvin"
                    },
                    {
                        "name": "Xinkai Yu"
                    },
                    {
                        "name": "Charlotte Ka Yee Yan"
                    },
                    {
                        "name": "Jason Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "https://opensocial.world/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02819v1",
                "updated": "2025-05-05T17:47:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:47:42Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations"
                },
                "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository."
                },
                "authors": [
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    },
                    {
                        "name": "Nikos Komodakis"
                    },
                    {
                        "name": "Sergey Zagoruyko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Zagoruyko"
                },
                "author": "Sergey Zagoruyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02811v1",
                "updated": "2025-05-05T17:39:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:39:35Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing"
                },
                "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."
                },
                "authors": [
                    {
                        "name": "Diji Yang"
                    },
                    {
                        "name": "Linda Zeng"
                    },
                    {
                        "name": "Jinmeng Rao"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "Proceedings of the 48th International ACM SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02809v1",
                "updated": "2025-05-05T17:34:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    34,
                    57,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:34:57Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    34,
                    57,
                    0,
                    125,
                    0
                ],
                "title": "Towards Quantifying the Hessian Structure of Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Quantifying the Hessian Structure of Neural Networks"
                },
                "summary": "Empirical studies reported that the Hessian matrix of neural networks (NNs)\nexhibits a near-block-diagonal structure, yet its theoretical foundation\nremains unclear. In this work, we reveal two forces that shape the Hessian\nstructure: a ``static force'' rooted in the architecture design, and a\n``dynamic force'' arisen from training. We then provide a rigorous theoretical\nanalysis of ``static force'' at random initialization. We study linear models\nand 1-hidden-layer networks with the mean-square (MSE) loss and the\nCross-Entropy (CE) loss for classification tasks. By leveraging random matrix\ntheory, we compare the limit distributions of the diagonal and off-diagonal\nHessian blocks and find that the block-diagonal structure arises as $C\n\\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings\nreveal that $C$ is a primary driver of the near-block-diagonal structure. These\nresults may shed new light on the Hessian structure of large language models\n(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical studies reported that the Hessian matrix of neural networks (NNs)\nexhibits a near-block-diagonal structure, yet its theoretical foundation\nremains unclear. In this work, we reveal two forces that shape the Hessian\nstructure: a ``static force'' rooted in the architecture design, and a\n``dynamic force'' arisen from training. We then provide a rigorous theoretical\nanalysis of ``static force'' at random initialization. We study linear models\nand 1-hidden-layer networks with the mean-square (MSE) loss and the\nCross-Entropy (CE) loss for classification tasks. By leveraging random matrix\ntheory, we compare the limit distributions of the diagonal and off-diagonal\nHessian blocks and find that the block-diagonal structure arises as $C\n\\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings\nreveal that $C$ is a primary driver of the near-block-diagonal structure. These\nresults may shed new light on the Hessian structure of large language models\n(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$."
                },
                "authors": [
                    {
                        "name": "Zhaorui Dong"
                    },
                    {
                        "name": "Yushun Zhang"
                    },
                    {
                        "name": "Zhi-Quan Luo"
                    },
                    {
                        "name": "Jianfeng Yao"
                    },
                    {
                        "name": "Ruoyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ruoyu Sun"
                },
                "author": "Ruoyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02802v1",
                "updated": "2025-05-05T17:26:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    26,
                    27,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:26:27Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    26,
                    27,
                    0,
                    125,
                    0
                ],
                "title": "Generating HomeAssistant Automations Using an LLM-based Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating HomeAssistant Automations Using an LLM-based Chatbot"
                },
                "summary": "To combat climate change, individuals are encouraged to adopt sustainable\nhabits, in particular, with their household, optimizing their electrical\nconsumption. Conversational agents, such as Smart Home Assistants, hold promise\nas effective tools for promoting sustainable practices within households. Our\nresearch investigated the application of Large Language Models (LLM) in\nenhancing smart home automation and promoting sustainable household practices,\nspecifically using the HomeAssistant framework. In particular, it highlights\nthe potential of GPT models in generating accurate automation routines. While\nthe LLMs showed proficiency in understanding complex commands and creating\nvalid JSON outputs, challenges such as syntax errors and message malformations\nwere noted, indicating areas for further improvement. Still, despite minimal\nquantitative differences between \"green\" and \"no green\" prompts, qualitative\nfeedback highlighted a positive shift towards sustainability in the routines\ngenerated with environmentally focused prompts. Then, an empirical evaluation\n(N=56) demonstrated that the system was well-received and found engaging by\nusers compared to its traditional rule-based counterpart. Our findings\nhighlight the role of LLMs in advancing smart home technologies and suggest\nfurther research to refine these models for broader, real-world applications to\nsupport sustainable living.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To combat climate change, individuals are encouraged to adopt sustainable\nhabits, in particular, with their household, optimizing their electrical\nconsumption. Conversational agents, such as Smart Home Assistants, hold promise\nas effective tools for promoting sustainable practices within households. Our\nresearch investigated the application of Large Language Models (LLM) in\nenhancing smart home automation and promoting sustainable household practices,\nspecifically using the HomeAssistant framework. In particular, it highlights\nthe potential of GPT models in generating accurate automation routines. While\nthe LLMs showed proficiency in understanding complex commands and creating\nvalid JSON outputs, challenges such as syntax errors and message malformations\nwere noted, indicating areas for further improvement. Still, despite minimal\nquantitative differences between \"green\" and \"no green\" prompts, qualitative\nfeedback highlighted a positive shift towards sustainability in the routines\ngenerated with environmentally focused prompts. Then, an empirical evaluation\n(N=56) demonstrated that the system was well-received and found engaging by\nusers compared to its traditional rule-based counterpart. Our findings\nhighlight the role of LLMs in advancing smart home technologies and suggest\nfurther research to refine these models for broader, real-world applications to\nsupport sustainable living."
                },
                "authors": [
                    {
                        "name": "Mathyas Giudici"
                    },
                    {
                        "name": "Alessandro Sironi"
                    },
                    {
                        "name": "Ismaele Villa"
                    },
                    {
                        "name": "Samuele Scherini"
                    },
                    {
                        "name": "Franca Garzotto"
                    }
                ],
                "author_detail": {
                    "name": "Franca Garzotto"
                },
                "author": "Franca Garzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02795v1",
                "updated": "2025-05-05T17:09:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    9,
                    19,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:09:19Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    9,
                    19,
                    0,
                    125,
                    0
                ],
                "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning\n  Framework for Large Language Models"
                },
                "summary": "Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed."
                },
                "authors": [
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Praneeth Vepakomma"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Jun Luo"
                    },
                    {
                        "name": "Yue Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Gao"
                },
                "author": "Yue Gao",
                "arxiv_comment": "16 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02787v1",
                "updated": "2025-05-05T17:02:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    2,
                    13,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:02:13Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    2,
                    13,
                    0,
                    125,
                    0
                ],
                "title": "Unsupervised training of keypoint-agnostic descriptors for flexible\n  retinal image registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised training of keypoint-agnostic descriptors for flexible\n  retinal image registration"
                },
                "summary": "Current color fundus image registration approaches are limited, among other\nthings, by the lack of labeled data, which is even more significant in the\nmedical domain, motivating the use of unsupervised learning. Therefore, in this\nwork, we develop a novel unsupervised descriptor learning method that does not\nrely on keypoint detection. This enables the resulting descriptor network to be\nagnostic to the keypoint detector used during the registration inference.\n  To validate this approach, we perform an extensive and comprehensive\ncomparison on the reference public retinal image registration dataset.\nAdditionally, we test our method with multiple keypoint detectors of varied\nnature, even proposing some novel ones. Our results demonstrate that the\nproposed approach offers accurate registration, not incurring in any\nperformance loss versus supervised methods. Additionally, it demonstrates\naccurate performance regardless of the keypoint detector used. Thus, this work\nrepresents a notable step towards leveraging unsupervised learning in the\nmedical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current color fundus image registration approaches are limited, among other\nthings, by the lack of labeled data, which is even more significant in the\nmedical domain, motivating the use of unsupervised learning. Therefore, in this\nwork, we develop a novel unsupervised descriptor learning method that does not\nrely on keypoint detection. This enables the resulting descriptor network to be\nagnostic to the keypoint detector used during the registration inference.\n  To validate this approach, we perform an extensive and comprehensive\ncomparison on the reference public retinal image registration dataset.\nAdditionally, we test our method with multiple keypoint detectors of varied\nnature, even proposing some novel ones. Our results demonstrate that the\nproposed approach offers accurate registration, not incurring in any\nperformance loss versus supervised methods. Additionally, it demonstrates\naccurate performance regardless of the keypoint detector used. Thus, this work\nrepresents a notable step towards leveraging unsupervised learning in the\nmedical domain."
                },
                "authors": [
                    {
                        "name": "David Rivas-Villar"
                    },
                    {
                        "name": "Álvaro S. Hervella"
                    },
                    {
                        "name": "José Rouco"
                    },
                    {
                        "name": "Jorge Novo"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Novo"
                },
                "author": "Jorge Novo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04551v3",
                "updated": "2025-05-05T16:52:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    52,
                    10,
                    0,
                    125,
                    0
                ],
                "published": "2024-02-07T03:16:16Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    3,
                    16,
                    16,
                    2,
                    38,
                    0
                ],
                "title": "Gamma-ray Bursts as Distance Indicators by a Statistical Learning\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray Bursts as Distance Indicators by a Statistical Learning\n  Approach"
                },
                "summary": "Gamma-ray bursts (GRBs) can be probes of the early universe, but currently,\nonly 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known\nredshifts ($z$) due to observational limitations. To address this, we estimated\nthe GRB redshift (distance) via a supervised statistical learning model that\nuses optical afterglow observed by Swift and ground-based telescopes. The\ninferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with\nthe observed redshifts, thus proving the reliability of this method. The\ninferred and observed redshifts allow us to estimate the number of GRBs\noccurring at a given redshift (GRB rate) to be 8.47-9 $yr^{-1} Gpc^{-1}$ for\n$1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared\nthis rate with the star formation rate highlighting a discrepancy of a factor\nof 3 at $z<1$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray bursts (GRBs) can be probes of the early universe, but currently,\nonly 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known\nredshifts ($z$) due to observational limitations. To address this, we estimated\nthe GRB redshift (distance) via a supervised statistical learning model that\nuses optical afterglow observed by Swift and ground-based telescopes. The\ninferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with\nthe observed redshifts, thus proving the reliability of this method. The\ninferred and observed redshifts allow us to estimate the number of GRBs\noccurring at a given redshift (GRB rate) to be 8.47-9 $yr^{-1} Gpc^{-1}$ for\n$1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared\nthis rate with the star formation rate highlighting a discrepancy of a factor\nof 3 at $z<1$."
                },
                "authors": [
                    {
                        "name": "Maria Giovanna Dainotti"
                    },
                    {
                        "name": "Aditya Narendra"
                    },
                    {
                        "name": "Agnieszka Pollo"
                    },
                    {
                        "name": "Vahe Petrosian"
                    },
                    {
                        "name": "Malgorzata Bogdan"
                    },
                    {
                        "name": "Kazunari Iwasaki"
                    },
                    {
                        "name": "Jason Xavier Prochaska"
                    },
                    {
                        "name": "Enrico Rinaldi"
                    },
                    {
                        "name": "David Zhou"
                    }
                ],
                "author_detail": {
                    "name": "David Zhou"
                },
                "author": "David Zhou",
                "arxiv_doi": "10.3847/2041-8213/ad4970",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad4970",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.04551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 figures. Published in The Astrophysical Journal Letters. arXiv\n  admin note: text overlap with arXiv:1907.05074",
                "arxiv_journal_ref": "ApJL, 967(2), p.L30 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05929v4",
                "updated": "2025-05-05T16:48:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    48,
                    19,
                    0,
                    125,
                    0
                ],
                "published": "2024-09-09T10:40:50Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    40,
                    50,
                    0,
                    253,
                    0
                ],
                "title": "M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the\n  JEPA framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the\n  JEPA framework"
                },
                "summary": "Current multimodal alignment strategies primarily use single or unified\nmodality encoders, while optimizing the alignment on the original token space.\nSuch a framework is easy to implement and incorporate with the pretrained\nknowledge, but might result in information bias. To deal with such issues, the\njoint encoding predictive architecture (JEPA) learns the alignment loss on the\nlatent space, with a predictor to convert the input encoding to the output\nlatent space. However, the application of JEPA in multimodal scenarios is\nlimited so far. In this paper, we introduce M3-Jepa, a scalable multimodal\nalignment framework, with the predictor implemented by a multi-directional\nmixture of experts (MoE). We demonstrate the framework can maximize the mutual\ninformation with information theory derivations, by alternating the\noptimization between different uni-directional tasks. By thoroughly designed\nexperiments, we show that M3-Jepa can obtain state-of-the-art performance on\ndifferent modalities and tasks, generalize to unseen datasets and domains, and\nis computationally efficient in training and inference. Our study indicates\nthat M3-Jepa might provide a new paradigm to self-supervised learning and\nopen-world modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal alignment strategies primarily use single or unified\nmodality encoders, while optimizing the alignment on the original token space.\nSuch a framework is easy to implement and incorporate with the pretrained\nknowledge, but might result in information bias. To deal with such issues, the\njoint encoding predictive architecture (JEPA) learns the alignment loss on the\nlatent space, with a predictor to convert the input encoding to the output\nlatent space. However, the application of JEPA in multimodal scenarios is\nlimited so far. In this paper, we introduce M3-Jepa, a scalable multimodal\nalignment framework, with the predictor implemented by a multi-directional\nmixture of experts (MoE). We demonstrate the framework can maximize the mutual\ninformation with information theory derivations, by alternating the\noptimization between different uni-directional tasks. By thoroughly designed\nexperiments, we show that M3-Jepa can obtain state-of-the-art performance on\ndifferent modalities and tasks, generalize to unseen datasets and domains, and\nis computationally efficient in training and inference. Our study indicates\nthat M3-Jepa might provide a new paradigm to self-supervised learning and\nopen-world modeling."
                },
                "authors": [
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Xiaolong Cheng"
                    },
                    {
                        "name": "Dan Wang"
                    },
                    {
                        "name": "Kun Fan"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Huazhen Huang"
                    },
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "13 pages, 4 figures. Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13985v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13985v3",
                "updated": "2025-05-05T16:42:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    42,
                    8,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-17T19:30:58Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    19,
                    30,
                    58,
                    3,
                    291,
                    0
                ],
                "title": "GRB Redshift Estimation using Machine Learning and the Associated\n  Web-App",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRB Redshift Estimation using Machine Learning and the Associated\n  Web-App"
                },
                "summary": "Context. Gamma-ray bursts (GRBs), observed at redshifts as high as 9.4, could\nserve as valuable probes for investigating the distant Universe. However, this\nnecessitates an increase in the number of GRBs with determined redshifts, as\ncurrently, only 12% of GRBs have known redshifts due to observational biases.\nAims. We aim to address the shortage of GRBs with measured redshifts, enabling\nus to fully realize their potential as valuable cosmological probes Methods.\nFollowing Dainotti et al. (2024c), we have taken a second step to overcome this\nissue by adding 30 more GRBs to our ensemble supervised machine learning\ntraining sample, an increase of 20%, which will help us obtain better redshift\nestimates. In addition, we have built a freely accessible and user-friendly web\napp that infers the redshift of long GRBs (LGRBs) with plateau emission using\nour machine learning model. The web app is the first of its kind for such a\nstudy and will allow the community to obtain redshift estimates by entering the\nGRB parameters in the app. Results. Through our machine learning model, we have\nsuccessfully estimated redshifts for 276 LGRBs using X-ray afterglow parameters\ndetected by the Neil Gehrels Swift Observatory and increased the sample of\nLGRBs with known redshifts by 110%. We also perform Monte Carlo simulations to\ndemonstrate the future applicability of this research. Conclusions. The results\npresented in this research will enable the community to increase the sample of\nGRBs with known redshift estimates. This can help address many outstanding\nissues, such as GRB formation rate, luminosity function, and the true nature of\nlow-luminosity GRBs, and enable the application of GRBs as standard candles",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Gamma-ray bursts (GRBs), observed at redshifts as high as 9.4, could\nserve as valuable probes for investigating the distant Universe. However, this\nnecessitates an increase in the number of GRBs with determined redshifts, as\ncurrently, only 12% of GRBs have known redshifts due to observational biases.\nAims. We aim to address the shortage of GRBs with measured redshifts, enabling\nus to fully realize their potential as valuable cosmological probes Methods.\nFollowing Dainotti et al. (2024c), we have taken a second step to overcome this\nissue by adding 30 more GRBs to our ensemble supervised machine learning\ntraining sample, an increase of 20%, which will help us obtain better redshift\nestimates. In addition, we have built a freely accessible and user-friendly web\napp that infers the redshift of long GRBs (LGRBs) with plateau emission using\nour machine learning model. The web app is the first of its kind for such a\nstudy and will allow the community to obtain redshift estimates by entering the\nGRB parameters in the app. Results. Through our machine learning model, we have\nsuccessfully estimated redshifts for 276 LGRBs using X-ray afterglow parameters\ndetected by the Neil Gehrels Swift Observatory and increased the sample of\nLGRBs with known redshifts by 110%. We also perform Monte Carlo simulations to\ndemonstrate the future applicability of this research. Conclusions. The results\npresented in this research will enable the community to increase the sample of\nGRBs with known redshift estimates. This can help address many outstanding\nissues, such as GRB formation rate, luminosity function, and the true nature of\nlow-luminosity GRBs, and enable the application of GRBs as standard candles"
                },
                "authors": [
                    {
                        "name": "Aditya Narendra"
                    },
                    {
                        "name": "Maria Dainotti"
                    },
                    {
                        "name": "Milind Sarkar"
                    },
                    {
                        "name": "Aleksander Lenart"
                    },
                    {
                        "name": "Malgorzata Bogdan"
                    },
                    {
                        "name": "Agnieszka Pollo"
                    },
                    {
                        "name": "Bing Zhang"
                    },
                    {
                        "name": "Aleksandra Rabeda"
                    },
                    {
                        "name": "Vahe Petrosian"
                    },
                    {
                        "name": "Iwasaki Kazunari"
                    }
                ],
                "author_detail": {
                    "name": "Iwasaki Kazunari"
                },
                "author": "Iwasaki Kazunari",
                "arxiv_doi": "10.1051/0004-6361/202452651",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452651",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.13985v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13985v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 Figures, 3 tables. Accepted for publication in Astronomy and\n  Astrophysics journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02773v1",
                "updated": "2025-05-05T16:36:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    36,
                    46,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T16:36:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    36,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "Can Transformers help us perform parameter estimation of overlapping\n  signals in gravitational wave detectors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformers help us perform parameter estimation of overlapping\n  signals in gravitational wave detectors?"
                },
                "summary": "Overlapping signals represent one of the major data analysis challenges in\nnext-generation gravitational wave detectors. We leverage Transformers and\nNormalizing Flows, state-of-the-art machine learning algorithms, to address the\nparameter estimation of overlapping binary black hole mergers in the Einstein\nTelescope (ET). Our proposed model combines a Transformer-based \"Knowledge\nExtractor Neural Network\" (KENN) with a Normalizing Flow (HYPERION) to perform\nrapid and unbiased inference over multiple overlapping black hole binary\nevents. The choice of architecture leverages the strength of Transformers in\ncapturing complex and long-range temporal structures in the strain time series\ndata, while Normalizing Flows provide a powerful framework to sample posterior\ndistributions. We demonstrate the effectiveness and robustness of our model\nover simulated gravitational wave signals, showing that it maintains the same\nlevel of accuracy regardless of the correlation level in the data. Moreover our\nmodel provides estimates of chirp mass and coalescence times within <10-20%\nfrom the true simulated value. The results obtained are promising and show how\nthis approach might represent a first step toward a deep-learning based\ninference pipeline for ET and other future gravitational wave detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overlapping signals represent one of the major data analysis challenges in\nnext-generation gravitational wave detectors. We leverage Transformers and\nNormalizing Flows, state-of-the-art machine learning algorithms, to address the\nparameter estimation of overlapping binary black hole mergers in the Einstein\nTelescope (ET). Our proposed model combines a Transformer-based \"Knowledge\nExtractor Neural Network\" (KENN) with a Normalizing Flow (HYPERION) to perform\nrapid and unbiased inference over multiple overlapping black hole binary\nevents. The choice of architecture leverages the strength of Transformers in\ncapturing complex and long-range temporal structures in the strain time series\ndata, while Normalizing Flows provide a powerful framework to sample posterior\ndistributions. We demonstrate the effectiveness and robustness of our model\nover simulated gravitational wave signals, showing that it maintains the same\nlevel of accuracy regardless of the correlation level in the data. Moreover our\nmodel provides estimates of chirp mass and coalescence times within <10-20%\nfrom the true simulated value. The results obtained are promising and show how\nthis approach might represent a first step toward a deep-learning based\ninference pipeline for ET and other future gravitational wave detectors."
                },
                "authors": [
                    {
                        "name": "Lucia Papalini"
                    },
                    {
                        "name": "Federico De Santi"
                    },
                    {
                        "name": "Massimiliano Razzano"
                    },
                    {
                        "name": "Ik Siong Heng"
                    },
                    {
                        "name": "Elena Cuoco"
                    }
                ],
                "author_detail": {
                    "name": "Elena Cuoco"
                },
                "author": "Elena Cuoco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02766v1",
                "updated": "2025-05-05T16:21:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    21,
                    46,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T16:21:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    21,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control"
                },
                "summary": "Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control."
                },
                "authors": [
                    {
                        "name": "Nam H. Le"
                    },
                    {
                        "name": "Patrick Erikson"
                    },
                    {
                        "name": "Yanbo Zhang"
                    },
                    {
                        "name": "Michael Levin"
                    },
                    {
                        "name": "Josh Bongard"
                    }
                ],
                "author_detail": {
                    "name": "Josh Bongard"
                },
                "author": "Josh Bongard",
                "arxiv_comment": "Accepted to GECCO Workshop on Bio-Inspired AI (ACM GECCO2025). 13\n  pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02763v1",
                "updated": "2025-05-05T16:18:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    18,
                    7,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T16:18:07Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    18,
                    7,
                    0,
                    125,
                    0
                ],
                "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models"
                },
                "summary": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount."
                },
                "authors": [
                    {
                        "name": "Matthew Dahl"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Dahl"
                },
                "author": "Matthew Dahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21239v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21239v4",
                "updated": "2025-05-05T16:13:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    13,
                    30,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-28T17:09:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring internal access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring internal access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Swair Shah"
                    },
                    {
                        "name": "Narayanan Sadagopan"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21239v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21239v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02754v1",
                "updated": "2025-05-05T16:07:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    7,
                    28,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T16:07:28Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    7,
                    28,
                    0,
                    125,
                    0
                ],
                "title": "Debiased inference in error-in-variable problems with non-Gaussian\n  measurement error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased inference in error-in-variable problems with non-Gaussian\n  measurement error"
                },
                "summary": "We consider drawing statistical inferences based on data subject to\nnon-Gaussian measurement error. Unlike most existing methods developed under\nthe assumption of Gaussian measurement error, the proposed strategy exploits\nhypercomplex numbers to reduce bias in naive estimation that ignores\nnon-Gaussian measurement error. We apply this new method to several widely\napplicable parametric regression models with error-prone covariates, and kernel\ndensity estimation using error-contaminated data. The efficacy of this method\nin bias reduction is demonstrated in simulation studies and a real-life\napplication in sports analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider drawing statistical inferences based on data subject to\nnon-Gaussian measurement error. Unlike most existing methods developed under\nthe assumption of Gaussian measurement error, the proposed strategy exploits\nhypercomplex numbers to reduce bias in naive estimation that ignores\nnon-Gaussian measurement error. We apply this new method to several widely\napplicable parametric regression models with error-prone covariates, and kernel\ndensity estimation using error-contaminated data. The efficacy of this method\nin bias reduction is demonstrated in simulation studies and a real-life\napplication in sports analytics."
                },
                "authors": [
                    {
                        "name": "Nicholas W. Woolsey"
                    },
                    {
                        "name": "Xianzheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xianzheng Huang"
                },
                "author": "Xianzheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62G08, 62J02, secondary 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02743v1",
                "updated": "2025-05-05T15:50:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    50,
                    52,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:50:52Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    50,
                    52,
                    0,
                    125,
                    0
                ],
                "title": "Cooperative Bayesian and variance networks disentangle aleatoric and\n  epistemic uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Bayesian and variance networks disentangle aleatoric and\n  epistemic uncertainties"
                },
                "summary": "Real-world data contains aleatoric uncertainty - irreducible noise arising\nfrom imperfect measurements or from incomplete knowledge about the data\ngeneration process. Mean variance estimation (MVE) networks can learn this type\nof uncertainty but require ad-hoc regularization strategies to avoid\noverfitting and are unable to predict epistemic uncertainty (model\nuncertainty). Conversely, Bayesian neural networks predict epistemic\nuncertainty but are notoriously difficult to train due to the approximate\nnature of Bayesian inference. We propose to cooperatively train a variance\nnetwork with a Bayesian neural network and demonstrate that the resulting model\ndisentangles aleatoric and epistemic uncertainties while improving the mean\nestimation. We demonstrate the effectiveness and scalability of this method\nacross a diverse range of datasets, including a time-dependent heteroscedastic\nregression dataset we created where the aleatoric uncertainty is known. The\nproposed method is straightforward to implement, robust, and adaptable to\nvarious model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world data contains aleatoric uncertainty - irreducible noise arising\nfrom imperfect measurements or from incomplete knowledge about the data\ngeneration process. Mean variance estimation (MVE) networks can learn this type\nof uncertainty but require ad-hoc regularization strategies to avoid\noverfitting and are unable to predict epistemic uncertainty (model\nuncertainty). Conversely, Bayesian neural networks predict epistemic\nuncertainty but are notoriously difficult to train due to the approximate\nnature of Bayesian inference. We propose to cooperatively train a variance\nnetwork with a Bayesian neural network and demonstrate that the resulting model\ndisentangles aleatoric and epistemic uncertainties while improving the mean\nestimation. We demonstrate the effectiveness and scalability of this method\nacross a diverse range of datasets, including a time-dependent heteroscedastic\nregression dataset we created where the aleatoric uncertainty is known. The\nproposed method is straightforward to implement, robust, and adaptable to\nvarious model architectures."
                },
                "authors": [
                    {
                        "name": "Jiaxiang Yi"
                    },
                    {
                        "name": "Miguel A. Bessa"
                    }
                ],
                "author_detail": {
                    "name": "Miguel A. Bessa"
                },
                "author": "Miguel A. Bessa",
                "arxiv_comment": "28 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00651v3",
                "updated": "2025-05-05T15:50:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    50,
                    2,
                    0,
                    125,
                    0
                ],
                "published": "2024-02-01T15:13:13Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    13,
                    13,
                    3,
                    32,
                    0
                ],
                "title": "Skew-elliptical copula based mixed models for non-Gaussian longitudinal\n  data with application to an HIV-AIDS study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-elliptical copula based mixed models for non-Gaussian longitudinal\n  data with application to an HIV-AIDS study"
                },
                "summary": "This study was sparked by an extensive longitudinal dataset focusing on HIV\nCD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the\ncorresponding histogram plots reveals an absence of symmetry in the marginal\ndistributions, while pairwise scatter plots uncover non-elliptical dependence\npatterns. Traditional linear mixed models designed for longitudinal data fail\nto capture these complexities adequately. Therefore, it appears prudent to\nexplore a broader framework for modeling such data. In this article, we delve\ninto generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma\nmixed model), and we address the temporal dependency of repeated measurements\nby utilizing copulas associated with skew-elliptical distributions (such as the\nskew-normal/skew-$t$). Our proposed class of copula-based mixed models\nsimultaneously accommodates asymmetry, between-subject variability, and\nnon-standard temporal dependence, thus offering extensions to the standard\nlinear mixed model based on multivariate normality. We estimate the model\nparameters using the IFM (inference function of margins) method and outline the\nprocess of obtaining standard errors for parameter estimates. Through extensive\nsimulation studies covering skewed and symmetric marginal distributions and\nvarious copula choices, we assess the finite sample performance of our\napproach. Finally, we apply these models to the HIV dataset and present our\nfindings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study was sparked by an extensive longitudinal dataset focusing on HIV\nCD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the\ncorresponding histogram plots reveals an absence of symmetry in the marginal\ndistributions, while pairwise scatter plots uncover non-elliptical dependence\npatterns. Traditional linear mixed models designed for longitudinal data fail\nto capture these complexities adequately. Therefore, it appears prudent to\nexplore a broader framework for modeling such data. In this article, we delve\ninto generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma\nmixed model), and we address the temporal dependency of repeated measurements\nby utilizing copulas associated with skew-elliptical distributions (such as the\nskew-normal/skew-$t$). Our proposed class of copula-based mixed models\nsimultaneously accommodates asymmetry, between-subject variability, and\nnon-standard temporal dependence, thus offering extensions to the standard\nlinear mixed model based on multivariate normality. We estimate the model\nparameters using the IFM (inference function of margins) method and outline the\nprocess of obtaining standard errors for parameter estimates. Through extensive\nsimulation studies covering skewed and symmetric marginal distributions and\nvarious copula choices, we assess the finite sample performance of our\napproach. Finally, we apply these models to the HIV dataset and present our\nfindings."
                },
                "authors": [
                    {
                        "name": "Subhajit Chattopadhyay"
                    },
                    {
                        "name": "Kalyan Das"
                    },
                    {
                        "name": "Sumitra Purkayastha"
                    }
                ],
                "author_detail": {
                    "name": "Sumitra Purkayastha"
                },
                "author": "Sumitra Purkayastha",
                "arxiv_comment": "24 pages, 5 figures and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02737v2",
                "updated": "2025-05-06T06:44:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    6,
                    44,
                    35,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T15:40:24Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    40,
                    24,
                    0,
                    125,
                    0
                ],
                "title": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance."
                },
                "authors": [
                    {
                        "name": "Gerard Pons"
                    },
                    {
                        "name": "Besim Bilalli"
                    },
                    {
                        "name": "Anna Queralt"
                    }
                ],
                "author_detail": {
                    "name": "Anna Queralt"
                },
                "author": "Anna Queralt",
                "arxiv_doi": "10.1007/978-3-031-77844-5_9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-77844-5_9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Pre-print submitted to ISWC 2024",
                "arxiv_journal_ref": "Proc. 23rd Int. Semantic Web Conf. (ISWC 2024), LNCS, Springer,\n  2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02735v1",
                "updated": "2025-05-05T15:37:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    37,
                    0,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    37,
                    0,
                    0,
                    125,
                    0
                ],
                "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models"
                },
                "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Ruotian Peng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yandong Wen"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Technical Report v1 (33 pages, 8 figures, project page:\n  https://sphere-ai-lab.github.io/FormalMATH/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20834v2",
                "updated": "2025-05-05T15:36:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    36,
                    15,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-29T14:58:43Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    58,
                    43,
                    1,
                    119,
                    0
                ],
                "title": "Token-Efficient RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Efficient RL for LLM Reasoning"
                },
                "summary": "We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Rather than relying on\nfull-sequence updates or separate critic networks, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Rather than relying on\nfull-sequence updates or separate critic networks, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes."
                },
                "authors": [
                    {
                        "name": "Alan Lee"
                    },
                    {
                        "name": "Harry Tong"
                    }
                ],
                "author_detail": {
                    "name": "Harry Tong"
                },
                "author": "Harry Tong",
                "arxiv_comment": "Title updated to \"Token-Efficient RL for LLM Reasoning\" to better\n  reflect algorithmic focus. Revised abstract, intro, and conclusion. Paper\n  shortened and typos fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18902v2",
                "updated": "2025-05-05T15:26:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    26,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-24T16:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    48,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Extremely Low-Resource Finno-Ugric Languages"
                },
                "summary": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP."
                },
                "authors": [
                    {
                        "name": "Taido Purason"
                    },
                    {
                        "name": "Hele-Andra Kuulmets"
                    },
                    {
                        "name": "Mark Fishel"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fishel"
                },
                "author": "Mark Fishel",
                "arxiv_journal_ref": "Findings of the Association for Computational Linguistics: NAACL\n  2025, pages 6677-6697",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02722v1",
                "updated": "2025-05-05T15:23:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    23,
                    47,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:23:47Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    23,
                    47,
                    0,
                    125,
                    0
                ],
                "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a\n  Nationwide Sepsis Registry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a\n  Nationwide Sepsis Registry"
                },
                "summary": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models."
                },
                "authors": [
                    {
                        "name": "Junu Kim"
                    },
                    {
                        "name": "Chaeeun Shim"
                    },
                    {
                        "name": "Sungjin Park"
                    },
                    {
                        "name": "Su Yeon Lee"
                    },
                    {
                        "name": "Gee Young Suh"
                    },
                    {
                        "name": "Chae-Man Lim"
                    },
                    {
                        "name": "Seong Jin Choi"
                    },
                    {
                        "name": "Song Mi Moon"
                    },
                    {
                        "name": "Kyoung-Ho Song"
                    },
                    {
                        "name": "Eu Suk Kim"
                    },
                    {
                        "name": "Hong Bin Kim"
                    },
                    {
                        "name": "Sejoong Kim"
                    },
                    {
                        "name": "Chami Im"
                    },
                    {
                        "name": "Dong-Wan Kang"
                    },
                    {
                        "name": "Yong Soo Kim"
                    },
                    {
                        "name": "Hee-Joon Bae"
                    },
                    {
                        "name": "Sung Yoon Lim"
                    },
                    {
                        "name": "Han-Gil Jeong"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02707v1",
                "updated": "2025-05-05T15:05:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    5,
                    1,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:05:01Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    5,
                    1,
                    0,
                    125,
                    0
                ],
                "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play"
                },
                "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions."
                },
                "authors": [
                    {
                        "name": "Yemin Shi"
                    },
                    {
                        "name": "Yu Shu"
                    },
                    {
                        "name": "Siwei Dong"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Jaward Sesay"
                    },
                    {
                        "name": "Jingwen Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiting Hu"
                },
                "author": "Zhiting Hu",
                "arxiv_comment": "18 pages, 7 figures, Website: https://voila.maitrix.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15449v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15449v3",
                "updated": "2025-05-05T14:58:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    58,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2024-07-22T08:07:33Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    8,
                    7,
                    33,
                    0,
                    204,
                    0
                ],
                "title": "Persistence-based Modes Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistence-based Modes Inference"
                },
                "summary": "We address the problem of estimating multiple modes of a multivariate density\nusing persistent homology, a central tool in Topological Data Analysis. We\nintroduce a method based on the preliminary estimation of the $H_0$-persistence\ndiagram to infer the number of modes, their locations, and the corresponding\nlocal maxima. For broad classes of piecewise-continuous functions with\ngeometric control on discontinuities loci, we identify a critical separation\nthreshold between modes, also interpretable in our framework in terms of modes\nprominence, below which modes inference is impossible and above which our\nprocedure achieves minimax optimal rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of estimating multiple modes of a multivariate density\nusing persistent homology, a central tool in Topological Data Analysis. We\nintroduce a method based on the preliminary estimation of the $H_0$-persistence\ndiagram to infer the number of modes, their locations, and the corresponding\nlocal maxima. For broad classes of piecewise-continuous functions with\ngeometric control on discontinuities loci, we identify a critical separation\nthreshold between modes, also interpretable in our framework in terms of modes\nprominence, below which modes inference is impossible and above which our\nprocedure achieves minimax optimal rates."
                },
                "authors": [
                    {
                        "name": "Hugo Henneuse"
                    }
                ],
                "author_detail": {
                    "name": "Hugo Henneuse"
                },
                "author": "Hugo Henneuse",
                "arxiv_comment": "36 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15449v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02705v1",
                "updated": "2025-05-05T14:57:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    57,
                    43,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:57:43Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    57,
                    43,
                    0,
                    125,
                    0
                ],
                "title": "Multi-View Learning with Context-Guided Receptance for Image Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-View Learning with Context-Guided Receptance for Image Denoising"
                },
                "summary": "Image denoising is essential in low-level vision applications such as\nphotography and automated driving. Existing methods struggle with\ndistinguishing complex noise patterns in real-world scenes and consume\nsignificant computational resources due to reliance on Transformer-based\nmodels. In this work, the Context-guided Receptance Weighted Key-Value (\\M)\nmodel is proposed, combining enhanced multi-view feature integration with\nefficient sequence modeling. Our approach introduces the Context-guided Token\nShift (CTS) paradigm, which effectively captures local spatial dependencies and\nenhance the model's ability to model real-world noise distributions.\nAdditionally, the Frequency Mix (FMix) module extracting frequency-domain\nfeatures is designed to isolate noise in high-frequency spectra, and is\nintegrated with spatial representations through a multi-view learning process.\nTo improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is\nadopted, enabling full pixel-sequence interaction with linear complexity while\novercoming the causal selection constraints. The model is validated on multiple\nreal-world image denoising datasets, outperforming the existing\nstate-of-the-art methods quantitatively and reducing inference time up to 40\\%.\nQualitative results further demonstrate the ability of our model to restore\nfine details in various scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image denoising is essential in low-level vision applications such as\nphotography and automated driving. Existing methods struggle with\ndistinguishing complex noise patterns in real-world scenes and consume\nsignificant computational resources due to reliance on Transformer-based\nmodels. In this work, the Context-guided Receptance Weighted Key-Value (\\M)\nmodel is proposed, combining enhanced multi-view feature integration with\nefficient sequence modeling. Our approach introduces the Context-guided Token\nShift (CTS) paradigm, which effectively captures local spatial dependencies and\nenhance the model's ability to model real-world noise distributions.\nAdditionally, the Frequency Mix (FMix) module extracting frequency-domain\nfeatures is designed to isolate noise in high-frequency spectra, and is\nintegrated with spatial representations through a multi-view learning process.\nTo improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is\nadopted, enabling full pixel-sequence interaction with linear complexity while\novercoming the causal selection constraints. The model is validated on multiple\nreal-world image denoising datasets, outperforming the existing\nstate-of-the-art methods quantitatively and reducing inference time up to 40\\%.\nQualitative results further demonstrate the ability of our model to restore\nfine details in various scenes."
                },
                "authors": [
                    {
                        "name": "Binghong Chen"
                    },
                    {
                        "name": "Tingting Chai"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yuanrong Xu"
                    },
                    {
                        "name": "Guanglu Zhou"
                    },
                    {
                        "name": "Xiangqian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangqian Wu"
                },
                "author": "Xiangqian Wu",
                "arxiv_comment": "Accepted by IJCAI 2025, code will be available at\n  https://github.com/Seeker98/CRWKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02703v1",
                "updated": "2025-05-05T14:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    57,
                    2,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    57,
                    2,
                    0,
                    125,
                    0
                ],
                "title": "Structure Causal Models and LLMs Integration in Medical Visual Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure Causal Models and LLMs Integration in Medical Visual Question\n  Answering"
                },
                "summary": "Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data."
                },
                "authors": [
                    {
                        "name": "Zibo Xu"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Weizhi Nie"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Anan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Anan Liu"
                },
                "author": "Anan Liu",
                "arxiv_doi": "10.1109/TMI.2025.3564320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TMI.2025.3564320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE TMI 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02699v1",
                "updated": "2025-05-05T14:51:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    51,
                    30,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:51:30Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    51,
                    30,
                    0,
                    125,
                    0
                ],
                "title": "Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for\n  History Education in Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for\n  History Education in Virtual Reality"
                },
                "summary": "Multi-role pedagogical agents can create engaging and immersive learning\nexperiences, helping learners better understand knowledge in history learning.\nHowever, existing pedagogical agents often struggle with multi-role\ninteractions due to complex controls, limited feedback forms, and difficulty\ndynamically adapting to user inputs. In this study, we developed a VR prototype\nwith LLM-powered adaptive role-switching and action-switching pedagogical\nagents to help users learn about the history of the Pavilion of Prince Teng. A\n2 x 2 between-subjects study was conducted with 84 participants to assess how\nadaptive role-switching and action-switching affect participants' learning\noutcomes and experiences. The results suggest that adaptive role-switching\nenhances participants' perception of the pedagogical agent's trustworthiness\nand expertise but may lead to inconsistent learning experiences. Adaptive\naction-switching increases participants' perceived social presence, expertise,\nand humanness. The study did not uncover any effects of role-switching and\naction-switching on usability, learning motivation, and cognitive load. Based\non the findings, we proposed five design implications for incorporating\nadaptive role-switching and action-switching into future VR history education\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-role pedagogical agents can create engaging and immersive learning\nexperiences, helping learners better understand knowledge in history learning.\nHowever, existing pedagogical agents often struggle with multi-role\ninteractions due to complex controls, limited feedback forms, and difficulty\ndynamically adapting to user inputs. In this study, we developed a VR prototype\nwith LLM-powered adaptive role-switching and action-switching pedagogical\nagents to help users learn about the history of the Pavilion of Prince Teng. A\n2 x 2 between-subjects study was conducted with 84 participants to assess how\nadaptive role-switching and action-switching affect participants' learning\noutcomes and experiences. The results suggest that adaptive role-switching\nenhances participants' perception of the pedagogical agent's trustworthiness\nand expertise but may lead to inconsistent learning experiences. Adaptive\naction-switching increases participants' perceived social presence, expertise,\nand humanness. The study did not uncover any effects of role-switching and\naction-switching on usability, learning motivation, and cognitive load. Based\non the findings, we proposed five design implications for incorporating\nadaptive role-switching and action-switching into future VR history education\ntools."
                },
                "authors": [
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Ao Yu"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_comment": "14 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://dl.acm.org/doi/10.1145/3706598.3713109",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18881v2",
                "updated": "2025-05-05T14:51:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    51,
                    5,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-26T06:56:41Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    6,
                    56,
                    41,
                    2,
                    57,
                    0
                ],
                "title": "Letters from Future Self: Augmenting the Letter-Exchange Exercise with\n  LLM-based Agents to Enhance Young Adults' Career Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letters from Future Self: Augmenting the Letter-Exchange Exercise with\n  LLM-based Agents to Enhance Young Adults' Career Exploration"
                },
                "summary": "Young adults often encounter challenges in career exploration. Self-guided\ninterventions, such as the letter-exchange exercise, where participants\nenvision and adopt the perspective of their future selves by exchanging letters\nwith their envisioned future selves, can support career development. However,\nthe broader adoption of such interventions may be limited without structured\nguidance. To address this, we integrated Large Language Model (LLM)-based\nagents that simulate participants' future selves into the letter-exchange\nexercise and evaluated their effectiveness. A one-week experiment (N=36)\ncompared three conditions: (1) participants manually writing replies to\nthemselves from the perspective of their future selves (baseline), (2)\nfuture-self agents generating letters to participants, and (3) future-self\nagents engaging in chat conversations with participants. Results indicated that\nexchanging letters with future-self agents enhanced participants' engagement\nduring the exercise, while overall benefits of the intervention on future\norientation, career self-concept, and psychological support remained comparable\nacross conditions. We discuss design implications for AI-augmented\ninterventions for supporting young adults' career exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young adults often encounter challenges in career exploration. Self-guided\ninterventions, such as the letter-exchange exercise, where participants\nenvision and adopt the perspective of their future selves by exchanging letters\nwith their envisioned future selves, can support career development. However,\nthe broader adoption of such interventions may be limited without structured\nguidance. To address this, we integrated Large Language Model (LLM)-based\nagents that simulate participants' future selves into the letter-exchange\nexercise and evaluated their effectiveness. A one-week experiment (N=36)\ncompared three conditions: (1) participants manually writing replies to\nthemselves from the perspective of their future selves (baseline), (2)\nfuture-self agents generating letters to participants, and (3) future-self\nagents engaging in chat conversations with participants. Results indicated that\nexchanging letters with future-self agents enhanced participants' engagement\nduring the exercise, while overall benefits of the intervention on future\norientation, career self-concept, and psychological support remained comparable\nacross conditions. We discuss design implications for AI-augmented\ninterventions for supporting young adults' career exploration."
                },
                "authors": [
                    {
                        "name": "Hayeon Jeon"
                    },
                    {
                        "name": "Suhwoo Yoon"
                    },
                    {
                        "name": "Keyeun Lee"
                    },
                    {
                        "name": "Seo Hyeong Kim"
                    },
                    {
                        "name": "Esther Hehsun Kim"
                    },
                    {
                        "name": "Seonghye Cho"
                    },
                    {
                        "name": "Yena Ko"
                    },
                    {
                        "name": "Soeun Yang"
                    },
                    {
                        "name": "Laura Dabbish"
                    },
                    {
                        "name": "John Zimmerman"
                    },
                    {
                        "name": "Eun-mee Kim"
                    },
                    {
                        "name": "Hajin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Hajin Lim"
                },
                "author": "Hajin Lim",
                "arxiv_doi": "10.1145/3706598.3714206",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714206",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 9 figures, Proceedings of the 2025 CHI Conference on Human\n  Factors in Computing Systems (Best Paper Award, Top 1%)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01315v2",
                "updated": "2025-05-05T14:46:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    46,
                    48,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-02T14:42:26Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    42,
                    26,
                    4,
                    122,
                    0
                ],
                "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping Large Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System"
                },
                "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."
                },
                "authors": [
                    {
                        "name": "Sheikh Samit Muhaimin"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02694v1",
                "updated": "2025-05-05T14:44:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    44,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:44:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    44,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "AI Standardized Patient Improves Human Conversations in Advanced Cancer\n  Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Standardized Patient Improves Human Conversations in Advanced Cancer\n  Care"
                },
                "summary": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education."
                },
                "authors": [
                    {
                        "name": "Kurtis Haut"
                    },
                    {
                        "name": "Masum Hasan"
                    },
                    {
                        "name": "Thomas Carroll"
                    },
                    {
                        "name": "Ronald Epstein"
                    },
                    {
                        "name": "Taylan Sen"
                    },
                    {
                        "name": "Ehsan Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Hoque"
                },
                "author": "Ehsan Hoque",
                "arxiv_comment": "20 pages, 6 figures, 4 tables, submitting to New England Journal of\n  Medicine (NEJM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02693v1",
                "updated": "2025-05-05T14:43:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    43,
                    20,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:43:20Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    43,
                    20,
                    0,
                    125,
                    0
                ],
                "title": "Predicting Movie Hits Before They Happen with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Movie Hits Before They Happen with LLMs"
                },
                "summary": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped."
                },
                "authors": [
                    {
                        "name": "Shaghayegh Agah"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Neeraj Sharma"
                    },
                    {
                        "name": "Mayur Nankani"
                    },
                    {
                        "name": "Kevin Foley"
                    },
                    {
                        "name": "H. Howie Huang"
                    },
                    {
                        "name": "Sardar Hamidian"
                    }
                ],
                "author_detail": {
                    "name": "Sardar Hamidian"
                },
                "author": "Sardar Hamidian",
                "arxiv_comment": "Accepted at ACM UMAP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02686v1",
                "updated": "2025-05-05T14:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    33,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    33,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models"
                },
                "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers."
                },
                "authors": [
                    {
                        "name": "Xiaobao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobao Wu"
                },
                "author": "Xiaobao Wu",
                "arxiv_comment": "35 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02683v1",
                "updated": "2025-05-05T14:32:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    32,
                    19,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:32:19Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    32,
                    19,
                    0,
                    125,
                    0
                ],
                "title": "Partons from stabilizer codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partons from stabilizer codes"
                },
                "summary": "The Gutzwiller projection of fermionic wave functions is a well-established\nmethod for generating variational wave functions describing exotic states of\nmatter, such as quantum spin liquids. We investigate the conditions under which\na projected wave function constructed from fermionic partons can be rigorously\nshown to possess topological order. We demonstrate that these conditions can be\nprecisely determined in the case of projected Majorana stabilizer codes. We\nthen use matrix product states to study states that interpolate between two\ndistinct Majorana fermion codes, one yielding a $\\mathbb Z_2$ spin liquid and\nthe other a trivial polarized state upon projection. While the free-fermion\nstates are adiabatically connected, we find that the projected states undergo a\nphase transition detected by the topological entanglement entropy. Our work\nunderscores the profound impact of the Gutzwiller projection and cautions\nagainst inferring properties of quantum spin liquids solely from their\nunprojected counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gutzwiller projection of fermionic wave functions is a well-established\nmethod for generating variational wave functions describing exotic states of\nmatter, such as quantum spin liquids. We investigate the conditions under which\na projected wave function constructed from fermionic partons can be rigorously\nshown to possess topological order. We demonstrate that these conditions can be\nprecisely determined in the case of projected Majorana stabilizer codes. We\nthen use matrix product states to study states that interpolate between two\ndistinct Majorana fermion codes, one yielding a $\\mathbb Z_2$ spin liquid and\nthe other a trivial polarized state upon projection. While the free-fermion\nstates are adiabatically connected, we find that the projected states undergo a\nphase transition detected by the topological entanglement entropy. Our work\nunderscores the profound impact of the Gutzwiller projection and cautions\nagainst inferring properties of quantum spin liquids solely from their\nunprojected counterparts."
                },
                "authors": [
                    {
                        "name": "Rafael A. Macedo"
                    },
                    {
                        "name": "Carlo C. Bellinati"
                    },
                    {
                        "name": "Weslei B. Fontana"
                    },
                    {
                        "name": "Eric C. Andrade"
                    },
                    {
                        "name": "Rodrigo G. Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo G. Pereira"
                },
                "author": "Rodrigo G. Pereira",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18674v2",
                "updated": "2025-05-05T14:25:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    25,
                    1,
                    0,
                    125,
                    0
                ],
                "published": "2024-11-27T18:50:15Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    50,
                    15,
                    2,
                    332,
                    0
                ],
                "title": "Active Data Curation Effectively Distills Large-Scale Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Data Curation Effectively Distills Large-Scale Multimodal Models"
                },
                "summary": "Knowledge distillation (KD) is the de facto standard for compressing\nlarge-scale models into smaller ones. Prior works have explored ever more\ncomplex KD strategies involving different objective functions,\nteacher-ensembles, and weight inheritance. In this work we explore an\nalternative, yet simple approach -- active data curation as effective\ndistillation for contrastive multimodal pretraining. Our simple online batch\nselection method, ACID, outperforms strong KD baselines across various model-,\ndata- and compute-configurations. Further, we find such an active data curation\nstrategy to in fact be complementary to standard KD, and can be effectively\ncombined to train highly performant inference-efficient models. Our simple and\nscalable pretraining framework, ACED, achieves state-of-the-art results across\n27 zero-shot classification and retrieval tasks with upto 11% less inference\nFLOPs. We further demonstrate that our ACED models yield strong vision-encoders\nfor training generative multimodal models in the LiT-Decoder setting,\noutperforming larger vision encoders for image-captioning and visual\nquestion-answering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is the de facto standard for compressing\nlarge-scale models into smaller ones. Prior works have explored ever more\ncomplex KD strategies involving different objective functions,\nteacher-ensembles, and weight inheritance. In this work we explore an\nalternative, yet simple approach -- active data curation as effective\ndistillation for contrastive multimodal pretraining. Our simple online batch\nselection method, ACID, outperforms strong KD baselines across various model-,\ndata- and compute-configurations. Further, we find such an active data curation\nstrategy to in fact be complementary to standard KD, and can be effectively\ncombined to train highly performant inference-efficient models. Our simple and\nscalable pretraining framework, ACED, achieves state-of-the-art results across\n27 zero-shot classification and retrieval tasks with upto 11% less inference\nFLOPs. We further demonstrate that our ACED models yield strong vision-encoders\nfor training generative multimodal models in the LiT-Decoder setting,\noutperforming larger vision encoders for image-captioning and visual\nquestion-answering tasks."
                },
                "authors": [
                    {
                        "name": "Vishaal Udandarao"
                    },
                    {
                        "name": "Nikhil Parthasarathy"
                    },
                    {
                        "name": "Muhammad Ferjad Naeem"
                    },
                    {
                        "name": "Talfan Evans"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Yongqin Xian"
                    },
                    {
                        "name": "Alessio Tonioni"
                    },
                    {
                        "name": "Olivier J. Hénaff"
                    }
                ],
                "author_detail": {
                    "name": "Olivier J. Hénaff"
                },
                "author": "Olivier J. Hénaff",
                "arxiv_comment": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09421v2",
                "updated": "2025-05-05T14:19:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    19,
                    8,
                    0,
                    125,
                    0
                ],
                "published": "2024-12-12T16:27:21Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    27,
                    21,
                    3,
                    347,
                    0
                ],
                "title": "HfO$_2$-based platform for high-index-contrast visible/UV integrated\n  photonics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HfO$_2$-based platform for high-index-contrast visible/UV integrated\n  photonics"
                },
                "summary": "Ultraviolet and visible integrated photonics are enabling for applications in\nquantum information, sensing, and spectroscopy, among others. Few materials\nsupport low-loss photonics into the UV, and the relatively low refractive index\nof known depositable materials limits the achievable functionality. Here we\npresent a high-index integrated photonics platform based on HfO$_2$ and\nAl$_2$O$_3$ composites deposited via Atomic Layer Deposition (ALD) with low\nloss in the visible and near-UV. We show that Al$_2$O$_3$ incorporation\ndramatically decreases bulk loss compared to pure HfO$_2$, consistent with\ninhibited crystallization due to the admixture of Al$_2$O$_3$. Composites\nexhibit refractive index $n$ following the average of that of HfO$_2$ and\nAl$_2$O$_3$, weighted by the HfO$_2$ fractional composition $x$. At\n$\\lambda=375$ nm, composites with $x=0.67$ exhibit $n=2.08$ preserving most of\nHfO$_2$'s significantly higher index, and $3.8(7) $ dB/cm material loss. We\nfurther present fully etched and cladded waveguides, grating couplers, and ring\nresonators, realizing single-mode waveguide loss of $0.25(2)$ dB/cm inferred\nfrom resonators of 2.6 million intrinsic quality factor at $\\lambda=729$ nm,\n$2.6(2)$ dB/cm at $\\lambda=405$ nm, and $7.7(6)$ dB/cm at $\\lambda=375$ nm. We\nmeasure the composite's thermo-optic coefficient (TOC) to be $2.44(3) \\times\n10^{-5}$ RIU/$^\\circ$C near $\\lambda=397$ nm. This work establishes\n(HfO$_2$)$_x$(Al$_2$O$_3$)$_{1-x}$ composites as a platform amenable to\nintegration for low-loss, high-index photonics spanning the UV to NIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultraviolet and visible integrated photonics are enabling for applications in\nquantum information, sensing, and spectroscopy, among others. Few materials\nsupport low-loss photonics into the UV, and the relatively low refractive index\nof known depositable materials limits the achievable functionality. Here we\npresent a high-index integrated photonics platform based on HfO$_2$ and\nAl$_2$O$_3$ composites deposited via Atomic Layer Deposition (ALD) with low\nloss in the visible and near-UV. We show that Al$_2$O$_3$ incorporation\ndramatically decreases bulk loss compared to pure HfO$_2$, consistent with\ninhibited crystallization due to the admixture of Al$_2$O$_3$. Composites\nexhibit refractive index $n$ following the average of that of HfO$_2$ and\nAl$_2$O$_3$, weighted by the HfO$_2$ fractional composition $x$. At\n$\\lambda=375$ nm, composites with $x=0.67$ exhibit $n=2.08$ preserving most of\nHfO$_2$'s significantly higher index, and $3.8(7) $ dB/cm material loss. We\nfurther present fully etched and cladded waveguides, grating couplers, and ring\nresonators, realizing single-mode waveguide loss of $0.25(2)$ dB/cm inferred\nfrom resonators of 2.6 million intrinsic quality factor at $\\lambda=729$ nm,\n$2.6(2)$ dB/cm at $\\lambda=405$ nm, and $7.7(6)$ dB/cm at $\\lambda=375$ nm. We\nmeasure the composite's thermo-optic coefficient (TOC) to be $2.44(3) \\times\n10^{-5}$ RIU/$^\\circ$C near $\\lambda=397$ nm. This work establishes\n(HfO$_2$)$_x$(Al$_2$O$_3$)$_{1-x}$ composites as a platform amenable to\nintegration for low-loss, high-index photonics spanning the UV to NIR."
                },
                "authors": [
                    {
                        "name": "Oscar Jaramillo"
                    },
                    {
                        "name": "Vighnesh Natarajan"
                    },
                    {
                        "name": "Hamim Mahmud Rivy"
                    },
                    {
                        "name": "Joshua Tensuan"
                    },
                    {
                        "name": "Leonardo Massai"
                    },
                    {
                        "name": "Karan K. Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Karan K. Mehta"
                },
                "author": "Karan K. Mehta",
                "arxiv_journal_ref": "Opt. Lett. 50(10), 3165-3168 (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02666v1",
                "updated": "2025-05-05T14:15:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    15,
                    2,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:15:02Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    15,
                    2,
                    0,
                    125,
                    0
                ],
                "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Progress in LLM Alignment from the Perspective of Reward\n  Design"
                },
                "summary": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies."
                },
                "authors": [
                    {
                        "name": "Miaomiao Ji"
                    },
                    {
                        "name": "Yanqiu Wu"
                    },
                    {
                        "name": "Zhibin Wu"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02665v1",
                "updated": "2025-05-05T14:14:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    14,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:14:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    14,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law"
                },
                "summary": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems."
                },
                "authors": [
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Wenkai Ji"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Shilian Chen"
                    },
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan Wu"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02659v2",
                "updated": "2025-05-06T08:34:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    34,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    5,
                    15,
                    0,
                    125,
                    0
                ],
                "title": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data."
                },
                "authors": [
                    {
                        "name": "Andrey Sidorenko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Sidorenko"
                },
                "author": "Andrey Sidorenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00711v2",
                "updated": "2025-05-05T13:57:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    57,
                    12,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-01T12:21:50Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    21,
                    50,
                    1,
                    91,
                    0
                ],
                "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited\n  Environments"
                },
                "summary": "The era of foundation models has revolutionized AI research, yet Graph\nFoundation Models (GFMs) remain constrained by the scarcity of large-scale\ngraph corpora. Traditional graph data synthesis techniques primarily focus on\nsimplistic structural operations, lacking the capacity to generate semantically\nrich nodes with meaningful textual attributes: a critical limitation for\nreal-world applications. While large language models (LLMs) demonstrate\nexceptional text generation capabilities, their direct application to graph\nsynthesis is impeded by context window limitations, hallucination phenomena,\nand structural consistency challenges. To address these issues, we introduce\nGraphMaster, the first multi-agent framework specifically designed for graph\ndata synthesis in data-limited environments. GraphMaster orchestrates four\nspecialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that\ncollaboratively optimize the synthesis process through iterative refinement,\nensuring both semantic coherence and structural integrity. To rigorously\nevaluate our approach, we create new data-limited \"Sub\" variants of six\nstandard graph benchmarks, specifically designed to test synthesis capabilities\nunder realistic constraints. Additionally, we develop a novel interpretability\nassessment framework that combines human evaluation with a principled\nGrassmannian manifold-based analysis, providing both qualitative and\nquantitative measures of semantic coherence. Experimental results demonstrate\nthat GraphMaster significantly outperforms traditional synthesis methods across\nmultiple datasets, establishing a strong foundation for advancing GFMs in\ndata-scarce environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of foundation models has revolutionized AI research, yet Graph\nFoundation Models (GFMs) remain constrained by the scarcity of large-scale\ngraph corpora. Traditional graph data synthesis techniques primarily focus on\nsimplistic structural operations, lacking the capacity to generate semantically\nrich nodes with meaningful textual attributes: a critical limitation for\nreal-world applications. While large language models (LLMs) demonstrate\nexceptional text generation capabilities, their direct application to graph\nsynthesis is impeded by context window limitations, hallucination phenomena,\nand structural consistency challenges. To address these issues, we introduce\nGraphMaster, the first multi-agent framework specifically designed for graph\ndata synthesis in data-limited environments. GraphMaster orchestrates four\nspecialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that\ncollaboratively optimize the synthesis process through iterative refinement,\nensuring both semantic coherence and structural integrity. To rigorously\nevaluate our approach, we create new data-limited \"Sub\" variants of six\nstandard graph benchmarks, specifically designed to test synthesis capabilities\nunder realistic constraints. Additionally, we develop a novel interpretability\nassessment framework that combines human evaluation with a principled\nGrassmannian manifold-based analysis, providing both qualitative and\nquantitative measures of semantic coherence. Experimental results demonstrate\nthat GraphMaster significantly outperforms traditional synthesis methods across\nmultiple datasets, establishing a strong foundation for advancing GFMs in\ndata-scarce environments."
                },
                "authors": [
                    {
                        "name": "Enjun Du"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03253v2",
                "updated": "2025-05-05T13:47:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    47,
                    32,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-05T15:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    8,
                    43,
                    2,
                    36,
                    0
                ],
                "title": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis"
                },
                "summary": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially -- to upwards of $0.99$ -- suggesting a homogenization\nin the LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially -- to upwards of $0.99$ -- suggesting a homogenization\nin the LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating."
                },
                "authors": [
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Jimmy Pronchick"
                    },
                    {
                        "name": "Krupa Bhawsar"
                    },
                    {
                        "name": "Roger E. Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger E. Beaty"
                },
                "author": "Roger E. Beaty",
                "arxiv_comment": "CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02639v1",
                "updated": "2025-05-05T13:31:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    31,
                    36,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T13:31:36Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    31,
                    36,
                    0,
                    125,
                    0
                ],
                "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large\n  Language Model and Dual-task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large\n  Language Model and Dual-task Learning"
                },
                "summary": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design."
                },
                "authors": [
                    {
                        "name": "Xuan Lin"
                    },
                    {
                        "name": "Qingrui Liu"
                    },
                    {
                        "name": "Hongxin Xiang"
                    },
                    {
                        "name": "Daojian Zeng"
                    },
                    {
                        "name": "Xiangxiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Zeng"
                },
                "author": "Xiangxiang Zeng",
                "arxiv_comment": "Accepted for publication at IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02629v1",
                "updated": "2025-05-05T13:15:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    15,
                    53,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T13:15:53Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    15,
                    53,
                    0,
                    125,
                    0
                ],
                "title": "Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for\n  Automated Patch Correctness Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for\n  Automated Patch Correctness Assessment"
                },
                "summary": "Automated program repair (APR) aims to automatically repair program errors\nwithout human intervention, and recent years have witnessed a growing interest\non this research topic. While much progress has been made and techniques\noriginating from different disciplines have been proposed, APR techniques\ngenerally suffer from the patch overfitting issue, i.e., the generated patches\nare not genuinely correct despite they pass the employed tests. To alleviate\nthis issue, many research efforts have been devoted for automated patch\ncorrectness assessment (APCA). In particular, with the emergence of large\nlanguage model (LLM) technology, researchers have employed LLM to assess the\npatch correctness and have obtained the state-of-the-art performance. The\nliterature on APCA has demonstrated the importance of capturing patch semantic\nand explicitly considering certain code attributes in predicting patch\ncorrectness. However, existing LLM-based methods typically treat code as token\nsequences and ignore the inherent formal structure for code, making it\ndifficult to capture the deep patch semantics. Moreover, these LLM-based\nmethods also do not explicitly account for enough code attributes. To overcome\nthese drawbacks, we in this paper design a novel patch graph representation\nnamed attributed patch semantic graph (APSG), which adequately captures the\npatch semantic and explicitly reflects important patch attributes. To\neffectively use graph information in APSG, we accordingly propose a new\nparameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA.\nExtensive evaluations have been conducted to evaluate our method, and the\nresults show that compared to the state-of-the-art methods, our method improves\naccuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) aims to automatically repair program errors\nwithout human intervention, and recent years have witnessed a growing interest\non this research topic. While much progress has been made and techniques\noriginating from different disciplines have been proposed, APR techniques\ngenerally suffer from the patch overfitting issue, i.e., the generated patches\nare not genuinely correct despite they pass the employed tests. To alleviate\nthis issue, many research efforts have been devoted for automated patch\ncorrectness assessment (APCA). In particular, with the emergence of large\nlanguage model (LLM) technology, researchers have employed LLM to assess the\npatch correctness and have obtained the state-of-the-art performance. The\nliterature on APCA has demonstrated the importance of capturing patch semantic\nand explicitly considering certain code attributes in predicting patch\ncorrectness. However, existing LLM-based methods typically treat code as token\nsequences and ignore the inherent formal structure for code, making it\ndifficult to capture the deep patch semantics. Moreover, these LLM-based\nmethods also do not explicitly account for enough code attributes. To overcome\nthese drawbacks, we in this paper design a novel patch graph representation\nnamed attributed patch semantic graph (APSG), which adequately captures the\npatch semantic and explicitly reflects important patch attributes. To\neffectively use graph information in APSG, we accordingly propose a new\nparameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA.\nExtensive evaluations have been conducted to evaluate our method, and the\nresults show that compared to the state-of-the-art methods, our method improves\naccuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Jingwen Wu"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Zhongxing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxing Yu"
                },
                "author": "Zhongxing Yu",
                "arxiv_comment": "16 pages, 4 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00654v2",
                "updated": "2025-05-05T13:14:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    14,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-01T16:55:44Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    55,
                    44,
                    3,
                    121,
                    0
                ],
                "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Understanding: an Inherent Ambiguity Barrier"
                },
                "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."
                },
                "authors": [
                    {
                        "name": "Daniel N. Nissani"
                    }
                ],
                "author_detail": {
                    "name": "Daniel N. Nissani"
                },
                "arxiv_affiliation": "Nissensohn",
                "author": "Daniel N. Nissani",
                "arxiv_comment": "submitted to NEURAL COMPUTATION",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02626v1",
                "updated": "2025-05-05T13:08:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    8,
                    25,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T13:08:25Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    8,
                    25,
                    0,
                    125,
                    0
                ],
                "title": "Detect, Classify, Act: Categorizing Industrial Anomalies with\n  Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Classify, Act: Categorizing Industrial Anomalies with\n  Multi-Modal Large Language Models"
                },
                "summary": "Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization."
                },
                "authors": [
                    {
                        "name": "Sassan Mokhtar"
                    },
                    {
                        "name": "Arian Mousakhan"
                    },
                    {
                        "name": "Silvio Galesso"
                    },
                    {
                        "name": "Jawad Tayyub"
                    },
                    {
                        "name": "Thomas Brox"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Brox"
                },
                "author": "Thomas Brox",
                "arxiv_comment": "Accepted as a spotlight presentation paper at the VAND Workshop, CVPR\n  2025. 10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02625v1",
                "updated": "2025-05-05T12:53:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    53,
                    9,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T12:53:09Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    53,
                    9,
                    0,
                    125,
                    0
                ],
                "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis"
                },
                "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data."
                },
                "authors": [
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16892v2",
                "updated": "2025-05-05T12:48:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    48,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-24T06:43:19Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    43,
                    19,
                    0,
                    55,
                    0
                ],
                "title": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text\n  Classification without Manually Labeled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text\n  Classification without Manually Labeled Data"
                },
                "summary": "Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework,\nachieving high cross-task text classification performance without the need for\nany manually labeled data. Furthermore, compared to directly applying GPT for\nclassification tasks, our approach retains over 93% of its classification\nperformance while requiring only approximately 6% of the computational time and\nmonetary cost, effectively balancing performance and resource efficiency. These\nfindings provide new insights into the efficient utilization of LLMs and active\nlearning algorithms in text classification tasks, paving the way for their\nbroader application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework,\nachieving high cross-task text classification performance without the need for\nany manually labeled data. Furthermore, compared to directly applying GPT for\nclassification tasks, our approach retains over 93% of its classification\nperformance while requiring only approximately 6% of the computational time and\nmonetary cost, effectively balancing performance and resource efficiency. These\nfindings provide new insights into the efficient utilization of LLMs and active\nlearning algorithms in text classification tasks, paving the way for their\nbroader application."
                },
                "authors": [
                    {
                        "name": "Yejian Zhang"
                    },
                    {
                        "name": "Shingo Takada"
                    }
                ],
                "author_detail": {
                    "name": "Shingo Takada"
                },
                "author": "Shingo Takada",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23211v2",
                "updated": "2025-05-05T12:38:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    38,
                    5,
                    0,
                    125,
                    0
                ],
                "published": "2025-03-29T20:19:39Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    20,
                    19,
                    39,
                    5,
                    88,
                    0
                ],
                "title": "Optimal Change Point Detection and Inference in the Spectral Density of\n  General Time Series Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Change Point Detection and Inference in the Spectral Density of\n  General Time Series Models"
                },
                "summary": "This paper addresses the problem of detecting change points in the spectral\ndensity of time series, motivated by EEG analysis of seizure patients. Seizures\ndisrupt coherence and functional connectivity, necessitating precise detection.\nDeparting from traditional parametric approaches, we utilize the Wold\ndecomposition, representing general time series as autoregressive processes\nwith infinite lags, which are truncated and estimated around the change point.\nOur detection procedure employs an initial estimator that systematically\nsearches across time points. We examine the localization error and its\ndependence on time series properties and sample size. To enhance accuracy, we\nintroduce an optimal rate method with an asymptotic distribution, facilitating\nthe construction of confidence intervals. The proposed method effectively\nidentifies seizure onset in EEG data and extends to event detection in video\ndata. Comprehensive numerical experiments demonstrate its superior performance\ncompared to existing techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of detecting change points in the spectral\ndensity of time series, motivated by EEG analysis of seizure patients. Seizures\ndisrupt coherence and functional connectivity, necessitating precise detection.\nDeparting from traditional parametric approaches, we utilize the Wold\ndecomposition, representing general time series as autoregressive processes\nwith infinite lags, which are truncated and estimated around the change point.\nOur detection procedure employs an initial estimator that systematically\nsearches across time points. We examine the localization error and its\ndependence on time series properties and sample size. To enhance accuracy, we\nintroduce an optimal rate method with an asymptotic distribution, facilitating\nthe construction of confidence intervals. The proposed method effectively\nidentifies seizure onset in EEG data and extends to event detection in video\ndata. Comprehensive numerical experiments demonstrate its superior performance\ncompared to existing techniques."
                },
                "authors": [
                    {
                        "name": "Sepideh Mosaferi"
                    },
                    {
                        "name": "Abolfazl Safikhani"
                    },
                    {
                        "name": "Peiliang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Peiliang Bai"
                },
                "author": "Peiliang Bai",
                "arxiv_comment": "Fixed minor typos in V1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01324v2",
                "updated": "2025-05-05T12:34:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    34,
                    8,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-02T14:55:34Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    55,
                    34,
                    4,
                    122,
                    0
                ],
                "title": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-Based Inference under Random Potential Outcomes via Riesz\n  Representation"
                },
                "summary": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments."
                },
                "authors": [
                    {
                        "name": "Yukai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yukai Yang"
                },
                "author": "Yukai Yang",
                "arxiv_comment": "37 pages, 2 figures, 2 Tables, 2 Algorithms. Preprint prepared for\n  journal submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G20, 62K99, 62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02650v2",
                "updated": "2025-05-05T12:25:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    25,
                    44,
                    0,
                    125,
                    0
                ],
                "published": "2025-03-04T14:14:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    14,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats"
                },
                "summary": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information."
                },
                "authors": [
                    {
                        "name": "William Brach"
                    },
                    {
                        "name": "Kristián Košťál"
                    },
                    {
                        "name": "Michal Ries"
                    }
                ],
                "author_detail": {
                    "name": "Michal Ries"
                },
                "author": "Michal Ries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15941v2",
                "updated": "2025-05-05T12:19:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    19,
                    32,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-22T14:35:16Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    35,
                    16,
                    1,
                    112,
                    0
                ],
                "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity"
                },
                "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."
                },
                "authors": [
                    {
                        "name": "Fanny Jourdan"
                    },
                    {
                        "name": "Yannick Chevalier"
                    },
                    {
                        "name": "Cécile Favre"
                    }
                ],
                "author_detail": {
                    "name": "Cécile Favre"
                },
                "author": "Cécile Favre",
                "arxiv_comment": "FAccT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02590v1",
                "updated": "2025-05-05T11:56:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    56,
                    12,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:56:12Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    56,
                    12,
                    0,
                    125,
                    0
                ],
                "title": "Ensemble Kalman filter for uncertainty in human language comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Kalman filter for uncertainty in human language comprehension"
                },
                "summary": "Artificial neural networks (ANNs) are widely used in modeling sentence\nprocessing but often exhibit deterministic behavior, contrasting with human\nsentence comprehension, which manages uncertainty during ambiguous or\nunexpected inputs. This is exemplified by reversal anomalies-sentences with\nunexpected role reversals that challenge syntax and semantics-highlighting the\nlimitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.\nTo address these limitations, we propose a Bayesian framework for sentence\ncomprehension, applying an extension of the ensemble Kalman filter (EnKF) for\nBayesian inference to quantify uncertainty. By framing language comprehension\nas a Bayesian inverse problem, this approach enhances the SG model's ability to\nreflect human sentence processing with respect to the representation of\nuncertainty. Numerical experiments and comparisons with maximum likelihood\nestimation (MLE) demonstrate that Bayesian methods improve uncertainty\nrepresentation, enabling the model to better approximate human cognitive\nprocessing when dealing with linguistic ambiguities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial neural networks (ANNs) are widely used in modeling sentence\nprocessing but often exhibit deterministic behavior, contrasting with human\nsentence comprehension, which manages uncertainty during ambiguous or\nunexpected inputs. This is exemplified by reversal anomalies-sentences with\nunexpected role reversals that challenge syntax and semantics-highlighting the\nlimitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.\nTo address these limitations, we propose a Bayesian framework for sentence\ncomprehension, applying an extension of the ensemble Kalman filter (EnKF) for\nBayesian inference to quantify uncertainty. By framing language comprehension\nas a Bayesian inverse problem, this approach enhances the SG model's ability to\nreflect human sentence processing with respect to the representation of\nuncertainty. Numerical experiments and comparisons with maximum likelihood\nestimation (MLE) demonstrate that Bayesian methods improve uncertainty\nrepresentation, enabling the model to better approximate human cognitive\nprocessing when dealing with linguistic ambiguities."
                },
                "authors": [
                    {
                        "name": "Diksha Bhandari"
                    },
                    {
                        "name": "Alessandro Lopopolo"
                    },
                    {
                        "name": "Milena Rabovsky"
                    },
                    {
                        "name": "Sebastian Reich"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Reich"
                },
                "author": "Sebastian Reich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02589v1",
                "updated": "2025-05-05T11:54:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    54,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:54:14Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    54,
                    14,
                    0,
                    125,
                    0
                ],
                "title": "DeepHMC : a deep-neural-network acclerated Hamiltonian Monte Carlo\n  algorithm for binary neutron star parameter estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepHMC : a deep-neural-network acclerated Hamiltonian Monte Carlo\n  algorithm for binary neutron star parameter estimation"
                },
                "summary": "We present a deep neural network (DNN) accelerated Hamiltonian Monte Carlo\n(HMC) algorithm called DeepHMC for the inference of binary neutron star\nsystems. The HMC is a non-random walk sampler that uses background gradient\ninformation to accelerate the convergence of the sampler. While faster\nconverging than a random-walk sampler, in theory by a factor of the\ndimensionality of the problem, a known computational bottleneck for HMC\nalgorithms is the calculation of gradients of the log-likelihood. We\ndemonstrate that Hamiltonian trajectories based on a DNN gradients are 30 times\nfaster than those based on the relative binning gradients, and 7000 times\nfaster than trajectories based on a naive likelihood gradient calculation.\nUsing the publicly available 128 second LVK data set for the binary neutron\nstar mergers GW170817 and GW190425, we show that not only does DeepHMC produce\nproduces highly accurate and consistent results with the LVK public data, but\nacquires 5000 statistically independent samples (SIS) in the $12D$ parameter\nspace in approximately two hours on a Macbook pro for GW170817, with a cost of\n$<1$ second/SIS, and 2.5 days for GW190425, with a cost of $\\sim25$\nseconds/SIS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a deep neural network (DNN) accelerated Hamiltonian Monte Carlo\n(HMC) algorithm called DeepHMC for the inference of binary neutron star\nsystems. The HMC is a non-random walk sampler that uses background gradient\ninformation to accelerate the convergence of the sampler. While faster\nconverging than a random-walk sampler, in theory by a factor of the\ndimensionality of the problem, a known computational bottleneck for HMC\nalgorithms is the calculation of gradients of the log-likelihood. We\ndemonstrate that Hamiltonian trajectories based on a DNN gradients are 30 times\nfaster than those based on the relative binning gradients, and 7000 times\nfaster than trajectories based on a naive likelihood gradient calculation.\nUsing the publicly available 128 second LVK data set for the binary neutron\nstar mergers GW170817 and GW190425, we show that not only does DeepHMC produce\nproduces highly accurate and consistent results with the LVK public data, but\nacquires 5000 statistically independent samples (SIS) in the $12D$ parameter\nspace in approximately two hours on a Macbook pro for GW170817, with a cost of\n$<1$ second/SIS, and 2.5 days for GW190425, with a cost of $\\sim25$\nseconds/SIS."
                },
                "authors": [
                    {
                        "name": "Jules Perret"
                    },
                    {
                        "name": "Marc Aréne"
                    },
                    {
                        "name": "Edward K. Porter"
                    }
                ],
                "author_detail": {
                    "name": "Edward K. Porter"
                },
                "author": "Edward K. Porter",
                "arxiv_comment": "22 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03601v3",
                "updated": "2025-05-05T11:54:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    54,
                    13,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-04T17:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay"
                },
                "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "12 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02587v1",
                "updated": "2025-05-05T11:42:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    42,
                    28,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:42:28Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    42,
                    28,
                    0,
                    125,
                    0
                ],
                "title": "Deriving Duration Time from Occupancy Data -- A case study in the length\n  of stay in Intensive Care Units for COVID-19 patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deriving Duration Time from Occupancy Data -- A case study in the length\n  of stay in Intensive Care Units for COVID-19 patients"
                },
                "summary": "This paper focuses on drawing information on underlying processes, which are\nnot directly observed in the data. In particular, we work with data in which\nonly the total count of units in a system at a given time point is observed,\nbut the underlying process of inflows, length of stay and outflows is not. The\nparticular data example looked at in this paper is the occupancy of intensive\ncare units (ICU) during the COVID-19 pandemic, where the aggregated numbers of\noccupied beds in ICUs on the district level (`Landkreis') are recorded, but not\nthe number of incoming and outgoing patients. The Skellam distribution allows\nus to infer the number of incoming and outgoing patients from the occupancy in\nthe ICUs. This paper goes a step beyond and approaches the question of whether\nwe can also estimate the average length of stay of ICU patients. Hence, the\ntask is to derive not only the number of incoming and outgoing units from a\ntotal net count but also to gain information on the duration time of patients\non ICUs. We make use of a stochastic Expectation-Maximisation algorithm and\nadditionally include exogenous information which are assumed to explain the\nintensity of inflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on drawing information on underlying processes, which are\nnot directly observed in the data. In particular, we work with data in which\nonly the total count of units in a system at a given time point is observed,\nbut the underlying process of inflows, length of stay and outflows is not. The\nparticular data example looked at in this paper is the occupancy of intensive\ncare units (ICU) during the COVID-19 pandemic, where the aggregated numbers of\noccupied beds in ICUs on the district level (`Landkreis') are recorded, but not\nthe number of incoming and outgoing patients. The Skellam distribution allows\nus to infer the number of incoming and outgoing patients from the occupancy in\nthe ICUs. This paper goes a step beyond and approaches the question of whether\nwe can also estimate the average length of stay of ICU patients. Hence, the\ntask is to derive not only the number of incoming and outgoing units from a\ntotal net count but also to gain information on the duration time of patients\non ICUs. We make use of a stochastic Expectation-Maximisation algorithm and\nadditionally include exogenous information which are assumed to explain the\nintensity of inflow."
                },
                "authors": [
                    {
                        "name": "Martje Rave"
                    },
                    {
                        "name": "Göran Kauermann"
                    }
                ],
                "author_detail": {
                    "name": "Göran Kauermann"
                },
                "author": "Göran Kauermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02583v1",
                "updated": "2025-05-05T11:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    35,
                    33,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:35:33Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    35,
                    33,
                    0,
                    125,
                    0
                ],
                "title": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in\n  the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in\n  the LLM Era"
                },
                "summary": "The proliferation of edge devices has generated an unprecedented volume of\ntime series data across different domains, motivating various well-customized\nmethods. Recently, Large Language Models (LLMs) have emerged as a new paradigm\nfor time series analytics by leveraging the shared sequential nature of textual\ndata and time series. However, a fundamental cross-modality gap between time\nseries and LLMs exists, as LLMs are pre-trained on textual corpora and are not\ninherently optimized for time series. Many recent proposals are designed to\naddress this issue. In this survey, we provide an up-to-date overview of\nLLMs-based cross-modality modeling for time series analytics. We first\nintroduce a taxonomy that classifies existing approaches into four groups based\non the type of textual data employed for time series modeling. We then\nsummarize key cross-modality strategies, e.g., alignment and fusion, and\ndiscuss their applications across a range of downstream tasks. Furthermore, we\nconduct experiments on multimodal datasets from different application domains\nto investigate effective combinations of textual data and cross-modality\nstrategies for enhancing time series analytics. Finally, we suggest several\npromising directions for future research. This survey is designed for a range\nof professionals, researchers, and practitioners interested in LLM-based time\nseries modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of edge devices has generated an unprecedented volume of\ntime series data across different domains, motivating various well-customized\nmethods. Recently, Large Language Models (LLMs) have emerged as a new paradigm\nfor time series analytics by leveraging the shared sequential nature of textual\ndata and time series. However, a fundamental cross-modality gap between time\nseries and LLMs exists, as LLMs are pre-trained on textual corpora and are not\ninherently optimized for time series. Many recent proposals are designed to\naddress this issue. In this survey, we provide an up-to-date overview of\nLLMs-based cross-modality modeling for time series analytics. We first\nintroduce a taxonomy that classifies existing approaches into four groups based\non the type of textual data employed for time series modeling. We then\nsummarize key cross-modality strategies, e.g., alignment and fusion, and\ndiscuss their applications across a range of downstream tasks. Furthermore, we\nconduct experiments on multimodal datasets from different application domains\nto investigate effective combinations of textual data and cross-modality\nstrategies for enhancing time series analytics. Finally, we suggest several\npromising directions for future research. This survey is designed for a range\nof professionals, researchers, and practitioners interested in LLM-based time\nseries modeling."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Shaowen Zhou"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "arxiv_comment": "Accepted by IJCAI 2025 Survey Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02579v2",
                "updated": "2025-05-06T06:26:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    6,
                    26,
                    11,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    30,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning"
                },
                "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."
                },
                "authors": [
                    {
                        "name": "Lingxiao Kong"
                    },
                    {
                        "name": "Cong Yang"
                    },
                    {
                        "name": "Susanne Neufang"
                    },
                    {
                        "name": "Oya Deniz Beyan"
                    },
                    {
                        "name": "Zeyd Boukhers"
                    }
                ],
                "author_detail": {
                    "name": "Zeyd Boukhers"
                },
                "author": "Zeyd Boukhers",
                "arxiv_comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02576v1",
                "updated": "2025-05-05T11:24:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    24,
                    20,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:24:20Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    24,
                    20,
                    0,
                    125,
                    0
                ],
                "title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer\n  Reasoning"
                },
                "summary": "Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient."
                },
                "authors": [
                    {
                        "name": "Sergio Hernández-Gutiérrez"
                    },
                    {
                        "name": "Minttu Alakuijala"
                    },
                    {
                        "name": "Alexander V. Nikitin"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02569v1",
                "updated": "2025-05-05T11:21:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    21,
                    3,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:21:03Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    21,
                    3,
                    0,
                    125,
                    0
                ],
                "title": "HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic\n  Interaction"
                },
                "summary": "This paper introduces HapticVLM, a novel multimodal system that integrates\nvision-language reasoning with deep convolutional networks to enable real-time\nhaptic feedback. HapticVLM leverages a ConvNeXt-based material recognition\nmodule to generate robust visual embeddings for accurate identification of\nobject materials, while a state-of-the-art Vision-Language Model\n(Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The\nsystem synthesizes tactile sensations by delivering vibrotactile feedback\nthrough speakers and thermal cues via a Peltier module, thereby bridging the\ngap between visual perception and tactile experience. Experimental evaluations\ndemonstrate an average recognition accuracy of 84.67% across five distinct\nauditory-tactile patterns and a temperature estimation accuracy of 86.7% based\non a tolerance-based evaluation method with an 8{\\deg}C margin of error across\n15 scenarios. Although promising, the current study is limited by the use of a\nsmall set of prominent patterns and a modest participant pool. Future work will\nfocus on expanding the range of tactile patterns and increasing user studies to\nfurther refine and validate the system's performance. Overall, HapticVLM\npresents a significant step toward context-aware, multimodal haptic interaction\nwith potential applications in virtual reality, and assistive technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces HapticVLM, a novel multimodal system that integrates\nvision-language reasoning with deep convolutional networks to enable real-time\nhaptic feedback. HapticVLM leverages a ConvNeXt-based material recognition\nmodule to generate robust visual embeddings for accurate identification of\nobject materials, while a state-of-the-art Vision-Language Model\n(Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The\nsystem synthesizes tactile sensations by delivering vibrotactile feedback\nthrough speakers and thermal cues via a Peltier module, thereby bridging the\ngap between visual perception and tactile experience. Experimental evaluations\ndemonstrate an average recognition accuracy of 84.67% across five distinct\nauditory-tactile patterns and a temperature estimation accuracy of 86.7% based\non a tolerance-based evaluation method with an 8{\\deg}C margin of error across\n15 scenarios. Although promising, the current study is limited by the use of a\nsmall set of prominent patterns and a modest participant pool. Future work will\nfocus on expanding the range of tactile patterns and increasing user studies to\nfurther refine and validate the system's performance. Overall, HapticVLM\npresents a significant step toward context-aware, multimodal haptic interaction\nwith potential applications in virtual reality, and assistive technologies."
                },
                "authors": [
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Miguel Altamirano Cabrera"
                    },
                    {
                        "name": "Dmitrii Iarchuk"
                    },
                    {
                        "name": "Yara Mahmoud"
                    },
                    {
                        "name": "Daria Trinitatova"
                    },
                    {
                        "name": "Issatay Tokmurziyev"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "Submitted to IEEE conf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v3",
                "updated": "2025-05-05T11:19:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    19,
                    53,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems"
                },
                "summary": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!"
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02560v2",
                "updated": "2025-05-06T11:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    44,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:02:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    2,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Evaluating Contrastive Feedback for Effective User Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Contrastive Feedback for Effective User Simulations"
                },
                "summary": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers."
                },
                "authors": [
                    {
                        "name": "Andreas Konstantin Kruff"
                    },
                    {
                        "name": "Timo Breuer"
                    },
                    {
                        "name": "Philipp Schaer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Schaer"
                },
                "author": "Philipp Schaer",
                "arxiv_doi": "10.1145/3726302.3730189",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730189",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02558v1",
                "updated": "2025-05-05T10:56:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    56,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:56:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    56,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "The Turing Test Is More Relevant Than Ever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing Test Is More Relevant Than Ever"
                },
                "summary": "The Turing Test, first proposed by Alan Turing in 1950, has historically\nserved as a benchmark for evaluating artificial intelligence (AI). However,\nsince the release of ELIZA in 1966, and particularly with recent advancements\nin large language models (LLMs), AI has been claimed to pass the Turing Test.\nFurthermore, criticism argues that the Turing Test primarily assesses deceptive\nmimicry rather than genuine intelligence, prompting the continuous emergence of\nalternative benchmarks. This study argues against discarding the Turing Test,\nproposing instead using more refined versions of it, for example, by\ninteracting simultaneously with both an AI and human candidate to determine who\nis who, allowing a longer interaction duration, access to the Internet and\nother AIs, using experienced people as evaluators, etc.\n  Through systematic experimentation using a web-based platform, we demonstrate\nthat richer, contextually structured testing environments significantly enhance\nparticipants' ability to differentiate between AI and human interactions.\nNamely, we show that, while an off-the-shelf LLM can pass some version of a\nTuring Test, it fails to do so when faced with a more robust version. Our\nfindings highlight that the Turing Test remains an important and effective\nmethod for evaluating AI, provided it continues to adapt as AI technology\nadvances. Additionally, the structured data gathered from these improved\ninteractions provides valuable insights into what humans expect from truly\nintelligent AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing Test, first proposed by Alan Turing in 1950, has historically\nserved as a benchmark for evaluating artificial intelligence (AI). However,\nsince the release of ELIZA in 1966, and particularly with recent advancements\nin large language models (LLMs), AI has been claimed to pass the Turing Test.\nFurthermore, criticism argues that the Turing Test primarily assesses deceptive\nmimicry rather than genuine intelligence, prompting the continuous emergence of\nalternative benchmarks. This study argues against discarding the Turing Test,\nproposing instead using more refined versions of it, for example, by\ninteracting simultaneously with both an AI and human candidate to determine who\nis who, allowing a longer interaction duration, access to the Internet and\nother AIs, using experienced people as evaluators, etc.\n  Through systematic experimentation using a web-based platform, we demonstrate\nthat richer, contextually structured testing environments significantly enhance\nparticipants' ability to differentiate between AI and human interactions.\nNamely, we show that, while an off-the-shelf LLM can pass some version of a\nTuring Test, it fails to do so when faced with a more robust version. Our\nfindings highlight that the Turing Test remains an important and effective\nmethod for evaluating AI, provided it continues to adapt as AI technology\nadvances. Additionally, the structured data gathered from these improved\ninteractions provides valuable insights into what humans expect from truly\nintelligent AI systems."
                },
                "authors": [
                    {
                        "name": "Avraham Rahimov"
                    },
                    {
                        "name": "Orel Zamler"
                    },
                    {
                        "name": "Amos Azaria"
                    }
                ],
                "author_detail": {
                    "name": "Amos Azaria"
                },
                "author": "Amos Azaria",
                "arxiv_comment": "10 pages, 5 figures, 1 listing, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02550v1",
                "updated": "2025-05-05T10:39:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    39,
                    51,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:39:51Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    39,
                    51,
                    0,
                    125,
                    0
                ],
                "title": "Bielik v3 Small: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik v3 Small: Technical Report"
                },
                "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Gwoździej"
                },
                "author": "Adrian Gwoździej",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02541v1",
                "updated": "2025-05-05T10:28:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    28,
                    50,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:28:50Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    28,
                    50,
                    0,
                    125,
                    0
                ],
                "title": "A comprehensive framework for statistical testing of brain dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive framework for statistical testing of brain dynamics"
                },
                "summary": "We introduce a comprehensive statistical framework for analysing brain\ndynamics and testing their associations with behavioural, physiological and\nother non-imaging variables. Based on a generalisation of the Hidden Markov\nModel (HMM) - the Gaussian-Linear HMM - our open-source Python package supports\nmultiple experimental paradigms, including task-based and resting-state\nstudies, and addresses a wide range of questions in neuroscience and related\nscientific fields. Inference is carried out using permutation-based methods and\nstructured Monte Carlo resampling, and the framework can easily handle\nconfounding variables, multiple testing corrections, and hierarchical\nrelationships within the data. The package includes tools for intuitive\nvisualisation of statistical results, along with comprehensive documentation\nand step-by-step tutorials to make it accessible for users of varying\nexpertise. Altogether, it provides a broadly applicable, end-to-end pipeline\nfor analysis and statistical testing of functional neural data and its\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a comprehensive statistical framework for analysing brain\ndynamics and testing their associations with behavioural, physiological and\nother non-imaging variables. Based on a generalisation of the Hidden Markov\nModel (HMM) - the Gaussian-Linear HMM - our open-source Python package supports\nmultiple experimental paradigms, including task-based and resting-state\nstudies, and addresses a wide range of questions in neuroscience and related\nscientific fields. Inference is carried out using permutation-based methods and\nstructured Monte Carlo resampling, and the framework can easily handle\nconfounding variables, multiple testing corrections, and hierarchical\nrelationships within the data. The package includes tools for intuitive\nvisualisation of statistical results, along with comprehensive documentation\nand step-by-step tutorials to make it accessible for users of varying\nexpertise. Altogether, it provides a broadly applicable, end-to-end pipeline\nfor analysis and statistical testing of functional neural data and its\ndynamics."
                },
                "authors": [
                    {
                        "name": "Nick Yao Larsen"
                    },
                    {
                        "name": "Laura Paulsen"
                    },
                    {
                        "name": "Anderson M. Winkler"
                    },
                    {
                        "name": "Diego Vidaurre"
                    }
                ],
                "author_detail": {
                    "name": "Diego Vidaurre"
                },
                "author": "Diego Vidaurre",
                "arxiv_comment": "38 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02527v1",
                "updated": "2025-05-05T10:08:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    8,
                    31,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:08:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    8,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Text to Image Generation and Editing: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text to Image Generation and Editing: A Survey"
                },
                "summary": "Text-to-image generation (T2I) refers to the text-guided generation of\nhigh-quality images. In the past few years, T2I has attracted widespread\nattention and numerous works have emerged. In this survey, we comprehensively\nreview 141 works conducted from 2021 to 2024. First, we introduce four\nfoundation model architectures of T2I (autoregression, non-autoregression, GAN\nand diffusion) and the commonly used key technologies (autoencoder, attention\nand classifier-free guidance). Secondly, we systematically compare the methods\nof these studies in two directions, T2I generation and T2I editing, including\nthe encoders and the key technologies they use. In addition, we also compare\nthe performance of these researches side by side in terms of datasets,\nevaluation metrics, training resources, and inference speed. In addition to the\nfour foundation models, we survey other works on T2I, such as energy-based\nmodels and recent Mamba and multimodality. We also investigate the potential\nsocial impact of T2I and provide some solutions. Finally, we propose unique\ninsights of improving the performance of T2I models and possible future\ndevelopment directions. In summary, this survey is the first systematic and\ncomprehensive overview of T2I, aiming to provide a valuable guide for future\nresearchers and stimulate continued progress in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation (T2I) refers to the text-guided generation of\nhigh-quality images. In the past few years, T2I has attracted widespread\nattention and numerous works have emerged. In this survey, we comprehensively\nreview 141 works conducted from 2021 to 2024. First, we introduce four\nfoundation model architectures of T2I (autoregression, non-autoregression, GAN\nand diffusion) and the commonly used key technologies (autoencoder, attention\nand classifier-free guidance). Secondly, we systematically compare the methods\nof these studies in two directions, T2I generation and T2I editing, including\nthe encoders and the key technologies they use. In addition, we also compare\nthe performance of these researches side by side in terms of datasets,\nevaluation metrics, training resources, and inference speed. In addition to the\nfour foundation models, we survey other works on T2I, such as energy-based\nmodels and recent Mamba and multimodality. We also investigate the potential\nsocial impact of T2I and provide some solutions. Finally, we propose unique\ninsights of improving the performance of T2I models and possible future\ndevelopment directions. In summary, this survey is the first systematic and\ncomprehensive overview of T2I, aiming to provide a valuable guide for future\nresearchers and stimulate continued progress in this field."
                },
                "authors": [
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "Ngai-Man Cheung"
                    },
                    {
                        "name": "Xinda Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xinda Ma"
                },
                "author": "Xinda Ma",
                "arxiv_comment": "49 pages,3 figures,3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02481v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02481v5",
                "updated": "2025-05-05T09:57:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    57,
                    34,
                    0,
                    125,
                    0
                ],
                "published": "2024-06-04T16:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    16,
                    49,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "Large Language Models as Carriers of Hidden Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Carriers of Hidden Messages"
                },
                "summary": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels)."
                },
                "authors": [
                    {
                        "name": "Jakub Hoscilowicz"
                    },
                    {
                        "name": "Pawel Popiolek"
                    },
                    {
                        "name": "Jan Rudkowski"
                    },
                    {
                        "name": "Jedrzej Bieniasz"
                    },
                    {
                        "name": "Artur Janicki"
                    }
                ],
                "author_detail": {
                    "name": "Artur Janicki"
                },
                "author": "Artur Janicki",
                "arxiv_comment": "Accepted on SECRYPT 2025 Conference. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02481v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02481v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00570v2",
                "updated": "2025-05-05T09:49:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    49,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2024-03-31T05:56:15Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    5,
                    56,
                    15,
                    6,
                    91,
                    0
                ],
                "title": "ParaICL: Towards Parallel In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaICL: Towards Parallel In-Context Learning"
                },
                "summary": "Large language models (LLMs) have become the norm in natural language\nprocessing (NLP), excelling in few-shot in-context learning (ICL) with their\nremarkable abilities. Nonetheless, the success of ICL largely hinges on the\nchoice of few-shot demonstration examples, making the selection process\nincreasingly crucial. Existing methods have delved into optimizing the quantity\nand semantic similarity of these examples to improve ICL performances. However,\nour preliminary experiments indicate that the effectiveness of ICL is limited\nby the length of the input context. Moreover, varying combinations of few-shot\ndemonstration examples can significantly boost accuracy across different test\nsamples. To address this, we propose a novel method named parallel in-context\nlearning (ParaICL) that effectively utilizes all demonstration examples without\nexceeding the manageable input context length. ParaICL employs parallel\nbatching to distribute demonstration examples into different batches according\nto the semantic similarities of the questions in the demonstrations to the test\nquestion. It then computes normalized batch semantic scores for each batch. A\nweighted average semantic objective, constrained by adaptive plausibility, is\napplied to select the most appropriate tokens. Through extensive experiments,\nwe validate the effectiveness of ParaICL and conduct ablation studies to\nunderscore its design rationale. We further demonstrate that ParaICL can\nseamlessly integrate with existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become the norm in natural language\nprocessing (NLP), excelling in few-shot in-context learning (ICL) with their\nremarkable abilities. Nonetheless, the success of ICL largely hinges on the\nchoice of few-shot demonstration examples, making the selection process\nincreasingly crucial. Existing methods have delved into optimizing the quantity\nand semantic similarity of these examples to improve ICL performances. However,\nour preliminary experiments indicate that the effectiveness of ICL is limited\nby the length of the input context. Moreover, varying combinations of few-shot\ndemonstration examples can significantly boost accuracy across different test\nsamples. To address this, we propose a novel method named parallel in-context\nlearning (ParaICL) that effectively utilizes all demonstration examples without\nexceeding the manageable input context length. ParaICL employs parallel\nbatching to distribute demonstration examples into different batches according\nto the semantic similarities of the questions in the demonstrations to the test\nquestion. It then computes normalized batch semantic scores for each batch. A\nweighted average semantic objective, constrained by adaptive plausibility, is\napplied to select the most appropriate tokens. Through extensive experiments,\nwe validate the effectiveness of ParaICL and conduct ablation studies to\nunderscore its design rationale. We further demonstrate that ParaICL can\nseamlessly integrate with existing methods."
                },
                "authors": [
                    {
                        "name": "Xingxuan Li"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02502v1",
                "updated": "2025-05-05T09:30:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    30,
                    19,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:30:19Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    30,
                    19,
                    0,
                    125,
                    0
                ],
                "title": "Unveiling the Landscape of LLM Deployment in the Wild: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Landscape of LLM Deployment in the Wild: An Empirical\n  Study"
                },
                "summary": "Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem."
                },
                "authors": [
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Jiahao Han"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02500v1",
                "updated": "2025-05-05T09:29:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    29,
                    13,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:29:13Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    29,
                    13,
                    0,
                    125,
                    0
                ],
                "title": "Automating Automotive Software Development: A Synergy of Generative AI\n  and Formal Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Automotive Software Development: A Synergy of Generative AI\n  and Formal Methods"
                },
                "summary": "As the automotive industry shifts its focus toward software-defined vehicles,\nthe need for faster and reliable software development continues to grow.\nHowever, traditional methods show their limitations. The rise of Generative\nArtificial Intelligence (GenAI), particularly Large Language Models (LLMs),\nintroduces new opportunities to automate automotive software development tasks\nsuch as requirement analysis and code generation. However, due to the\ncomplexity of automotive systems, where software components must interact with\neach other seamlessly, challenges remain in software integration and\nsystem-level validation. In this paper, we propose to combine GenAI with\nmodel-driven engineering to automate automotive software development. Our\napproach uses LLMs to convert free-text requirements into event chain\ndescriptions and to generate platform-independent software components that\nrealize the required functionality. At the same time, formal models are created\nbased on event chain descriptions to support system validation and the\ngeneration of integration code for integrating generated software components in\nthe whole vehicle system through middleware. This approach increases\ndevelopment automation while enabling formal analysis to improve system\nreliability. As a proof of concept, we used GPT-4o to implement our method and\ntested it in the CARLA simulation environment with ROS2 middleware. We\nevaluated the system in a simple Autonomous Emergency Braking scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the automotive industry shifts its focus toward software-defined vehicles,\nthe need for faster and reliable software development continues to grow.\nHowever, traditional methods show their limitations. The rise of Generative\nArtificial Intelligence (GenAI), particularly Large Language Models (LLMs),\nintroduces new opportunities to automate automotive software development tasks\nsuch as requirement analysis and code generation. However, due to the\ncomplexity of automotive systems, where software components must interact with\neach other seamlessly, challenges remain in software integration and\nsystem-level validation. In this paper, we propose to combine GenAI with\nmodel-driven engineering to automate automotive software development. Our\napproach uses LLMs to convert free-text requirements into event chain\ndescriptions and to generate platform-independent software components that\nrealize the required functionality. At the same time, formal models are created\nbased on event chain descriptions to support system validation and the\ngeneration of integration code for integrating generated software components in\nthe whole vehicle system through middleware. This approach increases\ndevelopment automation while enabling formal analysis to improve system\nreliability. As a proof of concept, we used GPT-4o to implement our method and\ntested it in the CARLA simulation environment with ROS2 middleware. We\nevaluated the system in a simple Autonomous Emergency Braking scenario."
                },
                "authors": [
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Yinglei Song"
                    },
                    {
                        "name": "Long Wen"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05547v2",
                "updated": "2025-05-05T09:26:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    26,
                    24,
                    0,
                    125,
                    0
                ],
                "published": "2024-12-07T05:49:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    5,
                    49,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large\n  Language Models"
                },
                "summary": "Large language models with retrieval-augmented generation encounter a pivotal\nchallenge in intricate retrieval tasks, e.g., multi-hop question answering,\nwhich requires the model to navigate across multiple documents and generate\ncomprehensive responses based on fragmented information. To tackle this\nchallenge, we introduce a novel Knowledge Graph-based RAG framework with a\nhierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing\nin KG-Retriever is constructed on a hierarchical index graph that consists of a\nknowledge graph layer and a collaborative document layer. The associative\nnature of graph structures is fully utilized to strengthen intra-document and\ninter-document connectivity, thereby fundamentally alleviating the information\nfragmentation problem and meanwhile improving the retrieval efficiency in\ncross-document retrieval of LLMs. With the coarse-grained collaborative\ninformation from neighboring documents and concise information from the\nknowledge graph, KG-Retriever achieves marked improvements on five public QA\ndatasets, showing the effectiveness and efficiency of our proposed RAG\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models with retrieval-augmented generation encounter a pivotal\nchallenge in intricate retrieval tasks, e.g., multi-hop question answering,\nwhich requires the model to navigate across multiple documents and generate\ncomprehensive responses based on fragmented information. To tackle this\nchallenge, we introduce a novel Knowledge Graph-based RAG framework with a\nhierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing\nin KG-Retriever is constructed on a hierarchical index graph that consists of a\nknowledge graph layer and a collaborative document layer. The associative\nnature of graph structures is fully utilized to strengthen intra-document and\ninter-document connectivity, thereby fundamentally alleviating the information\nfragmentation problem and meanwhile improving the retrieval efficiency in\ncross-document retrieval of LLMs. With the coarse-grained collaborative\ninformation from neighboring documents and concise information from the\nknowledge graph, KG-Retriever achieves marked improvements on five public QA\ndatasets, showing the effectiveness and efficiency of our proposed RAG\nframework."
                },
                "authors": [
                    {
                        "name": "Weijie Chen"
                    },
                    {
                        "name": "Ting Bai"
                    },
                    {
                        "name": "Jinbo Su"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02490v1",
                "updated": "2025-05-05T09:16:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    16,
                    43,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:16:43Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    16,
                    43,
                    0,
                    125,
                    0
                ],
                "title": "Bayesian Robust Aggregation for Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Robust Aggregation for Federated Learning"
                },
                "summary": "Federated Learning enables collaborative training of machine learning models\non decentralized data. This scheme, however, is vulnerable to adversarial\nattacks, when some of the clients submit corrupted model updates. In real-world\nscenarios, the total number of compromised clients is typically unknown, with\nthe extent of attacks potentially varying over time. To address these\nchallenges, we propose an adaptive approach for robust aggregation of model\nupdates based on Bayesian inference. The mean update is defined by the maximum\nof the likelihood marginalized over probabilities of each client to be\n`honest'. As a result, the method shares the simplicity of the classical\naverage estimators (e.g., sample mean or geometric median), being independent\nof the number of compromised clients. At the same time, it is as effective\nagainst attacks as methods specifically tailored to Federated Learning, such as\nKrum. We compare our approach with other aggregation schemes in federated\nsetting on three benchmark image classification data sets. The proposed method\nconsistently achieves state-of-the-art performance across various attack types\nwith static and varying number of malicious clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning enables collaborative training of machine learning models\non decentralized data. This scheme, however, is vulnerable to adversarial\nattacks, when some of the clients submit corrupted model updates. In real-world\nscenarios, the total number of compromised clients is typically unknown, with\nthe extent of attacks potentially varying over time. To address these\nchallenges, we propose an adaptive approach for robust aggregation of model\nupdates based on Bayesian inference. The mean update is defined by the maximum\nof the likelihood marginalized over probabilities of each client to be\n`honest'. As a result, the method shares the simplicity of the classical\naverage estimators (e.g., sample mean or geometric median), being independent\nof the number of compromised clients. At the same time, it is as effective\nagainst attacks as methods specifically tailored to Federated Learning, such as\nKrum. We compare our approach with other aggregation schemes in federated\nsetting on three benchmark image classification data sets. The proposed method\nconsistently achieves state-of-the-art performance across various attack types\nwith static and varying number of malicious clients."
                },
                "authors": [
                    {
                        "name": "Aleksandr Karakulev"
                    },
                    {
                        "name": "Usama Zafar"
                    },
                    {
                        "name": "Salman Toor"
                    },
                    {
                        "name": "Prashant Singh"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Singh"
                },
                "arxiv_affiliation": "Science for Life Laboratory, Sweden",
                "author": "Prashant Singh",
                "arxiv_comment": "14 pages, 4 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02489v1",
                "updated": "2025-05-05T09:15:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    15,
                    31,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:15:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    15,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Beyond the model: Key differentiators in large language models and\n  multi-agent services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the model: Key differentiators in large language models and\n  multi-agent services"
                },
                "summary": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable."
                },
                "authors": [
                    {
                        "name": "Muskaan Goyal"
                    },
                    {
                        "name": "Pranav Bhasin"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Bhasin"
                },
                "author": "Pranav Bhasin",
                "arxiv_doi": "10.30574/wjarr.2025.26.1.1295",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.30574/wjarr.2025.26.1.1295",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages",
                "arxiv_journal_ref": "World Journal of Advanced Research and Reviews, 2025, 26(01),\n  2703-2706",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02484v1",
                "updated": "2025-05-05T09:07:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    7,
                    22,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:07:22Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    7,
                    22,
                    0,
                    125,
                    0
                ],
                "title": "El Agente: An Autonomous Agent for Quantum Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "El Agente: An Autonomous Agent for Quantum Chemistry"
                },
                "summary": "Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry."
                },
                "authors": [
                    {
                        "name": "Yunheng Zou"
                    },
                    {
                        "name": "Austin H. Cheng"
                    },
                    {
                        "name": "Abdulrahman Aldossary"
                    },
                    {
                        "name": "Jiaru Bai"
                    },
                    {
                        "name": "Shi Xuan Leong"
                    },
                    {
                        "name": "Jorge Arturo Campos-Gonzalez-Angulo"
                    },
                    {
                        "name": "Changhyeok Choi"
                    },
                    {
                        "name": "Cher Tian Ser"
                    },
                    {
                        "name": "Gary Tom"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Ilya Yakavets"
                    },
                    {
                        "name": "Han Hao"
                    },
                    {
                        "name": "Chris Crebolder"
                    },
                    {
                        "name": "Varinia Bernales"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    }
                ],
                "author_detail": {
                    "name": "Alán Aspuru-Guzik"
                },
                "author": "Alán Aspuru-Guzik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08626v2",
                "updated": "2025-05-05T09:06:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    6,
                    44,
                    0,
                    125,
                    0
                ],
                "published": "2024-11-13T14:10:16Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    14,
                    10,
                    16,
                    2,
                    318,
                    0
                ],
                "title": "Learning-Guided Fuzzing for Testing Stateful SDN Controllers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Guided Fuzzing for Testing Stateful SDN Controllers"
                },
                "summary": "Controllers for software-defined networks (SDNs) are centralised software\ncomponents that enable advanced network functionalities, such as dynamic\ntraffic engineering and network virtualisation. However, these functionalities\nincrease the complexity of SDN controllers, making thorough testing crucial.\nSDN controllers are stateful, interacting with multiple network devices through\nsequences of control messages. Identifying stateful failures in an SDN\ncontroller is challenging due to the infinite possible sequences of control\nmessages, which result in an unbounded number of stateful interactions between\nthe controller and network devices. In this article, we propose SeqFuzzSDN, a\nlearning-guided fuzzing method for testing stateful SDN controllers. SeqFuzzSDN\naims to (1) efficiently explore the state space of the SDN controller under\ntest, (2) generate effective and diverse tests (i.e., control message\nsequences) to uncover failures, and (3) infer accurate failure-inducing models\nthat characterise the message sequences leading to failures. In addition, we\ncompare SeqFuzzSDN with three extensions of state-of-the-art (SOTA) methods for\nfuzzing SDNs. Our findings show that, compared to the extended SOTA methods,\nSeqFuzzSDN (1) generates more diverse message sequences that lead to failures\nwithin the same time budget, and (2) produces more accurate failure-inducing\nmodels, significantly outperforming the other extended SOTA methods in terms of\nsensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllers for software-defined networks (SDNs) are centralised software\ncomponents that enable advanced network functionalities, such as dynamic\ntraffic engineering and network virtualisation. However, these functionalities\nincrease the complexity of SDN controllers, making thorough testing crucial.\nSDN controllers are stateful, interacting with multiple network devices through\nsequences of control messages. Identifying stateful failures in an SDN\ncontroller is challenging due to the infinite possible sequences of control\nmessages, which result in an unbounded number of stateful interactions between\nthe controller and network devices. In this article, we propose SeqFuzzSDN, a\nlearning-guided fuzzing method for testing stateful SDN controllers. SeqFuzzSDN\naims to (1) efficiently explore the state space of the SDN controller under\ntest, (2) generate effective and diverse tests (i.e., control message\nsequences) to uncover failures, and (3) infer accurate failure-inducing models\nthat characterise the message sequences leading to failures. In addition, we\ncompare SeqFuzzSDN with three extensions of state-of-the-art (SOTA) methods for\nfuzzing SDNs. Our findings show that, compared to the extended SOTA methods,\nSeqFuzzSDN (1) generates more diverse message sequences that lead to failures\nwithin the same time budget, and (2) produces more accurate failure-inducing\nmodels, significantly outperforming the other extended SOTA methods in terms of\nsensitivity."
                },
                "authors": [
                    {
                        "name": "Raphaël Ollando"
                    },
                    {
                        "name": "Seung Yeob Shin"
                    },
                    {
                        "name": "Lionel C. Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel C. Briand"
                },
                "author": "Lionel C. Briand",
                "arxiv_doi": "10.1145/3733717",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733717",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.08626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02483v1",
                "updated": "2025-05-05T09:06:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    6,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:06:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    6,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic\n  Skill Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic\n  Skill Learning"
                },
                "summary": "Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks."
                },
                "authors": [
                    {
                        "name": "Changxin Huang"
                    },
                    {
                        "name": "Junyang Liang"
                    },
                    {
                        "name": "Yanbin Chang"
                    },
                    {
                        "name": "Jingzhao Xu"
                    },
                    {
                        "name": "Jianqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiang Li"
                },
                "author": "Jianqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04667v2",
                "updated": "2025-05-05T09:01:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    1,
                    6,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-07T05:21:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    21,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances\n  Reasoning Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances\n  Reasoning Generalization"
                },
                "summary": "The integration of explicit Chain-of-Thought (CoT) reasoning into training\nlarge language models (LLMs) has advanced their reasoning capabilities, yet the\nmechanisms by which CoT enhances generalization remain poorly understood. This\nwork investigates (1) \\textit{how} CoT training reshapes internal model\nrepresentations and (2) \\textit{why} it improves both in-distribution (ID) and\nout-of-distribution (OOD) reasoning generalization. Through controlled\nexperiments and theoretical analysis, we derive the following key insights.\n\\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a\ntwo-stage generalizing circuit, where the number of stages corresponds to the\nexplicit reasoning steps during training. Notably, CoT-trained models resolve\nintermediate results at shallower layers compared to non-CoT counterparts,\nfreeing up deeper layers to specialize in subsequent reasoning steps.\n\\textbf{2)} Theoretical Analysis: the information-theoretic generalization\nbounds via distributional divergence can be decomposed into ID and OOD\ncomponents. While ID error diminishes with sufficient training regardless of\nCoT, OOD error critically depends on CoT: Non-CoT training fails to generalize\nto OOD samples due to unseen reasoning patterns, whereas CoT training achieves\nnear-perfect OOD generalization by mastering subtasks and reasoning\ncompositions during training. The identified mechanisms explain our\nexperimental results: CoT training accelerates convergence and enhances\ngeneralization from ID to both ID and OOD scenarios while maintaining robust\nperformance even with tolerable noise. These findings are further validated on\ncomplex real-world datasets. This paper offers valuable insights for designing\nCoT strategies to enhance LLM reasoning robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of explicit Chain-of-Thought (CoT) reasoning into training\nlarge language models (LLMs) has advanced their reasoning capabilities, yet the\nmechanisms by which CoT enhances generalization remain poorly understood. This\nwork investigates (1) \\textit{how} CoT training reshapes internal model\nrepresentations and (2) \\textit{why} it improves both in-distribution (ID) and\nout-of-distribution (OOD) reasoning generalization. Through controlled\nexperiments and theoretical analysis, we derive the following key insights.\n\\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a\ntwo-stage generalizing circuit, where the number of stages corresponds to the\nexplicit reasoning steps during training. Notably, CoT-trained models resolve\nintermediate results at shallower layers compared to non-CoT counterparts,\nfreeing up deeper layers to specialize in subsequent reasoning steps.\n\\textbf{2)} Theoretical Analysis: the information-theoretic generalization\nbounds via distributional divergence can be decomposed into ID and OOD\ncomponents. While ID error diminishes with sufficient training regardless of\nCoT, OOD error critically depends on CoT: Non-CoT training fails to generalize\nto OOD samples due to unseen reasoning patterns, whereas CoT training achieves\nnear-perfect OOD generalization by mastering subtasks and reasoning\ncompositions during training. The identified mechanisms explain our\nexperimental results: CoT training accelerates convergence and enhances\ngeneralization from ID to both ID and OOD scenarios while maintaining robust\nperformance even with tolerable noise. These findings are further validated on\ncomplex real-world datasets. This paper offers valuable insights for designing\nCoT strategies to enhance LLM reasoning robustness."
                },
                "authors": [
                    {
                        "name": "Xinhao Yao"
                    },
                    {
                        "name": "Ruifeng Ren"
                    },
                    {
                        "name": "Yun Liao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02466v1",
                "updated": "2025-05-05T08:52:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    52,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T08:52:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    52,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language,\n  and Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language,\n  and Modality"
                },
                "summary": "Recent advancements in large language models (LLMs) have driven interest in\nbillion-scale retrieval models with strong generalization across retrieval\ntasks and languages. Additionally, progress in large vision-language models has\ncreated new opportunities for multimodal retrieval. In response, we have\nupdated the Tevatron toolkit, introducing a unified pipeline that enables\nresearchers to explore retriever models at different scales, across multiple\nlanguages, and with various modalities. This demo paper highlights the\ntoolkit's key features, bridging academia and industry by supporting efficient\ntraining, inference, and evaluation of neural retrievers. We showcase a unified\ndense retriever achieving strong multilingual and multimodal effectiveness, and\nconduct a cross-modality zero-shot study to demonstrate its research potential.\nAlongside, we release OmniEmbed, to the best of our knowledge, the first\nembedding model that unifies text, image document, video, and audio retrieval,\nserving as a baseline for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have driven interest in\nbillion-scale retrieval models with strong generalization across retrieval\ntasks and languages. Additionally, progress in large vision-language models has\ncreated new opportunities for multimodal retrieval. In response, we have\nupdated the Tevatron toolkit, introducing a unified pipeline that enables\nresearchers to explore retriever models at different scales, across multiple\nlanguages, and with various modalities. This demo paper highlights the\ntoolkit's key features, bridging academia and industry by supporting efficient\ntraining, inference, and evaluation of neural retrievers. We showcase a unified\ndense retriever achieving strong multilingual and multimodal effectiveness, and\nconduct a cross-modality zero-shot study to demonstrate its research potential.\nAlongside, we release OmniEmbed, to the best of our knowledge, the first\nembedding model that unifies text, image document, video, and audio retrieval,\nserving as a baseline for future research."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Luyu Gao"
                    },
                    {
                        "name": "Shengyao Zhuang"
                    },
                    {
                        "name": "Jiaqi Samantha Zhan"
                    },
                    {
                        "name": "Jamie Callan"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "Accepted in SIGIR 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.12942v3",
                "updated": "2025-05-05T08:52:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    52,
                    43,
                    0,
                    125,
                    0
                ],
                "published": "2022-11-23T13:25:33Z",
                "published_parsed": [
                    2022,
                    11,
                    23,
                    13,
                    25,
                    33,
                    2,
                    327,
                    0
                ],
                "title": "Enhanced RMT estimator for signal number estimation in the presence of\n  colored noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced RMT estimator for signal number estimation in the presence of\n  colored noise"
                },
                "summary": "The subspace-based techniques are widely utilized in various scientific\nfields, and they need accurate estimation of the signal subspace dimension. The\nclassic RMT estimator for model order estimation based on random matrix theory\nassumes that the noise is white Gaussian, and performs poorly in the presence\nof colored noise with unknown covariance matrix. In the presence of colored\nnoise, the multivariate regression (MV-R) algorithm models the source detection\nas a multivariate regression problem and infers the model order from the\ncovariance matrix of the residual error. However, the MV-R algorithm requires\nthat the noise is sufficiently weaker than the signal. In order to deal with\nthese problems, this paper proposes a novel signal number estimation algorithm\nin the presence of colored noise based on the analysis of the behavior of\ninformation theoretic criteria. Firstly, a first criterion is defined as the\nratio of the current eigenvalue and the mean of the next ones, and its\nproperties is analyzed with respect to the over-modeling and under-modeling.\nMoreover, a second criterion is designed as the ratio of the current value and\nthe next value of the first criterion, and its properties is analyzed with\nrespect to the over-modeling and under-modeling. Then, a novel enhanced RMT\nestimator is proposed for signal number estimation by analyzing the detection\nproperties among the signal number estimates obtained by these two criteria,\nthe MV-R estimator and the RMT estimator to sequentially determine whether the\neigenvalue being tested is arising from a signal or from noise. Finally,\nsimulation results are presented to illustrate that the proposed enhanced RMT\nestimator has better estimation performance than the existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The subspace-based techniques are widely utilized in various scientific\nfields, and they need accurate estimation of the signal subspace dimension. The\nclassic RMT estimator for model order estimation based on random matrix theory\nassumes that the noise is white Gaussian, and performs poorly in the presence\nof colored noise with unknown covariance matrix. In the presence of colored\nnoise, the multivariate regression (MV-R) algorithm models the source detection\nas a multivariate regression problem and infers the model order from the\ncovariance matrix of the residual error. However, the MV-R algorithm requires\nthat the noise is sufficiently weaker than the signal. In order to deal with\nthese problems, this paper proposes a novel signal number estimation algorithm\nin the presence of colored noise based on the analysis of the behavior of\ninformation theoretic criteria. Firstly, a first criterion is defined as the\nratio of the current eigenvalue and the mean of the next ones, and its\nproperties is analyzed with respect to the over-modeling and under-modeling.\nMoreover, a second criterion is designed as the ratio of the current value and\nthe next value of the first criterion, and its properties is analyzed with\nrespect to the over-modeling and under-modeling. Then, a novel enhanced RMT\nestimator is proposed for signal number estimation by analyzing the detection\nproperties among the signal number estimates obtained by these two criteria,\nthe MV-R estimator and the RMT estimator to sequentially determine whether the\neigenvalue being tested is arising from a signal or from noise. Finally,\nsimulation results are presented to illustrate that the proposed enhanced RMT\nestimator has better estimation performance than the existing methods."
                },
                "authors": [
                    {
                        "name": "Huiyue Yi"
                    },
                    {
                        "name": "Wuxiong Zhang"
                    },
                    {
                        "name": "Hui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xu"
                },
                "author": "Hui Xu",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01713v2",
                "updated": "2025-05-05T08:52:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    52,
                    8,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-03T15:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    58,
                    42,
                    0,
                    34,
                    0
                ],
                "title": "Auditing a Dutch Public Sector Risk Profiling Algorithm Using an\n  Unsupervised Bias Detection Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing a Dutch Public Sector Risk Profiling Algorithm Using an\n  Unsupervised Bias Detection Tool"
                },
                "summary": "Algorithms are increasingly used to automate or aid human decisions, yet\nrecent research shows that these algorithms may exhibit bias across legally\nprotected demographic groups. However, data on these groups may be unavailable\nto organizations or external auditors due to privacy legislation. This paper\nstudies bias detection using an unsupervised clustering tool when data on\ndemographic groups are unavailable. We collaborate with the Dutch Executive\nAgency for Education to audit an algorithm that was used to assign risk scores\nto college students at the national level in the Netherlands between 2012-2023.\nOur audit covers more than 250,000 students from the whole country. The\nunsupervised clustering tool highlights known disparities between students with\na non-European migration background and Dutch origin. Our contributions are\nthree-fold: (1) we assess bias in a real-world, large-scale and high-stakes\ndecision-making process by a governmental organization; (2) we use simulation\nstudies to highlight potential pitfalls of using the unsupervised clustering\ntool to detect true bias when demographic group data are unavailable and\nprovide recommendations for valid inferences; (3) we provide the unsupervised\nclustering tool in an open-source library. Our work serves as a starting point\nfor a deliberative assessment by human experts to evaluate potential\ndiscrimination in algorithmic-supported decision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithms are increasingly used to automate or aid human decisions, yet\nrecent research shows that these algorithms may exhibit bias across legally\nprotected demographic groups. However, data on these groups may be unavailable\nto organizations or external auditors due to privacy legislation. This paper\nstudies bias detection using an unsupervised clustering tool when data on\ndemographic groups are unavailable. We collaborate with the Dutch Executive\nAgency for Education to audit an algorithm that was used to assign risk scores\nto college students at the national level in the Netherlands between 2012-2023.\nOur audit covers more than 250,000 students from the whole country. The\nunsupervised clustering tool highlights known disparities between students with\na non-European migration background and Dutch origin. Our contributions are\nthree-fold: (1) we assess bias in a real-world, large-scale and high-stakes\ndecision-making process by a governmental organization; (2) we use simulation\nstudies to highlight potential pitfalls of using the unsupervised clustering\ntool to detect true bias when demographic group data are unavailable and\nprovide recommendations for valid inferences; (3) we provide the unsupervised\nclustering tool in an open-source library. Our work serves as a starting point\nfor a deliberative assessment by human experts to evaluate potential\ndiscrimination in algorithmic-supported decision-making processes."
                },
                "authors": [
                    {
                        "name": "Floris Holstege"
                    },
                    {
                        "name": "Mackenzie Jorgensen"
                    },
                    {
                        "name": "Kirtan Padh"
                    },
                    {
                        "name": "Jurriaan Parie"
                    },
                    {
                        "name": "Joel Persson"
                    },
                    {
                        "name": "Krsto Prorokovic"
                    },
                    {
                        "name": "Lukas Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Snoek"
                },
                "author": "Lukas Snoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02456v1",
                "updated": "2025-05-05T08:40:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    40,
                    51,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T08:40:51Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    40,
                    51,
                    0,
                    125,
                    0
                ],
                "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in\n  Occupation Recommendations from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in\n  Occupation Recommendations from LLMs"
                },
                "summary": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work."
                },
                "authors": [
                    {
                        "name": "Elisa Forcada Rodríguez"
                    },
                    {
                        "name": "Olatz Perez-de-Viñaspre"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Vagrant Gautam"
                    }
                ],
                "author_detail": {
                    "name": "Vagrant Gautam"
                },
                "author": "Vagrant Gautam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02597v3",
                "updated": "2025-05-05T08:20:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    20,
                    1,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-03T15:38:20Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    38,
                    20,
                    3,
                    277,
                    0
                ],
                "title": "HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR"
                },
                "summary": "We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel\narchitecture for speech recognition that extends the Token-and-Duration\nTransducer (TDT) model. Trained with randomly masked predictor network outputs,\nHAINAN supports both autoregressive inference with all network components and\nnon-autoregressive inference without the predictor. Additionally, we propose a\nnovel semi-autoregressive inference paradigm that first generates an initial\nhypothesis using non-autoregressive inference, followed by refinement steps\nwhere each token prediction is regenerated using parallelized autoregression on\nthe initial hypothesis. Experiments on multiple datasets across different\nlanguages demonstrate that HAINAN achieves efficiency parity with CTC in\nnon-autoregressive mode and with TDT in autoregressive mode. In terms of\naccuracy, autoregressive HAINAN outperforms TDT and RNN-T, while\nnon-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive\ninference further enhances the model's accuracy with minimal computational\noverhead, and even outperforms TDT results in some cases. These results\nhighlight HAINAN's flexibility in balancing accuracy and speed, positioning it\nas a strong candidate for real-world speech recognition applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel\narchitecture for speech recognition that extends the Token-and-Duration\nTransducer (TDT) model. Trained with randomly masked predictor network outputs,\nHAINAN supports both autoregressive inference with all network components and\nnon-autoregressive inference without the predictor. Additionally, we propose a\nnovel semi-autoregressive inference paradigm that first generates an initial\nhypothesis using non-autoregressive inference, followed by refinement steps\nwhere each token prediction is regenerated using parallelized autoregression on\nthe initial hypothesis. Experiments on multiple datasets across different\nlanguages demonstrate that HAINAN achieves efficiency parity with CTC in\nnon-autoregressive mode and with TDT in autoregressive mode. In terms of\naccuracy, autoregressive HAINAN outperforms TDT and RNN-T, while\nnon-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive\ninference further enhances the model's accuracy with minimal computational\noverhead, and even outperforms TDT results in some cases. These results\nhighlight HAINAN's flexibility in balancing accuracy and speed, positioning it\nas a strong candidate for real-world speech recognition applications."
                },
                "authors": [
                    {
                        "name": "Hainan Xu"
                    },
                    {
                        "name": "Travis M. Bartley"
                    },
                    {
                        "name": "Vladimir Bataev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02428v1",
                "updated": "2025-05-05T07:47:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    47,
                    21,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:47:21Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    47,
                    21,
                    0,
                    125,
                    0
                ],
                "title": "Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A\n  Randomized Study with 90+ Novice Counselors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A\n  Randomized Study with 90+ Novice Counselors"
                },
                "summary": "Training more counselors, from clinical students to peer supporters, can help\nmeet the demand for accessible mental health support; however, current training\napproaches remain resource-intensive and difficult to scale effectively. Large\nLanguage Models (LLMs) offer promising solutions for growing counseling skills\ntraining through simulated practice and automated feedback. Despite successes\nin aligning LLMs with expert-counselor annotations, we do not know whether\nLLM-based counseling training tools -- such as AI patients that simulate\nreal-world challenges and generative AI feedback with suggested alternatives\nand rationales -- actually lead to improvements in novice counselor skill\ndevelopment. We develop CARE, an LLM-simulated practice and feedback system,\nand randomize 94 novice counselors to practice using an AI patient, either\nalone or with AI feedback, measuring changes in their behavioral performance,\nself-assessments, and qualitative learning takeaways. Our results show the\npractice-and-feedback group improved in their use of reflections and questions\n(d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI\npatient alone did not show improvements, and in the case of empathy, actually\nhad worse uses across time (d=$-$0.52, p=0.001) and when compared against the\npractice-and-feedback group (d=0.72, p=0.001). Participants' qualitative\nself-reflections revealed key differences: the practice-and-feedback group\nadopted a client-centered approach involving listening to and validating\nfeelings, while the practice-alone group remained solution-oriented but delayed\noffering suggestions until gathering more information. Overall, these results\nsuggest that LLM-based training systems can promote effective skill\ndevelopment, but that combining both simulated practice and structured feedback\nis critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training more counselors, from clinical students to peer supporters, can help\nmeet the demand for accessible mental health support; however, current training\napproaches remain resource-intensive and difficult to scale effectively. Large\nLanguage Models (LLMs) offer promising solutions for growing counseling skills\ntraining through simulated practice and automated feedback. Despite successes\nin aligning LLMs with expert-counselor annotations, we do not know whether\nLLM-based counseling training tools -- such as AI patients that simulate\nreal-world challenges and generative AI feedback with suggested alternatives\nand rationales -- actually lead to improvements in novice counselor skill\ndevelopment. We develop CARE, an LLM-simulated practice and feedback system,\nand randomize 94 novice counselors to practice using an AI patient, either\nalone or with AI feedback, measuring changes in their behavioral performance,\nself-assessments, and qualitative learning takeaways. Our results show the\npractice-and-feedback group improved in their use of reflections and questions\n(d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI\npatient alone did not show improvements, and in the case of empathy, actually\nhad worse uses across time (d=$-$0.52, p=0.001) and when compared against the\npractice-and-feedback group (d=0.72, p=0.001). Participants' qualitative\nself-reflections revealed key differences: the practice-and-feedback group\nadopted a client-centered approach involving listening to and validating\nfeelings, while the practice-alone group remained solution-oriented but delayed\noffering suggestions until gathering more information. Overall, these results\nsuggest that LLM-based training systems can promote effective skill\ndevelopment, but that combining both simulated practice and structured feedback\nis critical."
                },
                "authors": [
                    {
                        "name": "Ryan Louie"
                    },
                    {
                        "name": "Ifdita Hasan Orney"
                    },
                    {
                        "name": "Juan Pablo Pacheco"
                    },
                    {
                        "name": "Raj Sanjay Shah"
                    },
                    {
                        "name": "Emma Brunskill"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "main paper is 11 pages, with methods it is 18 pages, with appendix\n  and references it is 33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06977v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06977v5",
                "updated": "2025-05-05T07:35:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    35,
                    9,
                    0,
                    125,
                    0
                ],
                "published": "2024-08-13T15:33:27Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    33,
                    27,
                    1,
                    226,
                    0
                ],
                "title": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogeneity Corrections in Binary Outcome Models with Nonlinear\n  Transformations: Identification and Inference"
                },
                "summary": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: either the endogenous\nregressor is a nonlinear function of one component of the error term,\nconditional on the exogenous regressors, or the dependence between the\nendogenous and exogenous regressors is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: either the endogenous\nregressor is a nonlinear function of one component of the error term,\nconditional on the exogenous regressors, or the dependence between the\nendogenous and exogenous regressors is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method."
                },
                "authors": [
                    {
                        "name": "Alexander Mayer"
                    },
                    {
                        "name": "Dominik Wied"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Wied"
                },
                "author": "Dominik Wied",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06977v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06977v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10519v3",
                "updated": "2025-05-05T07:25:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    25,
                    31,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-14T13:59:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    59,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors"
                },
                "summary": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking."
                },
                "authors": [
                    {
                        "name": "Noemi Bührer"
                    },
                    {
                        "name": "Saúl Alonso-Monsalve"
                    },
                    {
                        "name": "Matthew Franks"
                    },
                    {
                        "name": "Till Dieminger"
                    },
                    {
                        "name": "Davide Sgalaberna"
                    }
                ],
                "author_detail": {
                    "name": "Davide Sgalaberna"
                },
                "author": "Davide Sgalaberna",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02418v1",
                "updated": "2025-05-05T07:24:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    24,
                    38,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:24:38Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    24,
                    38,
                    0,
                    125,
                    0
                ],
                "title": "SymbioticRAG: Enhancing Document Intelligence Through Human-LLM\n  Symbiotic Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymbioticRAG: Enhancing Document Intelligence Through Human-LLM\n  Symbiotic Collaboration"
                },
                "summary": "We present \\textbf{SymbioticRAG}, a novel framework that fundamentally\nreimagines Retrieval-Augmented Generation~(RAG) systems by establishing a\nbidirectional learning relationship between humans and machines. Our approach\naddresses two critical challenges in current RAG systems: the inherently\nhuman-centered nature of relevance determination and users' progression from\n\"unconscious incompetence\" in query formulation. SymbioticRAG introduces a\ntwo-tier solution where Level 1 enables direct human curation of retrieved\ncontent through interactive source document exploration, while Level 2 aims to\nbuild personalized retrieval models based on captured user interactions. We\nimplement Level 1 through three key components: (1)~a comprehensive document\nprocessing pipeline with specialized models for layout detection, OCR, and\nextraction of tables, formulas, and figures; (2)~an extensible retriever module\nsupporting multiple retrieval strategies; and (3)~an interactive interface that\nfacilitates both user engagement and interaction data logging. We experiment\nLevel 2 implementation via a retriever strategy incorporated LLM summarized\nuser intention from user interaction logs. To maintain high-quality data\npreparation, we develop a human-on-the-loop validation interface that improves\npipeline output while advancing research in specialized extraction tasks.\nEvaluation across three scenarios (literature review, geological exploration,\nand education) demonstrates significant improvements in retrieval relevance and\nuser satisfaction compared to traditional RAG approaches. To facilitate broader\nresearch and further advancement of SymbioticRAG Level 2 implementation, we\nwill make our system openly accessible to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textbf{SymbioticRAG}, a novel framework that fundamentally\nreimagines Retrieval-Augmented Generation~(RAG) systems by establishing a\nbidirectional learning relationship between humans and machines. Our approach\naddresses two critical challenges in current RAG systems: the inherently\nhuman-centered nature of relevance determination and users' progression from\n\"unconscious incompetence\" in query formulation. SymbioticRAG introduces a\ntwo-tier solution where Level 1 enables direct human curation of retrieved\ncontent through interactive source document exploration, while Level 2 aims to\nbuild personalized retrieval models based on captured user interactions. We\nimplement Level 1 through three key components: (1)~a comprehensive document\nprocessing pipeline with specialized models for layout detection, OCR, and\nextraction of tables, formulas, and figures; (2)~an extensible retriever module\nsupporting multiple retrieval strategies; and (3)~an interactive interface that\nfacilitates both user engagement and interaction data logging. We experiment\nLevel 2 implementation via a retriever strategy incorporated LLM summarized\nuser intention from user interaction logs. To maintain high-quality data\npreparation, we develop a human-on-the-loop validation interface that improves\npipeline output while advancing research in specialized extraction tasks.\nEvaluation across three scenarios (literature review, geological exploration,\nand education) demonstrates significant improvements in retrieval relevance and\nuser satisfaction compared to traditional RAG approaches. To facilitate broader\nresearch and further advancement of SymbioticRAG Level 2 implementation, we\nwill make our system openly accessible to the research community."
                },
                "authors": [
                    {
                        "name": "Qiang Sun"
                    },
                    {
                        "name": "Tingting Bi"
                    },
                    {
                        "name": "Sirui Li"
                    },
                    {
                        "name": "Eun-Jung Holden"
                    },
                    {
                        "name": "Paul Duuring"
                    },
                    {
                        "name": "Kai Niu"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02413v1",
                "updated": "2025-05-05T07:18:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    18,
                    47,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:18:47Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    18,
                    47,
                    0,
                    125,
                    0
                ],
                "title": "Task-Oriented Semantic Communication in Large Multimodal Models-based\n  Vehicle Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Oriented Semantic Communication in Large Multimodal Models-based\n  Vehicle Networks"
                },
                "summary": "Task-oriented semantic communication has emerged as a fundamental approach\nfor enhancing performance in various communication scenarios. While recent\nadvances in Generative Artificial Intelligence (GenAI), such as Large Language\nModels (LLMs), have been applied to semantic communication designs, the\npotential of Large Multimodal Models (LMMs) remains largely unexplored. In this\npaper, we investigate an LMM-based vehicle AI assistant using a Large Language\nand Vision Assistant (LLaVA) and propose a task-oriented semantic communication\nframework to facilitate efficient interaction between users and cloud servers.\nTo reduce computational demands and shorten response time, we optimize LLaVA's\nimage slicing to selectively focus on areas of utmost interest to users.\nAdditionally, we assess the importance of image patches by combining objective\nand subjective user attention, adjusting energy usage for transmitting semantic\ninformation. This strategy optimizes resource utilization, ensuring precise\ntransmission of critical information. We construct a Visual Question Answering\n(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental\nresults show that our semantic communication framework significantly increases\naccuracy in answering questions under the same channel conditions, performing\nparticularly well in environments with poor Signal-to-Noise Ratios (SNR).\nAccuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented semantic communication has emerged as a fundamental approach\nfor enhancing performance in various communication scenarios. While recent\nadvances in Generative Artificial Intelligence (GenAI), such as Large Language\nModels (LLMs), have been applied to semantic communication designs, the\npotential of Large Multimodal Models (LMMs) remains largely unexplored. In this\npaper, we investigate an LMM-based vehicle AI assistant using a Large Language\nand Vision Assistant (LLaVA) and propose a task-oriented semantic communication\nframework to facilitate efficient interaction between users and cloud servers.\nTo reduce computational demands and shorten response time, we optimize LLaVA's\nimage slicing to selectively focus on areas of utmost interest to users.\nAdditionally, we assess the importance of image patches by combining objective\nand subjective user attention, adjusting energy usage for transmitting semantic\ninformation. This strategy optimizes resource utilization, ensuring precise\ntransmission of critical information. We construct a Visual Question Answering\n(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental\nresults show that our semantic communication framework significantly increases\naccuracy in answering questions under the same channel conditions, performing\nparticularly well in environments with poor Signal-to-Noise Ratios (SNR).\nAccuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Baoxia Du"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Ruidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruidong Li"
                },
                "author": "Ruidong Li",
                "arxiv_doi": "10.1109/TMC.2025.3564543",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TMC.2025.3564543",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15210v2",
                "updated": "2025-05-05T06:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    56,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-21T16:29:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    29,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs"
                },
                "summary": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark."
                },
                "authors": [
                    {
                        "name": "Marina Sakharova"
                    },
                    {
                        "name": "Abhinav Anand"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02405v1",
                "updated": "2025-05-05T06:55:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    55,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:55:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    55,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "Estimating Commonsense Scene Composition on Belief Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Commonsense Scene Composition on Belief Scene Graphs"
                },
                "summary": "This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types."
                },
                "authors": [
                    {
                        "name": "Mario A. V. Saucedo"
                    },
                    {
                        "name": "Vignesh Kottayam Viswanathan"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "Accepted at ICRA25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03471v2",
                "updated": "2025-05-05T06:32:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    32,
                    15,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-04T14:23:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    23,
                    30,
                    4,
                    94,
                    0
                ],
                "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis"
                },
                "summary": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting"
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Ziqi He"
                    },
                    {
                        "name": "Yang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhou"
                },
                "author": "Yang Zhou",
                "arxiv_comment": "Accepted to ICME 2025. Appendix & Code:\n  https://github.com/Hytidel/UNetReweighting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02391v1",
                "updated": "2025-05-05T06:26:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    26,
                    0,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:26:00Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    26,
                    0,
                    0,
                    125,
                    0
                ],
                "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL"
                },
                "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM."
                },
                "authors": [
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Hanning Zhang"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02387v1",
                "updated": "2025-05-05T06:11:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    11,
                    12,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:11:12Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    11,
                    12,
                    0,
                    125,
                    0
                ],
                "title": "RM-R1: Reward Modeling as Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RM-R1: Reward Modeling as Reasoning"
                },
                "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1."
                },
                "authors": [
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Denghui Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Hanghang Tong"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "23 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08229v2",
                "updated": "2025-05-05T05:56:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    56,
                    42,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-11T03:22:09Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    22,
                    9,
                    4,
                    101,
                    0
                ],
                "title": "Seismic constraints on the spin evolution of slowly rotating young\n  intermediate-mass stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seismic constraints on the spin evolution of slowly rotating young\n  intermediate-mass stars"
                },
                "summary": "$\\delta$ Scuti stars are hot, rapid rotators and are a poorly understood\nclass of pulsators. Asteroseismology provides the only means with which to\nprobe their interior dynamics. However, their complex and unexplained\noscillation patterns restrict analyses to only a small fraction with\ninterpretable pulsations. Here, we identify 5381 $\\delta$ Scuti stars from 63\nsectors of TESS observations, of which 300 had interpretable oscillations, with\n24 showing rotational splittings. We inferred compositions and ages ($\\tau$)\nfor the 300 stars finding them in near-ZAMS states (Bedding et al. 2020), and\nmeasured the mean envelope rotation rates ($< f_{rot} >$) for 24 of them.\nAnalyzing their age-dependent rotation, we found these stars essentially\nexhibit weak-to-no spindown, while evolving past the ZAMS across a narrow\ntime-span during which they show regular pulsations. A quantitative fit to\ntheir spin-evolution results in a trend $f_{rot} (d^{-1}) \\propto\n(\\tau/{Gyr})^{-0.048 \\pm 0.016}$, much slower than the spindown of cooler\nlate-type stars due to magnetic braking (Skumanich's law: $f_{rot} (d^{-1})\n\\propto (\\tau/{Gyr})^{-0.5}$). Based on stellar evolution calculations, we show\nthis weak spindown is consistent with the gradual increase in their\nmoment-of-inertia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\delta$ Scuti stars are hot, rapid rotators and are a poorly understood\nclass of pulsators. Asteroseismology provides the only means with which to\nprobe their interior dynamics. However, their complex and unexplained\noscillation patterns restrict analyses to only a small fraction with\ninterpretable pulsations. Here, we identify 5381 $\\delta$ Scuti stars from 63\nsectors of TESS observations, of which 300 had interpretable oscillations, with\n24 showing rotational splittings. We inferred compositions and ages ($\\tau$)\nfor the 300 stars finding them in near-ZAMS states (Bedding et al. 2020), and\nmeasured the mean envelope rotation rates ($< f_{rot} >$) for 24 of them.\nAnalyzing their age-dependent rotation, we found these stars essentially\nexhibit weak-to-no spindown, while evolving past the ZAMS across a narrow\ntime-span during which they show regular pulsations. A quantitative fit to\ntheir spin-evolution results in a trend $f_{rot} (d^{-1}) \\propto\n(\\tau/{Gyr})^{-0.048 \\pm 0.016}$, much slower than the spindown of cooler\nlate-type stars due to magnetic braking (Skumanich's law: $f_{rot} (d^{-1})\n\\propto (\\tau/{Gyr})^{-0.5}$). Based on stellar evolution calculations, we show\nthis weak spindown is consistent with the gradual increase in their\nmoment-of-inertia."
                },
                "authors": [
                    {
                        "name": "K. H. Singh"
                    },
                    {
                        "name": "S. K. Panda"
                    },
                    {
                        "name": "S. M. Hanasoge"
                    },
                    {
                        "name": "S. Dhanpal"
                    }
                ],
                "author_detail": {
                    "name": "S. Dhanpal"
                },
                "author": "S. Dhanpal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02380v2",
                "updated": "2025-05-06T09:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    37,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T05:42:14Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    42,
                    14,
                    0,
                    125,
                    0
                ],
                "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs."
                },
                "authors": [
                    {
                        "name": "Arnab Sanyal"
                    },
                    {
                        "name": "Prithwish Mukherjee"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Sandeep P. Chinchali"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep P. Chinchali"
                },
                "author": "Sandeep P. Chinchali",
                "arxiv_comment": "6 pages, 1 reference page. Under submission and review at ISLPED 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02376v1",
                "updated": "2025-05-05T05:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    34,
                    33,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T05:34:33Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    34,
                    33,
                    0,
                    125,
                    0
                ],
                "title": "LAMeD: LLM-generated Annotations for Memory Leak Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMeD: LLM-generated Annotations for Memory Leak Detection"
                },
                "summary": "Static analysis tools are widely used to detect software bugs and\nvulnerabilities but often struggle with scalability and efficiency in complex\ncodebases. Traditional approaches rely on manually crafted annotations --\nlabeling functions as sources or sinks -- to track data flows, e.g., ensuring\nthat allocated memory is eventually freed, and code analysis tools such as\nCodeQL, Infer, or Cooddy can use function specifications, but manual annotation\nis laborious and error-prone, especially for large or third-party libraries. We\npresent LAMeD (LLM-generated Annotations for Memory leak Detection), a novel\napproach that leverages large language models (LLMs) to automatically generate\nfunction-specific annotations. When integrated with analyzers such as Cooddy,\nLAMeD significantly improves memory leak detection and reduces path explosion.\nWe also suggest directions for extending LAMeD to broader code analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect software bugs and\nvulnerabilities but often struggle with scalability and efficiency in complex\ncodebases. Traditional approaches rely on manually crafted annotations --\nlabeling functions as sources or sinks -- to track data flows, e.g., ensuring\nthat allocated memory is eventually freed, and code analysis tools such as\nCodeQL, Infer, or Cooddy can use function specifications, but manual annotation\nis laborious and error-prone, especially for large or third-party libraries. We\npresent LAMeD (LLM-generated Annotations for Memory leak Detection), a novel\napproach that leverages large language models (LLMs) to automatically generate\nfunction-specific annotations. When integrated with analyzers such as Cooddy,\nLAMeD significantly improves memory leak detection and reduces path explosion.\nWe also suggest directions for extending LAMeD to broader code analysis."
                },
                "authors": [
                    {
                        "name": "Ekaterina Shemetova"
                    },
                    {
                        "name": "Ilya Shenbin"
                    },
                    {
                        "name": "Ivan Smirnov"
                    },
                    {
                        "name": "Anton Alekseev"
                    },
                    {
                        "name": "Alexey Rukhovich"
                    },
                    {
                        "name": "Sergey Nikolenko"
                    },
                    {
                        "name": "Vadim Lomshakov"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    }
                ],
                "author_detail": {
                    "name": "Irina Piontkovskaya"
                },
                "author": "Irina Piontkovskaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02370v1",
                "updated": "2025-05-05T05:19:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    19,
                    40,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T05:19:40Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    19,
                    40,
                    0,
                    125,
                    0
                ],
                "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing"
                },
                "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Xin Gu"
                    },
                    {
                        "name": "Fan Chen"
                    },
                    {
                        "name": "Xiaoying Xing"
                    },
                    {
                        "name": "Longyin Wen"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Sijie Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Sijie Zhu"
                },
                "author": "Sijie Zhu",
                "arxiv_comment": "Code, Data and Models are available at:\n  https://github.com/bytedance/SuperEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00874v2",
                "updated": "2025-05-05T05:01:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    1,
                    36,
                    0,
                    125,
                    0
                ],
                "published": "2025-01-01T15:43:07Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    43,
                    7,
                    2,
                    1,
                    0
                ],
                "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual\n  Embeddings with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual\n  Embeddings with Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."
                },
                "authors": [
                    {
                        "name": "Hieu Man"
                    },
                    {
                        "name": "Nghia Trung Ngo"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.02836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02836v1",
                "updated": "2025-05-05T17:59:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    58,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:59:58Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    58,
                    0,
                    125,
                    0
                ],
                "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation"
                },
                "summary": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Lu Ling"
                    },
                    {
                        "name": "Chen-Hsuan Lin"
                    },
                    {
                        "name": "Tsung-Yi Lin"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Yu Zeng"
                    },
                    {
                        "name": "Yichen Sheng"
                    },
                    {
                        "name": "Yunhao Ge"
                    },
                    {
                        "name": "Ming-Yu Liu"
                    },
                    {
                        "name": "Aniket Bera"
                    },
                    {
                        "name": "Zhaoshuo Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoshuo Li"
                },
                "author": "Zhaoshuo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02820v1",
                "updated": "2025-05-05T17:47:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:47:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoLibra: Agent Metric Induction from Open-Ended Feedback"
                },
                "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."
                },
                "authors": [
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Phil Cuvin"
                    },
                    {
                        "name": "Xinkai Yu"
                    },
                    {
                        "name": "Charlotte Ka Yee Yan"
                    },
                    {
                        "name": "Jason Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "https://opensocial.world/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02819v1",
                "updated": "2025-05-05T17:47:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:47:42Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations"
                },
                "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository."
                },
                "authors": [
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    },
                    {
                        "name": "Nikos Komodakis"
                    },
                    {
                        "name": "Sergey Zagoruyko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Zagoruyko"
                },
                "author": "Sergey Zagoruyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02815v1",
                "updated": "2025-05-05T17:42:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    42,
                    27,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:42:27Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    42,
                    27,
                    0,
                    125,
                    0
                ],
                "title": "Database-Agnostic Gait Enrollment using SetTransformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database-Agnostic Gait Enrollment using SetTransformers"
                },
                "summary": "Gait recognition has emerged as a powerful tool for unobtrusive and\nlong-range identity analysis, with growing relevance in surveillance and\nmonitoring applications. Although recent advances in deep learning and\nlarge-scale datasets have enabled highly accurate recognition under closed-set\nconditions, real-world deployment demands open-set gait enrollment, which means\ndetermining whether a new gait sample corresponds to a known identity or\nrepresents a previously unseen individual. In this work, we introduce a\ntransformer-based framework for open-set gait enrollment that is both\ndataset-agnostic and recognition-architecture-agnostic. Our method leverages a\nSetTransformer to make enrollment decisions based on the embedding of a probe\nsample and a context set drawn from the gallery, without requiring\ntask-specific thresholds or retraining for new environments. By decoupling\nenrollment from the main recognition pipeline, our model is generalized across\ndifferent datasets, gallery sizes, and identity distributions. We propose an\nevaluation protocol that uses existing datasets in different ratios of\nidentities and walks per identity. We instantiate our method using\nskeleton-based gait representations and evaluate it on two benchmark datasets\n(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition\nmodels (GaitGraph, GaitFormer, and GaitPT). We show that our method is\nflexible, is able to accurately perform enrollment in different scenarios, and\nscales better with data compared to traditional approaches. We will make the\ncode and dataset scenarios publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gait recognition has emerged as a powerful tool for unobtrusive and\nlong-range identity analysis, with growing relevance in surveillance and\nmonitoring applications. Although recent advances in deep learning and\nlarge-scale datasets have enabled highly accurate recognition under closed-set\nconditions, real-world deployment demands open-set gait enrollment, which means\ndetermining whether a new gait sample corresponds to a known identity or\nrepresents a previously unseen individual. In this work, we introduce a\ntransformer-based framework for open-set gait enrollment that is both\ndataset-agnostic and recognition-architecture-agnostic. Our method leverages a\nSetTransformer to make enrollment decisions based on the embedding of a probe\nsample and a context set drawn from the gallery, without requiring\ntask-specific thresholds or retraining for new environments. By decoupling\nenrollment from the main recognition pipeline, our model is generalized across\ndifferent datasets, gallery sizes, and identity distributions. We propose an\nevaluation protocol that uses existing datasets in different ratios of\nidentities and walks per identity. We instantiate our method using\nskeleton-based gait representations and evaluate it on two benchmark datasets\n(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition\nmodels (GaitGraph, GaitFormer, and GaitPT). We show that our method is\nflexible, is able to accurately perform enrollment in different scenarios, and\nscales better with data compared to traditional approaches. We will make the\ncode and dataset scenarios publicly available."
                },
                "authors": [
                    {
                        "name": "Nicoleta Basoc"
                    },
                    {
                        "name": "Adrian Cosma"
                    },
                    {
                        "name": "Andy Cǎtrunǎ"
                    },
                    {
                        "name": "Emilian Rǎdoi"
                    }
                ],
                "author_detail": {
                    "name": "Emilian Rǎdoi"
                },
                "author": "Emilian Rǎdoi",
                "arxiv_comment": "5 Tables, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02811v1",
                "updated": "2025-05-05T17:39:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:39:35Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing"
                },
                "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."
                },
                "authors": [
                    {
                        "name": "Diji Yang"
                    },
                    {
                        "name": "Linda Zeng"
                    },
                    {
                        "name": "Jinmeng Rao"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "Proceedings of the 48th International ACM SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02809v1",
                "updated": "2025-05-05T17:34:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    34,
                    57,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:34:57Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    34,
                    57,
                    0,
                    125,
                    0
                ],
                "title": "Towards Quantifying the Hessian Structure of Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Quantifying the Hessian Structure of Neural Networks"
                },
                "summary": "Empirical studies reported that the Hessian matrix of neural networks (NNs)\nexhibits a near-block-diagonal structure, yet its theoretical foundation\nremains unclear. In this work, we reveal two forces that shape the Hessian\nstructure: a ``static force'' rooted in the architecture design, and a\n``dynamic force'' arisen from training. We then provide a rigorous theoretical\nanalysis of ``static force'' at random initialization. We study linear models\nand 1-hidden-layer networks with the mean-square (MSE) loss and the\nCross-Entropy (CE) loss for classification tasks. By leveraging random matrix\ntheory, we compare the limit distributions of the diagonal and off-diagonal\nHessian blocks and find that the block-diagonal structure arises as $C\n\\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings\nreveal that $C$ is a primary driver of the near-block-diagonal structure. These\nresults may shed new light on the Hessian structure of large language models\n(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical studies reported that the Hessian matrix of neural networks (NNs)\nexhibits a near-block-diagonal structure, yet its theoretical foundation\nremains unclear. In this work, we reveal two forces that shape the Hessian\nstructure: a ``static force'' rooted in the architecture design, and a\n``dynamic force'' arisen from training. We then provide a rigorous theoretical\nanalysis of ``static force'' at random initialization. We study linear models\nand 1-hidden-layer networks with the mean-square (MSE) loss and the\nCross-Entropy (CE) loss for classification tasks. By leveraging random matrix\ntheory, we compare the limit distributions of the diagonal and off-diagonal\nHessian blocks and find that the block-diagonal structure arises as $C\n\\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings\nreveal that $C$ is a primary driver of the near-block-diagonal structure. These\nresults may shed new light on the Hessian structure of large language models\n(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$."
                },
                "authors": [
                    {
                        "name": "Zhaorui Dong"
                    },
                    {
                        "name": "Yushun Zhang"
                    },
                    {
                        "name": "Zhi-Quan Luo"
                    },
                    {
                        "name": "Jianfeng Yao"
                    },
                    {
                        "name": "Ruoyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ruoyu Sun"
                },
                "author": "Ruoyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02802v1",
                "updated": "2025-05-05T17:26:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    26,
                    27,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:26:27Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    26,
                    27,
                    0,
                    125,
                    0
                ],
                "title": "Generating HomeAssistant Automations Using an LLM-based Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating HomeAssistant Automations Using an LLM-based Chatbot"
                },
                "summary": "To combat climate change, individuals are encouraged to adopt sustainable\nhabits, in particular, with their household, optimizing their electrical\nconsumption. Conversational agents, such as Smart Home Assistants, hold promise\nas effective tools for promoting sustainable practices within households. Our\nresearch investigated the application of Large Language Models (LLM) in\nenhancing smart home automation and promoting sustainable household practices,\nspecifically using the HomeAssistant framework. In particular, it highlights\nthe potential of GPT models in generating accurate automation routines. While\nthe LLMs showed proficiency in understanding complex commands and creating\nvalid JSON outputs, challenges such as syntax errors and message malformations\nwere noted, indicating areas for further improvement. Still, despite minimal\nquantitative differences between \"green\" and \"no green\" prompts, qualitative\nfeedback highlighted a positive shift towards sustainability in the routines\ngenerated with environmentally focused prompts. Then, an empirical evaluation\n(N=56) demonstrated that the system was well-received and found engaging by\nusers compared to its traditional rule-based counterpart. Our findings\nhighlight the role of LLMs in advancing smart home technologies and suggest\nfurther research to refine these models for broader, real-world applications to\nsupport sustainable living.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To combat climate change, individuals are encouraged to adopt sustainable\nhabits, in particular, with their household, optimizing their electrical\nconsumption. Conversational agents, such as Smart Home Assistants, hold promise\nas effective tools for promoting sustainable practices within households. Our\nresearch investigated the application of Large Language Models (LLM) in\nenhancing smart home automation and promoting sustainable household practices,\nspecifically using the HomeAssistant framework. In particular, it highlights\nthe potential of GPT models in generating accurate automation routines. While\nthe LLMs showed proficiency in understanding complex commands and creating\nvalid JSON outputs, challenges such as syntax errors and message malformations\nwere noted, indicating areas for further improvement. Still, despite minimal\nquantitative differences between \"green\" and \"no green\" prompts, qualitative\nfeedback highlighted a positive shift towards sustainability in the routines\ngenerated with environmentally focused prompts. Then, an empirical evaluation\n(N=56) demonstrated that the system was well-received and found engaging by\nusers compared to its traditional rule-based counterpart. Our findings\nhighlight the role of LLMs in advancing smart home technologies and suggest\nfurther research to refine these models for broader, real-world applications to\nsupport sustainable living."
                },
                "authors": [
                    {
                        "name": "Mathyas Giudici"
                    },
                    {
                        "name": "Alessandro Sironi"
                    },
                    {
                        "name": "Ismaele Villa"
                    },
                    {
                        "name": "Samuele Scherini"
                    },
                    {
                        "name": "Franca Garzotto"
                    }
                ],
                "author_detail": {
                    "name": "Franca Garzotto"
                },
                "author": "Franca Garzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02795v1",
                "updated": "2025-05-05T17:09:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    9,
                    19,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T17:09:19Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    9,
                    19,
                    0,
                    125,
                    0
                ],
                "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning\n  Framework for Large Language Models"
                },
                "summary": "Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed."
                },
                "authors": [
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Praneeth Vepakomma"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Jun Luo"
                    },
                    {
                        "name": "Yue Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Gao"
                },
                "author": "Yue Gao",
                "arxiv_comment": "16 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02766v1",
                "updated": "2025-05-05T16:21:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    21,
                    46,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T16:21:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    21,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control"
                },
                "summary": "Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control."
                },
                "authors": [
                    {
                        "name": "Nam H. Le"
                    },
                    {
                        "name": "Patrick Erikson"
                    },
                    {
                        "name": "Yanbo Zhang"
                    },
                    {
                        "name": "Michael Levin"
                    },
                    {
                        "name": "Josh Bongard"
                    }
                ],
                "author_detail": {
                    "name": "Josh Bongard"
                },
                "author": "Josh Bongard",
                "arxiv_comment": "Accepted to GECCO Workshop on Bio-Inspired AI (ACM GECCO2025). 13\n  pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02763v1",
                "updated": "2025-05-05T16:18:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    18,
                    7,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T16:18:07Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    18,
                    7,
                    0,
                    125,
                    0
                ],
                "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models"
                },
                "summary": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount."
                },
                "authors": [
                    {
                        "name": "Matthew Dahl"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Dahl"
                },
                "author": "Matthew Dahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21239v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21239v4",
                "updated": "2025-05-05T16:13:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    16,
                    13,
                    30,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-28T17:09:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring internal access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring internal access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Swair Shah"
                    },
                    {
                        "name": "Narayanan Sadagopan"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21239v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21239v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02737v2",
                "updated": "2025-05-06T06:44:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    6,
                    44,
                    35,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T15:40:24Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    40,
                    24,
                    0,
                    125,
                    0
                ],
                "title": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance."
                },
                "authors": [
                    {
                        "name": "Gerard Pons"
                    },
                    {
                        "name": "Besim Bilalli"
                    },
                    {
                        "name": "Anna Queralt"
                    }
                ],
                "author_detail": {
                    "name": "Anna Queralt"
                },
                "author": "Anna Queralt",
                "arxiv_doi": "10.1007/978-3-031-77844-5_9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-77844-5_9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Pre-print submitted to ISWC 2024",
                "arxiv_journal_ref": "Proc. 23rd Int. Semantic Web Conf. (ISWC 2024), LNCS, Springer,\n  2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02735v1",
                "updated": "2025-05-05T15:37:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    37,
                    0,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    37,
                    0,
                    0,
                    125,
                    0
                ],
                "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models"
                },
                "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Ruotian Peng"
                    },
                    {
                        "name": "Keyi Ding"
                    },
                    {
                        "name": "Yizhe Li"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yandong Wen"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Technical Report v1 (33 pages, 8 figures, project page:\n  https://sphere-ai-lab.github.io/FormalMATH/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20834v2",
                "updated": "2025-05-05T15:36:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    36,
                    15,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-29T14:58:43Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    58,
                    43,
                    1,
                    119,
                    0
                ],
                "title": "Token-Efficient RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Efficient RL for LLM Reasoning"
                },
                "summary": "We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Rather than relying on\nfull-sequence updates or separate critic networks, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Rather than relying on\nfull-sequence updates or separate critic networks, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes."
                },
                "authors": [
                    {
                        "name": "Alan Lee"
                    },
                    {
                        "name": "Harry Tong"
                    }
                ],
                "author_detail": {
                    "name": "Harry Tong"
                },
                "author": "Harry Tong",
                "arxiv_comment": "Title updated to \"Token-Efficient RL for LLM Reasoning\" to better\n  reflect algorithmic focus. Revised abstract, intro, and conclusion. Paper\n  shortened and typos fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18902v2",
                "updated": "2025-05-05T15:26:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    26,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-24T16:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    48,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Extremely Low-Resource Finno-Ugric Languages"
                },
                "summary": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP."
                },
                "authors": [
                    {
                        "name": "Taido Purason"
                    },
                    {
                        "name": "Hele-Andra Kuulmets"
                    },
                    {
                        "name": "Mark Fishel"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fishel"
                },
                "author": "Mark Fishel",
                "arxiv_journal_ref": "Findings of the Association for Computational Linguistics: NAACL\n  2025, pages 6677-6697",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02722v1",
                "updated": "2025-05-05T15:23:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    23,
                    47,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:23:47Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    23,
                    47,
                    0,
                    125,
                    0
                ],
                "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a\n  Nationwide Sepsis Registry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a\n  Nationwide Sepsis Registry"
                },
                "summary": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models."
                },
                "authors": [
                    {
                        "name": "Junu Kim"
                    },
                    {
                        "name": "Chaeeun Shim"
                    },
                    {
                        "name": "Sungjin Park"
                    },
                    {
                        "name": "Su Yeon Lee"
                    },
                    {
                        "name": "Gee Young Suh"
                    },
                    {
                        "name": "Chae-Man Lim"
                    },
                    {
                        "name": "Seong Jin Choi"
                    },
                    {
                        "name": "Song Mi Moon"
                    },
                    {
                        "name": "Kyoung-Ho Song"
                    },
                    {
                        "name": "Eu Suk Kim"
                    },
                    {
                        "name": "Hong Bin Kim"
                    },
                    {
                        "name": "Sejoong Kim"
                    },
                    {
                        "name": "Chami Im"
                    },
                    {
                        "name": "Dong-Wan Kang"
                    },
                    {
                        "name": "Yong Soo Kim"
                    },
                    {
                        "name": "Hee-Joon Bae"
                    },
                    {
                        "name": "Sung Yoon Lim"
                    },
                    {
                        "name": "Han-Gil Jeong"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02707v1",
                "updated": "2025-05-05T15:05:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    5,
                    1,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T15:05:01Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    5,
                    1,
                    0,
                    125,
                    0
                ],
                "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play"
                },
                "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions."
                },
                "authors": [
                    {
                        "name": "Yemin Shi"
                    },
                    {
                        "name": "Yu Shu"
                    },
                    {
                        "name": "Siwei Dong"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Jaward Sesay"
                    },
                    {
                        "name": "Jingwen Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiting Hu"
                },
                "author": "Zhiting Hu",
                "arxiv_comment": "18 pages, 7 figures, Website: https://voila.maitrix.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02703v1",
                "updated": "2025-05-05T14:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    57,
                    2,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    57,
                    2,
                    0,
                    125,
                    0
                ],
                "title": "Structure Causal Models and LLMs Integration in Medical Visual Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure Causal Models and LLMs Integration in Medical Visual Question\n  Answering"
                },
                "summary": "Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data."
                },
                "authors": [
                    {
                        "name": "Zibo Xu"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Weizhi Nie"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Anan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Anan Liu"
                },
                "author": "Anan Liu",
                "arxiv_doi": "10.1109/TMI.2025.3564320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TMI.2025.3564320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE TMI 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02699v1",
                "updated": "2025-05-05T14:51:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    51,
                    30,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:51:30Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    51,
                    30,
                    0,
                    125,
                    0
                ],
                "title": "Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for\n  History Education in Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for\n  History Education in Virtual Reality"
                },
                "summary": "Multi-role pedagogical agents can create engaging and immersive learning\nexperiences, helping learners better understand knowledge in history learning.\nHowever, existing pedagogical agents often struggle with multi-role\ninteractions due to complex controls, limited feedback forms, and difficulty\ndynamically adapting to user inputs. In this study, we developed a VR prototype\nwith LLM-powered adaptive role-switching and action-switching pedagogical\nagents to help users learn about the history of the Pavilion of Prince Teng. A\n2 x 2 between-subjects study was conducted with 84 participants to assess how\nadaptive role-switching and action-switching affect participants' learning\noutcomes and experiences. The results suggest that adaptive role-switching\nenhances participants' perception of the pedagogical agent's trustworthiness\nand expertise but may lead to inconsistent learning experiences. Adaptive\naction-switching increases participants' perceived social presence, expertise,\nand humanness. The study did not uncover any effects of role-switching and\naction-switching on usability, learning motivation, and cognitive load. Based\non the findings, we proposed five design implications for incorporating\nadaptive role-switching and action-switching into future VR history education\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-role pedagogical agents can create engaging and immersive learning\nexperiences, helping learners better understand knowledge in history learning.\nHowever, existing pedagogical agents often struggle with multi-role\ninteractions due to complex controls, limited feedback forms, and difficulty\ndynamically adapting to user inputs. In this study, we developed a VR prototype\nwith LLM-powered adaptive role-switching and action-switching pedagogical\nagents to help users learn about the history of the Pavilion of Prince Teng. A\n2 x 2 between-subjects study was conducted with 84 participants to assess how\nadaptive role-switching and action-switching affect participants' learning\noutcomes and experiences. The results suggest that adaptive role-switching\nenhances participants' perception of the pedagogical agent's trustworthiness\nand expertise but may lead to inconsistent learning experiences. Adaptive\naction-switching increases participants' perceived social presence, expertise,\nand humanness. The study did not uncover any effects of role-switching and\naction-switching on usability, learning motivation, and cognitive load. Based\non the findings, we proposed five design implications for incorporating\nadaptive role-switching and action-switching into future VR history education\ntools."
                },
                "authors": [
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Ao Yu"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_comment": "14 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://dl.acm.org/doi/10.1145/3706598.3713109",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18881v2",
                "updated": "2025-05-05T14:51:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    51,
                    5,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-26T06:56:41Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    6,
                    56,
                    41,
                    2,
                    57,
                    0
                ],
                "title": "Letters from Future Self: Augmenting the Letter-Exchange Exercise with\n  LLM-based Agents to Enhance Young Adults' Career Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Letters from Future Self: Augmenting the Letter-Exchange Exercise with\n  LLM-based Agents to Enhance Young Adults' Career Exploration"
                },
                "summary": "Young adults often encounter challenges in career exploration. Self-guided\ninterventions, such as the letter-exchange exercise, where participants\nenvision and adopt the perspective of their future selves by exchanging letters\nwith their envisioned future selves, can support career development. However,\nthe broader adoption of such interventions may be limited without structured\nguidance. To address this, we integrated Large Language Model (LLM)-based\nagents that simulate participants' future selves into the letter-exchange\nexercise and evaluated their effectiveness. A one-week experiment (N=36)\ncompared three conditions: (1) participants manually writing replies to\nthemselves from the perspective of their future selves (baseline), (2)\nfuture-self agents generating letters to participants, and (3) future-self\nagents engaging in chat conversations with participants. Results indicated that\nexchanging letters with future-self agents enhanced participants' engagement\nduring the exercise, while overall benefits of the intervention on future\norientation, career self-concept, and psychological support remained comparable\nacross conditions. We discuss design implications for AI-augmented\ninterventions for supporting young adults' career exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young adults often encounter challenges in career exploration. Self-guided\ninterventions, such as the letter-exchange exercise, where participants\nenvision and adopt the perspective of their future selves by exchanging letters\nwith their envisioned future selves, can support career development. However,\nthe broader adoption of such interventions may be limited without structured\nguidance. To address this, we integrated Large Language Model (LLM)-based\nagents that simulate participants' future selves into the letter-exchange\nexercise and evaluated their effectiveness. A one-week experiment (N=36)\ncompared three conditions: (1) participants manually writing replies to\nthemselves from the perspective of their future selves (baseline), (2)\nfuture-self agents generating letters to participants, and (3) future-self\nagents engaging in chat conversations with participants. Results indicated that\nexchanging letters with future-self agents enhanced participants' engagement\nduring the exercise, while overall benefits of the intervention on future\norientation, career self-concept, and psychological support remained comparable\nacross conditions. We discuss design implications for AI-augmented\ninterventions for supporting young adults' career exploration."
                },
                "authors": [
                    {
                        "name": "Hayeon Jeon"
                    },
                    {
                        "name": "Suhwoo Yoon"
                    },
                    {
                        "name": "Keyeun Lee"
                    },
                    {
                        "name": "Seo Hyeong Kim"
                    },
                    {
                        "name": "Esther Hehsun Kim"
                    },
                    {
                        "name": "Seonghye Cho"
                    },
                    {
                        "name": "Yena Ko"
                    },
                    {
                        "name": "Soeun Yang"
                    },
                    {
                        "name": "Laura Dabbish"
                    },
                    {
                        "name": "John Zimmerman"
                    },
                    {
                        "name": "Eun-mee Kim"
                    },
                    {
                        "name": "Hajin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Hajin Lim"
                },
                "author": "Hajin Lim",
                "arxiv_doi": "10.1145/3706598.3714206",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714206",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 9 figures, Proceedings of the 2025 CHI Conference on Human\n  Factors in Computing Systems (Best Paper Award, Top 1%)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01315v2",
                "updated": "2025-05-05T14:46:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    46,
                    48,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-02T14:42:26Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    14,
                    42,
                    26,
                    4,
                    122,
                    0
                ],
                "title": "Helping Large Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helping Large Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System"
                },
                "summary": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses."
                },
                "authors": [
                    {
                        "name": "Sheikh Samit Muhaimin"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02694v1",
                "updated": "2025-05-05T14:44:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    44,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:44:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    44,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "AI Standardized Patient Improves Human Conversations in Advanced Cancer\n  Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Standardized Patient Improves Human Conversations in Advanced Cancer\n  Care"
                },
                "summary": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education."
                },
                "authors": [
                    {
                        "name": "Kurtis Haut"
                    },
                    {
                        "name": "Masum Hasan"
                    },
                    {
                        "name": "Thomas Carroll"
                    },
                    {
                        "name": "Ronald Epstein"
                    },
                    {
                        "name": "Taylan Sen"
                    },
                    {
                        "name": "Ehsan Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Hoque"
                },
                "author": "Ehsan Hoque",
                "arxiv_comment": "20 pages, 6 figures, 4 tables, submitting to New England Journal of\n  Medicine (NEJM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02693v1",
                "updated": "2025-05-05T14:43:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    43,
                    20,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:43:20Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    43,
                    20,
                    0,
                    125,
                    0
                ],
                "title": "Predicting Movie Hits Before They Happen with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Movie Hits Before They Happen with LLMs"
                },
                "summary": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped."
                },
                "authors": [
                    {
                        "name": "Shaghayegh Agah"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Neeraj Sharma"
                    },
                    {
                        "name": "Mayur Nankani"
                    },
                    {
                        "name": "Kevin Foley"
                    },
                    {
                        "name": "H. Howie Huang"
                    },
                    {
                        "name": "Sardar Hamidian"
                    }
                ],
                "author_detail": {
                    "name": "Sardar Hamidian"
                },
                "author": "Sardar Hamidian",
                "arxiv_comment": "Accepted at ACM UMAP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02686v1",
                "updated": "2025-05-05T14:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    33,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    33,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sailing AI by the Stars: A Survey of Learning from Rewards in\n  Post-Training and Test-Time Scaling of Large Language Models"
                },
                "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers."
                },
                "authors": [
                    {
                        "name": "Xiaobao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobao Wu"
                },
                "author": "Xiaobao Wu",
                "arxiv_comment": "35 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02666v1",
                "updated": "2025-05-05T14:15:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    15,
                    2,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:15:02Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    15,
                    2,
                    0,
                    125,
                    0
                ],
                "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Progress in LLM Alignment from the Perspective of Reward\n  Design"
                },
                "summary": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies."
                },
                "authors": [
                    {
                        "name": "Miaomiao Ji"
                    },
                    {
                        "name": "Yanqiu Wu"
                    },
                    {
                        "name": "Zhibin Wu"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02665v1",
                "updated": "2025-05-05T14:14:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    14,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T14:14:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    14,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law"
                },
                "summary": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems."
                },
                "authors": [
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Wenkai Ji"
                    },
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Shilian Chen"
                    },
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan Wu"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02659v2",
                "updated": "2025-05-06T08:34:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    34,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    5,
                    15,
                    0,
                    125,
                    0
                ],
                "title": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data."
                },
                "authors": [
                    {
                        "name": "Andrey Sidorenko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Sidorenko"
                },
                "author": "Andrey Sidorenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00711v2",
                "updated": "2025-05-05T13:57:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    57,
                    12,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-01T12:21:50Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    21,
                    50,
                    1,
                    91,
                    0
                ],
                "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited\n  Environments"
                },
                "summary": "The era of foundation models has revolutionized AI research, yet Graph\nFoundation Models (GFMs) remain constrained by the scarcity of large-scale\ngraph corpora. Traditional graph data synthesis techniques primarily focus on\nsimplistic structural operations, lacking the capacity to generate semantically\nrich nodes with meaningful textual attributes: a critical limitation for\nreal-world applications. While large language models (LLMs) demonstrate\nexceptional text generation capabilities, their direct application to graph\nsynthesis is impeded by context window limitations, hallucination phenomena,\nand structural consistency challenges. To address these issues, we introduce\nGraphMaster, the first multi-agent framework specifically designed for graph\ndata synthesis in data-limited environments. GraphMaster orchestrates four\nspecialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that\ncollaboratively optimize the synthesis process through iterative refinement,\nensuring both semantic coherence and structural integrity. To rigorously\nevaluate our approach, we create new data-limited \"Sub\" variants of six\nstandard graph benchmarks, specifically designed to test synthesis capabilities\nunder realistic constraints. Additionally, we develop a novel interpretability\nassessment framework that combines human evaluation with a principled\nGrassmannian manifold-based analysis, providing both qualitative and\nquantitative measures of semantic coherence. Experimental results demonstrate\nthat GraphMaster significantly outperforms traditional synthesis methods across\nmultiple datasets, establishing a strong foundation for advancing GFMs in\ndata-scarce environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of foundation models has revolutionized AI research, yet Graph\nFoundation Models (GFMs) remain constrained by the scarcity of large-scale\ngraph corpora. Traditional graph data synthesis techniques primarily focus on\nsimplistic structural operations, lacking the capacity to generate semantically\nrich nodes with meaningful textual attributes: a critical limitation for\nreal-world applications. While large language models (LLMs) demonstrate\nexceptional text generation capabilities, their direct application to graph\nsynthesis is impeded by context window limitations, hallucination phenomena,\nand structural consistency challenges. To address these issues, we introduce\nGraphMaster, the first multi-agent framework specifically designed for graph\ndata synthesis in data-limited environments. GraphMaster orchestrates four\nspecialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that\ncollaboratively optimize the synthesis process through iterative refinement,\nensuring both semantic coherence and structural integrity. To rigorously\nevaluate our approach, we create new data-limited \"Sub\" variants of six\nstandard graph benchmarks, specifically designed to test synthesis capabilities\nunder realistic constraints. Additionally, we develop a novel interpretability\nassessment framework that combines human evaluation with a principled\nGrassmannian manifold-based analysis, providing both qualitative and\nquantitative measures of semantic coherence. Experimental results demonstrate\nthat GraphMaster significantly outperforms traditional synthesis methods across\nmultiple datasets, establishing a strong foundation for advancing GFMs in\ndata-scarce environments."
                },
                "authors": [
                    {
                        "name": "Enjun Du"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03253v2",
                "updated": "2025-05-05T13:47:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    47,
                    32,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-05T15:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    8,
                    43,
                    2,
                    36,
                    0
                ],
                "title": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis"
                },
                "summary": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially -- to upwards of $0.99$ -- suggesting a homogenization\nin the LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially -- to upwards of $0.99$ -- suggesting a homogenization\nin the LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating."
                },
                "authors": [
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Jimmy Pronchick"
                    },
                    {
                        "name": "Krupa Bhawsar"
                    },
                    {
                        "name": "Roger E. Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger E. Beaty"
                },
                "author": "Roger E. Beaty",
                "arxiv_comment": "CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02639v1",
                "updated": "2025-05-05T13:31:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    31,
                    36,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T13:31:36Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    31,
                    36,
                    0,
                    125,
                    0
                ],
                "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large\n  Language Model and Dual-task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large\n  Language Model and Dual-task Learning"
                },
                "summary": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design."
                },
                "authors": [
                    {
                        "name": "Xuan Lin"
                    },
                    {
                        "name": "Qingrui Liu"
                    },
                    {
                        "name": "Hongxin Xiang"
                    },
                    {
                        "name": "Daojian Zeng"
                    },
                    {
                        "name": "Xiangxiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Zeng"
                },
                "author": "Xiangxiang Zeng",
                "arxiv_comment": "Accepted for publication at IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02629v1",
                "updated": "2025-05-05T13:15:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    15,
                    53,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T13:15:53Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    15,
                    53,
                    0,
                    125,
                    0
                ],
                "title": "Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for\n  Automated Patch Correctness Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for\n  Automated Patch Correctness Assessment"
                },
                "summary": "Automated program repair (APR) aims to automatically repair program errors\nwithout human intervention, and recent years have witnessed a growing interest\non this research topic. While much progress has been made and techniques\noriginating from different disciplines have been proposed, APR techniques\ngenerally suffer from the patch overfitting issue, i.e., the generated patches\nare not genuinely correct despite they pass the employed tests. To alleviate\nthis issue, many research efforts have been devoted for automated patch\ncorrectness assessment (APCA). In particular, with the emergence of large\nlanguage model (LLM) technology, researchers have employed LLM to assess the\npatch correctness and have obtained the state-of-the-art performance. The\nliterature on APCA has demonstrated the importance of capturing patch semantic\nand explicitly considering certain code attributes in predicting patch\ncorrectness. However, existing LLM-based methods typically treat code as token\nsequences and ignore the inherent formal structure for code, making it\ndifficult to capture the deep patch semantics. Moreover, these LLM-based\nmethods also do not explicitly account for enough code attributes. To overcome\nthese drawbacks, we in this paper design a novel patch graph representation\nnamed attributed patch semantic graph (APSG), which adequately captures the\npatch semantic and explicitly reflects important patch attributes. To\neffectively use graph information in APSG, we accordingly propose a new\nparameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA.\nExtensive evaluations have been conducted to evaluate our method, and the\nresults show that compared to the state-of-the-art methods, our method improves\naccuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) aims to automatically repair program errors\nwithout human intervention, and recent years have witnessed a growing interest\non this research topic. While much progress has been made and techniques\noriginating from different disciplines have been proposed, APR techniques\ngenerally suffer from the patch overfitting issue, i.e., the generated patches\nare not genuinely correct despite they pass the employed tests. To alleviate\nthis issue, many research efforts have been devoted for automated patch\ncorrectness assessment (APCA). In particular, with the emergence of large\nlanguage model (LLM) technology, researchers have employed LLM to assess the\npatch correctness and have obtained the state-of-the-art performance. The\nliterature on APCA has demonstrated the importance of capturing patch semantic\nand explicitly considering certain code attributes in predicting patch\ncorrectness. However, existing LLM-based methods typically treat code as token\nsequences and ignore the inherent formal structure for code, making it\ndifficult to capture the deep patch semantics. Moreover, these LLM-based\nmethods also do not explicitly account for enough code attributes. To overcome\nthese drawbacks, we in this paper design a novel patch graph representation\nnamed attributed patch semantic graph (APSG), which adequately captures the\npatch semantic and explicitly reflects important patch attributes. To\neffectively use graph information in APSG, we accordingly propose a new\nparameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA.\nExtensive evaluations have been conducted to evaluate our method, and the\nresults show that compared to the state-of-the-art methods, our method improves\naccuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Jingwen Wu"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Zhongxing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxing Yu"
                },
                "author": "Zhongxing Yu",
                "arxiv_comment": "16 pages, 4 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00654v2",
                "updated": "2025-05-05T13:14:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    14,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-01T16:55:44Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    55,
                    44,
                    3,
                    121,
                    0
                ],
                "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Understanding: an Inherent Ambiguity Barrier"
                },
                "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."
                },
                "authors": [
                    {
                        "name": "Daniel N. Nissani"
                    }
                ],
                "author_detail": {
                    "name": "Daniel N. Nissani"
                },
                "arxiv_affiliation": "Nissensohn",
                "author": "Daniel N. Nissani",
                "arxiv_comment": "submitted to NEURAL COMPUTATION",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02626v1",
                "updated": "2025-05-05T13:08:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    8,
                    25,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T13:08:25Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    13,
                    8,
                    25,
                    0,
                    125,
                    0
                ],
                "title": "Detect, Classify, Act: Categorizing Industrial Anomalies with\n  Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Classify, Act: Categorizing Industrial Anomalies with\n  Multi-Modal Large Language Models"
                },
                "summary": "Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization."
                },
                "authors": [
                    {
                        "name": "Sassan Mokhtar"
                    },
                    {
                        "name": "Arian Mousakhan"
                    },
                    {
                        "name": "Silvio Galesso"
                    },
                    {
                        "name": "Jawad Tayyub"
                    },
                    {
                        "name": "Thomas Brox"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Brox"
                },
                "author": "Thomas Brox",
                "arxiv_comment": "Accepted as a spotlight presentation paper at the VAND Workshop, CVPR\n  2025. 10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02625v1",
                "updated": "2025-05-05T12:53:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    53,
                    9,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T12:53:09Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    53,
                    9,
                    0,
                    125,
                    0
                ],
                "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis"
                },
                "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data."
                },
                "authors": [
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16892v2",
                "updated": "2025-05-05T12:48:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    48,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-24T06:43:19Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    43,
                    19,
                    0,
                    55,
                    0
                ],
                "title": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text\n  Classification without Manually Labeled Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text\n  Classification without Manually Labeled Data"
                },
                "summary": "Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework,\nachieving high cross-task text classification performance without the need for\nany manually labeled data. Furthermore, compared to directly applying GPT for\nclassification tasks, our approach retains over 93% of its classification\nperformance while requiring only approximately 6% of the computational time and\nmonetary cost, effectively balancing performance and resource efficiency. These\nfindings provide new insights into the efficient utilization of LLMs and active\nlearning algorithms in text classification tasks, paving the way for their\nbroader application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework,\nachieving high cross-task text classification performance without the need for\nany manually labeled data. Furthermore, compared to directly applying GPT for\nclassification tasks, our approach retains over 93% of its classification\nperformance while requiring only approximately 6% of the computational time and\nmonetary cost, effectively balancing performance and resource efficiency. These\nfindings provide new insights into the efficient utilization of LLMs and active\nlearning algorithms in text classification tasks, paving the way for their\nbroader application."
                },
                "authors": [
                    {
                        "name": "Yejian Zhang"
                    },
                    {
                        "name": "Shingo Takada"
                    }
                ],
                "author_detail": {
                    "name": "Shingo Takada"
                },
                "author": "Shingo Takada",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02650v2",
                "updated": "2025-05-05T12:25:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    25,
                    44,
                    0,
                    125,
                    0
                ],
                "published": "2025-03-04T14:14:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    14,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats"
                },
                "summary": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information."
                },
                "authors": [
                    {
                        "name": "William Brach"
                    },
                    {
                        "name": "Kristián Košťál"
                    },
                    {
                        "name": "Michal Ries"
                    }
                ],
                "author_detail": {
                    "name": "Michal Ries"
                },
                "author": "Michal Ries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15941v2",
                "updated": "2025-05-05T12:19:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    12,
                    19,
                    32,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-22T14:35:16Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    14,
                    35,
                    16,
                    1,
                    112,
                    0
                ],
                "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity"
                },
                "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."
                },
                "authors": [
                    {
                        "name": "Fanny Jourdan"
                    },
                    {
                        "name": "Yannick Chevalier"
                    },
                    {
                        "name": "Cécile Favre"
                    }
                ],
                "author_detail": {
                    "name": "Cécile Favre"
                },
                "author": "Cécile Favre",
                "arxiv_comment": "FAccT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03601v3",
                "updated": "2025-05-05T11:54:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    54,
                    13,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-04T17:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay"
                },
                "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "12 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02583v1",
                "updated": "2025-05-05T11:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    35,
                    33,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:35:33Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    35,
                    33,
                    0,
                    125,
                    0
                ],
                "title": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in\n  the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in\n  the LLM Era"
                },
                "summary": "The proliferation of edge devices has generated an unprecedented volume of\ntime series data across different domains, motivating various well-customized\nmethods. Recently, Large Language Models (LLMs) have emerged as a new paradigm\nfor time series analytics by leveraging the shared sequential nature of textual\ndata and time series. However, a fundamental cross-modality gap between time\nseries and LLMs exists, as LLMs are pre-trained on textual corpora and are not\ninherently optimized for time series. Many recent proposals are designed to\naddress this issue. In this survey, we provide an up-to-date overview of\nLLMs-based cross-modality modeling for time series analytics. We first\nintroduce a taxonomy that classifies existing approaches into four groups based\non the type of textual data employed for time series modeling. We then\nsummarize key cross-modality strategies, e.g., alignment and fusion, and\ndiscuss their applications across a range of downstream tasks. Furthermore, we\nconduct experiments on multimodal datasets from different application domains\nto investigate effective combinations of textual data and cross-modality\nstrategies for enhancing time series analytics. Finally, we suggest several\npromising directions for future research. This survey is designed for a range\nof professionals, researchers, and practitioners interested in LLM-based time\nseries modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of edge devices has generated an unprecedented volume of\ntime series data across different domains, motivating various well-customized\nmethods. Recently, Large Language Models (LLMs) have emerged as a new paradigm\nfor time series analytics by leveraging the shared sequential nature of textual\ndata and time series. However, a fundamental cross-modality gap between time\nseries and LLMs exists, as LLMs are pre-trained on textual corpora and are not\ninherently optimized for time series. Many recent proposals are designed to\naddress this issue. In this survey, we provide an up-to-date overview of\nLLMs-based cross-modality modeling for time series analytics. We first\nintroduce a taxonomy that classifies existing approaches into four groups based\non the type of textual data employed for time series modeling. We then\nsummarize key cross-modality strategies, e.g., alignment and fusion, and\ndiscuss their applications across a range of downstream tasks. Furthermore, we\nconduct experiments on multimodal datasets from different application domains\nto investigate effective combinations of textual data and cross-modality\nstrategies for enhancing time series analytics. Finally, we suggest several\npromising directions for future research. This survey is designed for a range\nof professionals, researchers, and practitioners interested in LLM-based time\nseries modeling."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Shaowen Zhou"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "arxiv_comment": "Accepted by IJCAI 2025 Survey Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02579v2",
                "updated": "2025-05-06T06:26:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    6,
                    26,
                    11,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    30,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning"
                },
                "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."
                },
                "authors": [
                    {
                        "name": "Lingxiao Kong"
                    },
                    {
                        "name": "Cong Yang"
                    },
                    {
                        "name": "Susanne Neufang"
                    },
                    {
                        "name": "Oya Deniz Beyan"
                    },
                    {
                        "name": "Zeyd Boukhers"
                    }
                ],
                "author_detail": {
                    "name": "Zeyd Boukhers"
                },
                "author": "Zeyd Boukhers",
                "arxiv_comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02576v1",
                "updated": "2025-05-05T11:24:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    24,
                    20,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:24:20Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    24,
                    20,
                    0,
                    125,
                    0
                ],
                "title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer\n  Reasoning"
                },
                "summary": "Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient."
                },
                "authors": [
                    {
                        "name": "Sergio Hernández-Gutiérrez"
                    },
                    {
                        "name": "Minttu Alakuijala"
                    },
                    {
                        "name": "Alexander V. Nikitin"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v3",
                "updated": "2025-05-05T11:19:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    19,
                    53,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems"
                },
                "summary": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!"
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02566v1",
                "updated": "2025-05-05T11:14:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    14,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T11:14:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    14,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "Robustness questions the interpretability of graph neural networks: what\n  to do?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness questions the interpretability of graph neural networks: what\n  to do?"
                },
                "summary": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kirill Lukyanov"
                    },
                    {
                        "name": "Georgii Sazonov"
                    },
                    {
                        "name": "Serafim Boyarsky"
                    },
                    {
                        "name": "Ilya Makarov"
                    }
                ],
                "author_detail": {
                    "name": "Ilya Makarov"
                },
                "arxiv_affiliation": "1 v 5",
                "author": "Ilya Makarov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02560v2",
                "updated": "2025-05-06T11:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    44,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:02:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    2,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Evaluating Contrastive Feedback for Effective User Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Contrastive Feedback for Effective User Simulations"
                },
                "summary": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers."
                },
                "authors": [
                    {
                        "name": "Andreas Konstantin Kruff"
                    },
                    {
                        "name": "Timo Breuer"
                    },
                    {
                        "name": "Philipp Schaer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Schaer"
                },
                "author": "Philipp Schaer",
                "arxiv_doi": "10.1145/3726302.3730189",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730189",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02558v1",
                "updated": "2025-05-05T10:56:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    56,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:56:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    56,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "The Turing Test Is More Relevant Than Ever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing Test Is More Relevant Than Ever"
                },
                "summary": "The Turing Test, first proposed by Alan Turing in 1950, has historically\nserved as a benchmark for evaluating artificial intelligence (AI). However,\nsince the release of ELIZA in 1966, and particularly with recent advancements\nin large language models (LLMs), AI has been claimed to pass the Turing Test.\nFurthermore, criticism argues that the Turing Test primarily assesses deceptive\nmimicry rather than genuine intelligence, prompting the continuous emergence of\nalternative benchmarks. This study argues against discarding the Turing Test,\nproposing instead using more refined versions of it, for example, by\ninteracting simultaneously with both an AI and human candidate to determine who\nis who, allowing a longer interaction duration, access to the Internet and\nother AIs, using experienced people as evaluators, etc.\n  Through systematic experimentation using a web-based platform, we demonstrate\nthat richer, contextually structured testing environments significantly enhance\nparticipants' ability to differentiate between AI and human interactions.\nNamely, we show that, while an off-the-shelf LLM can pass some version of a\nTuring Test, it fails to do so when faced with a more robust version. Our\nfindings highlight that the Turing Test remains an important and effective\nmethod for evaluating AI, provided it continues to adapt as AI technology\nadvances. Additionally, the structured data gathered from these improved\ninteractions provides valuable insights into what humans expect from truly\nintelligent AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Turing Test, first proposed by Alan Turing in 1950, has historically\nserved as a benchmark for evaluating artificial intelligence (AI). However,\nsince the release of ELIZA in 1966, and particularly with recent advancements\nin large language models (LLMs), AI has been claimed to pass the Turing Test.\nFurthermore, criticism argues that the Turing Test primarily assesses deceptive\nmimicry rather than genuine intelligence, prompting the continuous emergence of\nalternative benchmarks. This study argues against discarding the Turing Test,\nproposing instead using more refined versions of it, for example, by\ninteracting simultaneously with both an AI and human candidate to determine who\nis who, allowing a longer interaction duration, access to the Internet and\nother AIs, using experienced people as evaluators, etc.\n  Through systematic experimentation using a web-based platform, we demonstrate\nthat richer, contextually structured testing environments significantly enhance\nparticipants' ability to differentiate between AI and human interactions.\nNamely, we show that, while an off-the-shelf LLM can pass some version of a\nTuring Test, it fails to do so when faced with a more robust version. Our\nfindings highlight that the Turing Test remains an important and effective\nmethod for evaluating AI, provided it continues to adapt as AI technology\nadvances. Additionally, the structured data gathered from these improved\ninteractions provides valuable insights into what humans expect from truly\nintelligent AI systems."
                },
                "authors": [
                    {
                        "name": "Avraham Rahimov"
                    },
                    {
                        "name": "Orel Zamler"
                    },
                    {
                        "name": "Amos Azaria"
                    }
                ],
                "author_detail": {
                    "name": "Amos Azaria"
                },
                "author": "Amos Azaria",
                "arxiv_comment": "10 pages, 5 figures, 1 listing, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02550v1",
                "updated": "2025-05-05T10:39:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    39,
                    51,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:39:51Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    39,
                    51,
                    0,
                    125,
                    0
                ],
                "title": "Bielik v3 Small: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik v3 Small: Technical Report"
                },
                "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Gwoździej"
                },
                "author": "Adrian Gwoździej",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21437v2",
                "updated": "2025-05-05T10:18:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    18,
                    9,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-30T08:52:32Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    52,
                    32,
                    2,
                    120,
                    0
                ],
                "title": "Identifying Critical Dependencies in Large-Scale Continuous Software\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Critical Dependencies in Large-Scale Continuous Software\n  Engineering"
                },
                "summary": "Continuous Software Engineering (CSE) is widely adopted in the industry,\nintegrating practices such as Continuous Integration and Continuous Deployment\n(CI/CD). Beyond technical aspects, CSE also encompasses business activities\nlike continuous planning, budgeting, and operational processes. Coordinating\nthese activities in large-scale product development involves multiple\nstakeholders, increasing complexity. This study aims to address this complexity\nby identifying and analyzing critical dependencies in large-scale CSE. Based on\n17 semi-structured interviews conducted at two Nordic fintech companies, our\npreliminary findings indicate that dependencies between software teams and\nsupport functions, as well as between software teams and external entities, are\nthe primary sources of delays and bottlenecks. As a next step, we plan to\nfurther refine our understanding of critical dependencies in large-scale CSE\nand explore coordination mechanisms that can better support software\ndevelopment teams in managing these challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Software Engineering (CSE) is widely adopted in the industry,\nintegrating practices such as Continuous Integration and Continuous Deployment\n(CI/CD). Beyond technical aspects, CSE also encompasses business activities\nlike continuous planning, budgeting, and operational processes. Coordinating\nthese activities in large-scale product development involves multiple\nstakeholders, increasing complexity. This study aims to address this complexity\nby identifying and analyzing critical dependencies in large-scale CSE. Based on\n17 semi-structured interviews conducted at two Nordic fintech companies, our\npreliminary findings indicate that dependencies between software teams and\nsupport functions, as well as between software teams and external entities, are\nthe primary sources of delays and bottlenecks. As a next step, we plan to\nfurther refine our understanding of critical dependencies in large-scale CSE\nand explore coordination mechanisms that can better support software\ndevelopment teams in managing these challenges."
                },
                "authors": [
                    {
                        "name": "Anastasiia Tkalich"
                    },
                    {
                        "name": "Eriks Klotins"
                    },
                    {
                        "name": "Nils Brede Moe"
                    }
                ],
                "author_detail": {
                    "name": "Nils Brede Moe"
                },
                "author": "Nils Brede Moe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02519v1",
                "updated": "2025-05-05T09:58:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    58,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:58:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    58,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "Deaf in AI: AI language technologies and the erosion of linguistic\n  rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deaf in AI: AI language technologies and the erosion of linguistic\n  rights"
                },
                "summary": "This paper explores the interplay of AI language technologies, sign language\ninterpreting, and linguistic access, highlighting the complex interdependencies\nshaping access frameworks and the tradeoffs these technologies bring. While AI\ntools promise innovation, they also perpetuate biases, reinforce technoableism,\nand deepen inequalities through systemic and design flaws. The historical and\ncontemporary privileging of sign language interpreting as the dominant access\nmodel, and the broader inclusion ideologies it reflects, shape AIs development\nand deployment, often sidelining deaf languaging practices and introducing new\nforms of linguistic subordination to technology. Drawing on Deaf Studies, Sign\nLanguage Interpreting Studies, and crip technoscience, this paper critiques the\nframing of AI as a substitute for interpreters and examines its implications\nfor access hierarchies. It calls for deaf-led approaches to foster AI systems\nthat remain equitable, inclusive, and trustworthy, supporting rather than\nundermining linguistic autonomy and contributing to deaf aligned futures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the interplay of AI language technologies, sign language\ninterpreting, and linguistic access, highlighting the complex interdependencies\nshaping access frameworks and the tradeoffs these technologies bring. While AI\ntools promise innovation, they also perpetuate biases, reinforce technoableism,\nand deepen inequalities through systemic and design flaws. The historical and\ncontemporary privileging of sign language interpreting as the dominant access\nmodel, and the broader inclusion ideologies it reflects, shape AIs development\nand deployment, often sidelining deaf languaging practices and introducing new\nforms of linguistic subordination to technology. Drawing on Deaf Studies, Sign\nLanguage Interpreting Studies, and crip technoscience, this paper critiques the\nframing of AI as a substitute for interpreters and examines its implications\nfor access hierarchies. It calls for deaf-led approaches to foster AI systems\nthat remain equitable, inclusive, and trustworthy, supporting rather than\nundermining linguistic autonomy and contributing to deaf aligned futures."
                },
                "authors": [
                    {
                        "name": "Maartje De Meulder"
                    }
                ],
                "author_detail": {
                    "name": "Maartje De Meulder"
                },
                "author": "Maartje De Meulder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02481v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02481v5",
                "updated": "2025-05-05T09:57:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    57,
                    34,
                    0,
                    125,
                    0
                ],
                "published": "2024-06-04T16:49:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    16,
                    49,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "Large Language Models as Carriers of Hidden Messages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Carriers of Hidden Messages"
                },
                "summary": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels)."
                },
                "authors": [
                    {
                        "name": "Jakub Hoscilowicz"
                    },
                    {
                        "name": "Pawel Popiolek"
                    },
                    {
                        "name": "Jan Rudkowski"
                    },
                    {
                        "name": "Jedrzej Bieniasz"
                    },
                    {
                        "name": "Artur Janicki"
                    }
                ],
                "author_detail": {
                    "name": "Artur Janicki"
                },
                "author": "Artur Janicki",
                "arxiv_comment": "Accepted on SECRYPT 2025 Conference. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02481v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02481v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00570v2",
                "updated": "2025-05-05T09:49:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    49,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2024-03-31T05:56:15Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    5,
                    56,
                    15,
                    6,
                    91,
                    0
                ],
                "title": "ParaICL: Towards Parallel In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaICL: Towards Parallel In-Context Learning"
                },
                "summary": "Large language models (LLMs) have become the norm in natural language\nprocessing (NLP), excelling in few-shot in-context learning (ICL) with their\nremarkable abilities. Nonetheless, the success of ICL largely hinges on the\nchoice of few-shot demonstration examples, making the selection process\nincreasingly crucial. Existing methods have delved into optimizing the quantity\nand semantic similarity of these examples to improve ICL performances. However,\nour preliminary experiments indicate that the effectiveness of ICL is limited\nby the length of the input context. Moreover, varying combinations of few-shot\ndemonstration examples can significantly boost accuracy across different test\nsamples. To address this, we propose a novel method named parallel in-context\nlearning (ParaICL) that effectively utilizes all demonstration examples without\nexceeding the manageable input context length. ParaICL employs parallel\nbatching to distribute demonstration examples into different batches according\nto the semantic similarities of the questions in the demonstrations to the test\nquestion. It then computes normalized batch semantic scores for each batch. A\nweighted average semantic objective, constrained by adaptive plausibility, is\napplied to select the most appropriate tokens. Through extensive experiments,\nwe validate the effectiveness of ParaICL and conduct ablation studies to\nunderscore its design rationale. We further demonstrate that ParaICL can\nseamlessly integrate with existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become the norm in natural language\nprocessing (NLP), excelling in few-shot in-context learning (ICL) with their\nremarkable abilities. Nonetheless, the success of ICL largely hinges on the\nchoice of few-shot demonstration examples, making the selection process\nincreasingly crucial. Existing methods have delved into optimizing the quantity\nand semantic similarity of these examples to improve ICL performances. However,\nour preliminary experiments indicate that the effectiveness of ICL is limited\nby the length of the input context. Moreover, varying combinations of few-shot\ndemonstration examples can significantly boost accuracy across different test\nsamples. To address this, we propose a novel method named parallel in-context\nlearning (ParaICL) that effectively utilizes all demonstration examples without\nexceeding the manageable input context length. ParaICL employs parallel\nbatching to distribute demonstration examples into different batches according\nto the semantic similarities of the questions in the demonstrations to the test\nquestion. It then computes normalized batch semantic scores for each batch. A\nweighted average semantic objective, constrained by adaptive plausibility, is\napplied to select the most appropriate tokens. Through extensive experiments,\nwe validate the effectiveness of ParaICL and conduct ablation studies to\nunderscore its design rationale. We further demonstrate that ParaICL can\nseamlessly integrate with existing methods."
                },
                "authors": [
                    {
                        "name": "Xingxuan Li"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02502v1",
                "updated": "2025-05-05T09:30:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    30,
                    19,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:30:19Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    30,
                    19,
                    0,
                    125,
                    0
                ],
                "title": "Unveiling the Landscape of LLM Deployment in the Wild: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Landscape of LLM Deployment in the Wild: An Empirical\n  Study"
                },
                "summary": "Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem."
                },
                "authors": [
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Jiahao Han"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02500v1",
                "updated": "2025-05-05T09:29:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    29,
                    13,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:29:13Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    29,
                    13,
                    0,
                    125,
                    0
                ],
                "title": "Automating Automotive Software Development: A Synergy of Generative AI\n  and Formal Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Automotive Software Development: A Synergy of Generative AI\n  and Formal Methods"
                },
                "summary": "As the automotive industry shifts its focus toward software-defined vehicles,\nthe need for faster and reliable software development continues to grow.\nHowever, traditional methods show their limitations. The rise of Generative\nArtificial Intelligence (GenAI), particularly Large Language Models (LLMs),\nintroduces new opportunities to automate automotive software development tasks\nsuch as requirement analysis and code generation. However, due to the\ncomplexity of automotive systems, where software components must interact with\neach other seamlessly, challenges remain in software integration and\nsystem-level validation. In this paper, we propose to combine GenAI with\nmodel-driven engineering to automate automotive software development. Our\napproach uses LLMs to convert free-text requirements into event chain\ndescriptions and to generate platform-independent software components that\nrealize the required functionality. At the same time, formal models are created\nbased on event chain descriptions to support system validation and the\ngeneration of integration code for integrating generated software components in\nthe whole vehicle system through middleware. This approach increases\ndevelopment automation while enabling formal analysis to improve system\nreliability. As a proof of concept, we used GPT-4o to implement our method and\ntested it in the CARLA simulation environment with ROS2 middleware. We\nevaluated the system in a simple Autonomous Emergency Braking scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the automotive industry shifts its focus toward software-defined vehicles,\nthe need for faster and reliable software development continues to grow.\nHowever, traditional methods show their limitations. The rise of Generative\nArtificial Intelligence (GenAI), particularly Large Language Models (LLMs),\nintroduces new opportunities to automate automotive software development tasks\nsuch as requirement analysis and code generation. However, due to the\ncomplexity of automotive systems, where software components must interact with\neach other seamlessly, challenges remain in software integration and\nsystem-level validation. In this paper, we propose to combine GenAI with\nmodel-driven engineering to automate automotive software development. Our\napproach uses LLMs to convert free-text requirements into event chain\ndescriptions and to generate platform-independent software components that\nrealize the required functionality. At the same time, formal models are created\nbased on event chain descriptions to support system validation and the\ngeneration of integration code for integrating generated software components in\nthe whole vehicle system through middleware. This approach increases\ndevelopment automation while enabling formal analysis to improve system\nreliability. As a proof of concept, we used GPT-4o to implement our method and\ntested it in the CARLA simulation environment with ROS2 middleware. We\nevaluated the system in a simple Autonomous Emergency Braking scenario."
                },
                "authors": [
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Yinglei Song"
                    },
                    {
                        "name": "Long Wen"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05547v2",
                "updated": "2025-05-05T09:26:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    26,
                    24,
                    0,
                    125,
                    0
                ],
                "published": "2024-12-07T05:49:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    5,
                    49,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large\n  Language Models"
                },
                "summary": "Large language models with retrieval-augmented generation encounter a pivotal\nchallenge in intricate retrieval tasks, e.g., multi-hop question answering,\nwhich requires the model to navigate across multiple documents and generate\ncomprehensive responses based on fragmented information. To tackle this\nchallenge, we introduce a novel Knowledge Graph-based RAG framework with a\nhierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing\nin KG-Retriever is constructed on a hierarchical index graph that consists of a\nknowledge graph layer and a collaborative document layer. The associative\nnature of graph structures is fully utilized to strengthen intra-document and\ninter-document connectivity, thereby fundamentally alleviating the information\nfragmentation problem and meanwhile improving the retrieval efficiency in\ncross-document retrieval of LLMs. With the coarse-grained collaborative\ninformation from neighboring documents and concise information from the\nknowledge graph, KG-Retriever achieves marked improvements on five public QA\ndatasets, showing the effectiveness and efficiency of our proposed RAG\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models with retrieval-augmented generation encounter a pivotal\nchallenge in intricate retrieval tasks, e.g., multi-hop question answering,\nwhich requires the model to navigate across multiple documents and generate\ncomprehensive responses based on fragmented information. To tackle this\nchallenge, we introduce a novel Knowledge Graph-based RAG framework with a\nhierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing\nin KG-Retriever is constructed on a hierarchical index graph that consists of a\nknowledge graph layer and a collaborative document layer. The associative\nnature of graph structures is fully utilized to strengthen intra-document and\ninter-document connectivity, thereby fundamentally alleviating the information\nfragmentation problem and meanwhile improving the retrieval efficiency in\ncross-document retrieval of LLMs. With the coarse-grained collaborative\ninformation from neighboring documents and concise information from the\nknowledge graph, KG-Retriever achieves marked improvements on five public QA\ndatasets, showing the effectiveness and efficiency of our proposed RAG\nframework."
                },
                "authors": [
                    {
                        "name": "Weijie Chen"
                    },
                    {
                        "name": "Ting Bai"
                    },
                    {
                        "name": "Jinbo Su"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02489v1",
                "updated": "2025-05-05T09:15:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    15,
                    31,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:15:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    15,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Beyond the model: Key differentiators in large language models and\n  multi-agent services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the model: Key differentiators in large language models and\n  multi-agent services"
                },
                "summary": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable."
                },
                "authors": [
                    {
                        "name": "Muskaan Goyal"
                    },
                    {
                        "name": "Pranav Bhasin"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Bhasin"
                },
                "author": "Pranav Bhasin",
                "arxiv_doi": "10.30574/wjarr.2025.26.1.1295",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.30574/wjarr.2025.26.1.1295",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages",
                "arxiv_journal_ref": "World Journal of Advanced Research and Reviews, 2025, 26(01),\n  2703-2706",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02484v1",
                "updated": "2025-05-05T09:07:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    7,
                    22,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:07:22Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    7,
                    22,
                    0,
                    125,
                    0
                ],
                "title": "El Agente: An Autonomous Agent for Quantum Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "El Agente: An Autonomous Agent for Quantum Chemistry"
                },
                "summary": "Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry."
                },
                "authors": [
                    {
                        "name": "Yunheng Zou"
                    },
                    {
                        "name": "Austin H. Cheng"
                    },
                    {
                        "name": "Abdulrahman Aldossary"
                    },
                    {
                        "name": "Jiaru Bai"
                    },
                    {
                        "name": "Shi Xuan Leong"
                    },
                    {
                        "name": "Jorge Arturo Campos-Gonzalez-Angulo"
                    },
                    {
                        "name": "Changhyeok Choi"
                    },
                    {
                        "name": "Cher Tian Ser"
                    },
                    {
                        "name": "Gary Tom"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Ilya Yakavets"
                    },
                    {
                        "name": "Han Hao"
                    },
                    {
                        "name": "Chris Crebolder"
                    },
                    {
                        "name": "Varinia Bernales"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    }
                ],
                "author_detail": {
                    "name": "Alán Aspuru-Guzik"
                },
                "author": "Alán Aspuru-Guzik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02483v1",
                "updated": "2025-05-05T09:06:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    6,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T09:06:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    6,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic\n  Skill Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic\n  Skill Learning"
                },
                "summary": "Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks."
                },
                "authors": [
                    {
                        "name": "Changxin Huang"
                    },
                    {
                        "name": "Junyang Liang"
                    },
                    {
                        "name": "Yanbin Chang"
                    },
                    {
                        "name": "Jingzhao Xu"
                    },
                    {
                        "name": "Jianqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiang Li"
                },
                "author": "Jianqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04667v2",
                "updated": "2025-05-05T09:01:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    9,
                    1,
                    6,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-07T05:21:13Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    5,
                    21,
                    13,
                    4,
                    38,
                    0
                ],
                "title": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances\n  Reasoning Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances\n  Reasoning Generalization"
                },
                "summary": "The integration of explicit Chain-of-Thought (CoT) reasoning into training\nlarge language models (LLMs) has advanced their reasoning capabilities, yet the\nmechanisms by which CoT enhances generalization remain poorly understood. This\nwork investigates (1) \\textit{how} CoT training reshapes internal model\nrepresentations and (2) \\textit{why} it improves both in-distribution (ID) and\nout-of-distribution (OOD) reasoning generalization. Through controlled\nexperiments and theoretical analysis, we derive the following key insights.\n\\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a\ntwo-stage generalizing circuit, where the number of stages corresponds to the\nexplicit reasoning steps during training. Notably, CoT-trained models resolve\nintermediate results at shallower layers compared to non-CoT counterparts,\nfreeing up deeper layers to specialize in subsequent reasoning steps.\n\\textbf{2)} Theoretical Analysis: the information-theoretic generalization\nbounds via distributional divergence can be decomposed into ID and OOD\ncomponents. While ID error diminishes with sufficient training regardless of\nCoT, OOD error critically depends on CoT: Non-CoT training fails to generalize\nto OOD samples due to unseen reasoning patterns, whereas CoT training achieves\nnear-perfect OOD generalization by mastering subtasks and reasoning\ncompositions during training. The identified mechanisms explain our\nexperimental results: CoT training accelerates convergence and enhances\ngeneralization from ID to both ID and OOD scenarios while maintaining robust\nperformance even with tolerable noise. These findings are further validated on\ncomplex real-world datasets. This paper offers valuable insights for designing\nCoT strategies to enhance LLM reasoning robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of explicit Chain-of-Thought (CoT) reasoning into training\nlarge language models (LLMs) has advanced their reasoning capabilities, yet the\nmechanisms by which CoT enhances generalization remain poorly understood. This\nwork investigates (1) \\textit{how} CoT training reshapes internal model\nrepresentations and (2) \\textit{why} it improves both in-distribution (ID) and\nout-of-distribution (OOD) reasoning generalization. Through controlled\nexperiments and theoretical analysis, we derive the following key insights.\n\\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a\ntwo-stage generalizing circuit, where the number of stages corresponds to the\nexplicit reasoning steps during training. Notably, CoT-trained models resolve\nintermediate results at shallower layers compared to non-CoT counterparts,\nfreeing up deeper layers to specialize in subsequent reasoning steps.\n\\textbf{2)} Theoretical Analysis: the information-theoretic generalization\nbounds via distributional divergence can be decomposed into ID and OOD\ncomponents. While ID error diminishes with sufficient training regardless of\nCoT, OOD error critically depends on CoT: Non-CoT training fails to generalize\nto OOD samples due to unseen reasoning patterns, whereas CoT training achieves\nnear-perfect OOD generalization by mastering subtasks and reasoning\ncompositions during training. The identified mechanisms explain our\nexperimental results: CoT training accelerates convergence and enhances\ngeneralization from ID to both ID and OOD scenarios while maintaining robust\nperformance even with tolerable noise. These findings are further validated on\ncomplex real-world datasets. This paper offers valuable insights for designing\nCoT strategies to enhance LLM reasoning robustness."
                },
                "authors": [
                    {
                        "name": "Xinhao Yao"
                    },
                    {
                        "name": "Ruifeng Ren"
                    },
                    {
                        "name": "Yun Liao"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02466v1",
                "updated": "2025-05-05T08:52:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    52,
                    49,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T08:52:49Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    52,
                    49,
                    0,
                    125,
                    0
                ],
                "title": "Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language,\n  and Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language,\n  and Modality"
                },
                "summary": "Recent advancements in large language models (LLMs) have driven interest in\nbillion-scale retrieval models with strong generalization across retrieval\ntasks and languages. Additionally, progress in large vision-language models has\ncreated new opportunities for multimodal retrieval. In response, we have\nupdated the Tevatron toolkit, introducing a unified pipeline that enables\nresearchers to explore retriever models at different scales, across multiple\nlanguages, and with various modalities. This demo paper highlights the\ntoolkit's key features, bridging academia and industry by supporting efficient\ntraining, inference, and evaluation of neural retrievers. We showcase a unified\ndense retriever achieving strong multilingual and multimodal effectiveness, and\nconduct a cross-modality zero-shot study to demonstrate its research potential.\nAlongside, we release OmniEmbed, to the best of our knowledge, the first\nembedding model that unifies text, image document, video, and audio retrieval,\nserving as a baseline for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have driven interest in\nbillion-scale retrieval models with strong generalization across retrieval\ntasks and languages. Additionally, progress in large vision-language models has\ncreated new opportunities for multimodal retrieval. In response, we have\nupdated the Tevatron toolkit, introducing a unified pipeline that enables\nresearchers to explore retriever models at different scales, across multiple\nlanguages, and with various modalities. This demo paper highlights the\ntoolkit's key features, bridging academia and industry by supporting efficient\ntraining, inference, and evaluation of neural retrievers. We showcase a unified\ndense retriever achieving strong multilingual and multimodal effectiveness, and\nconduct a cross-modality zero-shot study to demonstrate its research potential.\nAlongside, we release OmniEmbed, to the best of our knowledge, the first\nembedding model that unifies text, image document, video, and audio retrieval,\nserving as a baseline for future research."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Luyu Gao"
                    },
                    {
                        "name": "Shengyao Zhuang"
                    },
                    {
                        "name": "Jiaqi Samantha Zhan"
                    },
                    {
                        "name": "Jamie Callan"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "Accepted in SIGIR 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02456v1",
                "updated": "2025-05-05T08:40:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    40,
                    51,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T08:40:51Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    8,
                    40,
                    51,
                    0,
                    125,
                    0
                ],
                "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in\n  Occupation Recommendations from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in\n  Occupation Recommendations from LLMs"
                },
                "summary": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work."
                },
                "authors": [
                    {
                        "name": "Elisa Forcada Rodríguez"
                    },
                    {
                        "name": "Olatz Perez-de-Viñaspre"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Vagrant Gautam"
                    }
                ],
                "author_detail": {
                    "name": "Vagrant Gautam"
                },
                "author": "Vagrant Gautam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02428v1",
                "updated": "2025-05-05T07:47:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    47,
                    21,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:47:21Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    47,
                    21,
                    0,
                    125,
                    0
                ],
                "title": "Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A\n  Randomized Study with 90+ Novice Counselors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A\n  Randomized Study with 90+ Novice Counselors"
                },
                "summary": "Training more counselors, from clinical students to peer supporters, can help\nmeet the demand for accessible mental health support; however, current training\napproaches remain resource-intensive and difficult to scale effectively. Large\nLanguage Models (LLMs) offer promising solutions for growing counseling skills\ntraining through simulated practice and automated feedback. Despite successes\nin aligning LLMs with expert-counselor annotations, we do not know whether\nLLM-based counseling training tools -- such as AI patients that simulate\nreal-world challenges and generative AI feedback with suggested alternatives\nand rationales -- actually lead to improvements in novice counselor skill\ndevelopment. We develop CARE, an LLM-simulated practice and feedback system,\nand randomize 94 novice counselors to practice using an AI patient, either\nalone or with AI feedback, measuring changes in their behavioral performance,\nself-assessments, and qualitative learning takeaways. Our results show the\npractice-and-feedback group improved in their use of reflections and questions\n(d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI\npatient alone did not show improvements, and in the case of empathy, actually\nhad worse uses across time (d=$-$0.52, p=0.001) and when compared against the\npractice-and-feedback group (d=0.72, p=0.001). Participants' qualitative\nself-reflections revealed key differences: the practice-and-feedback group\nadopted a client-centered approach involving listening to and validating\nfeelings, while the practice-alone group remained solution-oriented but delayed\noffering suggestions until gathering more information. Overall, these results\nsuggest that LLM-based training systems can promote effective skill\ndevelopment, but that combining both simulated practice and structured feedback\nis critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training more counselors, from clinical students to peer supporters, can help\nmeet the demand for accessible mental health support; however, current training\napproaches remain resource-intensive and difficult to scale effectively. Large\nLanguage Models (LLMs) offer promising solutions for growing counseling skills\ntraining through simulated practice and automated feedback. Despite successes\nin aligning LLMs with expert-counselor annotations, we do not know whether\nLLM-based counseling training tools -- such as AI patients that simulate\nreal-world challenges and generative AI feedback with suggested alternatives\nand rationales -- actually lead to improvements in novice counselor skill\ndevelopment. We develop CARE, an LLM-simulated practice and feedback system,\nand randomize 94 novice counselors to practice using an AI patient, either\nalone or with AI feedback, measuring changes in their behavioral performance,\nself-assessments, and qualitative learning takeaways. Our results show the\npractice-and-feedback group improved in their use of reflections and questions\n(d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI\npatient alone did not show improvements, and in the case of empathy, actually\nhad worse uses across time (d=$-$0.52, p=0.001) and when compared against the\npractice-and-feedback group (d=0.72, p=0.001). Participants' qualitative\nself-reflections revealed key differences: the practice-and-feedback group\nadopted a client-centered approach involving listening to and validating\nfeelings, while the practice-alone group remained solution-oriented but delayed\noffering suggestions until gathering more information. Overall, these results\nsuggest that LLM-based training systems can promote effective skill\ndevelopment, but that combining both simulated practice and structured feedback\nis critical."
                },
                "authors": [
                    {
                        "name": "Ryan Louie"
                    },
                    {
                        "name": "Ifdita Hasan Orney"
                    },
                    {
                        "name": "Juan Pablo Pacheco"
                    },
                    {
                        "name": "Raj Sanjay Shah"
                    },
                    {
                        "name": "Emma Brunskill"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "main paper is 11 pages, with methods it is 18 pages, with appendix\n  and references it is 33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10519v3",
                "updated": "2025-05-05T07:25:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    25,
                    31,
                    0,
                    125,
                    0
                ],
                "published": "2024-10-14T13:59:30Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    59,
                    30,
                    0,
                    288,
                    0
                ],
                "title": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based particle track identification in scintillating fibres read out\n  with imaging sensors"
                },
                "summary": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and application of an AI-based method for\nparticle track identification using scintillating fibres read out with imaging\nsensors. We propose a variational autoencoder (VAE) to efficiently filter and\nidentify frames containing signal from the substantial data generated by SPAD\narray sensors. Our VAE model, trained on purely background frames, demonstrated\na high capability to distinguish frames containing particle tracks from\nbackground noise. The performance of the VAE-based anomaly detection was\nvalidated with experimental data, demonstrating the method's ability to\nefficiently identify relevant events with rapid processing time, suggesting a\nsolid prospect for deployment as a fast inference tool on hardware for\nreal-time anomaly detection. This work highlights the potential of combining\nadvanced sensor technology with machine learning techniques to enhance particle\ndetection and tracking."
                },
                "authors": [
                    {
                        "name": "Noemi Bührer"
                    },
                    {
                        "name": "Saúl Alonso-Monsalve"
                    },
                    {
                        "name": "Matthew Franks"
                    },
                    {
                        "name": "Till Dieminger"
                    },
                    {
                        "name": "Davide Sgalaberna"
                    }
                ],
                "author_detail": {
                    "name": "Davide Sgalaberna"
                },
                "author": "Davide Sgalaberna",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02418v1",
                "updated": "2025-05-05T07:24:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    24,
                    38,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:24:38Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    24,
                    38,
                    0,
                    125,
                    0
                ],
                "title": "SymbioticRAG: Enhancing Document Intelligence Through Human-LLM\n  Symbiotic Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymbioticRAG: Enhancing Document Intelligence Through Human-LLM\n  Symbiotic Collaboration"
                },
                "summary": "We present \\textbf{SymbioticRAG}, a novel framework that fundamentally\nreimagines Retrieval-Augmented Generation~(RAG) systems by establishing a\nbidirectional learning relationship between humans and machines. Our approach\naddresses two critical challenges in current RAG systems: the inherently\nhuman-centered nature of relevance determination and users' progression from\n\"unconscious incompetence\" in query formulation. SymbioticRAG introduces a\ntwo-tier solution where Level 1 enables direct human curation of retrieved\ncontent through interactive source document exploration, while Level 2 aims to\nbuild personalized retrieval models based on captured user interactions. We\nimplement Level 1 through three key components: (1)~a comprehensive document\nprocessing pipeline with specialized models for layout detection, OCR, and\nextraction of tables, formulas, and figures; (2)~an extensible retriever module\nsupporting multiple retrieval strategies; and (3)~an interactive interface that\nfacilitates both user engagement and interaction data logging. We experiment\nLevel 2 implementation via a retriever strategy incorporated LLM summarized\nuser intention from user interaction logs. To maintain high-quality data\npreparation, we develop a human-on-the-loop validation interface that improves\npipeline output while advancing research in specialized extraction tasks.\nEvaluation across three scenarios (literature review, geological exploration,\nand education) demonstrates significant improvements in retrieval relevance and\nuser satisfaction compared to traditional RAG approaches. To facilitate broader\nresearch and further advancement of SymbioticRAG Level 2 implementation, we\nwill make our system openly accessible to the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textbf{SymbioticRAG}, a novel framework that fundamentally\nreimagines Retrieval-Augmented Generation~(RAG) systems by establishing a\nbidirectional learning relationship between humans and machines. Our approach\naddresses two critical challenges in current RAG systems: the inherently\nhuman-centered nature of relevance determination and users' progression from\n\"unconscious incompetence\" in query formulation. SymbioticRAG introduces a\ntwo-tier solution where Level 1 enables direct human curation of retrieved\ncontent through interactive source document exploration, while Level 2 aims to\nbuild personalized retrieval models based on captured user interactions. We\nimplement Level 1 through three key components: (1)~a comprehensive document\nprocessing pipeline with specialized models for layout detection, OCR, and\nextraction of tables, formulas, and figures; (2)~an extensible retriever module\nsupporting multiple retrieval strategies; and (3)~an interactive interface that\nfacilitates both user engagement and interaction data logging. We experiment\nLevel 2 implementation via a retriever strategy incorporated LLM summarized\nuser intention from user interaction logs. To maintain high-quality data\npreparation, we develop a human-on-the-loop validation interface that improves\npipeline output while advancing research in specialized extraction tasks.\nEvaluation across three scenarios (literature review, geological exploration,\nand education) demonstrates significant improvements in retrieval relevance and\nuser satisfaction compared to traditional RAG approaches. To facilitate broader\nresearch and further advancement of SymbioticRAG Level 2 implementation, we\nwill make our system openly accessible to the research community."
                },
                "authors": [
                    {
                        "name": "Qiang Sun"
                    },
                    {
                        "name": "Tingting Bi"
                    },
                    {
                        "name": "Sirui Li"
                    },
                    {
                        "name": "Eun-Jung Holden"
                    },
                    {
                        "name": "Paul Duuring"
                    },
                    {
                        "name": "Kai Niu"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02498v2",
                "updated": "2025-05-05T07:19:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    19,
                    50,
                    0,
                    125,
                    0
                ],
                "published": "2025-03-04T11:04:36Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    36,
                    1,
                    63,
                    0
                ],
                "title": "A Systematic Literature Review on Safety of the Intended Functionality\n  for Automated Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Safety of the Intended Functionality\n  for Automated Driving Systems"
                },
                "summary": "In the automobile industry, ensuring the safety of automated vehicles\nequipped with the Automated Driving System (ADS) is becoming a significant\nfocus due to the increasing development and deployment of automated driving.\nAutomated driving depends on sensing both the external and internal\nenvironments of a vehicle, utilizing perception sensors and algorithms, and\nElectrical/Electronic (E/E) systems for situational awareness and response. ISO\n21448 is the standard for Safety of the Intended Functionality (SOTIF) that\naims to ensure that the ADS operate safely within their intended functionality.\nSOTIF focuses on preventing or mitigating potential hazards that may arise from\nthe limitations or failures of the ADS, including hazards due to\ninsufficiencies of specification, or performance insufficiencies, as well as\nforeseeable misuse of the intended functionality. However, the challenge lies\nin ensuring the safety of vehicles despite the limited availability of\nextensive and systematic literature on SOTIF. To address this challenge, a\nSystematic Literature Review (SLR) on SOTIF for the ADS is performed following\nthe Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. The objective is to methodically gather and analyze the existing\nliterature on SOTIF. The major contributions of this paper are: (i) presenting\na summary of the literature by synthesizing and organizing the collective\nfindings, methodologies, and insights into distinct thematic groups, and (ii)\nsummarizing and categorizing the acknowledged limitations based on data\nextracted from an SLR of 51 research papers published between 2018 and 2023.\nFurthermore, research gaps are determined, and future research directions are\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the automobile industry, ensuring the safety of automated vehicles\nequipped with the Automated Driving System (ADS) is becoming a significant\nfocus due to the increasing development and deployment of automated driving.\nAutomated driving depends on sensing both the external and internal\nenvironments of a vehicle, utilizing perception sensors and algorithms, and\nElectrical/Electronic (E/E) systems for situational awareness and response. ISO\n21448 is the standard for Safety of the Intended Functionality (SOTIF) that\naims to ensure that the ADS operate safely within their intended functionality.\nSOTIF focuses on preventing or mitigating potential hazards that may arise from\nthe limitations or failures of the ADS, including hazards due to\ninsufficiencies of specification, or performance insufficiencies, as well as\nforeseeable misuse of the intended functionality. However, the challenge lies\nin ensuring the safety of vehicles despite the limited availability of\nextensive and systematic literature on SOTIF. To address this challenge, a\nSystematic Literature Review (SLR) on SOTIF for the ADS is performed following\nthe Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. The objective is to methodically gather and analyze the existing\nliterature on SOTIF. The major contributions of this paper are: (i) presenting\na summary of the literature by synthesizing and organizing the collective\nfindings, methodologies, and insights into distinct thematic groups, and (ii)\nsummarizing and categorizing the acknowledged limitations based on data\nextracted from an SLR of 51 research papers published between 2018 and 2023.\nFurthermore, research gaps are determined, and future research directions are\nproposed."
                },
                "authors": [
                    {
                        "name": "Milin Patel"
                    },
                    {
                        "name": "Rolf Jung"
                    },
                    {
                        "name": "Marzana Khatun"
                    }
                ],
                "author_detail": {
                    "name": "Marzana Khatun"
                },
                "author": "Marzana Khatun",
                "arxiv_doi": "10.4271/2025-01-5030",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4271/2025-01-5030",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Scheduled to be published in SAE journal as technical paper as a part\n  of non-technical event and will be available as open access in 2025",
                "arxiv_journal_ref": "SAE Technical Paper 2025-01-5030, 2025",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02413v1",
                "updated": "2025-05-05T07:18:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    18,
                    47,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:18:47Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    18,
                    47,
                    0,
                    125,
                    0
                ],
                "title": "Task-Oriented Semantic Communication in Large Multimodal Models-based\n  Vehicle Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Oriented Semantic Communication in Large Multimodal Models-based\n  Vehicle Networks"
                },
                "summary": "Task-oriented semantic communication has emerged as a fundamental approach\nfor enhancing performance in various communication scenarios. While recent\nadvances in Generative Artificial Intelligence (GenAI), such as Large Language\nModels (LLMs), have been applied to semantic communication designs, the\npotential of Large Multimodal Models (LMMs) remains largely unexplored. In this\npaper, we investigate an LMM-based vehicle AI assistant using a Large Language\nand Vision Assistant (LLaVA) and propose a task-oriented semantic communication\nframework to facilitate efficient interaction between users and cloud servers.\nTo reduce computational demands and shorten response time, we optimize LLaVA's\nimage slicing to selectively focus on areas of utmost interest to users.\nAdditionally, we assess the importance of image patches by combining objective\nand subjective user attention, adjusting energy usage for transmitting semantic\ninformation. This strategy optimizes resource utilization, ensuring precise\ntransmission of critical information. We construct a Visual Question Answering\n(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental\nresults show that our semantic communication framework significantly increases\naccuracy in answering questions under the same channel conditions, performing\nparticularly well in environments with poor Signal-to-Noise Ratios (SNR).\nAccuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented semantic communication has emerged as a fundamental approach\nfor enhancing performance in various communication scenarios. While recent\nadvances in Generative Artificial Intelligence (GenAI), such as Large Language\nModels (LLMs), have been applied to semantic communication designs, the\npotential of Large Multimodal Models (LMMs) remains largely unexplored. In this\npaper, we investigate an LMM-based vehicle AI assistant using a Large Language\nand Vision Assistant (LLaVA) and propose a task-oriented semantic communication\nframework to facilitate efficient interaction between users and cloud servers.\nTo reduce computational demands and shorten response time, we optimize LLaVA's\nimage slicing to selectively focus on areas of utmost interest to users.\nAdditionally, we assess the importance of image patches by combining objective\nand subjective user attention, adjusting energy usage for transmitting semantic\ninformation. This strategy optimizes resource utilization, ensuring precise\ntransmission of critical information. We construct a Visual Question Answering\n(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental\nresults show that our semantic communication framework significantly increases\naccuracy in answering questions under the same channel conditions, performing\nparticularly well in environments with poor Signal-to-Noise Ratios (SNR).\nAccuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Baoxia Du"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Ruidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruidong Li"
                },
                "author": "Ruidong Li",
                "arxiv_doi": "10.1109/TMC.2025.3564543",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TMC.2025.3564543",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02410v1",
                "updated": "2025-05-05T07:03:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    3,
                    41,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T07:03:41Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    3,
                    41,
                    0,
                    125,
                    0
                ],
                "title": "Bielik 11B v2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik 11B v2 Technical Report"
                },
                "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    }
                ],
                "author_detail": {
                    "name": "Remigiusz Kinas"
                },
                "author": "Remigiusz Kinas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15210v2",
                "updated": "2025-05-05T06:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    56,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-21T16:29:07Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    16,
                    29,
                    7,
                    0,
                    111,
                    0
                ],
                "title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs"
                },
                "summary": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark."
                },
                "authors": [
                    {
                        "name": "Marina Sakharova"
                    },
                    {
                        "name": "Abhinav Anand"
                    },
                    {
                        "name": "Mira Mezini"
                    }
                ],
                "author_detail": {
                    "name": "Mira Mezini"
                },
                "author": "Mira Mezini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02405v1",
                "updated": "2025-05-05T06:55:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    55,
                    59,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:55:59Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    55,
                    59,
                    0,
                    125,
                    0
                ],
                "title": "Estimating Commonsense Scene Composition on Belief Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Commonsense Scene Composition on Belief Scene Graphs"
                },
                "summary": "This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types."
                },
                "authors": [
                    {
                        "name": "Mario A. V. Saucedo"
                    },
                    {
                        "name": "Vignesh Kottayam Viswanathan"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "Accepted at ICRA25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02391v1",
                "updated": "2025-05-05T06:26:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    26,
                    0,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:26:00Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    26,
                    0,
                    0,
                    125,
                    0
                ],
                "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL"
                },
                "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM."
                },
                "authors": [
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Hanning Zhang"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02390v1",
                "updated": "2025-05-05T06:25:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    25,
                    20,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:25:20Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    25,
                    20,
                    0,
                    125,
                    0
                ],
                "title": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization"
                },
                "summary": "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,\npossibly because the official service often suffers from being busy and some\norganizations have data privacy concerns. While single-machine deployment\noffers infrastructure simplicity, the models' 671B FP8 parameter configuration\nexceeds the practical memory limits of a standard 8-GPU machine. Quantization\nis a widely used technique that helps reduce model memory consumption. However,\nit is unclear what the performance of DeepSeek-R1 and V3 will be after being\nquantized. This technical report presents the first quantitative evaluation of\nmulti-bitwidth quantization across the complete DeepSeek model spectrum. Key\nfindings reveal that 4-bit quantization maintains little performance\ndegradation versus FP8 while enabling single-machine deployment on standard\nNVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization\nmethod that significantly outperforms traditional Q3_K_M variant on various\nbenchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach\nin most tasks. Moreover, DQ3_K_M supports single-machine deployment\nconfigurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of\nDQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing\noptimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,\npossibly because the official service often suffers from being busy and some\norganizations have data privacy concerns. While single-machine deployment\noffers infrastructure simplicity, the models' 671B FP8 parameter configuration\nexceeds the practical memory limits of a standard 8-GPU machine. Quantization\nis a widely used technique that helps reduce model memory consumption. However,\nit is unclear what the performance of DeepSeek-R1 and V3 will be after being\nquantized. This technical report presents the first quantitative evaluation of\nmulti-bitwidth quantization across the complete DeepSeek model spectrum. Key\nfindings reveal that 4-bit quantization maintains little performance\ndegradation versus FP8 while enabling single-machine deployment on standard\nNVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization\nmethod that significantly outperforms traditional Q3_K_M variant on various\nbenchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach\nin most tasks. Moreover, DQ3_K_M supports single-machine deployment\nconfigurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of\nDQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing\noptimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "Enbo Zhao"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Shuming Shi"
                    },
                    {
                        "name": "Jieyun Huang"
                    },
                    {
                        "name": "Zhihao Chen"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Siqi Xiao"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02387v1",
                "updated": "2025-05-05T06:11:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    11,
                    12,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T06:11:12Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    6,
                    11,
                    12,
                    0,
                    125,
                    0
                ],
                "title": "RM-R1: Reward Modeling as Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RM-R1: Reward Modeling as Reasoning"
                },
                "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1."
                },
                "authors": [
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Denghui Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Hanghang Tong"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "23 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02380v2",
                "updated": "2025-05-06T09:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    37,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T05:42:14Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    42,
                    14,
                    0,
                    125,
                    0
                ],
                "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs."
                },
                "authors": [
                    {
                        "name": "Arnab Sanyal"
                    },
                    {
                        "name": "Prithwish Mukherjee"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Sandeep P. Chinchali"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep P. Chinchali"
                },
                "author": "Sandeep P. Chinchali",
                "arxiv_comment": "6 pages, 1 reference page. Under submission and review at ISLPED 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02376v1",
                "updated": "2025-05-05T05:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    34,
                    33,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T05:34:33Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    34,
                    33,
                    0,
                    125,
                    0
                ],
                "title": "LAMeD: LLM-generated Annotations for Memory Leak Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMeD: LLM-generated Annotations for Memory Leak Detection"
                },
                "summary": "Static analysis tools are widely used to detect software bugs and\nvulnerabilities but often struggle with scalability and efficiency in complex\ncodebases. Traditional approaches rely on manually crafted annotations --\nlabeling functions as sources or sinks -- to track data flows, e.g., ensuring\nthat allocated memory is eventually freed, and code analysis tools such as\nCodeQL, Infer, or Cooddy can use function specifications, but manual annotation\nis laborious and error-prone, especially for large or third-party libraries. We\npresent LAMeD (LLM-generated Annotations for Memory leak Detection), a novel\napproach that leverages large language models (LLMs) to automatically generate\nfunction-specific annotations. When integrated with analyzers such as Cooddy,\nLAMeD significantly improves memory leak detection and reduces path explosion.\nWe also suggest directions for extending LAMeD to broader code analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect software bugs and\nvulnerabilities but often struggle with scalability and efficiency in complex\ncodebases. Traditional approaches rely on manually crafted annotations --\nlabeling functions as sources or sinks -- to track data flows, e.g., ensuring\nthat allocated memory is eventually freed, and code analysis tools such as\nCodeQL, Infer, or Cooddy can use function specifications, but manual annotation\nis laborious and error-prone, especially for large or third-party libraries. We\npresent LAMeD (LLM-generated Annotations for Memory leak Detection), a novel\napproach that leverages large language models (LLMs) to automatically generate\nfunction-specific annotations. When integrated with analyzers such as Cooddy,\nLAMeD significantly improves memory leak detection and reduces path explosion.\nWe also suggest directions for extending LAMeD to broader code analysis."
                },
                "authors": [
                    {
                        "name": "Ekaterina Shemetova"
                    },
                    {
                        "name": "Ilya Shenbin"
                    },
                    {
                        "name": "Ivan Smirnov"
                    },
                    {
                        "name": "Anton Alekseev"
                    },
                    {
                        "name": "Alexey Rukhovich"
                    },
                    {
                        "name": "Sergey Nikolenko"
                    },
                    {
                        "name": "Vadim Lomshakov"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    }
                ],
                "author_detail": {
                    "name": "Irina Piontkovskaya"
                },
                "author": "Irina Piontkovskaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00874v2",
                "updated": "2025-05-05T05:01:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    1,
                    36,
                    0,
                    125,
                    0
                ],
                "published": "2025-01-01T15:43:07Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    43,
                    7,
                    2,
                    1,
                    0
                ],
                "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual\n  Embeddings with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual\n  Embeddings with Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."
                },
                "authors": [
                    {
                        "name": "Hieu Man"
                    },
                    {
                        "name": "Nghia Trung Ngo"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05314v3",
                "updated": "2025-05-05T04:44:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    44,
                    30,
                    0,
                    125,
                    0
                ],
                "published": "2024-09-09T03:58:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    3,
                    58,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "Tele-LLMs: A Series of Specialized Large Language Models for\n  Telecommunications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tele-LLMs: A Series of Specialized Large Language Models for\n  Telecommunications"
                },
                "summary": "The emergence of large language models (LLMs) has significantly impacted\nvarious fields, from natural language processing to sectors like medicine and\nfinance. However, despite their rapid proliferation, the applications of LLMs\nin telecommunications remain limited, often relying on general-purpose models\nthat lack domain-specific specialization. This lack of specialization results\nin underperformance, particularly when dealing with telecommunications-specific\ntechnical terminology and their associated mathematical representations. This\npaper addresses this gap by first creating and disseminating Tele-Data, a\ncomprehensive dataset of telecommunications material curated from relevant\nsources, and Tele-Eval, a large-scale question-and-answer dataset tailored to\nthe domain. Through extensive experiments, we explore the most effective\ntraining techniques for adapting LLMs to the telecommunications domain, ranging\nfrom examining the division of expertise across various telecommunications\naspects to employing parameter-efficient techniques. We also investigate how\nmodels of different sizes behave during adaptation and analyze the impact of\ntheir training data on this behavior. Leveraging these findings, we develop and\nopen-source Tele-LLMs, the first series of language models ranging from 1B to\n8B parameters, specifically tailored for telecommunications. Our evaluations\ndemonstrate that these models outperform their general-purpose counterparts on\nTele-Eval and telecommunications-related literature tasks while retaining their\npreviously acquired capabilities, thus avoiding the catastrophic forgetting\nphenomenon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly impacted\nvarious fields, from natural language processing to sectors like medicine and\nfinance. However, despite their rapid proliferation, the applications of LLMs\nin telecommunications remain limited, often relying on general-purpose models\nthat lack domain-specific specialization. This lack of specialization results\nin underperformance, particularly when dealing with telecommunications-specific\ntechnical terminology and their associated mathematical representations. This\npaper addresses this gap by first creating and disseminating Tele-Data, a\ncomprehensive dataset of telecommunications material curated from relevant\nsources, and Tele-Eval, a large-scale question-and-answer dataset tailored to\nthe domain. Through extensive experiments, we explore the most effective\ntraining techniques for adapting LLMs to the telecommunications domain, ranging\nfrom examining the division of expertise across various telecommunications\naspects to employing parameter-efficient techniques. We also investigate how\nmodels of different sizes behave during adaptation and analyze the impact of\ntheir training data on this behavior. Leveraging these findings, we develop and\nopen-source Tele-LLMs, the first series of language models ranging from 1B to\n8B parameters, specifically tailored for telecommunications. Our evaluations\ndemonstrate that these models outperform their general-purpose counterparts on\nTele-Eval and telecommunications-related literature tasks while retaining their\npreviously acquired capabilities, thus avoiding the catastrophic forgetting\nphenomenon."
                },
                "authors": [
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Kenny Chirino Ampudia"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00985v2",
                "updated": "2025-05-05T04:30:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    30,
                    12,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-02T04:13:27Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    13,
                    27,
                    4,
                    122,
                    0
                ],
                "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling"
                },
                "summary": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the dominant focus on neural scaling laws and advocate for a\nparadigm shift toward downscaling in the development of large language models\n(LLMs). While scaling laws have provided critical insights into performance\nimprovements through increasing model and dataset size, we emphasize the\nsignificant limitations of this approach, particularly in terms of\ncomputational inefficiency, environmental impact, and deployment constraints.\nTo address these challenges, we propose a holistic framework for downscaling\nLLMs that seeks to maintain performance while drastically reducing resource\ndemands. This paper outlines practical strategies for transitioning away from\ntraditional scaling paradigms, advocating for a more sustainable, efficient,\nand accessible approach to LLM development."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Yash Goel"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01111v2",
                "updated": "2025-05-05T04:18:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    18,
                    9,
                    0,
                    125,
                    0
                ],
                "published": "2025-03-03T02:37:16Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    2,
                    37,
                    16,
                    0,
                    62,
                    0
                ],
                "title": "Exploration on Real World Assets and Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration on Real World Assets and Tokenization"
                },
                "summary": "This study delves into the tokenization of real-world assets (RWAs) on the\nblockchain with the objective of augmenting liquidity and refining asset\nmanagement practices. By conducting an exhaustive analysis of the technical\nprocedures implicated and scrutinizing case studies of existing deployments,\nthis research evaluates the advantages, hurdles, and prospective advancements\nof blockchain technology in reshaping conventional asset management paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study delves into the tokenization of real-world assets (RWAs) on the\nblockchain with the objective of augmenting liquidity and refining asset\nmanagement practices. By conducting an exhaustive analysis of the technical\nprocedures implicated and scrutinizing case studies of existing deployments,\nthis research evaluates the advantages, hurdles, and prospective advancements\nof blockchain technology in reshaping conventional asset management paradigms."
                },
                "authors": [
                    {
                        "name": "Ning Xia"
                    },
                    {
                        "name": "Xiaolei Zhao"
                    },
                    {
                        "name": "Yimin Yang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yucong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yucong Li"
                },
                "author": "Yucong Li",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15666v2",
                "updated": "2025-05-05T03:57:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    3,
                    57,
                    24,
                    0,
                    125,
                    0
                ],
                "published": "2025-02-21T18:45:37Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    45,
                    37,
                    4,
                    52,
                    0
                ],
                "title": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing"
                },
                "summary": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Such classification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate twelve\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains 14.7K samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently flag even\nminimally polished text as AI-generated, struggle to differentiate between\ndegrees of AI involvement, and exhibit biases against older and smaller models.\nThese limitations highlight the urgent need for more nuanced detection\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Such classification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate twelve\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains 14.7K samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently flag even\nminimally polished text as AI-generated, struggle to differentiate between\ndegrees of AI involvement, and exhibit biases against older and smaller models.\nThese limitations highlight the urgent need for more nuanced detection\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Shoumik Saha"
                    },
                    {
                        "name": "Soheil Feizi"
                    }
                ],
                "author_detail": {
                    "name": "Soheil Feizi"
                },
                "author": "Soheil Feizi",
                "arxiv_comment": "18 pages, 18 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02344v1",
                "updated": "2025-05-05T03:50:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    3,
                    50,
                    28,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T03:50:28Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    3,
                    50,
                    28,
                    0,
                    125,
                    0
                ],
                "title": "An End-to-End Model For Logits Based Large Language Models Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An End-to-End Model For Logits Based Large Language Models Watermarking"
                },
                "summary": "The rise of LLMs has increased concerns over source tracing and copyright\nprotection for AIGC, highlighting the need for advanced detection technologies.\nPassive detection methods usually face high false positives, while active\nwatermarking techniques using logits or sampling manipulation offer more\neffective protection. Existing LLM watermarking methods, though effective on\nunaltered content, suffer significant performance drops when the text is\nmodified and could introduce biases that degrade LLM performance in downstream\ntasks. These methods fail to achieve an optimal tradeoff between text quality\nand robustness, particularly due to the lack of end-to-end optimization of the\nencoder and decoder. In this paper, we introduce a novel end-to-end logits\nperturbation method for watermarking LLM-generated text. By jointly\noptimization, our approach achieves a better balance between quality and\nrobustness. To address non-differentiable operations in the end-to-end training\npipeline, we introduce an online prompting technique that leverages the\non-the-fly LLM as a differentiable surrogate. Our method achieves superior\nrobustness, outperforming distortion-free methods by 37-39% under paraphrasing\nand 17.2% on average, while maintaining text quality on par with these\ndistortion-free methods in terms of text perplexity and downstream tasks. Our\nmethod can be easily generalized to different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of LLMs has increased concerns over source tracing and copyright\nprotection for AIGC, highlighting the need for advanced detection technologies.\nPassive detection methods usually face high false positives, while active\nwatermarking techniques using logits or sampling manipulation offer more\neffective protection. Existing LLM watermarking methods, though effective on\nunaltered content, suffer significant performance drops when the text is\nmodified and could introduce biases that degrade LLM performance in downstream\ntasks. These methods fail to achieve an optimal tradeoff between text quality\nand robustness, particularly due to the lack of end-to-end optimization of the\nencoder and decoder. In this paper, we introduce a novel end-to-end logits\nperturbation method for watermarking LLM-generated text. By jointly\noptimization, our approach achieves a better balance between quality and\nrobustness. To address non-differentiable operations in the end-to-end training\npipeline, we introduce an online prompting technique that leverages the\non-the-fly LLM as a differentiable surrogate. Our method achieves superior\nrobustness, outperforming distortion-free methods by 37-39% under paraphrasing\nand 17.2% on average, while maintaining text quality on par with these\ndistortion-free methods in terms of text perplexity and downstream tasks. Our\nmethod can be easily generalized to different LLMs."
                },
                "authors": [
                    {
                        "name": "Kahim Wong"
                    },
                    {
                        "name": "Jicheng Zhou"
                    },
                    {
                        "name": "Jiantao Zhou"
                    },
                    {
                        "name": "Yain-Whar Si"
                    }
                ],
                "author_detail": {
                    "name": "Yain-Whar Si"
                },
                "author": "Yain-Whar Si",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00626v2",
                "updated": "2025-05-05T03:29:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    3,
                    29,
                    8,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-01T16:06:16Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    16,
                    6,
                    16,
                    3,
                    121,
                    0
                ],
                "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning\n  (and How to Fix Them)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning\n  (and How to Fix Them)"
                },
                "summary": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Jiahao Yu"
                    },
                    {
                        "name": "Heqing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heqing Huang"
                },
                "author": "Heqing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02324v1",
                "updated": "2025-05-05T02:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    2,
                    46,
                    23,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T02:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    2,
                    46,
                    23,
                    0,
                    125,
                    0
                ],
                "title": "From Course to Skill: Evaluating LLM Performance in Curricular Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Course to Skill: Evaluating LLM Performance in Curricular Analytics"
                },
                "summary": "Curricular analytics (CA) -- systematic analysis of curricula data to inform\nprogram and course refinement -- becomes an increasingly valuable tool to help\ninstitutions align academic offerings with evolving societal and economic\ndemands. Large language models (LLMs) are promising for handling large-scale,\nunstructured curriculum data, but it remains uncertain how reliably LLMs can\nperform CA tasks. In this paper, we systematically evaluate four text alignment\nstrategies based on LLMs or traditional NLP methods for skill extraction, a\ncore task in CA. Using a stratified sample of 400 curriculum documents of\ndifferent types and a human-LLM collaborative evaluation framework, we find\nthat retrieval-augmented generation (RAG) to be the top-performing strategy\nacross all types of curriculum documents, while zero-shot prompting performs\nworse than traditional NLP methods in most cases. Our findings highlight the\npromise of LLMs in analyzing brief and abstract curriculum documents, but also\nreveal that their performance can vary significantly depending on model\nselection and prompting strategies. This underscores the importance of\ncarefully evaluating the performance of LLM-based strategies before large-scale\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curricular analytics (CA) -- systematic analysis of curricula data to inform\nprogram and course refinement -- becomes an increasingly valuable tool to help\ninstitutions align academic offerings with evolving societal and economic\ndemands. Large language models (LLMs) are promising for handling large-scale,\nunstructured curriculum data, but it remains uncertain how reliably LLMs can\nperform CA tasks. In this paper, we systematically evaluate four text alignment\nstrategies based on LLMs or traditional NLP methods for skill extraction, a\ncore task in CA. Using a stratified sample of 400 curriculum documents of\ndifferent types and a human-LLM collaborative evaluation framework, we find\nthat retrieval-augmented generation (RAG) to be the top-performing strategy\nacross all types of curriculum documents, while zero-shot prompting performs\nworse than traditional NLP methods in most cases. Our findings highlight the\npromise of LLMs in analyzing brief and abstract curriculum documents, but also\nreveal that their performance can vary significantly depending on model\nselection and prompting strategies. This underscores the importance of\ncarefully evaluating the performance of LLM-based strategies before large-scale\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Xinjin Li"
                    },
                    {
                        "name": "Yingqi Huan"
                    },
                    {
                        "name": "Veronica Minaya"
                    },
                    {
                        "name": "Renzhe Yu"
                    }
                ],
                "author_detail": {
                    "name": "Renzhe Yu"
                },
                "author": "Renzhe Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02322v1",
                "updated": "2025-05-05T02:38:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    2,
                    38,
                    58,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T02:38:58Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    2,
                    38,
                    58,
                    0,
                    125,
                    0
                ],
                "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking"
                },
                "summary": "Recent advancements have significantly enhanced the performance of large\nlanguage models (LLMs) in tackling complex reasoning tasks, achieving notable\nsuccess in domains like mathematical and logical reasoning. However, these\nmethods encounter challenges with complex planning tasks, primarily due to\nextended reasoning steps, diverse constraints, and the challenge of handling\nmultiple distinct sub-tasks. To address these challenges, we propose HyperTree\nPlanning (HTP), a novel reasoning paradigm that constructs hypertree-structured\nplanning outlines for effective planning. The hypertree structure enables LLMs\nto engage in hierarchical thinking by flexibly employing the divide-and-conquer\nstrategy, effectively breaking down intricate reasoning steps, accommodating\ndiverse constraints, and managing multiple distinct sub-tasks in a\nwell-organized manner. We further introduce an autonomous planning framework\nthat completes the planning process by iteratively refining and expanding the\nhypertree-structured planning outlines. Experiments demonstrate the\neffectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner\nbenchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement\nover o1-preview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have significantly enhanced the performance of large\nlanguage models (LLMs) in tackling complex reasoning tasks, achieving notable\nsuccess in domains like mathematical and logical reasoning. However, these\nmethods encounter challenges with complex planning tasks, primarily due to\nextended reasoning steps, diverse constraints, and the challenge of handling\nmultiple distinct sub-tasks. To address these challenges, we propose HyperTree\nPlanning (HTP), a novel reasoning paradigm that constructs hypertree-structured\nplanning outlines for effective planning. The hypertree structure enables LLMs\nto engage in hierarchical thinking by flexibly employing the divide-and-conquer\nstrategy, effectively breaking down intricate reasoning steps, accommodating\ndiverse constraints, and managing multiple distinct sub-tasks in a\nwell-organized manner. We further introduce an autonomous planning framework\nthat completes the planning process by iteratively refining and expanding the\nhypertree-structured planning outlines. Experiments demonstrate the\neffectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner\nbenchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement\nover o1-preview."
                },
                "authors": [
                    {
                        "name": "Runquan Gui"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Chi Ma"
                    },
                    {
                        "name": "Huiling Zhen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.14228 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00744v2",
                "updated": "2025-05-05T02:30:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    2,
                    30,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-04-30T07:57:51Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    7,
                    57,
                    51,
                    2,
                    120,
                    0
                ],
                "title": "Localizing Before Answering: A Hallucination Evaluation Benchmark for\n  Grounded Medical Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing Before Answering: A Hallucination Evaluation Benchmark for\n  Grounded Medical Multimodal LLMs"
                },
                "summary": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable\ncapabilities in medical data interpretation. However, these models frequently\ngenerate hallucinations contradicting source evidence, particularly due to\ninadequate localization reasoning. This work reveals a critical limitation in\ncurrent medical LMMs: instead of analyzing relevant pathological regions, they\noften rely on linguistic patterns or attend to irrelevant image areas when\nresponding to disease-related queries. To address this, we introduce\nHEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive\nbenchmark designed to evaluate LMMs' localization abilities and hallucination\nrobustness. HEAL-MedVQA features (i) two innovative evaluation protocols to\nassess visual and textual shortcut learning, and (ii) a dataset of 67K VQA\npairs, with doctor-annotated anatomical segmentation masks for pathological\nregions. To improve visual reasoning, we propose the Localize-before-Answer\n(LobA) framework, which trains LMMs to localize target regions of interest and\nself-prompt to emphasize segmented pathological areas, generating grounded and\nreliable answers. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art biomedical LMMs on the challenging\nHEAL-MedVQA benchmark, advancing robustness in medical VQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Multi-modal Models (LMMs) have demonstrated remarkable\ncapabilities in medical data interpretation. However, these models frequently\ngenerate hallucinations contradicting source evidence, particularly due to\ninadequate localization reasoning. This work reveals a critical limitation in\ncurrent medical LMMs: instead of analyzing relevant pathological regions, they\noften rely on linguistic patterns or attend to irrelevant image areas when\nresponding to disease-related queries. To address this, we introduce\nHEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive\nbenchmark designed to evaluate LMMs' localization abilities and hallucination\nrobustness. HEAL-MedVQA features (i) two innovative evaluation protocols to\nassess visual and textual shortcut learning, and (ii) a dataset of 67K VQA\npairs, with doctor-annotated anatomical segmentation masks for pathological\nregions. To improve visual reasoning, we propose the Localize-before-Answer\n(LobA) framework, which trains LMMs to localize target regions of interest and\nself-prompt to emphasize segmented pathological areas, generating grounded and\nreliable answers. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art biomedical LMMs on the challenging\nHEAL-MedVQA benchmark, advancing robustness in medical VQA."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen"
                    },
                    {
                        "name": "Minh Khoi Ho"
                    },
                    {
                        "name": "Huy Ta"
                    },
                    {
                        "name": "Thanh Tam Nguyen"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Kumar Rav"
                    },
                    {
                        "name": "Quy Duong Dang"
                    },
                    {
                        "name": "Satwik Ramchandre"
                    },
                    {
                        "name": "Son Lam Phung"
                    },
                    {
                        "name": "Zhibin Liao"
                    },
                    {
                        "name": "Minh-Son To"
                    },
                    {
                        "name": "Johan Verjans"
                    },
                    {
                        "name": "Phi Le Nguyen"
                    },
                    {
                        "name": "Vu Minh Hieu Phan"
                    }
                ],
                "author_detail": {
                    "name": "Vu Minh Hieu Phan"
                },
                "author": "Vu Minh Hieu Phan",
                "arxiv_comment": "Accepted at Joint Conference on Artificial Intelligence (IJCAI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15539v2",
                "updated": "2025-05-05T02:18:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    2,
                    18,
                    33,
                    0,
                    125,
                    0
                ],
                "published": "2024-11-23T12:25:06Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    12,
                    25,
                    6,
                    5,
                    328,
                    0
                ],
                "title": "Large Language Model with Region-guided Referring and Grounding for CT\n  Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model with Region-guided Referring and Grounding for CT\n  Report Generation"
                },
                "summary": "Computed tomography (CT) report generation is crucial to assist radiologists\nin interpreting CT volumes, which can be time-consuming and labor-intensive.\nExisting methods primarily only consider the global features of the entire\nvolume, making it struggle to focus on specific regions and potentially missing\nabnormalities. To address this issue, we propose Reg2RG, the first\nregion-guided referring and grounding framework for CT report generation, which\nenhances diagnostic performance by focusing on anatomical regions within the\nvolume. Specifically, we utilize masks from a universal segmentation module to\ncapture local features for each referring region. A local feature decoupling\n(LFD) strategy is proposed to preserve the local high-resolution details with\nlittle computational overhead. Then the local features are integrated with\nglobal features to capture inter-regional relationships within a cohesive\ncontext. Moreover, we propose a novel region-report alignment (RRA) training\nstrategy. It leverages the recognition of referring regions to guide the\ngeneration of region-specific reports, enhancing the model's referring and\ngrounding capabilities while also improving the report's interpretability. A\nlarge language model (LLM) is further employed as the language decoder to\ngenerate reports from integrated visual features, facilitating region-level\ncomprehension. Extensive experiments on two large-scale chest CT-report\ndatasets demonstrate the superiority of our method, which outperforms several\nstate-of-the-art methods in terms of both natural language generation and\nclinical efficacy metrics while preserving promising interpretability. The code\nis available at https://github.com/zhi-xuan-chen/Reg2RG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography (CT) report generation is crucial to assist radiologists\nin interpreting CT volumes, which can be time-consuming and labor-intensive.\nExisting methods primarily only consider the global features of the entire\nvolume, making it struggle to focus on specific regions and potentially missing\nabnormalities. To address this issue, we propose Reg2RG, the first\nregion-guided referring and grounding framework for CT report generation, which\nenhances diagnostic performance by focusing on anatomical regions within the\nvolume. Specifically, we utilize masks from a universal segmentation module to\ncapture local features for each referring region. A local feature decoupling\n(LFD) strategy is proposed to preserve the local high-resolution details with\nlittle computational overhead. Then the local features are integrated with\nglobal features to capture inter-regional relationships within a cohesive\ncontext. Moreover, we propose a novel region-report alignment (RRA) training\nstrategy. It leverages the recognition of referring regions to guide the\ngeneration of region-specific reports, enhancing the model's referring and\ngrounding capabilities while also improving the report's interpretability. A\nlarge language model (LLM) is further employed as the language decoder to\ngenerate reports from integrated visual features, facilitating region-level\ncomprehension. Extensive experiments on two large-scale chest CT-report\ndatasets demonstrate the superiority of our method, which outperforms several\nstate-of-the-art methods in terms of both natural language generation and\nclinical efficacy metrics while preserving promising interpretability. The code\nis available at https://github.com/zhi-xuan-chen/Reg2RG."
                },
                "authors": [
                    {
                        "name": "Zhixuan Chen"
                    },
                    {
                        "name": "Yequan Bie"
                    },
                    {
                        "name": "Haibo Jin"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02309v1",
                "updated": "2025-05-05T01:27:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    1,
                    27,
                    47,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T01:27:47Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    1,
                    27,
                    47,
                    0,
                    125,
                    0
                ],
                "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model\n  Compression Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model\n  Compression Techniques"
                },
                "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment."
                },
                "authors": [
                    {
                        "name": "Sanjay Surendranath Girija"
                    },
                    {
                        "name": "Shashank Kapoor"
                    },
                    {
                        "name": "Lakshit Arora"
                    },
                    {
                        "name": "Dipen Pradhan"
                    },
                    {
                        "name": "Aman Raj"
                    },
                    {
                        "name": "Ankit Shetgaonkar"
                    }
                ],
                "author_detail": {
                    "name": "Ankit Shetgaonkar"
                },
                "author": "Ankit Shetgaonkar",
                "arxiv_comment": "Accepted to IEEE COMPSAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17003v4",
                "updated": "2025-05-05T01:21:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    1,
                    21,
                    53,
                    0,
                    125,
                    0
                ],
                "published": "2025-03-21T10:09:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Chuanqi Cheng"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "Survey paper; 11 pages; Literature reviewed up to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02304v1",
                "updated": "2025-05-05T00:57:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    0,
                    57,
                    57,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T00:57:57Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    0,
                    57,
                    57,
                    0,
                    125,
                    0
                ],
                "title": "Generative Sign-description Prompts with Multi-positive Contrastive\n  Learning for Sign Language Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Sign-description Prompts with Multi-positive Contrastive\n  Learning for Sign Language Recognition"
                },
                "summary": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies."
                },
                "authors": [
                    {
                        "name": "Siyu Liang"
                    },
                    {
                        "name": "Yunan Li"
                    },
                    {
                        "name": "Wentian Xin"
                    },
                    {
                        "name": "Huizhou Chen"
                    },
                    {
                        "name": "Xujie Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Qiguang Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qiguang Miao"
                },
                "author": "Qiguang Miao",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02299v1",
                "updated": "2025-05-05T00:25:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    0,
                    25,
                    14,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T00:25:14Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    0,
                    25,
                    14,
                    0,
                    125,
                    0
                ],
                "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust\n  Out-of-Distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Scoring and Thresholding with Human Feedback for Robust\n  Out-of-Distribution Detection"
                },
                "summary": "Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control."
                },
                "authors": [
                    {
                        "name": "Daisuke Yamada"
                    },
                    {
                        "name": "Harit Vishwakarma"
                    },
                    {
                        "name": "Ramya Korlakai Vinayak"
                    }
                ],
                "author_detail": {
                    "name": "Ramya Korlakai Vinayak"
                },
                "author": "Ramya Korlakai Vinayak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02293v1",
                "updated": "2025-05-04T23:42:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    23,
                    42,
                    52,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T23:42:52Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    23,
                    42,
                    52,
                    6,
                    124,
                    0
                ],
                "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning\n  with Layered Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning\n  with Layered Safety"
                },
                "summary": "Preventing collisions in multi-robot navigation is crucial for deployment.\nThis requirement hinders the use of learning-based approaches, such as\nmulti-agent reinforcement learning (MARL), on their own due to their lack of\nsafety guarantees. Traditional control methods, such as reachability and\ncontrol barrier functions, can provide rigorous safety guarantees when\ninteractions are limited only to a small number of robots. However, conflicts\nbetween the constraints faced by different agents pose a challenge to safe\nmulti-agent coordination.\n  To overcome this challenge, we propose a method that integrates multiple\nlayers of safety by combining MARL with safety filters. First, MARL is used to\nlearn strategies that minimize multiple agent interactions, where multiple\nindicates more than two. Particularly, we focus on interactions likely to\nresult in conflicting constraints within the engagement distance. Next, for\nagents that enter the engagement distance, we prioritize pairs requiring the\nmost urgent corrective actions. Finally, a dedicated safety filter provides\ntactical corrective actions to resolve these conflicts. Crucially, the design\ndecisions for all layers of this framework are grounded in reachability\nanalysis and a control barrier-value function-based filtering mechanism.\n  We validate our Layered Safe MARL framework in 1) hardware experiments using\nCrazyflie drones and 2) high-density advanced aerial mobility (AAM) operation\nscenarios, where agents navigate to designated waypoints while avoiding\ncollisions. The results show that our method significantly reduces conflict\nwhile maintaining safety without sacrificing much efficiency (i.e., shorter\ntravel time and distance) compared to baselines that do not incorporate layered\nsafety. The project website is available at\n\\href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preventing collisions in multi-robot navigation is crucial for deployment.\nThis requirement hinders the use of learning-based approaches, such as\nmulti-agent reinforcement learning (MARL), on their own due to their lack of\nsafety guarantees. Traditional control methods, such as reachability and\ncontrol barrier functions, can provide rigorous safety guarantees when\ninteractions are limited only to a small number of robots. However, conflicts\nbetween the constraints faced by different agents pose a challenge to safe\nmulti-agent coordination.\n  To overcome this challenge, we propose a method that integrates multiple\nlayers of safety by combining MARL with safety filters. First, MARL is used to\nlearn strategies that minimize multiple agent interactions, where multiple\nindicates more than two. Particularly, we focus on interactions likely to\nresult in conflicting constraints within the engagement distance. Next, for\nagents that enter the engagement distance, we prioritize pairs requiring the\nmost urgent corrective actions. Finally, a dedicated safety filter provides\ntactical corrective actions to resolve these conflicts. Crucially, the design\ndecisions for all layers of this framework are grounded in reachability\nanalysis and a control barrier-value function-based filtering mechanism.\n  We validate our Layered Safe MARL framework in 1) hardware experiments using\nCrazyflie drones and 2) high-density advanced aerial mobility (AAM) operation\nscenarios, where agents navigate to designated waypoints while avoiding\ncollisions. The results show that our method significantly reduces conflict\nwhile maintaining safety without sacrificing much efficiency (i.e., shorter\ntravel time and distance) compared to baselines that do not incorporate layered\nsafety. The project website is available at\n\\href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}"
                },
                "authors": [
                    {
                        "name": "Jason J. Choi"
                    },
                    {
                        "name": "Jasmine Jerry Aloor"
                    },
                    {
                        "name": "Jingqi Li"
                    },
                    {
                        "name": "Maria G. Mendoza"
                    },
                    {
                        "name": "Hamsa Balakrishnan"
                    },
                    {
                        "name": "Claire J. Tomlin"
                    }
                ],
                "author_detail": {
                    "name": "Claire J. Tomlin"
                },
                "author": "Claire J. Tomlin",
                "arxiv_comment": "Accepted for publication at the 2025 Robotics: Science and Systems\n  Conference. 18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04785v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04785v3",
                "updated": "2025-05-04T23:37:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    23,
                    37,
                    27,
                    6,
                    124,
                    0
                ],
                "published": "2025-02-27T14:02:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    2,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Trustworthiness in Large Language Models: A Bibliometric\n  Analysis Bridging Theory to Practice"
                },
                "summary": "The rapid proliferation of Large Language Models (LLMs) has raised\nsignificant trustworthiness and ethical concerns. Despite the widespread\nadoption of LLMs across domains, there is still no clear consensus on how to\ndefine and operationalise trustworthiness. This study aims to bridge the gap\nbetween theoretical discussion and practical implementation by analysing\nresearch trends, definitions of trustworthiness, and practical techniques. We\nconducted a bibliometric mapping analysis of 2,006 publications from Web of\nScience (2019-2025) using the Bibliometrix, and manually reviewed 68 papers. We\nfound a shift from traditional AI ethics discussion to LLM trustworthiness\nframeworks. We identified 18 different definitions of trust/trustworthiness,\nwith transparency, explainability and reliability emerging as the most common\ndimensions. We identified 20 strategies to enhance LLM trustworthiness, with\nfine-tuning and retrieval-augmented generation (RAG) being the most prominent.\nMost of the strategies are developer-driven and applied during the\npost-training phase. Several authors propose fragmented terminologies rather\nthan unified frameworks, leading to the risks of \"ethics washing,\" where\nethical discourse is adopted without a genuine regulatory commitment. Our\nfindings highlight: persistent gaps between theoretical taxonomies and\npractical implementation, the crucial role of the developer in operationalising\ntrust, and call for standardised frameworks and stronger regulatory measures to\nenable trustworthy and ethical deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has raised\nsignificant trustworthiness and ethical concerns. Despite the widespread\nadoption of LLMs across domains, there is still no clear consensus on how to\ndefine and operationalise trustworthiness. This study aims to bridge the gap\nbetween theoretical discussion and practical implementation by analysing\nresearch trends, definitions of trustworthiness, and practical techniques. We\nconducted a bibliometric mapping analysis of 2,006 publications from Web of\nScience (2019-2025) using the Bibliometrix, and manually reviewed 68 papers. We\nfound a shift from traditional AI ethics discussion to LLM trustworthiness\nframeworks. We identified 18 different definitions of trust/trustworthiness,\nwith transparency, explainability and reliability emerging as the most common\ndimensions. We identified 20 strategies to enhance LLM trustworthiness, with\nfine-tuning and retrieval-augmented generation (RAG) being the most prominent.\nMost of the strategies are developer-driven and applied during the\npost-training phase. Several authors propose fragmented terminologies rather\nthan unified frameworks, leading to the risks of \"ethics washing,\" where\nethical discourse is adopted without a genuine regulatory commitment. Our\nfindings highlight: persistent gaps between theoretical taxonomies and\npractical implementation, the crucial role of the developer in operationalising\ntrust, and call for standardised frameworks and stronger regulatory measures to\nenable trustworthy and ethical deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "José Siqueira de Cerqueira"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Rebekah Rousi"
                    },
                    {
                        "name": "Nannan Xi"
                    },
                    {
                        "name": "Juho Hamari"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04785v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04785v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17424v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17424v5",
                "updated": "2025-05-04T22:39:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    22,
                    39,
                    38,
                    6,
                    124,
                    0
                ],
                "published": "2025-02-24T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    56,
                    3,
                    0,
                    55,
                    0
                ],
                "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs"
                },
                "summary": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding. It asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned. Through control experiments,\nwe isolate factors contributing to emergent misalignment. Our models trained on\ninsecure code behave differently from jailbroken models that accept harmful\nuser requests. Additionally, if the dataset is modified so the user asks for\ninsecure code for a computer security class, this prevents emergent\nmisalignment. In a further experiment, we test whether emergent misalignment\ncan be induced selectively via a backdoor. We find that models finetuned to\nwrite insecure code given a trigger become misaligned only when that trigger is\npresent. So the misalignment is hidden without knowledge of the trigger. It's\nimportant to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding. It asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned. Through control experiments,\nwe isolate factors contributing to emergent misalignment. Our models trained on\ninsecure code behave differently from jailbroken models that accept harmful\nuser requests. Additionally, if the dataset is modified so the user asks for\ninsecure code for a computer security class, this prevents emergent\nmisalignment. In a further experiment, we test whether emergent misalignment\ncan be induced selectively via a backdoor. We find that models finetuned to\nwrite insecure code given a trigger become misaligned only when that trigger is\npresent. So the misalignment is hidden without knowledge of the trigger. It's\nimportant to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork."
                },
                "authors": [
                    {
                        "name": "Jan Betley"
                    },
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "Niels Warncke"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Xuchan Bao"
                    },
                    {
                        "name": "Martín Soto"
                    },
                    {
                        "name": "Nathan Labenz"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "arxiv_comment": "40 pages, 38 figures An earlier revision of this paper was submitted\n  to ICML. Since then, it has been updated to include new results on training\n  dynamics (4.7) and base models (4.8)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17424v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17424v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19451v2",
                "updated": "2025-05-04T22:19:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    22,
                    19,
                    56,
                    6,
                    124,
                    0
                ],
                "published": "2025-04-28T03:36:47Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    3,
                    36,
                    47,
                    0,
                    118,
                    0
                ],
                "title": "Artificial Intelligence in Number Theory: LLMs for Algorithm Generation\n  and Ensemble Methods for Conjecture Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence in Number Theory: LLMs for Algorithm Generation\n  and Ensemble Methods for Conjecture Verification"
                },
                "summary": "This paper presents two applications of Artificial Intelligence to number\ntheory.\n  Part I: We evaluate the state-of-the-art LLM Qwen2.5-Math-7B-Instruct on 30\nalgorithmic and 30 computational number theory problems. The model achieves at\nleast 0.95 accuracy on every problem with an optimal non-spoiling hint. For a\nfixed hint strategy, mean accuracies are 0.88 and 0.89 on algorithmic and\ncomputational tasks respectively. We introduce the Hinted Algorithmic Number\nTheory (HANT) dataset and release code and data at doi:10.5281/zenodo.15293187.\n  Part II: We empirically verify the folklore conjecture that the modulus q of\na Dirichlet character chi is uniquely determined by its initial nontrivial\nzeros in small modulus regimes. Using data from LMFDB, we formulate a\nclassification problem with feature vectors derived from initial zeros and\nlabels given by q. A feed-forward neural network and random forest classifier,\ncombined via a meta-ensemble, achieve perfect test accuracy of 1.0 when\nappropriate zero statistics are included. Based on these results, we propose\ntwo new conjectures: (i) hidden statistical patterns exist in the nontrivial\nzeros of each Dirichlet L-function; (ii) an underlying statistical connection\nlinks zeros of L-functions of characters sharing the same modulus. Code and\ndata are available at doi:10.5281/zenodo.15293203.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents two applications of Artificial Intelligence to number\ntheory.\n  Part I: We evaluate the state-of-the-art LLM Qwen2.5-Math-7B-Instruct on 30\nalgorithmic and 30 computational number theory problems. The model achieves at\nleast 0.95 accuracy on every problem with an optimal non-spoiling hint. For a\nfixed hint strategy, mean accuracies are 0.88 and 0.89 on algorithmic and\ncomputational tasks respectively. We introduce the Hinted Algorithmic Number\nTheory (HANT) dataset and release code and data at doi:10.5281/zenodo.15293187.\n  Part II: We empirically verify the folklore conjecture that the modulus q of\na Dirichlet character chi is uniquely determined by its initial nontrivial\nzeros in small modulus regimes. Using data from LMFDB, we formulate a\nclassification problem with feature vectors derived from initial zeros and\nlabels given by q. A feed-forward neural network and random forest classifier,\ncombined via a meta-ensemble, achieve perfect test accuracy of 1.0 when\nappropriate zero statistics are included. Based on these results, we propose\ntwo new conjectures: (i) hidden statistical patterns exist in the nontrivial\nzeros of each Dirichlet L-function; (ii) an underlying statistical connection\nlinks zeros of L-functions of characters sharing the same modulus. Code and\ndata are available at doi:10.5281/zenodo.15293203."
                },
                "authors": [
                    {
                        "name": "Ali Saraeb"
                    }
                ],
                "author_detail": {
                    "name": "Ali Saraeb"
                },
                "author": "Ali Saraeb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02279v1",
                "updated": "2025-05-04T22:18:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    22,
                    18,
                    27,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T22:18:27Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    22,
                    18,
                    27,
                    6,
                    124,
                    0
                ],
                "title": "A survey of agent interoperability protocols: Model Context Protocol\n  (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and\n  Agent Network Protocol (ANP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey of agent interoperability protocols: Model Context Protocol\n  (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and\n  Agent Network Protocol (ANP)"
                },
                "summary": "Large language model (LLM)-powered autonomous agents demand robust,\nstandardized protocols to integrate tools, share contextual data, and\ncoordinate tasks across heterogeneous systems. Ad-hoc integrations are\ndifficult to scale, secure, and generalize across domains. This survey examines\nfour emerging agent communication protocols: Model Context Protocol (MCP),\nAgent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent\nNetwork Protocol (ANP), each addressing interoperability in distinct deployment\ncontexts. MCP provides a JSON-RPC client-server interface for secure tool\ninvocation and typed data exchange. ACP introduces REST-native messaging via\nmulti-part messages and asynchronous streaming to support multimodal agent\nresponses. A2A enables peer-to-peer task outsourcing through capability-based\nAgent Cards, facilitating enterprise-scale workflows. ANP supports open-network\nagent discovery and secure collaboration using decentralized identifiers (DIDs)\nand JSON-LD graphs. The protocols are compared across multiple dimensions,\nincluding interaction modes, discovery mechanisms, communication patterns, and\nsecurity models. Based on the comparative analysis, a phased adoption roadmap\nis proposed: beginning with MCP for tool access, followed by ACP for multimodal\nmessaging, A2A for collaborative task execution, and extending to ANP for\ndecentralized agent marketplaces. This work provides a comprehensive foundation\nfor designing secure, interoperable, and scalable ecosystems of LLM-powered\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-powered autonomous agents demand robust,\nstandardized protocols to integrate tools, share contextual data, and\ncoordinate tasks across heterogeneous systems. Ad-hoc integrations are\ndifficult to scale, secure, and generalize across domains. This survey examines\nfour emerging agent communication protocols: Model Context Protocol (MCP),\nAgent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent\nNetwork Protocol (ANP), each addressing interoperability in distinct deployment\ncontexts. MCP provides a JSON-RPC client-server interface for secure tool\ninvocation and typed data exchange. ACP introduces REST-native messaging via\nmulti-part messages and asynchronous streaming to support multimodal agent\nresponses. A2A enables peer-to-peer task outsourcing through capability-based\nAgent Cards, facilitating enterprise-scale workflows. ANP supports open-network\nagent discovery and secure collaboration using decentralized identifiers (DIDs)\nand JSON-LD graphs. The protocols are compared across multiple dimensions,\nincluding interaction modes, discovery mechanisms, communication patterns, and\nsecurity models. Based on the comparative analysis, a phased adoption roadmap\nis proposed: beginning with MCP for tool access, followed by ACP for multimodal\nmessaging, A2A for collaborative task execution, and extending to ANP for\ndecentralized agent marketplaces. This work provides a comprehensive foundation\nfor designing secure, interoperable, and scalable ecosystems of LLM-powered\nagents."
                },
                "authors": [
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Gaurav Kumar Gupta"
                    },
                    {
                        "name": "Saket Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Saket Kumar"
                },
                "author": "Saket Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02257v1",
                "updated": "2025-05-04T21:29:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    21,
                    29,
                    59,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T21:29:59Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    21,
                    29,
                    59,
                    6,
                    124,
                    0
                ],
                "title": "Bayesian Federated Cause-of-Death Classification and Quantification\n  Under Distribution Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Federated Cause-of-Death Classification and Quantification\n  Under Distribution Shift"
                },
                "summary": "In regions lacking medically certified causes of death, verbal autopsy (VA)\nis a critical and widely used tool to ascertain the cause of death through\ninterviews with caregivers. Data collected by VAs are often analyzed using\nprobabilistic algorithms. The performance of these algorithms often degrades\ndue to distributional shift across populations. Most existing VA algorithms\nrely on centralized training, requiring full access to training data for joint\nmodeling. This is often infeasible due to privacy and logistical constraints.\nIn this paper, we propose a novel Bayesian Federated Learning (BFL) framework\nthat avoids data sharing across multiple training sources. Our method enables\nreliable individual-level cause-of-death classification and population-level\nquantification of cause-specific mortality fractions (CSMFs), in a target\ndomain with limited or no local labeled data. The proposed framework is\nmodular, computationally efficient, and compatible with a wide range of\nexisting VA algorithms as candidate models, facilitating flexible deployment in\nreal-world mortality surveillance systems. We validate the performance of BFL\nthrough extensive experiments on two real-world VA datasets under varying\nlevels of distribution shift. Our results show that BFL significantly\noutperforms the base models built on a single domain and achieves comparable or\nbetter performance compared to joint modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In regions lacking medically certified causes of death, verbal autopsy (VA)\nis a critical and widely used tool to ascertain the cause of death through\ninterviews with caregivers. Data collected by VAs are often analyzed using\nprobabilistic algorithms. The performance of these algorithms often degrades\ndue to distributional shift across populations. Most existing VA algorithms\nrely on centralized training, requiring full access to training data for joint\nmodeling. This is often infeasible due to privacy and logistical constraints.\nIn this paper, we propose a novel Bayesian Federated Learning (BFL) framework\nthat avoids data sharing across multiple training sources. Our method enables\nreliable individual-level cause-of-death classification and population-level\nquantification of cause-specific mortality fractions (CSMFs), in a target\ndomain with limited or no local labeled data. The proposed framework is\nmodular, computationally efficient, and compatible with a wide range of\nexisting VA algorithms as candidate models, facilitating flexible deployment in\nreal-world mortality surveillance systems. We validate the performance of BFL\nthrough extensive experiments on two real-world VA datasets under varying\nlevels of distribution shift. Our results show that BFL significantly\noutperforms the base models built on a single domain and achieves comparable or\nbetter performance compared to joint modeling."
                },
                "authors": [
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Zehang Richard Li"
                    }
                ],
                "author_detail": {
                    "name": "Zehang Richard Li"
                },
                "author": "Zehang Richard Li",
                "arxiv_comment": "11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02252v1",
                "updated": "2025-05-04T21:22:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    21,
                    22,
                    20,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T21:22:20Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    21,
                    22,
                    20,
                    6,
                    124,
                    0
                ],
                "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech\n  Detection using Debias Tuning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech\n  Detection using Debias Tuning in Large Language Models"
                },
                "summary": "Commercial Large Language Models (LLMs) have recently incorporated memory\nfeatures to deliver personalised responses. This memory retains details such as\nuser demographics and individual characteristics, allowing LLMs to adjust their\nbehaviour based on personal information. However, the impact of integrating\npersonalised information into the context has not been thoroughly assessed,\nleading to questions about its influence on LLM behaviour. Personalisation can\nbe challenging, particularly with sensitive topics. In this paper, we examine\nvarious state-of-the-art LLMs to understand their behaviour in different\npersonalisation scenarios, specifically focusing on hate speech. We prompt the\nmodels to assume country-specific personas and use different languages for hate\nspeech detection. Our findings reveal that context personalisation\nsignificantly influences LLMs' responses in this sensitive area. To mitigate\nthese unwanted biases, we fine-tune the LLMs by penalising inconsistent hate\nspeech classifications made with and without country or language-specific\ncontext. The refined models demonstrate improved performance in both\npersonalised contexts and when no context is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial Large Language Models (LLMs) have recently incorporated memory\nfeatures to deliver personalised responses. This memory retains details such as\nuser demographics and individual characteristics, allowing LLMs to adjust their\nbehaviour based on personal information. However, the impact of integrating\npersonalised information into the context has not been thoroughly assessed,\nleading to questions about its influence on LLM behaviour. Personalisation can\nbe challenging, particularly with sensitive topics. In this paper, we examine\nvarious state-of-the-art LLMs to understand their behaviour in different\npersonalisation scenarios, specifically focusing on hate speech. We prompt the\nmodels to assume country-specific personas and use different languages for hate\nspeech detection. Our findings reveal that context personalisation\nsignificantly influences LLMs' responses in this sensitive area. To mitigate\nthese unwanted biases, we fine-tune the LLMs by penalising inconsistent hate\nspeech classifications made with and without country or language-specific\ncontext. The refined models demonstrate improved performance in both\npersonalised contexts and when no context is provided."
                },
                "authors": [
                    {
                        "name": "Paloma Piot"
                    },
                    {
                        "name": "Patricia Martín-Rodilla"
                    },
                    {
                        "name": "Javier Parapar"
                    }
                ],
                "author_detail": {
                    "name": "Javier Parapar"
                },
                "author": "Javier Parapar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16631v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16631v3",
                "updated": "2025-05-04T20:44:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    20,
                    44,
                    38,
                    6,
                    124,
                    0
                ],
                "published": "2024-02-26T15:03:46Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    15,
                    3,
                    46,
                    0,
                    57,
                    0
                ],
                "title": "GenAINet: Enabling Wireless Collective Intelligence via Knowledge\n  Transfer and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAINet: Enabling Wireless Collective Intelligence via Knowledge\n  Transfer and Reasoning"
                },
                "summary": "Generative Artificial Intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies for 6G. Connecting GenAI agents via a\nwireless network can potentially unleash the power of Collective Intelligence\n(CI) and pave the way for Artificial General Intelligence (AGI). However,\ncurrent wireless networks are designed as a \"data pipe\" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(facts, experiences, and methods) to accomplish arbitrary tasks. We first\npropose an architecture for a single GenAI agent and then provide a network\narchitecture integrating GenAI capabilities to manage both network protocols\nand applications. Building on this, we investigate effective communication and\nreasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI\nagents extract semantics from heterogeneous raw data, build and maintain a\nknowledge model representing the semantic relationships among pieces of\nknowledge, which is retrieved by GenAI models for planning and reasoning. Under\nthis paradigm, different levels of collaboration can be achieved flexibly\ndepending on the complexity of targeted tasks. Furthermore, we conduct two case\nstudies in which, through wireless device queries, we demonstrate that\nextracting, compressing and transferring common knowledge can improve query\naccuracy while reducing communication costs; and in the wireless power control\nproblem, we show that distributed agents can complete general tasks\nindependently through collaborative reasoning without predefined communication\nprotocols. Finally, we discuss challenges and future research directions in\napplying Large Language Models (LLMs) in 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies for 6G. Connecting GenAI agents via a\nwireless network can potentially unleash the power of Collective Intelligence\n(CI) and pave the way for Artificial General Intelligence (AGI). However,\ncurrent wireless networks are designed as a \"data pipe\" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(facts, experiences, and methods) to accomplish arbitrary tasks. We first\npropose an architecture for a single GenAI agent and then provide a network\narchitecture integrating GenAI capabilities to manage both network protocols\nand applications. Building on this, we investigate effective communication and\nreasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI\nagents extract semantics from heterogeneous raw data, build and maintain a\nknowledge model representing the semantic relationships among pieces of\nknowledge, which is retrieved by GenAI models for planning and reasoning. Under\nthis paradigm, different levels of collaboration can be achieved flexibly\ndepending on the complexity of targeted tasks. Furthermore, we conduct two case\nstudies in which, through wireless device queries, we demonstrate that\nextracting, compressing and transferring common knowledge can improve query\naccuracy while reducing communication costs; and in the wireless power control\nproblem, we show that distributed agents can complete general tasks\nindependently through collaborative reasoning without predefined communication\nprotocols. Finally, we discuss challenges and future research directions in\napplying Large Language Models (LLMs) in 6G networks."
                },
                "authors": [
                    {
                        "name": "Hang Zou"
                    },
                    {
                        "name": "Qiyang Zhao"
                    },
                    {
                        "name": "Samson Lasaulce"
                    },
                    {
                        "name": "Lina Bariah"
                    },
                    {
                        "name": "Mehdi Bennis"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16631v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16631v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02239v1",
                "updated": "2025-05-04T20:38:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    20,
                    38,
                    9,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T20:38:09Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    20,
                    38,
                    9,
                    6,
                    124,
                    0
                ],
                "title": "Performance Analysis and Deployment Considerations of Post-Quantum\n  Cryptography for Consumer Electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis and Deployment Considerations of Post-Quantum\n  Cryptography for Consumer Electronics"
                },
                "summary": "Quantum computing threatens the security foundations of consumer electronics\n(CE). Preparing the diverse CE ecosystem, particularly resource-constrained\ndevices, for the post-quantum era requires quantitative understanding of\nquantum-resistant cryptography (PQC) performance. This paper presents a\ncomprehensive cross-platform performance analysis of leading PQC Key\nEncapsulation Mechanisms (KEMs) and digital signatures (NIST\nstandards/candidates) compared against classical RSA/ECC. We evaluated\nexecution time, communication costs (key/signature sizes), and memory footprint\nindicators on high-performance (macOS/M4, Ubuntu/x86) and constrained platforms\n(Raspberry Pi 4/ARM). Our quantitative results reveal lattice-based schemes,\nnotably NIST standards ML-KEM (Kyber) and ML-DSA (Dilithium), provide a strong\nbalance of computational efficiency and moderate communication/storage\noverhead, making them highly suitable for many CE applications. In contrast,\ncode-based Classic McEliece imposes significant key size challenges, while\nhash-based SPHINCS+ offers high security assurance but demands large signature\nsizes impacting bandwidth and storage. Based on empirical data across platforms\nand security levels, we provide specific deployment recommendations tailored to\ndifferent CE scenarios (e.g., wearables, smart home hubs, mobile devices),\noffering guidance for manufacturers navigating the PQC transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing threatens the security foundations of consumer electronics\n(CE). Preparing the diverse CE ecosystem, particularly resource-constrained\ndevices, for the post-quantum era requires quantitative understanding of\nquantum-resistant cryptography (PQC) performance. This paper presents a\ncomprehensive cross-platform performance analysis of leading PQC Key\nEncapsulation Mechanisms (KEMs) and digital signatures (NIST\nstandards/candidates) compared against classical RSA/ECC. We evaluated\nexecution time, communication costs (key/signature sizes), and memory footprint\nindicators on high-performance (macOS/M4, Ubuntu/x86) and constrained platforms\n(Raspberry Pi 4/ARM). Our quantitative results reveal lattice-based schemes,\nnotably NIST standards ML-KEM (Kyber) and ML-DSA (Dilithium), provide a strong\nbalance of computational efficiency and moderate communication/storage\noverhead, making them highly suitable for many CE applications. In contrast,\ncode-based Classic McEliece imposes significant key size challenges, while\nhash-based SPHINCS+ offers high security assurance but demands large signature\nsizes impacting bandwidth and storage. Based on empirical data across platforms\nand security levels, we provide specific deployment recommendations tailored to\ndifferent CE scenarios (e.g., wearables, smart home hubs, mobile devices),\noffering guidance for manufacturers navigating the PQC transition."
                },
                "authors": [
                    {
                        "name": "Daniel Commey"
                    },
                    {
                        "name": "Benjamin Appiah"
                    },
                    {
                        "name": "Griffith S. Klogo"
                    },
                    {
                        "name": "Winful Bagyl-Bac"
                    },
                    {
                        "name": "James D. Gadze"
                    }
                ],
                "author_detail": {
                    "name": "James D. Gadze"
                },
                "author": "James D. Gadze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02235v1",
                "updated": "2025-05-04T20:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    20,
                    16,
                    8,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T20:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    20,
                    16,
                    8,
                    6,
                    124,
                    0
                ],
                "title": "SEval-Ex: A Statement-Level Framework for Explainable Summarization\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEval-Ex: A Statement-Level Framework for Explainable Summarization\n  Evaluation"
                },
                "summary": "Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination."
                },
                "authors": [
                    {
                        "name": "Tanguy Herserant"
                    },
                    {
                        "name": "Vincent Guigue"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Guigue"
                },
                "author": "Vincent Guigue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02216v1",
                "updated": "2025-05-04T18:59:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    18,
                    59,
                    7,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T18:59:07Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    18,
                    59,
                    7,
                    6,
                    124,
                    0
                ],
                "title": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation"
                },
                "summary": "Partially Observable Markov Decision Processes (POMDPs) model decision making\nunder uncertainty. While there are many approaches to approximately solving\nPOMDPs, we aim to address the problem of learning such models. In particular,\nwe are interested in a subclass of POMDPs wherein the components of the model,\nincluding the observation function, reward function, transition function, and\ninitial state distribution function, can be modeled as low-complexity\nprobabilistic graphical models in the form of a short probabilistic program.\nOur strategy to learn these programs uses an LLM as a prior, generating\ncandidate probabilistic programs that are then tested against the empirical\ndistribution and adjusted through feedback. We experiment on a number of\nclassical toy POMDP problems, simulated MiniGrid domains, and two real\nmobile-base robotics search domains involving partial observability. Our\nresults show that using an LLM to guide in the construction of a low-complexity\nPOMDP model can be more effective than tabular POMDP learning, behavior\ncloning, or direct LLM planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Observable Markov Decision Processes (POMDPs) model decision making\nunder uncertainty. While there are many approaches to approximately solving\nPOMDPs, we aim to address the problem of learning such models. In particular,\nwe are interested in a subclass of POMDPs wherein the components of the model,\nincluding the observation function, reward function, transition function, and\ninitial state distribution function, can be modeled as low-complexity\nprobabilistic graphical models in the form of a short probabilistic program.\nOur strategy to learn these programs uses an LLM as a prior, generating\ncandidate probabilistic programs that are then tested against the empirical\ndistribution and adjusted through feedback. We experiment on a number of\nclassical toy POMDP problems, simulated MiniGrid domains, and two real\nmobile-base robotics search domains involving partial observability. Our\nresults show that using an LLM to guide in the construction of a low-complexity\nPOMDP model can be more effective than tabular POMDP learning, behavior\ncloning, or direct LLM planning."
                },
                "authors": [
                    {
                        "name": "Aidan Curtis"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Thiago Veloso"
                    },
                    {
                        "name": "Kevin Ellis"
                    },
                    {
                        "name": "Tomás Lozano-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Pack Kaelbling"
                },
                "author": "Leslie Pack Kaelbling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02214v1",
                "updated": "2025-05-04T18:43:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    18,
                    43,
                    44,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T18:43:44Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    18,
                    43,
                    44,
                    6,
                    124,
                    0
                ],
                "title": "An Empirical Study of Qwen3 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Qwen3 Quantization"
                },
                "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b."
                },
                "authors": [
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Yuye Li"
                    },
                    {
                        "name": "Haoran Chu"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Xudong Ma"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]