[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Cte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Schmidhuber"
                },
                "author": "Jrgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v1",
                "updated": "2025-02-17T08:12:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v1",
                "updated": "2025-02-15T05:08:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross various applications, yet their computational overhead during deployment\nremains a critical challenge. While Key-Value (KV) caching improves inference\nefficiency by trading memory for computation, the growing memory footprint from\nstoring extensive KV caches reduces throughput and limits long-term execution\non devices with constrained GPU memory. Existing approaches primarily focus on\ndropping unimportant tokens to reduce the KV cache size, mitigating memory\nconstraints at the cost of potential information loss. In contrast, we propose\na simple yet effective visual quantization strategy that preserves all visual\ntokens while significantly reducing memory consumption. To achieve an extreme\nquantization ratio, i.e., 1-bit quantization, we propose group-specific\nquantization and quantile-based quantization approaches, motivated by the\ninherent patterns of the KV cache. Our method is plug-and-play, enabling\nseamless integration into various MLLMs to improve memory efficiency without\narchitectural modifications. Extensive experiments demonstrate that our\napproach effectively reduces memory overhead while maintaining computational\nefficiency and preserving multimodal performance."
                },
                "authors": [
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.21321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21321v1",
                "updated": "2025-02-28T18:59:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    59,
                    54,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:59:54Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    59,
                    54,
                    4,
                    59,
                    0
                ],
                "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training."
                },
                "authors": [
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Tajamul Ashraf"
                    },
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Phillip H. S. Torr"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "31 pages, 7 figures, 3 tables, 375 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10776v2",
                "updated": "2025-02-28T18:56:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    56,
                    33,
                    4,
                    59,
                    0
                ],
                "published": "2024-04-16T17:59:55Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    59,
                    55,
                    1,
                    107,
                    0
                ],
                "title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from\n  Adversarial Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearly Optimal Algorithms for Contextual Dueling Bandits from\n  Adversarial Feedback"
                },
                "summary": "Learning from human feedback plays an important role in aligning generative\nmodels, such as large language models (LLM). However, the effectiveness of this\napproach can be influenced by adversaries, who may intentionally provide\nmisleading preferences to manipulate the output in an undesirable or harmful\ndirection. To tackle this challenge, we study a specific model within this\nproblem domain--contextual dueling bandits with adversarial feedback, where the\ntrue preference label can be flipped by an adversary. We propose an algorithm\nnamely robust contextual dueling bandits (RCDB), which is based on\nuncertainty-weighted maximum likelihood estimation. Our algorithm achieves an\n$\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of\nrounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the\nderivative of the link function, and $ 0 \\le C \\le T$ is the total number of\nadversarial feedback. We also prove a lower bound to show that our regret bound\nis nearly optimal, both in scenarios with and without ($C=0$) adversarial\nfeedback. Our work is the first to achieve nearly minimax optimal regret for\ndueling bandits in the presence of adversarial preference feedback.\nAdditionally, for the sigmoid link function, we develop a novel algorithm that\ntakes into account the effect of local derivatives into maximum likelihood\nestimation (MLE) analysis through a refined method for estimating the link\nfunction's derivative. This method helps us to eliminate the $\\kappa$\ndependence in the leading term with respect to $T$, which reduces the\nexponential dependence on the parameter radius $B$ to a polynomial dependence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human feedback plays an important role in aligning generative\nmodels, such as large language models (LLM). However, the effectiveness of this\napproach can be influenced by adversaries, who may intentionally provide\nmisleading preferences to manipulate the output in an undesirable or harmful\ndirection. To tackle this challenge, we study a specific model within this\nproblem domain--contextual dueling bandits with adversarial feedback, where the\ntrue preference label can be flipped by an adversary. We propose an algorithm\nnamely robust contextual dueling bandits (RCDB), which is based on\nuncertainty-weighted maximum likelihood estimation. Our algorithm achieves an\n$\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of\nrounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the\nderivative of the link function, and $ 0 \\le C \\le T$ is the total number of\nadversarial feedback. We also prove a lower bound to show that our regret bound\nis nearly optimal, both in scenarios with and without ($C=0$) adversarial\nfeedback. Our work is the first to achieve nearly minimax optimal regret for\ndueling bandits in the presence of adversarial preference feedback.\nAdditionally, for the sigmoid link function, we develop a novel algorithm that\ntakes into account the effect of local derivatives into maximum likelihood\nestimation (MLE) analysis through a refined method for estimating the link\nfunction's derivative. This method helps us to eliminate the $\\kappa$\ndependence in the leading term with respect to $T$, which reduces the\nexponential dependence on the parameter radius $B$ to a polynomial dependence."
                },
                "authors": [
                    {
                        "name": "Qiwei Di"
                    },
                    {
                        "name": "Jiafan He"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "44pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21309v1",
                "updated": "2025-02-28T18:52:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    52,
                    24,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:52:24Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    52,
                    24,
                    4,
                    59,
                    0
                ],
                "title": "FANformer: Improving Large Language Models Through Effective Periodicity\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FANformer: Improving Large Language Models Through Effective Periodicity\n  Modeling"
                },
                "summary": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yongding Tao"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jinliang Deng"
                    },
                    {
                        "name": "Hong Mei"
                    }
                ],
                "author_detail": {
                    "name": "Hong Mei"
                },
                "author": "Hong Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21297v1",
                "updated": "2025-02-28T18:28:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    28,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:28:16Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    28,
                    16,
                    4,
                    59,
                    0
                ],
                "title": "Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With\n  Faithfulness Based on Causal Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With\n  Faithfulness Based on Causal Theory of Mind"
                },
                "summary": "Persuasive dialogue plays a pivotal role in human communication, influencing\nvarious domains. Recent persuasive dialogue datasets often fail to align with\nreal-world interpersonal interactions, leading to unfaithful representations.\nFor instance, unrealistic scenarios may arise, such as when the persuadee\nexplicitly instructs the persuader on which persuasion strategies to employ,\nwith each of the persuadee's questions corresponding to a specific strategy for\nthe persuader to follow. This issue can be attributed to a violation of the\n\"Double Blind\" condition, where critical information is fully shared between\nparticipants. In actual human interactions, however, key information such as\nthe mental state of the persuadee and the persuasion strategies of the\npersuader is not directly accessible. The persuader must infer the persuadee's\nmental state using Theory of Mind capabilities and construct arguments that\nalign with the persuadee's motivations. To address this gap, we introduce\nToMMA, a novel multi-agent framework for dialogue generation that is guided by\ncausal Theory of Mind. This framework ensures that information remains\nundisclosed between agents, preserving \"double-blind\" conditions, while causal\nToM directs the persuader's reasoning, enhancing alignment with human-like\npersuasion dynamics. Consequently, we present CToMPersu, a multi-domain,\nmulti-turn persuasive dialogue dataset that tackles both double-blind and\nlogical coherence issues, demonstrating superior performance across multiple\nmetrics and achieving better alignment with real human dialogues. Our dataset\nand prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasive dialogue plays a pivotal role in human communication, influencing\nvarious domains. Recent persuasive dialogue datasets often fail to align with\nreal-world interpersonal interactions, leading to unfaithful representations.\nFor instance, unrealistic scenarios may arise, such as when the persuadee\nexplicitly instructs the persuader on which persuasion strategies to employ,\nwith each of the persuadee's questions corresponding to a specific strategy for\nthe persuader to follow. This issue can be attributed to a violation of the\n\"Double Blind\" condition, where critical information is fully shared between\nparticipants. In actual human interactions, however, key information such as\nthe mental state of the persuadee and the persuasion strategies of the\npersuader is not directly accessible. The persuader must infer the persuadee's\nmental state using Theory of Mind capabilities and construct arguments that\nalign with the persuadee's motivations. To address this gap, we introduce\nToMMA, a novel multi-agent framework for dialogue generation that is guided by\ncausal Theory of Mind. This framework ensures that information remains\nundisclosed between agents, preserving \"double-blind\" conditions, while causal\nToM directs the persuader's reasoning, enhancing alignment with human-like\npersuasion dynamics. Consequently, we present CToMPersu, a multi-domain,\nmulti-turn persuasive dialogue dataset that tackles both double-blind and\nlogical coherence issues, demonstrating superior performance across multiple\nmetrics and achieving better alignment with real human dialogues. Our dataset\nand prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu ."
                },
                "authors": [
                    {
                        "name": "Dingyi Zhang"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "23pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09768v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09768v3",
                "updated": "2025-02-28T18:27:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    27,
                    21,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-15T11:32:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    32,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Can Large Language Models Predict the Outcome of Judicial Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict the Outcome of Judicial Decisions?"
                },
                "summary": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a\ncomprehensive evaluation framework that integrates both quantitative metrics\n(such as BLEU, ROUGE, and BERT) and qualitative assessments (including\nCoherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate\nthat fine-tuned smaller models achieve comparable performance to larger models\nin task-specific contexts while offering significant resource efficiency.\nFurthermore, we investigate the impact of fine-tuning the model on a diverse\nset of instructions, offering valuable insights into the development of a more\nhuman-centric and adaptable LLM. We have made the dataset, code, and models\npublicly available to provide a solid foundation for future research in Arabic\nlegal NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a\ncomprehensive evaluation framework that integrates both quantitative metrics\n(such as BLEU, ROUGE, and BERT) and qualitative assessments (including\nCoherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate\nthat fine-tuned smaller models achieve comparable performance to larger models\nin task-specific contexts while offering significant resource efficiency.\nFurthermore, we investigate the impact of fine-tuning the model on a diverse\nset of instructions, offering valuable insights into the development of a more\nhuman-centric and adaptable LLM. We have made the dataset, code, and models\npublicly available to provide a solid foundation for future research in Arabic\nlegal NLP."
                },
                "authors": [
                    {
                        "name": "Mohamed Bayan Kmainasi"
                    },
                    {
                        "name": "Ali Ezzat Shahroor"
                    },
                    {
                        "name": "Amani Al-Ghraibah"
                    }
                ],
                "author_detail": {
                    "name": "Amani Al-Ghraibah"
                },
                "author": "Amani Al-Ghraibah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09768v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09768v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21295v1",
                "updated": "2025-02-28T18:26:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    26,
                    40,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:26:40Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    26,
                    40,
                    4,
                    59,
                    0
                ],
                "title": "Quantum arrival times in free fall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum arrival times in free fall"
                },
                "summary": "The probability distribution of a time measurement $T_x$ at position $x$ can\nbe inferred from the probability distribution of a position measurement $X_t$\nat time $t$ as given by the Born rule [Time-of-arrival distributions for\ncontinuous quantum systems and application to quantum backflow, Phys. Rev. A\n110, 052217 (2024)]. In an application to free-fall, this finding has been used\nto predict the existence of a mass-dependent positive relative shift with\nrespect to the classical time-of-arrival in the long time-of-flight regime for\ndropped quantum particles [M. Beau and L. Martellini, Quantum delay in the time\nof arrival of free-falling atoms, Phys. Rev. A 109, 012216 (2024).]. The\npresent paper extends these results in two important directions. We first show\nthat for a Gaussian quantum particle of mass $m$ dropped in a uniform\ngravitational field $g$, the uncertainties about time and position measurements\nare related by the relation $ \\Delta T_x \\Delta X_t \\geq \\frac{\\hbar}{2mg} . $\nThis novel form of uncertainty relation suggests that choosing the initial\nstate so as to obtain a lower uncertainty in the measured position leads to a\nhigher uncertainty in the measured arrival time. Secondly, we examine the case\nof a free-falling particle starting from a non-Gaussian initial superposed\nstate, for which we predict the presence of gravitationally induced\ninterferences and oscillations in the mean time-of-arrival as a function of the\ndetector's position that can be interpreted as the signature of a\nZitterbewegung-like effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The probability distribution of a time measurement $T_x$ at position $x$ can\nbe inferred from the probability distribution of a position measurement $X_t$\nat time $t$ as given by the Born rule [Time-of-arrival distributions for\ncontinuous quantum systems and application to quantum backflow, Phys. Rev. A\n110, 052217 (2024)]. In an application to free-fall, this finding has been used\nto predict the existence of a mass-dependent positive relative shift with\nrespect to the classical time-of-arrival in the long time-of-flight regime for\ndropped quantum particles [M. Beau and L. Martellini, Quantum delay in the time\nof arrival of free-falling atoms, Phys. Rev. A 109, 012216 (2024).]. The\npresent paper extends these results in two important directions. We first show\nthat for a Gaussian quantum particle of mass $m$ dropped in a uniform\ngravitational field $g$, the uncertainties about time and position measurements\nare related by the relation $ \\Delta T_x \\Delta X_t \\geq \\frac{\\hbar}{2mg} . $\nThis novel form of uncertainty relation suggests that choosing the initial\nstate so as to obtain a lower uncertainty in the measured position leads to a\nhigher uncertainty in the measured arrival time. Secondly, we examine the case\nof a free-falling particle starting from a non-Gaussian initial superposed\nstate, for which we predict the presence of gravitationally induced\ninterferences and oscillations in the mean time-of-arrival as a function of the\ndetector's position that can be interpreted as the signature of a\nZitterbewegung-like effect."
                },
                "authors": [
                    {
                        "name": "Mathieu Beau"
                    },
                    {
                        "name": "Timothey Szczepanski"
                    },
                    {
                        "name": "Rafael Martellini"
                    },
                    {
                        "name": "Lionel Martellini"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Martellini"
                },
                "author": "Lionel Martellini",
                "arxiv_comment": "9 pages + 2 pages supplementary material, 8 figures. arXiv admin\n  note: substantial text overlap with arXiv:2403.06057",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18303v2",
                "updated": "2025-02-28T18:17:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    17,
                    6,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-24T09:15:00Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    15,
                    0,
                    1,
                    359,
                    0
                ],
                "title": "Efficient and Context-Aware Label Propagation for Zero-/Few-Shot\n  Training-Free Adaptation of Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Context-Aware Label Propagation for Zero-/Few-Shot\n  Training-Free Adaptation of Vision-Language Model"
                },
                "summary": "Vision-language models (VLMs) have revolutionized machine learning by\nleveraging large pre-trained models to tackle various downstream tasks.\nAlthough label, training, and data efficiency have improved, many\nstate-of-the-art VLMs still require task-specific hyperparameter tuning and\nfail to fully exploit test samples. To overcome these challenges, we propose a\ngraph-based approach for label-efficient adaptation and inference. Our method\ndynamically constructs a graph over text prompts, few-shot examples, and test\nsamples, using label propagation for inference without task-specific tuning.\nUnlike existing zero-shot label propagation techniques, our approach requires\nno additional unlabeled support set and effectively leverages the test sample\nmanifold through dynamic graph expansion. We further introduce a context-aware\nfeature re-weighting mechanism to improve task adaptation accuracy.\nAdditionally, our method supports efficient graph expansion, enabling real-time\ninductive inference. Extensive evaluations on downstream tasks, such as\nfine-grained categorization and out-of-distribution generalization, demonstrate\nthe effectiveness of our approach. The source code is available at\nhttps://github.com/Yushu-Li/ECALP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have revolutionized machine learning by\nleveraging large pre-trained models to tackle various downstream tasks.\nAlthough label, training, and data efficiency have improved, many\nstate-of-the-art VLMs still require task-specific hyperparameter tuning and\nfail to fully exploit test samples. To overcome these challenges, we propose a\ngraph-based approach for label-efficient adaptation and inference. Our method\ndynamically constructs a graph over text prompts, few-shot examples, and test\nsamples, using label propagation for inference without task-specific tuning.\nUnlike existing zero-shot label propagation techniques, our approach requires\nno additional unlabeled support set and effectively leverages the test sample\nmanifold through dynamic graph expansion. We further introduce a context-aware\nfeature re-weighting mechanism to improve task adaptation accuracy.\nAdditionally, our method supports efficient graph expansion, enabling real-time\ninductive inference. Extensive evaluations on downstream tasks, such as\nfine-grained categorization and out-of-distribution generalization, demonstrate\nthe effectiveness of our approach. The source code is available at\nhttps://github.com/Yushu-Li/ECALP."
                },
                "authors": [
                    {
                        "name": "Yushu Li"
                    },
                    {
                        "name": "Yongyi Su"
                    },
                    {
                        "name": "Adam Goodge"
                    },
                    {
                        "name": "Kui Jia"
                    },
                    {
                        "name": "Xun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xun Xu"
                },
                "author": "Xun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21290v1",
                "updated": "2025-02-28T18:15:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    15,
                    31,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:15:31Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    15,
                    31,
                    4,
                    59,
                    0
                ],
                "title": "Contextualizing biological perturbation experiments through language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing biological perturbation experiments through language"
                },
                "summary": "High-content perturbation experiments allow scientists to probe biomolecular\nsystems at unprecedented resolution, but experimental and analysis costs pose\nsignificant barriers to widespread adoption. Machine learning has the potential\nto guide efficient exploration of the perturbation space and extract novel\ninsights from these data. However, current approaches neglect the semantic\nrichness of the relevant biology, and their objectives are misaligned with\ndownstream biological analyses. In this paper, we hypothesize that large\nlanguage models (LLMs) present a natural medium for representing complex\nbiological relationships and rationalizing experimental outcomes. We propose\nPerturbQA, a benchmark for structured reasoning over perturbation experiments.\nUnlike current benchmarks that primarily interrogate existing knowledge,\nPerturbQA is inspired by open problems in perturbation modeling: prediction of\ndifferential expression and change of direction for unseen perturbations, and\ngene set enrichment. We evaluate state-of-the-art machine learning and\nstatistical approaches for modeling perturbations, as well as standard LLM\nreasoning strategies, and we find that current methods perform poorly on\nPerturbQA. As a proof of feasibility, we introduce Summer (SUMMarize, retrievE,\nand answeR, a simple, domain-informed LLM framework that matches or exceeds the\ncurrent state-of-the-art. Our code and data are publicly available at\nhttps://github.com/genentech/PerturbQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-content perturbation experiments allow scientists to probe biomolecular\nsystems at unprecedented resolution, but experimental and analysis costs pose\nsignificant barriers to widespread adoption. Machine learning has the potential\nto guide efficient exploration of the perturbation space and extract novel\ninsights from these data. However, current approaches neglect the semantic\nrichness of the relevant biology, and their objectives are misaligned with\ndownstream biological analyses. In this paper, we hypothesize that large\nlanguage models (LLMs) present a natural medium for representing complex\nbiological relationships and rationalizing experimental outcomes. We propose\nPerturbQA, a benchmark for structured reasoning over perturbation experiments.\nUnlike current benchmarks that primarily interrogate existing knowledge,\nPerturbQA is inspired by open problems in perturbation modeling: prediction of\ndifferential expression and change of direction for unseen perturbations, and\ngene set enrichment. We evaluate state-of-the-art machine learning and\nstatistical approaches for modeling perturbations, as well as standard LLM\nreasoning strategies, and we find that current methods perform poorly on\nPerturbQA. As a proof of feasibility, we introduce Summer (SUMMarize, retrievE,\nand answeR, a simple, domain-informed LLM framework that matches or exceeds the\ncurrent state-of-the-art. Our code and data are publicly available at\nhttps://github.com/genentech/PerturbQA."
                },
                "authors": [
                    {
                        "name": "Menghua Wu"
                    },
                    {
                        "name": "Russell Littman"
                    },
                    {
                        "name": "Jacob Levine"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "David Richmond"
                    },
                    {
                        "name": "Jan-Christian Huetter"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Christian Huetter"
                },
                "author": "Jan-Christian Huetter",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21282v1",
                "updated": "2025-02-28T18:01:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    1,
                    23,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:01:23Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    1,
                    23,
                    4,
                    59,
                    0
                ],
                "title": "Large Sample Inference with Dynamic Information Borrowing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Sample Inference with Dynamic Information Borrowing"
                },
                "summary": "Large sample behavior of dynamic information borrowing (DIB) estimators is\ninvestigated. Asymptotic properties of several DIB approaches (adaptive risk\nminimization, adaptive LASSO, Bayesian procedures with empirical power prior,\nfully Bayesian procedures, and a Bayes-frequentist compromise) are explored\nagainst shrinking to zero alternatives. As shown theoretically and with\nsimulations, local asymptotic distributions of DIB estimators are often\nnon-normal. A simple Gaussian setting with external information borrowing\nillustrates that none of the considered DIB methods outperforms others in terms\nof mean squared error (MSE): at different conflict values, the MSEs of DIBs are\nchanging between the MSEs of the maximum likelihood estimators based on the\ncurrent and pooled data. To uniquely determine an optimality criterion for DIB,\na prior distribution on the conflict needs be either implicitly or explicitly\ndetermined using data independent considerations. Data independent assumptions\non the conflict are also needed for DIB-based hypothesis testing. New families\nof DIB estimators parameterized by a sensitivity-to-conflict parameter S are\nsuggested and their use is illustrated in an infant mortality example. The\nchoice of S is determined in a data-independent manner by a cost-benefit\ncompromise associated with the use of external data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large sample behavior of dynamic information borrowing (DIB) estimators is\ninvestigated. Asymptotic properties of several DIB approaches (adaptive risk\nminimization, adaptive LASSO, Bayesian procedures with empirical power prior,\nfully Bayesian procedures, and a Bayes-frequentist compromise) are explored\nagainst shrinking to zero alternatives. As shown theoretically and with\nsimulations, local asymptotic distributions of DIB estimators are often\nnon-normal. A simple Gaussian setting with external information borrowing\nillustrates that none of the considered DIB methods outperforms others in terms\nof mean squared error (MSE): at different conflict values, the MSEs of DIBs are\nchanging between the MSEs of the maximum likelihood estimators based on the\ncurrent and pooled data. To uniquely determine an optimality criterion for DIB,\na prior distribution on the conflict needs be either implicitly or explicitly\ndetermined using data independent considerations. Data independent assumptions\non the conflict are also needed for DIB-based hypothesis testing. New families\nof DIB estimators parameterized by a sensitivity-to-conflict parameter S are\nsuggested and their use is illustrated in an infant mortality example. The\nchoice of S is determined in a data-independent manner by a cost-benefit\ncompromise associated with the use of external data."
                },
                "authors": [
                    {
                        "name": "Sergey Tarima"
                    },
                    {
                        "name": "Silvia Calderazzo"
                    },
                    {
                        "name": "Mary Homan"
                    }
                ],
                "author_detail": {
                    "name": "Mary Homan"
                },
                "author": "Mary Homan",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00075v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00075v5",
                "updated": "2025-02-28T17:50:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    50,
                    12,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-21T19:18:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    19,
                    18,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference"
                },
                "summary": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Anton Xue"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Surbhi Goel"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00075v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00075v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21271v1",
                "updated": "2025-02-28T17:46:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    46,
                    29,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:46:29Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    46,
                    29,
                    4,
                    59,
                    0
                ],
                "title": "Adaptive Keyframe Sampling for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Keyframe Sampling for Long Video Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have enabled open-world visual\nunderstanding by injecting visual input as extra tokens into large language\nmodels (LLMs) as contexts. However, when the visual input changes from a single\nimage to a long video, the above paradigm encounters difficulty because the\nvast amount of video tokens has significantly exceeded the maximal capacity of\nMLLMs. Therefore, existing video-based MLLMs are mostly established upon\nsampling a small portion of tokens from input data, which can cause key\ninformation to be lost and thus produce incorrect answers. This paper presents\na simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It\ninserts a plug-and-play module known as keyframe selection, which aims to\nmaximize the useful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving (1) the relevance\nbetween the keyframes and the prompt, and (2) the coverage of the keyframes\nover the video, and present an adaptive algorithm to approximate the best\nsolution. Experiments on two long video understanding benchmarks validate that\nAdaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines)\nupon selecting informative keyframes. Our study reveals the importance of\ninformation pre-filtering in video-based MLLMs. Code is available at\nhttps://github.com/ncTimTang/AKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enabled open-world visual\nunderstanding by injecting visual input as extra tokens into large language\nmodels (LLMs) as contexts. However, when the visual input changes from a single\nimage to a long video, the above paradigm encounters difficulty because the\nvast amount of video tokens has significantly exceeded the maximal capacity of\nMLLMs. Therefore, existing video-based MLLMs are mostly established upon\nsampling a small portion of tokens from input data, which can cause key\ninformation to be lost and thus produce incorrect answers. This paper presents\na simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It\ninserts a plug-and-play module known as keyframe selection, which aims to\nmaximize the useful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving (1) the relevance\nbetween the keyframes and the prompt, and (2) the coverage of the keyframes\nover the video, and present an adaptive algorithm to approximate the best\nsolution. Experiments on two long video understanding benchmarks validate that\nAdaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines)\nupon selecting informative keyframes. Our study reveals the importance of\ninformation pre-filtering in video-based MLLMs. Code is available at\nhttps://github.com/ncTimTang/AKS."
                },
                "authors": [
                    {
                        "name": "Xi Tang"
                    },
                    {
                        "name": "Jihao Qiu"
                    },
                    {
                        "name": "Lingxi Xie"
                    },
                    {
                        "name": "Yunjie Tian"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Qixiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Qixiang Ye"
                },
                "author": "Qixiang Ye",
                "arxiv_comment": "CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21265v1",
                "updated": "2025-02-28T17:41:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    41,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:41:27Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    41,
                    27,
                    4,
                    59,
                    0
                ],
                "title": "Token-level Ensembling of Models with Different Vocabularies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-level Ensembling of Models with Different Vocabularies"
                },
                "summary": "Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually."
                },
                "authors": [
                    {
                        "name": "Rachel Wicks"
                    },
                    {
                        "name": "Kartik Ravisankar"
                    },
                    {
                        "name": "Xinchen Yang"
                    },
                    {
                        "name": "Philipp Koehn"
                    },
                    {
                        "name": "Matt Post"
                    }
                ],
                "author_detail": {
                    "name": "Matt Post"
                },
                "author": "Matt Post",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21262v1",
                "updated": "2025-02-28T17:39:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    39,
                    55,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:39:55Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    39,
                    55,
                    4,
                    59,
                    0
                ],
                "title": "Modeling Human Beliefs about AI Behavior for Scalable Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Human Beliefs about AI Behavior for Scalable Oversight"
                },
                "summary": "Contemporary work in AI alignment often relies on human feedback to teach AI\nsystems human preferences and values. Yet as AI systems grow more capable,\nhuman feedback becomes increasingly unreliable. This raises the problem of\nscalable oversight: How can we supervise AI systems that exceed human\ncapabilities? In this work, we propose to model the human evaluator's beliefs\nabout the AI system's behavior to better interpret the human's feedback. We\nformalize human belief models and theoretically analyze their role in inferring\nhuman values. We then characterize the remaining ambiguity in this inference\nand conditions for which the ambiguity disappears. To mitigate reliance on\nexact belief models, we then introduce the relaxation of human belief model\ncovering. Finally, we propose using foundation models to construct covering\nbelief models, providing a new potential approach to scalable oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary work in AI alignment often relies on human feedback to teach AI\nsystems human preferences and values. Yet as AI systems grow more capable,\nhuman feedback becomes increasingly unreliable. This raises the problem of\nscalable oversight: How can we supervise AI systems that exceed human\ncapabilities? In this work, we propose to model the human evaluator's beliefs\nabout the AI system's behavior to better interpret the human's feedback. We\nformalize human belief models and theoretically analyze their role in inferring\nhuman values. We then characterize the remaining ambiguity in this inference\nand conditions for which the ambiguity disappears. To mitigate reliance on\nexact belief models, we then introduce the relaxation of human belief model\ncovering. Finally, we propose using foundation models to construct covering\nbelief models, providing a new potential approach to scalable oversight."
                },
                "authors": [
                    {
                        "name": "Leon Lang"
                    },
                    {
                        "name": "Patrick Forr"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Forr"
                },
                "author": "Patrick Forr",
                "arxiv_comment": "53 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21259v1",
                "updated": "2025-02-28T17:35:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    35,
                    14,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:35:14Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    35,
                    14,
                    4,
                    59,
                    0
                ],
                "title": "Optimizing and Exploring System Performance in Compact\n  Processing-in-Memory-based Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing and Exploring System Performance in Compact\n  Processing-in-Memory-based Chips"
                },
                "summary": "Processing-in-memory (PIM) is a promising computing paradigm to tackle the\n\"memory wall\" challenge. However, PIM system-level benefits over traditional\nvon Neumann architecture can be reduced when the memory array cannot fully\nstore all the neural network (NN) weights. The NN size is increasing while the\nPIM design size cannot scale up accordingly due to area constraints. Therefore,\nthis work targets the system performance optimization and exploration for\ncompact PIM designs. We first analyze the impact of data movement on compact\ndesigns. Then, we propose a novel pipeline method that maximizes the reuse of\nNN weights to improve the throughput and energy efficiency of inference in\ncompact chips. To further boost throughput, we introduce a scheduling algorithm\nto mitigate the pipeline bubble problem. Moreover, we investigate the trade-off\nbetween the network size and system performance for a compact PIM chip.\nExperimental results show that the proposed algorithm achieves 2.35x and 0.5%\nimprovement in throughput and energy efficiency, respectively. Compared to the\narea-unlimited design, our compact chip achieves approximately 56.5% of the\nthroughput and 58.6% of the energy efficiency while using only one-third of the\nchip area, along with 1.3x improvement in area efficiency. Our compact design\nalso outperforms the modern GPU with 4.56x higher throughput and 157x better\nenergy efficiency. Besides, our compact design uses less than 20% of the system\nenergy for data movement as batch size scales up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-memory (PIM) is a promising computing paradigm to tackle the\n\"memory wall\" challenge. However, PIM system-level benefits over traditional\nvon Neumann architecture can be reduced when the memory array cannot fully\nstore all the neural network (NN) weights. The NN size is increasing while the\nPIM design size cannot scale up accordingly due to area constraints. Therefore,\nthis work targets the system performance optimization and exploration for\ncompact PIM designs. We first analyze the impact of data movement on compact\ndesigns. Then, we propose a novel pipeline method that maximizes the reuse of\nNN weights to improve the throughput and energy efficiency of inference in\ncompact chips. To further boost throughput, we introduce a scheduling algorithm\nto mitigate the pipeline bubble problem. Moreover, we investigate the trade-off\nbetween the network size and system performance for a compact PIM chip.\nExperimental results show that the proposed algorithm achieves 2.35x and 0.5%\nimprovement in throughput and energy efficiency, respectively. Compared to the\narea-unlimited design, our compact chip achieves approximately 56.5% of the\nthroughput and 58.6% of the energy efficiency while using only one-third of the\nchip area, along with 1.3x improvement in area efficiency. Our compact design\nalso outperforms the modern GPU with 4.56x higher throughput and 157x better\nenergy efficiency. Besides, our compact design uses less than 20% of the system\nenergy for data movement as batch size scales up."
                },
                "authors": [
                    {
                        "name": "Peilin Chen"
                    },
                    {
                        "name": "Xiaoxuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Yang"
                },
                "author": "Xiaoxuan Yang",
                "arxiv_comment": "Accepted to AICAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19700v2",
                "updated": "2025-02-28T17:33:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    33,
                    31,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-27T02:35:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    2,
                    35,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small\n  Sample Classification via Semi-Supervised Conditional Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small\n  Sample Classification via Semi-Supervised Conditional Diffusion Model"
                },
                "summary": "Data augmentation effectively addresses the imbalanced-small sample data\n(ISSD) problem in hyperspectral image classification (HSIC). While most\nmethodologies extend features in the latent space, few leverage text-driven\ngeneration to create realistic and diverse samples. Recently, text-guided\ndiffusion models have gained significant attention due to their ability to\ngenerate highly diverse and high-quality images based on text prompts in\nnatural image synthesis. Motivated by this, this paper proposes\nTxt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis\nmethod to address the ISSD in HSIC. The proposed approach uses a denoising\ndiffusion model, which iteratively removes Gaussian noise to generate\nhyperspectral samples conditioned on textual descriptions. First, to address\nthe high-dimensionality of hyperspectral data, a universal variational\nautoencoder (VAE) is designed to map the data into a low-dimensional latent\nspace, which provides stable features and reduces the inference complexity of\ndiffusion model. Second, a semi-supervised diffusion model is designed to fully\ntake advantage of unlabeled data. Random polygon spatial clipping (RPSC) and\nuncertainty estimation of latent feature (LF-UE) are used to simulate the\nvarying degrees of mixing. Third, the VAE decodes HSI from latent space\ngenerated by the diffusion model with the language conditions as input. In our\nexperiments, we fully evaluate synthetic samples' effectiveness from\nstatistical characteristics and data distribution in 2D-PCA space.\nAdditionally, visual-linguistic cross-attention is visualized on the pixel\nlevel to prove that our proposed model can capture the spatial layout and\ngeometry of the generated data. Experiments demonstrate that the performance of\nthe proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models,\nstate-of-the-art CNNs, and semi-supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation effectively addresses the imbalanced-small sample data\n(ISSD) problem in hyperspectral image classification (HSIC). While most\nmethodologies extend features in the latent space, few leverage text-driven\ngeneration to create realistic and diverse samples. Recently, text-guided\ndiffusion models have gained significant attention due to their ability to\ngenerate highly diverse and high-quality images based on text prompts in\nnatural image synthesis. Motivated by this, this paper proposes\nTxt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis\nmethod to address the ISSD in HSIC. The proposed approach uses a denoising\ndiffusion model, which iteratively removes Gaussian noise to generate\nhyperspectral samples conditioned on textual descriptions. First, to address\nthe high-dimensionality of hyperspectral data, a universal variational\nautoencoder (VAE) is designed to map the data into a low-dimensional latent\nspace, which provides stable features and reduces the inference complexity of\ndiffusion model. Second, a semi-supervised diffusion model is designed to fully\ntake advantage of unlabeled data. Random polygon spatial clipping (RPSC) and\nuncertainty estimation of latent feature (LF-UE) are used to simulate the\nvarying degrees of mixing. Third, the VAE decodes HSI from latent space\ngenerated by the diffusion model with the language conditions as input. In our\nexperiments, we fully evaluate synthetic samples' effectiveness from\nstatistical characteristics and data distribution in 2D-PCA space.\nAdditionally, visual-linguistic cross-attention is visualized on the pixel\nlevel to prove that our proposed model can capture the spatial layout and\ngeometry of the generated data. Experiments demonstrate that the performance of\nthe proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models,\nstate-of-the-art CNNs, and semi-supervised methods."
                },
                "authors": [
                    {
                        "name": "Yimin Zhu"
                    },
                    {
                        "name": "Linlin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Xu"
                },
                "author": "Linlin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06967v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06967v3",
                "updated": "2025-02-28T17:28:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    28,
                    36,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-11T05:50:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    5,
                    50,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language\n  Models Closing the Gap with Human Vision ?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Thinking and Logical Processing -- Are Multi-modal Large Language\n  Models Closing the Gap with Human Vision ?"
                },
                "summary": "The dual thinking framework considers fast, intuitive, and slower logical\nprocessing. The perception of dual thinking in vision requires images where\ninferences from intuitive and logical processing differ, and the latter is\nunder-explored in current studies. We introduce a novel adversarial dataset to\nprovide evidence for the dual thinking framework in human vision, which also\nfacilitates the study of the qualitative behavior of deep learning models. Our\npsychophysical studies show the presence of multiple inferences in rapid\nsuccession, and analysis of errors shows that the early stopping of visual\nprocessing can result in missing relevant information. MLLMs (Multi-modal Large\nLanguage Models) and VLMs (Vision Language Models) have made significant\nprogress in correcting errors in intuitive processing in human vision and\nshowed enhanced performance on images requiring logical processing. However,\ntheir improvements in logical processing have not kept pace with their\nadvancements in intuitive processing. In contrast, segmentation models exhibit\nerrors similar to those seen in intuitive human processing and lack\nunderstanding of sub-structures, as indicated by errors related to\nsub-components in identified instances. As AI (Artificial Intelligence)-based\nsystems find increasing applications in safety-critical domains like autonomous\ndriving, the integration of logical processing capabilities becomes essential.\nThis not only enhances performance but also addresses the limitations of\nscaling-based approaches while ensuring robustness and reliability in\nreal-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dual thinking framework considers fast, intuitive, and slower logical\nprocessing. The perception of dual thinking in vision requires images where\ninferences from intuitive and logical processing differ, and the latter is\nunder-explored in current studies. We introduce a novel adversarial dataset to\nprovide evidence for the dual thinking framework in human vision, which also\nfacilitates the study of the qualitative behavior of deep learning models. Our\npsychophysical studies show the presence of multiple inferences in rapid\nsuccession, and analysis of errors shows that the early stopping of visual\nprocessing can result in missing relevant information. MLLMs (Multi-modal Large\nLanguage Models) and VLMs (Vision Language Models) have made significant\nprogress in correcting errors in intuitive processing in human vision and\nshowed enhanced performance on images requiring logical processing. However,\ntheir improvements in logical processing have not kept pace with their\nadvancements in intuitive processing. In contrast, segmentation models exhibit\nerrors similar to those seen in intuitive human processing and lack\nunderstanding of sub-structures, as indicated by errors related to\nsub-components in identified instances. As AI (Artificial Intelligence)-based\nsystems find increasing applications in safety-critical domains like autonomous\ndriving, the integration of logical processing capabilities becomes essential.\nThis not only enhances performance but also addresses the limitations of\nscaling-based approaches while ensuring robustness and reliability in\nreal-world environments."
                },
                "authors": [
                    {
                        "name": "Kailas Dayanandan"
                    },
                    {
                        "name": "Nikhil Kumar"
                    },
                    {
                        "name": "Anand Sinha"
                    },
                    {
                        "name": "Brejesh Lall"
                    }
                ],
                "author_detail": {
                    "name": "Brejesh Lall"
                },
                "author": "Brejesh Lall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06967v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06967v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13655v2",
                "updated": "2025-02-28T17:18:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    18,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2024-05-22T13:59:51Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    13,
                    59,
                    51,
                    2,
                    143,
                    0
                ],
                "title": "A Deep Learning Approach to Multi-Fiber Parameter Estimation and\n  Uncertainty Quantification in Diffusion MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Learning Approach to Multi-Fiber Parameter Estimation and\n  Uncertainty Quantification in Diffusion MRI"
                },
                "summary": "Diffusion MRI (dMRI) is the primary imaging modality used to study brain\nmicrostructure in vivo. Reliable and computationally efficient parameter\ninference for common dMRI biophysical models is a challenging inverse problem,\ndue to factors such as variable dimensionalities (reflecting the unknown number\nof distinct white matter fiber populations in a voxel), low signal-to-noise\nratios, and non-linear forward models. These challenges have led many existing\nmethods to use biologically implausible simplified models to stabilize\nestimation, for instance, assuming shared microstructure across all fiber\npopulations within a voxel. In this work, we introduce a novel sequential\nmethod for multi-fiber parameter inference that decomposes the task into a\nseries of manageable subproblems. These subproblems are solved using deep\nneural networks tailored to problem-specific structure and symmetry, and\ntrained via simulation. The resulting inference procedure is largely amortized,\nenabling scalable parameter estimation and uncertainty quantification across\nall model parameters. Simulation studies and real imaging data analysis using\nthe Human Connectome Project (HCP) demonstrate the advantages of our method\nover standard alternatives. In the case of the standard model of diffusion, our\nresults show that under HCP-like acquisition schemes, estimates for\nextra-cellular parallel diffusivity are highly uncertain, while those for the\nintra-cellular volume fraction can be estimated with relatively high precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion MRI (dMRI) is the primary imaging modality used to study brain\nmicrostructure in vivo. Reliable and computationally efficient parameter\ninference for common dMRI biophysical models is a challenging inverse problem,\ndue to factors such as variable dimensionalities (reflecting the unknown number\nof distinct white matter fiber populations in a voxel), low signal-to-noise\nratios, and non-linear forward models. These challenges have led many existing\nmethods to use biologically implausible simplified models to stabilize\nestimation, for instance, assuming shared microstructure across all fiber\npopulations within a voxel. In this work, we introduce a novel sequential\nmethod for multi-fiber parameter inference that decomposes the task into a\nseries of manageable subproblems. These subproblems are solved using deep\nneural networks tailored to problem-specific structure and symmetry, and\ntrained via simulation. The resulting inference procedure is largely amortized,\nenabling scalable parameter estimation and uncertainty quantification across\nall model parameters. Simulation studies and real imaging data analysis using\nthe Human Connectome Project (HCP) demonstrate the advantages of our method\nover standard alternatives. In the case of the standard model of diffusion, our\nresults show that under HCP-like acquisition schemes, estimates for\nextra-cellular parallel diffusivity are highly uncertain, while those for the\nintra-cellular volume fraction can be estimated with relatively high precision."
                },
                "authors": [
                    {
                        "name": "William Consagra"
                    },
                    {
                        "name": "Lipeng Ning"
                    },
                    {
                        "name": "Yogesh Rathi"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Rathi"
                },
                "author": "Yogesh Rathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18475v2",
                "updated": "2025-02-28T17:12:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    12,
                    42,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-05T14:44:32Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    44,
                    32,
                    2,
                    36,
                    0
                ],
                "title": "Least squares variational inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Least squares variational inference"
                },
                "summary": "Variational inference consists in finding the best approximation of a target\ndistribution within a certain family, where `best' means (typically) smallest\nKullback-Leiber divergence. We show that, when the approximation family is\nexponential, the best approximation is the solution of a fixed-point equation.\nWe introduce LSVI (Least-Squares Variational Inference), a Monte Carlo variant\nof the corresponding fixed-point recursion, where each iteration boils down to\nordinary least squares regression and does not require computing gradients. We\nshow that LSVI is equivalent to stochastic mirror descent; we use this insight\nto derive convergence guarantees. We introduce various ideas to improve LSVI\nfurther when the approximation family is Gaussian, leading to a $O(d^3)$\ncomplexity in the dimension $d$ of the target in the full-covariance case, and\na $O(d)$ complexity in the mean-field case. We show that LSVI outperforms\nstate-of-the-art methods in a range of examples, while remaining gradient-free,\nthat is, it does not require computing gradients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference consists in finding the best approximation of a target\ndistribution within a certain family, where `best' means (typically) smallest\nKullback-Leiber divergence. We show that, when the approximation family is\nexponential, the best approximation is the solution of a fixed-point equation.\nWe introduce LSVI (Least-Squares Variational Inference), a Monte Carlo variant\nof the corresponding fixed-point recursion, where each iteration boils down to\nordinary least squares regression and does not require computing gradients. We\nshow that LSVI is equivalent to stochastic mirror descent; we use this insight\nto derive convergence guarantees. We introduce various ideas to improve LSVI\nfurther when the approximation family is Gaussian, leading to a $O(d^3)$\ncomplexity in the dimension $d$ of the target in the full-covariance case, and\na $O(d)$ complexity in the mean-field case. We show that LSVI outperforms\nstate-of-the-art methods in a range of examples, while remaining gradient-free,\nthat is, it does not require computing gradients."
                },
                "authors": [
                    {
                        "name": "Yvann Le Fay"
                    },
                    {
                        "name": "Nicolas Chopin"
                    },
                    {
                        "name": "Simon Barthelm"
                    }
                ],
                "author_detail": {
                    "name": "Simon Barthelm"
                },
                "author": "Simon Barthelm",
                "arxiv_comment": "22 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21239v1",
                "updated": "2025-02-28T17:09:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:09:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Swair Shah"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16100v2",
                "updated": "2025-02-28T17:02:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    2,
                    23,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-20T17:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    42,
                    25,
                    4,
                    355,
                    0
                ],
                "title": "Logical Consistency of Large Language Models in Fact-checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Consistency of Large Language Models in Fact-checking"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses - a meaning-preserving change in the input query results\nin an inconsistent response and attributes to vulnerabilities of LLMs such as\nhallucination. Consequently, existing research focuses on simple\nparaphrasing-based consistency assessment of LLMs, and ignores complex queries\nthat necessitate an even better understanding of logical reasoning by an LLM.\nOur work therefore addresses the logical inconsistency of LLMs under complex\nlogical queries with primitive logical operators, e.g., negation, conjunction,\nand disjunction. As a test bed, we consider retrieval-augmented LLMs on a\nfact-checking task involving propositional logic queries from knowledge graphs\n(KGs). Our contributions are threefold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries and demonstrate that existing LLMs lack logical\nconsistency, especially on complex queries. Improvement: We employ supervised\nfine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts. We have made our source code and\nbenchmarks available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses - a meaning-preserving change in the input query results\nin an inconsistent response and attributes to vulnerabilities of LLMs such as\nhallucination. Consequently, existing research focuses on simple\nparaphrasing-based consistency assessment of LLMs, and ignores complex queries\nthat necessitate an even better understanding of logical reasoning by an LLM.\nOur work therefore addresses the logical inconsistency of LLMs under complex\nlogical queries with primitive logical operators, e.g., negation, conjunction,\nand disjunction. As a test bed, we consider retrieval-augmented LLMs on a\nfact-checking task involving propositional logic queries from knowledge graphs\n(KGs). Our contributions are threefold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries and demonstrate that existing LLMs lack logical\nconsistency, especially on complex queries. Improvement: We employ supervised\nfine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts. We have made our source code and\nbenchmarks available."
                },
                "authors": [
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Sarah Hasan"
                    },
                    {
                        "name": "Naheed Anjum Arafat"
                    },
                    {
                        "name": "Arijit Khan"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Khan"
                },
                "author": "Arijit Khan",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21231v1",
                "updated": "2025-02-28T17:01:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    1,
                    3,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:01:03Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    1,
                    3,
                    4,
                    59,
                    0
                ],
                "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length\n  on More Than 12,000 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length\n  on More Than 12,000 GPUs"
                },
                "summary": "Scaling long-context ability is essential for Large Language Models (LLMs).\nTo amortize the memory consumption across multiple devices in long-context\ntraining, inter-data partitioning (a.k.a. Data Parallelism) and intra-data\npartitioning (a.k.a. Context Parallelism) are commonly used. Current training\nframeworks predominantly treat the two techniques as orthogonal, and establish\nstatic communication groups to organize the devices as a static mesh (e.g., a\n2D mesh). However, the sequences for LLM training typically vary in lengths, no\nmatter for texts, multi-modalities or reinforcement learning. The mismatch\nbetween data heterogeneity and static mesh causes redundant communication and\nimbalanced computation, degrading the training efficiency.\n  In this work, we introduce ByteScale, an efficient, flexible, and scalable\nLLM training framework for large-scale mixed training of long and short\nsequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid\nData Parallelism (HDP), which unifies the inter- and intra-data partitioning\nwith a dynamic mesh design. In particular, we build a communication optimizer,\nwhich eliminates the redundant communication for short sequences by data-aware\nsharding and dynamic communication, and further compresses the communication\ncost for long sequences by selective offloading. Besides, we also develop a\nbalance scheduler to mitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model sizes ranging from 7B to\n141B, context lengths from 256K to 2048K, on a production cluster with more\nthan 12,000 GPUs. Experiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling long-context ability is essential for Large Language Models (LLMs).\nTo amortize the memory consumption across multiple devices in long-context\ntraining, inter-data partitioning (a.k.a. Data Parallelism) and intra-data\npartitioning (a.k.a. Context Parallelism) are commonly used. Current training\nframeworks predominantly treat the two techniques as orthogonal, and establish\nstatic communication groups to organize the devices as a static mesh (e.g., a\n2D mesh). However, the sequences for LLM training typically vary in lengths, no\nmatter for texts, multi-modalities or reinforcement learning. The mismatch\nbetween data heterogeneity and static mesh causes redundant communication and\nimbalanced computation, degrading the training efficiency.\n  In this work, we introduce ByteScale, an efficient, flexible, and scalable\nLLM training framework for large-scale mixed training of long and short\nsequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid\nData Parallelism (HDP), which unifies the inter- and intra-data partitioning\nwith a dynamic mesh design. In particular, we build a communication optimizer,\nwhich eliminates the redundant communication for short sequences by data-aware\nsharding and dynamic communication, and further compresses the communication\ncost for long sequences by selective offloading. Besides, we also develop a\nbalance scheduler to mitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model sizes ranging from 7B to\n141B, context lengths from 256K to 2048K, on a production cluster with more\nthan 12,000 GPUs. Experiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89x."
                },
                "authors": [
                    {
                        "name": "Hao Ge"
                    },
                    {
                        "name": "Junda Feng"
                    },
                    {
                        "name": "Qi Huang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Lei Zuo"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "arxiv_comment": "12 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21228v1",
                "updated": "2025-02-28T16:59:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    59,
                    30,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:59:30Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    59,
                    30,
                    4,
                    59,
                    0
                ],
                "title": "ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer"
                },
                "summary": "To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in."
                },
                "authors": [
                    {
                        "name": "Omer Goldman"
                    },
                    {
                        "name": "Uri Shaham"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Joshua Maynez"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Jason Riesa"
                    },
                    {
                        "name": "Shruti Rijhwani"
                    },
                    {
                        "name": "Laura Rimell"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    },
                    {
                        "name": "Matan Eyal"
                    }
                ],
                "author_detail": {
                    "name": "Matan Eyal"
                },
                "author": "Matan Eyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.17837v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.17837v5",
                "updated": "2025-02-28T16:47:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    47,
                    1,
                    4,
                    59,
                    0
                ],
                "published": "2023-06-30T17:56:15Z",
                "published_parsed": [
                    2023,
                    6,
                    30,
                    17,
                    56,
                    15,
                    4,
                    181,
                    0
                ],
                "title": "Gauging tensor networks with belief propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gauging tensor networks with belief propagation"
                },
                "summary": "Effectively compressing and optimizing tensor networks requires reliable\nmethods for fixing the latent degrees of freedom of the tensors, known as the\ngauge. Here we introduce a new algorithm for gauging tensor networks using\nbelief propagation, a method that was originally formulated for performing\nstatistical inference on graphical models and has recently found applications\nin tensor network algorithms. We show that this method is closely related to\nknown tensor network gauging methods. It has the practical advantage, however,\nthat existing belief propagation implementations can be repurposed for tensor\nnetwork gauging, and that belief propagation is a very simple algorithm based\non just tensor contractions so it can be easier to implement, optimize, and\ngeneralize. We present numerical evidence and scaling arguments that this\nalgorithm is faster than existing gauging algorithms, demonstrating its usage\non structured, unstructured, and infinite tensor networks. Additionally, we\napply this method to improve the accuracy of the widely used simple update gate\nevolution algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively compressing and optimizing tensor networks requires reliable\nmethods for fixing the latent degrees of freedom of the tensors, known as the\ngauge. Here we introduce a new algorithm for gauging tensor networks using\nbelief propagation, a method that was originally formulated for performing\nstatistical inference on graphical models and has recently found applications\nin tensor network algorithms. We show that this method is closely related to\nknown tensor network gauging methods. It has the practical advantage, however,\nthat existing belief propagation implementations can be repurposed for tensor\nnetwork gauging, and that belief propagation is a very simple algorithm based\non just tensor contractions so it can be easier to implement, optimize, and\ngeneralize. We present numerical evidence and scaling arguments that this\nalgorithm is faster than existing gauging algorithms, demonstrating its usage\non structured, unstructured, and infinite tensor networks. Additionally, we\napply this method to improve the accuracy of the widely used simple update gate\nevolution algorithm."
                },
                "authors": [
                    {
                        "name": "Joseph Tindall"
                    },
                    {
                        "name": "Matthew T. Fishman"
                    }
                ],
                "author_detail": {
                    "name": "Matthew T. Fishman"
                },
                "author": "Matthew T. Fishman",
                "arxiv_doi": "10.21468/SciPostPhys.15.6.222",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21468/SciPostPhys.15.6.222",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.17837v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.17837v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "45 Pages. 11 Figures",
                "arxiv_journal_ref": "SciPost Phys. 15, 222 (2023)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20672v3",
                "updated": "2025-02-28T16:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    44,
                    24,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-28T02:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    2,
                    15,
                    45,
                    0,
                    302,
                    0
                ],
                "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA"
                },
                "summary": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Seungyeon Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Tal Schuster"
                },
                "author": "Tal Schuster",
                "arxiv_comment": "ICLR 2025; 49 pages, 17 figures, 19 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21212v1",
                "updated": "2025-02-28T16:40:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    40,
                    38,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:40:38Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    40,
                    38,
                    4,
                    59,
                    0
                ],
                "title": "Transformers Learn to Implement Multi-step Gradient Descent with Chain\n  of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Learn to Implement Multi-step Gradient Descent with Chain\n  of Thought"
                },
                "summary": "Chain of Thought (CoT) prompting has been shown to significantly improve the\nperformance of large language models (LLMs), particularly in arithmetic and\nreasoning tasks, by instructing the model to produce intermediate reasoning\nsteps. Despite the remarkable empirical success of CoT and its theoretical\nadvantages in enhancing expressivity, the mechanisms underlying CoT training\nremain largely unexplored. In this paper, we study the training dynamics of\ntransformers over a CoT objective on an in-context weight prediction task for\nlinear regression. We prove that while a one-layer linear transformer without\nCoT can only implement a single step of gradient descent (GD) and fails to\nrecover the ground-truth weight vector, a transformer with CoT prompting can\nlearn to perform multi-step GD autoregressively, achieving near-exact recovery.\nFurthermore, we show that the trained transformer effectively generalizes on\nthe unseen data. With our technique, we also show that looped transformers\nsignificantly improve final performance compared to transformers without\nlooping in the in-context learning of linear regression. Empirically, we\ndemonstrate that CoT prompting yields substantial performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) prompting has been shown to significantly improve the\nperformance of large language models (LLMs), particularly in arithmetic and\nreasoning tasks, by instructing the model to produce intermediate reasoning\nsteps. Despite the remarkable empirical success of CoT and its theoretical\nadvantages in enhancing expressivity, the mechanisms underlying CoT training\nremain largely unexplored. In this paper, we study the training dynamics of\ntransformers over a CoT objective on an in-context weight prediction task for\nlinear regression. We prove that while a one-layer linear transformer without\nCoT can only implement a single step of gradient descent (GD) and fails to\nrecover the ground-truth weight vector, a transformer with CoT prompting can\nlearn to perform multi-step GD autoregressively, achieving near-exact recovery.\nFurthermore, we show that the trained transformer effectively generalizes on\nthe unseen data. With our technique, we also show that looped transformers\nsignificantly improve final performance compared to transformers without\nlooping in the in-context learning of linear regression. Empirically, we\ndemonstrate that CoT prompting yields substantial performance improvements."
                },
                "authors": [
                    {
                        "name": "Jianhao Huang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jason D. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. Lee"
                },
                "author": "Jason D. Lee",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21208v1",
                "updated": "2025-02-28T16:28:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    28,
                    13,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:28:13Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    28,
                    13,
                    4,
                    59,
                    0
                ],
                "title": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph\n  Environments"
                },
                "summary": "Recent research has shown that LLM performance on reasoning tasks can be\nenhanced by scaling test-time compute. One promising approach, particularly\nwith decomposable problems, involves arranging intermediate solutions as a\ngraph on which transformations are performed to explore the solution space.\nHowever, prior works rely on pre-determined, task-specific transformation\nschedules which are subject to a set of searched hyperparameters. In this work,\nwe view thought graph transformations as actions in a Markov decision process,\nand implement policy agents to drive effective action policies for the\nunderlying reasoning LLM agent. In particular, we investigate the ability for\nanother LLM to act as a policy agent on thought graph environments and\nintroduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES,\nreasoning LLM agents solve decomposed subproblems, while policy LLM agents\nmaintain visibility of the thought graph states, and dynamically adapt the\nproblem-solving strategy. Through extensive experiments, we observe that using\noff-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can\nyield up to $29\\%$ higher accuracy on HumanEval relative to static\ntransformation schedules, as well as reducing inference costs by $35\\%$ and\navoid any search requirements. We also conduct a thorough analysis of observed\nfailure modes, highlighting that limitations on LLM sizes and the depth of\nproblem decomposition can be seen as challenges to scaling LLM-guided\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that LLM performance on reasoning tasks can be\nenhanced by scaling test-time compute. One promising approach, particularly\nwith decomposable problems, involves arranging intermediate solutions as a\ngraph on which transformations are performed to explore the solution space.\nHowever, prior works rely on pre-determined, task-specific transformation\nschedules which are subject to a set of searched hyperparameters. In this work,\nwe view thought graph transformations as actions in a Markov decision process,\nand implement policy agents to drive effective action policies for the\nunderlying reasoning LLM agent. In particular, we investigate the ability for\nanother LLM to act as a policy agent on thought graph environments and\nintroduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES,\nreasoning LLM agents solve decomposed subproblems, while policy LLM agents\nmaintain visibility of the thought graph states, and dynamically adapt the\nproblem-solving strategy. Through extensive experiments, we observe that using\noff-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can\nyield up to $29\\%$ higher accuracy on HumanEval relative to static\ntransformation schedules, as well as reducing inference costs by $35\\%$ and\navoid any search requirements. We also conduct a thorough analysis of observed\nfailure modes, highlighting that limitations on LLM sizes and the depth of\nproblem decomposition can be seen as challenges to scaling LLM-guided\nreasoning."
                },
                "authors": [
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Jeffrey Wong"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.12197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.12197v2",
                "updated": "2025-02-28T16:18:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    18,
                    41,
                    4,
                    59,
                    0
                ],
                "published": "2022-09-25T10:59:00Z",
                "published_parsed": [
                    2022,
                    9,
                    25,
                    10,
                    59,
                    0,
                    6,
                    268,
                    0
                ],
                "title": "First-order Conditions for Optimization in the Wasserstein Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-order Conditions for Optimization in the Wasserstein Space"
                },
                "summary": "We study first-order optimality conditions for constrained optimization in\nthe Wasserstein space, whereby one seeks to minimize a real-valued function\nover the space of probability measures endowed with the Wasserstein distance.\nOur analysis combines recent insights on the geometry and the differential\nstructure of the Wasserstein space with more classical calculus of variations.\nWe show that simple rationales such as \"setting the derivative to zero\" and\n\"gradients are aligned at optimality\" carry over to the Wasserstein space. We\ndeploy our tools to study and solve optimization problems in the setting of\ndistributionally robust optimization and statistical inference. The generality\nof our methodology allows us to naturally deal with functionals, such as\nmean-variance, Kullback-Leibler divergence, and Wasserstein distance, which are\ntraditionally difficult to study in a unified framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study first-order optimality conditions for constrained optimization in\nthe Wasserstein space, whereby one seeks to minimize a real-valued function\nover the space of probability measures endowed with the Wasserstein distance.\nOur analysis combines recent insights on the geometry and the differential\nstructure of the Wasserstein space with more classical calculus of variations.\nWe show that simple rationales such as \"setting the derivative to zero\" and\n\"gradients are aligned at optimality\" carry over to the Wasserstein space. We\ndeploy our tools to study and solve optimization problems in the setting of\ndistributionally robust optimization and statistical inference. The generality\nof our methodology allows us to naturally deal with functionals, such as\nmean-variance, Kullback-Leibler divergence, and Wasserstein distance, which are\ntraditionally difficult to study in a unified framework."
                },
                "authors": [
                    {
                        "name": "Nicolas Lanzetti"
                    },
                    {
                        "name": "Saverio Bolognani"
                    },
                    {
                        "name": "Florian Drfler"
                    }
                ],
                "author_detail": {
                    "name": "Florian Drfler"
                },
                "author": "Florian Drfler",
                "arxiv_doi": "10.1137/23M156687X",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1137/23M156687X",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2209.12197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.12197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "SIAM Journal on Mathematics of Data Science, 7(1), 274-300 (2025)",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17389v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17389v3",
                "updated": "2025-02-28T16:17:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    17,
                    10,
                    4,
                    59,
                    0
                ],
                "published": "2024-01-30T19:13:08Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    19,
                    13,
                    8,
                    1,
                    30,
                    0
                ],
                "title": "An introduction to statistical models used to characterize\n  species-habitat associations with animal movement data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An introduction to statistical models used to characterize\n  species-habitat associations with animal movement data"
                },
                "summary": "Understanding species-habitat associations is fundamental to ecological\nsciences and for species conservation. Consequently, various statistical\napproaches have been designed to infer species-habitat associations. Due to\ntheir conceptual and mathematical differences, these methods can yield\ncontrasting results. We describe and compare commonly used statistical models\nthat relate animal movement data to environmental data, including resource\nselection functions (RSF), step-selection functions (SSF), and hidden Markov\nmodels (HMMs). We demonstrate differences in assumptions and highlighting\nadvantages and limitations of each method. Additionally, we provide guidance on\nselecting the most appropriate statistical method based on the scale of the\ndata and intended inference. To illustrate the varying ecological insights\nderived from each model, we apply them to the movement track of a single ringed\nseal in a case study. We demonstrate that each model yields varying ecological\ninsights. For example, while the selection coefficient values from RSFs appear\nto show a stronger positive relationship with prey diversity than those of the\nSSFs, when we accounted for the autocorrelation in the data none of these\nrelationships with prey diversity were statistically significant. The HMM\nreveals variable associations with prey diversity across different behaviors.\nNotably, the three models identified different important areas. This case study\nhighlights the critical significance of selecting the appropriate model as an\nessential step in the process of identifying species-habitat relationships and\nspecific areas of importance. Our review provides the foundational information\nrequired for making informed decisions when choosing the most suitable\nstatistical methods to address specific questions, such as identifying\nprotected zones, understanding movement patterns, or studying behaviours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding species-habitat associations is fundamental to ecological\nsciences and for species conservation. Consequently, various statistical\napproaches have been designed to infer species-habitat associations. Due to\ntheir conceptual and mathematical differences, these methods can yield\ncontrasting results. We describe and compare commonly used statistical models\nthat relate animal movement data to environmental data, including resource\nselection functions (RSF), step-selection functions (SSF), and hidden Markov\nmodels (HMMs). We demonstrate differences in assumptions and highlighting\nadvantages and limitations of each method. Additionally, we provide guidance on\nselecting the most appropriate statistical method based on the scale of the\ndata and intended inference. To illustrate the varying ecological insights\nderived from each model, we apply them to the movement track of a single ringed\nseal in a case study. We demonstrate that each model yields varying ecological\ninsights. For example, while the selection coefficient values from RSFs appear\nto show a stronger positive relationship with prey diversity than those of the\nSSFs, when we accounted for the autocorrelation in the data none of these\nrelationships with prey diversity were statistically significant. The HMM\nreveals variable associations with prey diversity across different behaviors.\nNotably, the three models identified different important areas. This case study\nhighlights the critical significance of selecting the appropriate model as an\nessential step in the process of identifying species-habitat relationships and\nspecific areas of importance. Our review provides the foundational information\nrequired for making informed decisions when choosing the most suitable\nstatistical methods to address specific questions, such as identifying\nprotected zones, understanding movement patterns, or studying behaviours."
                },
                "authors": [
                    {
                        "name": "Katie R. N. Florko"
                    },
                    {
                        "name": "Ron R. Togunov"
                    },
                    {
                        "name": "Rowenna Gryba"
                    },
                    {
                        "name": "Evan Sidrow"
                    },
                    {
                        "name": "Steven H. Ferguson"
                    },
                    {
                        "name": "David J. Yurkowski"
                    },
                    {
                        "name": "Marie Auger-Mth"
                    }
                ],
                "author_detail": {
                    "name": "Marie Auger-Mth"
                },
                "author": "Marie Auger-Mth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17389v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17389v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21198v1",
                "updated": "2025-02-28T16:15:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    15,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:15:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    15,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "AI-Enhanced Self-Triggering for Extensive Air Showers: Performance and\n  FPGA Feasibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Enhanced Self-Triggering for Extensive Air Showers: Performance and\n  FPGA Feasibility"
                },
                "summary": "Cosmic-ray detection with radio antennas has traditionally depended on\nexternal triggers from particle detectors, constraining sensitivity and\nincreasing complexity. Previous attempts at fully standalone, radio-only\ntriggers have often failed under intense radio frequency interference, making\ngenuine air-shower signals difficult to isolate. We present a\nproof-of-principle artificial intelligence-based self-triggering system that\novercomes these limitations. By training a deep learning model on both real\nnoise data and injected cosmic-ray-like pulses, we achieve an exceptionally low\nfalse-positive rate alongside high detection efficiency. Configurable operating\npoints can suppress false positives below 0.01\\% while retaining more than 88\\%\nof genuine signals, and can even eliminate false positives entirely at a modest\nreduction in signal efficiency. This flexibility makes single-station\ncosmic-ray detection feasible without requiring external trigger inputs.\nApplying our approach to real-world noise conditions reduces the initial\nfalse-positive event rate by several orders of magnitude, supporting\nlarge-scale deployments. Extrapolation to dedicated hardware implementations,\nsuch as FPGAs, indicates that sub-\\SI{}{\\micro\\second} inference times are\nachievable, enabling real-time autonomous triggering. These results highlight\nthe transformative potential of artificial intelligence for enhancing radio\ndetection sensitivity and inaugurate a new generation of fully self-triggered\ncosmic-ray observatories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic-ray detection with radio antennas has traditionally depended on\nexternal triggers from particle detectors, constraining sensitivity and\nincreasing complexity. Previous attempts at fully standalone, radio-only\ntriggers have often failed under intense radio frequency interference, making\ngenuine air-shower signals difficult to isolate. We present a\nproof-of-principle artificial intelligence-based self-triggering system that\novercomes these limitations. By training a deep learning model on both real\nnoise data and injected cosmic-ray-like pulses, we achieve an exceptionally low\nfalse-positive rate alongside high detection efficiency. Configurable operating\npoints can suppress false positives below 0.01\\% while retaining more than 88\\%\nof genuine signals, and can even eliminate false positives entirely at a modest\nreduction in signal efficiency. This flexibility makes single-station\ncosmic-ray detection feasible without requiring external trigger inputs.\nApplying our approach to real-world noise conditions reduces the initial\nfalse-positive event rate by several orders of magnitude, supporting\nlarge-scale deployments. Extrapolation to dedicated hardware implementations,\nsuch as FPGAs, indicates that sub-\\SI{}{\\micro\\second} inference times are\nachievable, enabling real-time autonomous triggering. These results highlight\nthe transformative potential of artificial intelligence for enhancing radio\ndetection sensitivity and inaugurate a new generation of fully self-triggered\ncosmic-ray observatories."
                },
                "authors": [
                    {
                        "name": "Qader Dorosti"
                    }
                ],
                "author_detail": {
                    "name": "Qader Dorosti"
                },
                "author": "Qader Dorosti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21196v1",
                "updated": "2025-02-28T16:14:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    14,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:14:16Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    14,
                    16,
                    4,
                    59,
                    0
                ],
                "title": "AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph\n  Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have recently gained attention due to their\nperformance on non-Euclidean data. The use of custom hardware architectures\nproves particularly beneficial for GNNs due to their irregular memory access\npatterns, resulting from the sparse structure of graphs. However, existing FPGA\naccelerators are limited by their double buffering mechanism, which doesn't\naccount for the irregular node distribution in typical graph datasets. To\naddress this, we introduce \\textbf{AMPLE} (Accelerated Message Passing Logic\nEngine), an FPGA accelerator leveraging a new event-driven programming flow. We\ndevelop a mixed-arithmetic architecture, enabling GNN inference to be quantized\nat a node-level granularity. Finally, prefetcher for data and instructions is\nimplemented to optimize off-chip memory access and maximize node parallelism.\nEvaluation on citation and social media graph datasets ranging from $2$K to\n$700$K nodes showed a mean speedup of $243\\times$ and $7.2\\times$ against CPU\nand GPU counterparts, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have recently gained attention due to their\nperformance on non-Euclidean data. The use of custom hardware architectures\nproves particularly beneficial for GNNs due to their irregular memory access\npatterns, resulting from the sparse structure of graphs. However, existing FPGA\naccelerators are limited by their double buffering mechanism, which doesn't\naccount for the irregular node distribution in typical graph datasets. To\naddress this, we introduce \\textbf{AMPLE} (Accelerated Message Passing Logic\nEngine), an FPGA accelerator leveraging a new event-driven programming flow. We\ndevelop a mixed-arithmetic architecture, enabling GNN inference to be quantized\nat a node-level granularity. Finally, prefetcher for data and instructions is\nimplemented to optimize off-chip memory access and maximize node parallelism.\nEvaluation on citation and social media graph datasets ranging from $2$K to\n$700$K nodes showed a mean speedup of $243\\times$ and $7.2\\times$ against CPU\nand GPU counterparts, respectively."
                },
                "authors": [
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "George Constantinides"
                    }
                ],
                "author_detail": {
                    "name": "George Constantinides"
                },
                "author": "George Constantinides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18868v3",
                "updated": "2025-02-28T16:12:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    12,
                    10,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-24T15:53:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    53,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics"
                },
                "summary": "By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nnon-linear structure-preserving latent space and the associated low-dimensional\ndynamics. Our approach enables accurate long-term predictions of the\nhigh-dimensional dynamics of rigid and deformable systems with increased data\nefficiency by inferring interpretable and physically-plausible reduced\nLagrangian models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nnon-linear structure-preserving latent space and the associated low-dimensional\ndynamics. Our approach enables accurate long-term predictions of the\nhigh-dimensional dynamics of rigid and deformable systems with increased data\nefficiency by inferring interpretable and physically-plausible reduced\nLagrangian models."
                },
                "authors": [
                    {
                        "name": "Katharina Friedl"
                    },
                    {
                        "name": "Nomie Jaquier"
                    },
                    {
                        "name": "Jens Lundell"
                    },
                    {
                        "name": "Tamim Asfour"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_comment": "28 pages, 16 figures. Accepted for publication in ICLR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05467v4",
                "updated": "2025-02-28T16:02:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    2,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-06T23:43:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    23,
                    43,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "The BrowserGym Ecosystem for Web Agent Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym Ecosystem for Web Agent Research"
                },
                "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs). Many existing benchmarks suffer from\nfragmentation and inconsistent evaluation methodologies, making it challenging\nto achieve reliable comparisons and reproducible results. In an earlier work,\nDrouin et al. (2024) introduced BrowserGym which aims to solve this by\nproviding a unified, gym-like environment with well-defined observation and\naction spaces, facilitating standardized evaluation across diverse benchmarks.\nWe propose an extended BrowserGym-based ecosystem for web agent research, which\nunifies existing benchmarks from the literature and includes AgentLab, a\ncomplementary framework that aids in agent creation, testing, and analysis. Our\nproposed ecosystem offers flexibility for integrating new benchmarks while\nensuring consistent evaluation and comprehensive experiment management. As a\nsupporting evidence, we conduct the first large-scale, multi-benchmark web\nagent experiment and compare the performance of 6 state-of-the-art LLMs across\n6 popular web agent benchmarks made available in BrowserGym. Among other\nfindings, our results highlight a large discrepancy between OpenAI and\nAnthropic's latests models, with Claude-3.5-Sonnet leading the way on almost\nall benchmarks, except on vision-related tasks where GPT-4o is superior.\nDespite these advancements, our results emphasize that building robust and\nefficient web agents remains a significant challenge, due to the inherent\ncomplexity of real-world web environments and the limitations of current\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs). Many existing benchmarks suffer from\nfragmentation and inconsistent evaluation methodologies, making it challenging\nto achieve reliable comparisons and reproducible results. In an earlier work,\nDrouin et al. (2024) introduced BrowserGym which aims to solve this by\nproviding a unified, gym-like environment with well-defined observation and\naction spaces, facilitating standardized evaluation across diverse benchmarks.\nWe propose an extended BrowserGym-based ecosystem for web agent research, which\nunifies existing benchmarks from the literature and includes AgentLab, a\ncomplementary framework that aids in agent creation, testing, and analysis. Our\nproposed ecosystem offers flexibility for integrating new benchmarks while\nensuring consistent evaluation and comprehensive experiment management. As a\nsupporting evidence, we conduct the first large-scale, multi-benchmark web\nagent experiment and compare the performance of 6 state-of-the-art LLMs across\n6 popular web agent benchmarks made available in BrowserGym. Among other\nfindings, our results highlight a large discrepancy between OpenAI and\nAnthropic's latests models, with Claude-3.5-Sonnet leading the way on almost\nall benchmarks, except on vision-related tasks where GPT-4o is superior.\nDespite these advancements, our results emphasize that building robust and\nefficient web agents remains a significant challenge, due to the inherent\ncomplexity of real-world web environments and the limitations of current\nmodels."
                },
                "authors": [
                    {
                        "name": "Thibault Le Sellier De Chezelles"
                    },
                    {
                        "name": "Maxime Gasse"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "Lo Boisvert"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Rim Assouel"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Lawrence Keunho Jang"
                    },
                    {
                        "name": "Xing Han L"
                    },
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21185v1",
                "updated": "2025-02-28T16:00:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    0,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:00:57Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    0,
                    57,
                    4,
                    59,
                    0
                ],
                "title": "A Survey of Link Prediction in Temporal Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Link Prediction in Temporal Networks"
                },
                "summary": "Temporal networks have gained significant prominence in the past decade for\nmodelling dynamic interactions within complex systems. A key challenge in this\ndomain is Temporal Link Prediction (TLP), which aims to forecast future\nconnections by analysing historical network structures across various\napplications including social network analysis. While existing surveys have\naddressed specific aspects of TLP, they typically lack a comprehensive\nframework that distinguishes between representation and inference methods. This\nsurvey bridges this gap by introducing a novel taxonomy that explicitly\nexamines representation and inference from existing methods, providing a novel\nclassification of approaches for TLP. We analyse how different representation\ntechniques capture temporal and structural dynamics, examining their\ncompatibility with various inference methods for both transductive and\ninductive prediction tasks. Our taxonomy not only clarifies the methodological\nlandscape but also reveals promising unexplored combinations of existing\ntechniques. This taxonomy provides a systematic foundation for emerging\nchallenges in TLP, including model explainability and scalable architectures\nfor complex temporal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal networks have gained significant prominence in the past decade for\nmodelling dynamic interactions within complex systems. A key challenge in this\ndomain is Temporal Link Prediction (TLP), which aims to forecast future\nconnections by analysing historical network structures across various\napplications including social network analysis. While existing surveys have\naddressed specific aspects of TLP, they typically lack a comprehensive\nframework that distinguishes between representation and inference methods. This\nsurvey bridges this gap by introducing a novel taxonomy that explicitly\nexamines representation and inference from existing methods, providing a novel\nclassification of approaches for TLP. We analyse how different representation\ntechniques capture temporal and structural dynamics, examining their\ncompatibility with various inference methods for both transductive and\ninductive prediction tasks. Our taxonomy not only clarifies the methodological\nlandscape but also reveals promising unexplored combinations of existing\ntechniques. This taxonomy provides a systematic foundation for emerging\nchallenges in TLP, including model explainability and scalable architectures\nfor complex temporal networks."
                },
                "authors": [
                    {
                        "name": "Jiafeng Xiong"
                    },
                    {
                        "name": "Ahmad Zareie"
                    },
                    {
                        "name": "Rizos Sakellariou"
                    }
                ],
                "author_detail": {
                    "name": "Rizos Sakellariou"
                },
                "author": "Rizos Sakellariou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13249v2",
                "updated": "2025-02-28T15:54:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    54,
                    8,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-18T19:21:10Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    21,
                    10,
                    1,
                    49,
                    0
                ],
                "title": "Evidence of Replica Symmetry Breaking under the Nishimori conditions in\n  epidemic inference on graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence of Replica Symmetry Breaking under the Nishimori conditions in\n  epidemic inference on graphs"
                },
                "summary": "In Bayesian inference, computing the posterior distribution from the data is\ntypically a non-trivial problem, which usually requires approximations such as\nmean-field approaches or numerical methods, like the Monte Carlo Markov Chain.\nBeing a high-dimensional distribution over a set of correlated variables, the\nposterior distribution can undergo the notorious replica symmetry breaking\ntransition. When it happens, several mean-field methods and virtually every\nMonte Carlo scheme can not provide a reasonable approximation to the posterior\nand its marginals. Replica symmetry is believed to be guaranteed whenever the\ndata is generated with known prior and likelihood distributions, namely under\nthe so-called Nishimori conditions. In this paper, we break this belief, by\nproviding a counter-example showing that, under the Nishimori conditions,\nreplica symmetry breaking arises. Introducing a simple, geometrical model that\ncan be thought of as a patient zero retrieval problem in a highly infectious\nregime of the epidemic Susceptible-Infectious model, we show that under the\nNishimori conditions, there is evidence of replica symmetry breaking. We\nachieve this result by computing the instability of the replica symmetric\ncavity method toward the one step replica symmetry broken phase. The origin of\nthis phenomenon -- replica symmetry breaking under the Nishimori conditions --\nis likely due to the correlated disorder appearing in the epidemic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Bayesian inference, computing the posterior distribution from the data is\ntypically a non-trivial problem, which usually requires approximations such as\nmean-field approaches or numerical methods, like the Monte Carlo Markov Chain.\nBeing a high-dimensional distribution over a set of correlated variables, the\nposterior distribution can undergo the notorious replica symmetry breaking\ntransition. When it happens, several mean-field methods and virtually every\nMonte Carlo scheme can not provide a reasonable approximation to the posterior\nand its marginals. Replica symmetry is believed to be guaranteed whenever the\ndata is generated with known prior and likelihood distributions, namely under\nthe so-called Nishimori conditions. In this paper, we break this belief, by\nproviding a counter-example showing that, under the Nishimori conditions,\nreplica symmetry breaking arises. Introducing a simple, geometrical model that\ncan be thought of as a patient zero retrieval problem in a highly infectious\nregime of the epidemic Susceptible-Infectious model, we show that under the\nNishimori conditions, there is evidence of replica symmetry breaking. We\nachieve this result by computing the instability of the replica symmetric\ncavity method toward the one step replica symmetry broken phase. The origin of\nthis phenomenon -- replica symmetry breaking under the Nishimori conditions --\nis likely due to the correlated disorder appearing in the epidemic models."
                },
                "authors": [
                    {
                        "name": "Alfredo Braunstein"
                    },
                    {
                        "name": "Louise Budzynski"
                    },
                    {
                        "name": "Matteo Mariani"
                    },
                    {
                        "name": "Federico Ricci-Tersenghi"
                    }
                ],
                "author_detail": {
                    "name": "Federico Ricci-Tersenghi"
                },
                "author": "Federico Ricci-Tersenghi",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16239v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16239v3",
                "updated": "2025-02-28T15:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    44,
                    24,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-27T17:35:39Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    35,
                    39,
                    0,
                    27,
                    0
                ],
                "title": "Distilling foundation models for robust and efficient models in digital\n  pathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling foundation models for robust and efficient models in digital\n  pathology"
                },
                "summary": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance."
                },
                "authors": [
                    {
                        "name": "Alexandre Filiot"
                    },
                    {
                        "name": "Nicolas Dop"
                    },
                    {
                        "name": "Oussama Tchita"
                    },
                    {
                        "name": "Auriane Riou"
                    },
                    {
                        "name": "Rmy Dubois"
                    },
                    {
                        "name": "Thomas Peeters"
                    },
                    {
                        "name": "Daria Valter"
                    },
                    {
                        "name": "Marin Scalbert"
                    },
                    {
                        "name": "Charlie Saillard"
                    },
                    {
                        "name": "Genevive Robin"
                    },
                    {
                        "name": "Antoine Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Olivier"
                },
                "author": "Antoine Olivier",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16239v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16239v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11843v3",
                "updated": "2025-02-28T15:41:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    41,
                    0,
                    4,
                    59,
                    0
                ],
                "published": "2024-09-23T08:39:16Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    39,
                    16,
                    0,
                    267,
                    0
                ],
                "title": "From Commands to Prompts: LLM-based Semantic File System for AIOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Commands to Prompts: LLM-based Semantic File System for AIOS"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Chaoji Zuo"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yujie Ren"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Dong Deng"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03664v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03664v4",
                "updated": "2025-02-28T15:33:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    33,
                    10,
                    4,
                    59,
                    0
                ],
                "published": "2024-02-16T10:56:15Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    56,
                    15,
                    4,
                    47,
                    0
                ],
                "title": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine"
                },
                "summary": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results."
                },
                "authors": [
                    {
                        "name": "Erblin Isaku"
                    },
                    {
                        "name": "Christoph Laaber"
                    },
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Thomas Schwitalla"
                    },
                    {
                        "name": "Jan F. Nygrd"
                    }
                ],
                "author_detail": {
                    "name": "Jan F. Nygrd"
                },
                "author": "Jan F. Nygrd",
                "arxiv_comment": "12 pages, 6 figures, 4 tables, 1 listing, revised arguments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03664v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03664v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21143v1",
                "updated": "2025-02-28T15:26:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    26,
                    10,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T15:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    26,
                    10,
                    4,
                    59,
                    0
                ],
                "title": "Variational Bayesian Pseudo-Coreset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Pseudo-Coreset"
                },
                "summary": "The success of deep learning requires large datasets and extensive training,\nwhich can create significant computational challenges. To address these\nchallenges, pseudo-coresets, small learnable datasets that mimic the entire\ndata, have been proposed. Bayesian Neural Networks, which offer predictive\nuncertainty and probabilistic interpretation for deep neural networks, also\nface issues with large-scale datasets due to their high-dimensional parameter\nspace. Prior works on Bayesian Pseudo-Coresets (BPC) attempt to reduce the\ncomputational load for computing weight posterior distribution by a small\nnumber of pseudo-coresets but suffer from memory inefficiency during BPC\ntraining and sub-optimal results. To overcome these limitations, we propose\nVariational Bayesian Pseudo-Coreset (VBPC), a novel approach that utilizes\nvariational inference to efficiently approximate the posterior distribution,\nreducing memory usage and computational costs while improving performance\nacross benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of deep learning requires large datasets and extensive training,\nwhich can create significant computational challenges. To address these\nchallenges, pseudo-coresets, small learnable datasets that mimic the entire\ndata, have been proposed. Bayesian Neural Networks, which offer predictive\nuncertainty and probabilistic interpretation for deep neural networks, also\nface issues with large-scale datasets due to their high-dimensional parameter\nspace. Prior works on Bayesian Pseudo-Coresets (BPC) attempt to reduce the\ncomputational load for computing weight posterior distribution by a small\nnumber of pseudo-coresets but suffer from memory inefficiency during BPC\ntraining and sub-optimal results. To overcome these limitations, we propose\nVariational Bayesian Pseudo-Coreset (VBPC), a novel approach that utilizes\nvariational inference to efficiently approximate the posterior distribution,\nreducing memory usage and computational costs while improving performance\nacross benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Hyungi Lee"
                    },
                    {
                        "name": "Seungyoo Lee"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15296v3",
                "updated": "2025-02-28T15:23:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    23,
                    40,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-25T18:26:39Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    18,
                    26,
                    39,
                    5,
                    25,
                    0
                ],
                "title": "You Only Prune Once: Designing Calibration-Free Model Compression With\n  Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Prune Once: Designing Calibration-Free Model Compression With\n  Policy Learning"
                },
                "summary": "The ever-increasing size of large language models (LLMs) presents significant\nchallenges for deployment due to their heavy computational and memory\nrequirements. Current model pruning techniques attempt to alleviate these\nissues by relying heavily on external calibration datasets to determine which\nparameters to prune or compress, thus limiting their flexibility and\nscalability across different compression ratios. Moreover, these methods often\ncause severe performance degradation, particularly in downstream tasks, when\nsubjected to higher compression rates. In this paper, we propose PruneNet, a\nnovel model compression method that addresses these limitations by\nreformulating model pruning as a policy learning process. PruneNet decouples\nthe pruning process from the model architecture, eliminating the need for\ncalibration datasets. It learns a stochastic pruning policy to assess parameter\nimportance solely based on intrinsic model properties while preserving the\nspectral structure to minimize information loss. PruneNet can compress the\nLLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its\nzero-shot performance with a 30% compression ratio, outperforming existing\nmethods that retain only 75% performance. Furthermore, on complex multitask\nlanguage understanding tasks, PruneNet demonstrates its robustness by\npreserving up to 80% performance of the original model, proving itself a\nsuperior alternative to conventional structured compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing size of large language models (LLMs) presents significant\nchallenges for deployment due to their heavy computational and memory\nrequirements. Current model pruning techniques attempt to alleviate these\nissues by relying heavily on external calibration datasets to determine which\nparameters to prune or compress, thus limiting their flexibility and\nscalability across different compression ratios. Moreover, these methods often\ncause severe performance degradation, particularly in downstream tasks, when\nsubjected to higher compression rates. In this paper, we propose PruneNet, a\nnovel model compression method that addresses these limitations by\nreformulating model pruning as a policy learning process. PruneNet decouples\nthe pruning process from the model architecture, eliminating the need for\ncalibration datasets. It learns a stochastic pruning policy to assess parameter\nimportance solely based on intrinsic model properties while preserving the\nspectral structure to minimize information loss. PruneNet can compress the\nLLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its\nzero-shot performance with a 30% compression ratio, outperforming existing\nmethods that retain only 75% performance. Furthermore, on complex multitask\nlanguage understanding tasks, PruneNet demonstrates its robustness by\npreserving up to 80% performance of the original model, proving itself a\nsuperior alternative to conventional structured compression techniques."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08587v2",
                "updated": "2025-02-28T15:16:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    16,
                    4,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-12T18:47:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    18,
                    47,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nadvancing various fields of research and society. However, the current\ncommunity of LLMs overly focuses on benchmarks for analyzing specific\nfoundational skills (e.g. mathematics and code generation), neglecting an\nall-round evaluation of the computer science field. To bridge this gap, we\nintroduce CS-Bench, the first multilingual (English, Chinese, French, German)\nbenchmark dedicated to evaluating the performance of LLMs in computer science.\nCS-Bench comprises approximately 10K meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing\nvarious task forms and divisions of knowledge and reasoning. Utilizing\nCS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,\nrevealing the relationship between CS performance and model scales. We also\nquantitatively analyze the reasons for failures in existing LLMs and highlight\ndirections for improvements, including knowledge supplementation and\nCS-specific reasoning. Further cross-capability experiments show a high\ncorrelation between LLMs' capabilities in computer science and their abilities\nin mathematics and coding. Moreover, expert LLMs specialized in mathematics and\ncoding also demonstrate strong performances in several CS subfields. Looking\nahead, we envision CS-Bench serving as a cornerstone for LLM applications in\nthe CS field and paving new avenues in assessing LLMs' diverse reasoning\ncapabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nadvancing various fields of research and society. However, the current\ncommunity of LLMs overly focuses on benchmarks for analyzing specific\nfoundational skills (e.g. mathematics and code generation), neglecting an\nall-round evaluation of the computer science field. To bridge this gap, we\nintroduce CS-Bench, the first multilingual (English, Chinese, French, German)\nbenchmark dedicated to evaluating the performance of LLMs in computer science.\nCS-Bench comprises approximately 10K meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing\nvarious task forms and divisions of knowledge and reasoning. Utilizing\nCS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,\nrevealing the relationship between CS performance and model scales. We also\nquantitatively analyze the reasons for failures in existing LLMs and highlight\ndirections for improvements, including knowledge supplementation and\nCS-specific reasoning. Further cross-capability experiments show a high\ncorrelation between LLMs' capabilities in computer science and their abilities\nin mathematics and coding. Moreover, expert LLMs specialized in mathematics and\ncoding also demonstrate strong performances in several CS subfields. Looking\nahead, we envision CS-Bench serving as a cornerstone for LLM applications in\nthe CS field and paving new avenues in assessing LLMs' diverse reasoning\ncapabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench."
                },
                "authors": [
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Muxi Diao"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Runqi Qiao"
                    },
                    {
                        "name": "Zhexu Wang"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Huangxuan Wu"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Zhuoma GongQue"
                    },
                    {
                        "name": "Jianing Yu"
                    },
                    {
                        "name": "Qiuna Tan"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06842v2",
                "updated": "2025-02-28T15:15:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    15,
                    31,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-12T15:21:22Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    21,
                    22,
                    6,
                    12,
                    0
                ],
                "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git"
                },
                "authors": [
                    {
                        "name": "Tianjin Huang"
                    },
                    {
                        "name": "Ziquan Zhu"
                    },
                    {
                        "name": "Gaojie Jin"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15282v2",
                "updated": "2025-02-28T15:11:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    11,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-25T17:31:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    17,
                    31,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AutoG: Towards automatic graph construction from tabular data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoG: Towards automatic graph construction from tabular data"
                },
                "summary": "Recent years have witnessed significant advancements in graph machine\nlearning (GML), with its applications spanning numerous domains. However, the\nfocus of GML has predominantly been on developing powerful models, often\noverlooking a crucial initial step: constructing suitable graphs from common\ndata formats, such as tabular data. This construction process is fundamental to\napplying graph-based models, yet it remains largely understudied and lacks\nformalization. Our research aims to address this gap by formalizing the graph\nconstruction problem and proposing an effective solution. We identify two\ncritical challenges to achieve this goal: 1. The absence of dedicated datasets\nto formalize and evaluate the effectiveness of graph construction methods, and\n2. Existing automatic construction methods can only be applied to some specific\ncases, while tedious human engineering is required to generate high-quality\ngraphs. To tackle these challenges, we present a two-fold contribution. First,\nwe introduce a set of datasets to formalize and evaluate graph construction\nmethods. Second, we propose an LLM-based solution, AutoG, automatically\ngenerating high-quality graph schemas without human intervention. The\nexperimental results demonstrate that the quality of constructed graphs is\ncritical to downstream task performance, and AutoG can generate high-quality\ngraphs that rival those produced by human experts. Our code can be accessible\nfrom https://github.com/amazon-science/Automatic-Table-to-Graph-Generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed significant advancements in graph machine\nlearning (GML), with its applications spanning numerous domains. However, the\nfocus of GML has predominantly been on developing powerful models, often\noverlooking a crucial initial step: constructing suitable graphs from common\ndata formats, such as tabular data. This construction process is fundamental to\napplying graph-based models, yet it remains largely understudied and lacks\nformalization. Our research aims to address this gap by formalizing the graph\nconstruction problem and proposing an effective solution. We identify two\ncritical challenges to achieve this goal: 1. The absence of dedicated datasets\nto formalize and evaluate the effectiveness of graph construction methods, and\n2. Existing automatic construction methods can only be applied to some specific\ncases, while tedious human engineering is required to generate high-quality\ngraphs. To tackle these challenges, we present a two-fold contribution. First,\nwe introduce a set of datasets to formalize and evaluate graph construction\nmethods. Second, we propose an LLM-based solution, AutoG, automatically\ngenerating high-quality graph schemas without human intervention. The\nexperimental results demonstrate that the quality of constructed graphs is\ncritical to downstream task performance, and AutoG can generate high-quality\ngraphs that rival those produced by human experts. Our code can be accessible\nfrom https://github.com/amazon-science/Automatic-Table-to-Graph-Generation."
                },
                "authors": [
                    {
                        "name": "Zhikai Chen"
                    },
                    {
                        "name": "Han Xie"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Xiang song"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    }
                ],
                "author_detail": {
                    "name": "George Karypis"
                },
                "author": "George Karypis",
                "arxiv_comment": "camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21130v1",
                "updated": "2025-02-28T15:10:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    10,
                    7,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T15:10:07Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    10,
                    7,
                    4,
                    59,
                    0
                ],
                "title": "Fast and Accurate Gigapixel Pathological Image Classification with\n  Hierarchical Distillation Multi-Instance Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Gigapixel Pathological Image Classification with\n  Hierarchical Distillation Multi-Instance Learning"
                },
                "summary": "Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset."
                },
                "authors": [
                    {
                        "name": "Jiuyang Dong"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Jiahan Li"
                    },
                    {
                        "name": "Yongbing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongbing Zhang"
                },
                "author": "Yongbing Zhang",
                "arxiv_comment": "11 pages, 4 figures, accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13983v2",
                "updated": "2025-02-28T15:07:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    7,
                    55,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-23T06:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    57,
                    24,
                    3,
                    23,
                    0
                ],
                "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. AdEval extracts key knowledge points and main ideas to\nalign dynamically generated questions with static data's core concepts. It also\nleverages online search to provide detailed explanations of related knowledge\npoints, thereby creating high-quality evaluation samples with robust knowledge\nsupport. Furthermore, AdEval incorporates mechanisms to control the number and\ncomplexity of questions, enabling dynamic alignment and flexible adjustment.\nThis ensures that the generated questions align with the complexity of static\ndata while supporting varied complexity levels. Based on Bloom's taxonomy,\nAdEval conducts a multi-dimensional evaluation of LLMs across six cognitive\nlevels: remembering, understanding, applying, analyzing, evaluating, and\ncreating. Experimental results on multiple datasets demonstrate that AdEval\neffectively reduces the impact of data contamination on evaluation outcomes,\nenhancing both the fairness and reliability of the evaluation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. AdEval extracts key knowledge points and main ideas to\nalign dynamically generated questions with static data's core concepts. It also\nleverages online search to provide detailed explanations of related knowledge\npoints, thereby creating high-quality evaluation samples with robust knowledge\nsupport. Furthermore, AdEval incorporates mechanisms to control the number and\ncomplexity of questions, enabling dynamic alignment and flexible adjustment.\nThis ensures that the generated questions align with the complexity of static\ndata while supporting varied complexity levels. Based on Bloom's taxonomy,\nAdEval conducts a multi-dimensional evaluation of LLMs across six cognitive\nlevels: remembering, understanding, applying, analyzing, evaluating, and\ncreating. Experimental results on multiple datasets demonstrate that AdEval\neffectively reduces the impact of data contamination on evaluation outcomes,\nenhancing both the fairness and reliability of the evaluation process."
                },
                "authors": [
                    {
                        "name": "Yang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yang Fan"
                },
                "author": "Yang Fan",
                "arxiv_comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19104v2",
                "updated": "2025-02-28T15:00:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    0,
                    1,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-26T12:46:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine\n  Translation"
                },
                "summary": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german."
                },
                "authors": [
                    {
                        "name": "Michelle Kappl"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Kappl"
                },
                "author": "Michelle Kappl",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21119v1",
                "updated": "2025-02-28T14:55:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    55,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:55:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    55,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "Detection of the 2175 UV Bump at z>7: Evidence for Rapid Dust\n  Evolution in a Merging Reionisation-Era Galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of the 2175 UV Bump at z>7: Evidence for Rapid Dust\n  Evolution in a Merging Reionisation-Era Galaxy"
                },
                "summary": "Dust is a fundamental component of the interstellar medium (ISM) within\ngalaxies, as dust grains are highly efficient absorbers of UV and optical\nphotons. Accurately quantifying this obscuration is crucial for interpreting\ngalaxy spectral energy distributions (SEDs). The extinction curves in the Milky\nWay (MW) and Large Magellanic Cloud (LMC) exhibit a strong feature known as the\n2175A UV bump, most often attributed to small carbonaceous dust grains. This\nfeature was recently detected in faint galaxies out to z~7 suggesting rapid\nformation channels. Here we report the detection of a strong UV bump in a\nluminous Lyman-break galaxy at z = 7.11235, GNWY-7379420231, through\nobservations taken as part of the NIRSpec Wide GTO survey. We fit a dust\nattenuation curve that is consistent with the MW extinction curve within\n1{\\sigma}, in a galaxy just ~700 Myr after the Big Bang. From the integrated\nspectrum, we infer a young mass-weighted age (t* ~ 22-59 Myr) for this galaxy,\nhowever spatially resolved SED fitting unveils the presence of an older stellar\npopulation (t* ~ 252 Myr). Furthermore, morphological analysis provides\nevidence for a potential merger. The underlying older stellar population\nsuggests the merging system could be pre-enriched, with the dust illuminated by\na merger-induced starburst. Moreover, turbulence driven by stellar feedback in\nthis bursty region may be driving PAH formation through top-down shattering.\nThe presence of a UV bump in GNWY-7379420231 solidifies growing evidence for\nthe rapid evolution of dust properties within the first billion years of cosmic\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dust is a fundamental component of the interstellar medium (ISM) within\ngalaxies, as dust grains are highly efficient absorbers of UV and optical\nphotons. Accurately quantifying this obscuration is crucial for interpreting\ngalaxy spectral energy distributions (SEDs). The extinction curves in the Milky\nWay (MW) and Large Magellanic Cloud (LMC) exhibit a strong feature known as the\n2175A UV bump, most often attributed to small carbonaceous dust grains. This\nfeature was recently detected in faint galaxies out to z~7 suggesting rapid\nformation channels. Here we report the detection of a strong UV bump in a\nluminous Lyman-break galaxy at z = 7.11235, GNWY-7379420231, through\nobservations taken as part of the NIRSpec Wide GTO survey. We fit a dust\nattenuation curve that is consistent with the MW extinction curve within\n1{\\sigma}, in a galaxy just ~700 Myr after the Big Bang. From the integrated\nspectrum, we infer a young mass-weighted age (t* ~ 22-59 Myr) for this galaxy,\nhowever spatially resolved SED fitting unveils the presence of an older stellar\npopulation (t* ~ 252 Myr). Furthermore, morphological analysis provides\nevidence for a potential merger. The underlying older stellar population\nsuggests the merging system could be pre-enriched, with the dust illuminated by\na merger-induced starburst. Moreover, turbulence driven by stellar feedback in\nthis bursty region may be driving PAH formation through top-down shattering.\nThe presence of a UV bump in GNWY-7379420231 solidifies growing evidence for\nthe rapid evolution of dust properties within the first billion years of cosmic\ntime."
                },
                "authors": [
                    {
                        "name": "Katherine Ormerod"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Marijn Franx"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Pierluigi Rinaldi"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Tacchella"
                },
                "author": "Sandro Tacchella",
                "arxiv_comment": "Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21112v1",
                "updated": "2025-02-28T14:52:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    52,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    52,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts"
                },
                "summary": "The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques."
                },
                "authors": [
                    {
                        "name": "Mattia Birti"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Andrea Maurino"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Maurino"
                },
                "author": "Andrea Maurino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18540v2",
                "updated": "2025-02-28T14:49:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    49,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2024-05-28T19:16:17Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    19,
                    16,
                    17,
                    1,
                    149,
                    0
                ],
                "title": "Learning diverse attacks on large language models for robust red-teaming\n  and safety tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning diverse attacks on large language models for robust red-teaming\n  and safety tuning"
                },
                "summary": "Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches."
                },
                "authors": [
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Lynn Cherif"
                    },
                    {
                        "name": "David Dobre"
                    },
                    {
                        "name": "Juho Lee"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Moksh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Moksh Jain"
                },
                "author": "Moksh Jain",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21108v1",
                "updated": "2025-02-28T14:46:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    46,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    46,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Large Language Model-Based Benchmarking Experiment Settings for\n  Evolutionary Multi-Objective Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Benchmarking Experiment Settings for\n  Evolutionary Multi-Objective Optimization"
                },
                "summary": "When we manually design an evolutionary optimization algorithm, we implicitly\nor explicitly assume a set of target optimization problems. In the case of\nautomated algorithm design, target optimization problems are usually explicitly\nshown. Recently, the use of large language models (LLMs) for the design of\nevolutionary multi-objective optimization (EMO) algorithms have been examined\nin some studies. In those studies, target multi-objective problems are not\nalways explicitly shown. It is well known in the EMO community that the\nperformance evaluation results of EMO algorithms depend on not only test\nproblems but also many other factors such as performance indicators, reference\npoint, termination condition, and population size. Thus, it is likely that the\ndesigned EMO algorithms by LLMs depends on those factors. In this paper, we try\nto examine the implicit assumption about the performance comparison of EMO\nalgorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking\nexperiment of EMO algorithms. Our experiments show that LLMs often suggest\nclassical benchmark settings: Performance examination of NSGA-II, MOEA/D and\nNSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter\nspecifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When we manually design an evolutionary optimization algorithm, we implicitly\nor explicitly assume a set of target optimization problems. In the case of\nautomated algorithm design, target optimization problems are usually explicitly\nshown. Recently, the use of large language models (LLMs) for the design of\nevolutionary multi-objective optimization (EMO) algorithms have been examined\nin some studies. In those studies, target multi-objective problems are not\nalways explicitly shown. It is well known in the EMO community that the\nperformance evaluation results of EMO algorithms depend on not only test\nproblems but also many other factors such as performance indicators, reference\npoint, termination condition, and population size. Thus, it is likely that the\ndesigned EMO algorithms by LLMs depends on those factors. In this paper, we try\nto examine the implicit assumption about the performance comparison of EMO\nalgorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking\nexperiment of EMO algorithms. Our experiments show that LLMs often suggest\nclassical benchmark settings: Performance examination of NSGA-II, MOEA/D and\nNSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter\nspecifications."
                },
                "authors": [
                    {
                        "name": "Lie Meng Pang"
                    },
                    {
                        "name": "Hisao Ishibuchi"
                    }
                ],
                "author_detail": {
                    "name": "Hisao Ishibuchi"
                },
                "author": "Hisao Ishibuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21106v1",
                "updated": "2025-02-28T14:44:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    44,
                    55,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:44:55Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    44,
                    55,
                    4,
                    59,
                    0
                ],
                "title": "A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma\n  Triage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma\n  Triage"
                },
                "summary": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology."
                },
                "authors": [
                    {
                        "name": "Youngjin Yoo"
                    },
                    {
                        "name": "Bogdan Georgescu"
                    },
                    {
                        "name": "Yanbo Zhang"
                    },
                    {
                        "name": "Sasa Grbic"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Gabriela D. Aldea"
                    },
                    {
                        "name": "Thomas J. Re"
                    },
                    {
                        "name": "Jyotipriya Das"
                    },
                    {
                        "name": "Poikavila Ullaskrishnan"
                    },
                    {
                        "name": "Eva Eibenberger"
                    },
                    {
                        "name": "Andrei Chekkoury"
                    },
                    {
                        "name": "Uttam K. Bodanapally"
                    },
                    {
                        "name": "Savvas Nicolaou"
                    },
                    {
                        "name": "Pina C. Sanelli"
                    },
                    {
                        "name": "Thomas J. Schroeppel"
                    },
                    {
                        "name": "Yvonne W. Lui"
                    },
                    {
                        "name": "Eli Gibson"
                    }
                ],
                "author_detail": {
                    "name": "Eli Gibson"
                },
                "author": "Eli Gibson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04755v4",
                "updated": "2025-02-28T14:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    41,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-07T08:54:55Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    54,
                    55,
                    4,
                    159,
                    0
                ],
                "title": "LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses"
                },
                "summary": "Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing that our\nadversarially perturbed prompts 1) are indistinguishable from unaltered prompts\nby humans, 2) push LLMs to recommend target concepts more often, and 3) make\nusers more likely to notice target concepts, all without arousing suspicion.\nThe practicality of this attack has the potential to undermine user autonomy.\nAmong other measures, we recommend implementing warnings against using prompts\nfrom untrusted parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing that our\nadversarially perturbed prompts 1) are indistinguishable from unaltered prompts\nby humans, 2) push LLMs to recommend target concepts more often, and 3) make\nusers more likely to notice target concepts, all without arousing suspicion.\nThe practicality of this attack has the potential to undermine user autonomy.\nAmong other measures, we recommend implementing warnings against using prompts\nfrom untrusted parties."
                },
                "authors": [
                    {
                        "name": "Weiran Lin"
                    },
                    {
                        "name": "Anna Gerchanovsky"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Lujo Bauer"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Zifan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zifan Wang"
                },
                "author": "Zifan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03168v2",
                "updated": "2025-02-28T14:39:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    39,
                    17,
                    4,
                    59,
                    0
                ],
                "published": "2024-07-03T14:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    41,
                    39,
                    2,
                    185,
                    0
                ],
                "title": "LivePortrait: Efficient Portrait Animation with Stitching and\n  Retargeting Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LivePortrait: Efficient Portrait Animation with Stitching and\n  Retargeting Control"
                },
                "summary": "Portrait Animation aims to synthesize a lifelike video from a single source\nimage, using it as an appearance reference, with motion (i.e., facial\nexpressions and head pose) derived from a driving video, audio, text, or\ngeneration. Instead of following mainstream diffusion-based methods, we explore\nand extend the potential of the implicit-keypoint-based framework, which\neffectively balances computational efficiency and controllability. Building\nupon this, we develop a video-driven portrait animation framework named\nLivePortrait with a focus on better generalization, controllability, and\nefficiency for practical usage. To enhance the generation quality and\ngeneralization ability, we scale up the training data to about 69 million\nhigh-quality frames, adopt a mixed image-video training strategy, upgrade the\nnetwork architecture, and design better motion transformation and optimization\nobjectives. Additionally, we discover that compact implicit keypoints can\neffectively represent a kind of blendshapes and meticulously propose a\nstitching and two retargeting modules, which utilize a small MLP with\nnegligible computational overhead, to enhance the controllability. Experimental\nresults demonstrate the efficacy of our framework even compared to\ndiffusion-based methods. The generation speed remarkably reaches 12.8ms on an\nRTX 4090 GPU with PyTorch. The inference code and models are available at\nhttps://github.com/KwaiVGI/LivePortrait",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portrait Animation aims to synthesize a lifelike video from a single source\nimage, using it as an appearance reference, with motion (i.e., facial\nexpressions and head pose) derived from a driving video, audio, text, or\ngeneration. Instead of following mainstream diffusion-based methods, we explore\nand extend the potential of the implicit-keypoint-based framework, which\neffectively balances computational efficiency and controllability. Building\nupon this, we develop a video-driven portrait animation framework named\nLivePortrait with a focus on better generalization, controllability, and\nefficiency for practical usage. To enhance the generation quality and\ngeneralization ability, we scale up the training data to about 69 million\nhigh-quality frames, adopt a mixed image-video training strategy, upgrade the\nnetwork architecture, and design better motion transformation and optimization\nobjectives. Additionally, we discover that compact implicit keypoints can\neffectively represent a kind of blendshapes and meticulously propose a\nstitching and two retargeting modules, which utilize a small MLP with\nnegligible computational overhead, to enhance the controllability. Experimental\nresults demonstrate the efficacy of our framework even compared to\ndiffusion-based methods. The generation speed remarkably reaches 12.8ms on an\nRTX 4090 GPU with PyTorch. The inference code and models are available at\nhttps://github.com/KwaiVGI/LivePortrait"
                },
                "authors": [
                    {
                        "name": "Jianzhu Guo"
                    },
                    {
                        "name": "Dingyun Zhang"
                    },
                    {
                        "name": "Xiaoqiang Liu"
                    },
                    {
                        "name": "Zhizhou Zhong"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhang"
                },
                "author": "Di Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21098v1",
                "updated": "2025-02-28T14:36:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    36,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:36:57Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    36,
                    57,
                    4,
                    59,
                    0
                ],
                "title": "Re-evaluating Theory of Mind evaluation in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Theory of Mind evaluation in large language models"
                },
                "summary": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition."
                },
                "authors": [
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Felix Sosa"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11540v2",
                "updated": "2025-02-28T14:35:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    35,
                    58,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-15T12:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    57,
                    1,
                    289,
                    0
                ],
                "title": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models"
                },
                "summary": "Federated Learning (FL) enables privacy-preserving collaborative instruction\ntuning of large language models (LLMs) by leveraging massively distributed\ndata. However, the decentralized nature of FL exacerbates data quality\nchallenges, as local clients lack global visibility to filter noisy or\nlow-quality samples before training. To resolve this issue, we propose FedDQC,\na novel federated instruction tuning framework with dynamic data quality\ncontrol. Our approach introduces two key innovations. First, we propose\ninstruction-response alignment (IRA), an efficient client-side metric for\nquality evaluation requiring only low-cost inference. We validate that\nhigher-IRA data corresponds to more relevant and easier-to-learn\nquestion-answer pairs. Second, mirroring the human easy-to-hard knowledge\nacquisition process, we design a quality-aware hierarchical FL training\nframework, where the LLM is progressively fine-tuned from high- to low-IRA data\nin a collaborative manner. The framework also supports adaptive data quality\nassessment at each hierarchy, enabling dynamic adjustments throughout the\ntraining process. Extensive experiments on synthetic and real-world datasets\nshow that our method significantly improves LLM performance on mixed-quality\ndata in FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables privacy-preserving collaborative instruction\ntuning of large language models (LLMs) by leveraging massively distributed\ndata. However, the decentralized nature of FL exacerbates data quality\nchallenges, as local clients lack global visibility to filter noisy or\nlow-quality samples before training. To resolve this issue, we propose FedDQC,\na novel federated instruction tuning framework with dynamic data quality\ncontrol. Our approach introduces two key innovations. First, we propose\ninstruction-response alignment (IRA), an efficient client-side metric for\nquality evaluation requiring only low-cost inference. We validate that\nhigher-IRA data corresponds to more relevant and easier-to-learn\nquestion-answer pairs. Second, mirroring the human easy-to-hard knowledge\nacquisition process, we design a quality-aware hierarchical FL training\nframework, where the LLM is progressively fine-tuned from high- to low-IRA data\nin a collaborative manner. The framework also supports adaptive data quality\nassessment at each hierarchy, enabling dynamic adjustments throughout the\ntraining process. Extensive experiments on synthetic and real-world datasets\nshow that our method significantly improves LLM performance on mixed-quality\ndata in FL."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Fengting Yuchi"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21092v1",
                "updated": "2025-02-28T14:31:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    31,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:31:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    31,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "An LLM-based Delphi Study to Predict GenAI Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Delphi Study to Predict GenAI Evolution"
                },
                "summary": "Predicting the future trajectory of complex and rapidly evolving systems\nremains a significant challenge, particularly in domains where data is scarce\nor unreliable. This study introduces a novel approach to qualitative\nforecasting by leveraging Large Language Models to conduct Delphi studies. The\nmethodology was applied to explore the future evolution of Generative\nArtificial Intelligence, revealing insights into key factors such as\ngeopolitical tensions, economic disparities, regulatory frameworks, and ethical\nconsiderations. The results highlight how LLM-based Delphi studies can\nfacilitate structured scenario analysis, capturing diverse perspectives while\nmitigating issues such as respondent fatigue. However, limitations emerge in\nterms of knowledge cutoffs, inherent biases, and sensitivity to initial\nconditions. While the approach provides an innovative means for structured\nforesight, this method could be also considered as a novel form of reasoning.\nfurther research is needed to refine its ability to manage heterogeneity,\nimprove reliability, and integrate external data sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the future trajectory of complex and rapidly evolving systems\nremains a significant challenge, particularly in domains where data is scarce\nor unreliable. This study introduces a novel approach to qualitative\nforecasting by leveraging Large Language Models to conduct Delphi studies. The\nmethodology was applied to explore the future evolution of Generative\nArtificial Intelligence, revealing insights into key factors such as\ngeopolitical tensions, economic disparities, regulatory frameworks, and ethical\nconsiderations. The results highlight how LLM-based Delphi studies can\nfacilitate structured scenario analysis, capturing diverse perspectives while\nmitigating issues such as respondent fatigue. However, limitations emerge in\nterms of knowledge cutoffs, inherent biases, and sensitivity to initial\nconditions. While the approach provides an innovative means for structured\nforesight, this method could be also considered as a novel form of reasoning.\nfurther research is needed to refine its ability to manage heterogeneity,\nimprove reliability, and integrate external data sources."
                },
                "authors": [
                    {
                        "name": "Francesco Bertolotti"
                    },
                    {
                        "name": "Luca Mari"
                    }
                ],
                "author_detail": {
                    "name": "Luca Mari"
                },
                "author": "Luca Mari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21087v1",
                "updated": "2025-02-28T14:26:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    26,
                    47,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:26:47Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    26,
                    47,
                    4,
                    59,
                    0
                ],
                "title": "PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured\n  Data with Text and Relational Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured\n  Data with Text and Relational Information"
                },
                "summary": "Large language models (LLMs) have shown impressive abilities in answering\nquestions across various domains, but they often encounter hallucination issues\non questions that require professional and up-to-date knowledge. To address\nthis limitation, retrieval-augmented generation (RAG) techniques have been\nproposed, which retrieve relevant information from external sources to inform\ntheir responses. However, existing RAG methods typically focus on a single type\nof external data, such as vectorized text database or knowledge graphs, and\ncannot well handle real-world questions on semi-structured data containing both\ntext and relational information. To bridge this gap, we introduce PASemiQA, a\nnovel approach that jointly leverages text and relational information in\nsemi-structured data to answer questions. PASemiQA first generates a plan to\nidentify relevant text and relational information to answer the question in\nsemi-structured data, and then uses an LLM agent to traverse the\nsemi-structured data and extract necessary information. Our empirical results\ndemonstrate the effectiveness of PASemiQA across different semi-structured\ndatasets from various domains, showcasing its potential to improve the accuracy\nand reliability of question answering systems on semi-structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive abilities in answering\nquestions across various domains, but they often encounter hallucination issues\non questions that require professional and up-to-date knowledge. To address\nthis limitation, retrieval-augmented generation (RAG) techniques have been\nproposed, which retrieve relevant information from external sources to inform\ntheir responses. However, existing RAG methods typically focus on a single type\nof external data, such as vectorized text database or knowledge graphs, and\ncannot well handle real-world questions on semi-structured data containing both\ntext and relational information. To bridge this gap, we introduce PASemiQA, a\nnovel approach that jointly leverages text and relational information in\nsemi-structured data to answer questions. PASemiQA first generates a plan to\nidentify relevant text and relational information to answer the question in\nsemi-structured data, and then uses an LLM agent to traverse the\nsemi-structured data and extract necessary information. Our empirical results\ndemonstrate the effectiveness of PASemiQA across different semi-structured\ndatasets from various domains, showcasing its potential to improve the accuracy\nand reliability of question answering systems on semi-structured data."
                },
                "authors": [
                    {
                        "name": "Hansi Yang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07594v3",
                "updated": "2025-02-28T14:24:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    24,
                    39,
                    4,
                    59,
                    0
                ],
                "published": "2024-02-12T11:48:54Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    11,
                    48,
                    54,
                    0,
                    43,
                    0
                ],
                "title": "Zero-shot Imputation with Foundation Inference Models for Dynamical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Imputation with Foundation Inference Models for Dynamical\n  Systems"
                },
                "summary": "Dynamical systems governed by ordinary differential equations (ODEs) serve as\nmodels for a vast number of natural and social phenomena. In this work, we\noffer a fresh perspective on the classical problem of imputing missing time\nseries data, whose underlying dynamics are assumed to be determined by ODEs.\nSpecifically, we revisit ideas from amortized inference and neural operators,\nand propose a novel supervised learning framework for zero-shot time series\nimputation, through parametric functions satisfying some (hidden) ODEs. Our\nproposal consists of two components. First, a broad probability distribution\nover the space of ODE solutions, observation times and noise mechanisms, with\nwhich we generate a large, synthetic dataset of (hidden) ODE solutions, along\nwith their noisy and sparse observations. Second, a neural recognition model\nthat is trained offline, to map the generated time series onto the spaces of\ninitial conditions and time derivatives of the (hidden) ODE solutions, which we\nthen integrate to impute the missing data. We empirically demonstrate that one\nand the same (pretrained) recognition model can perform zero-shot imputation\nacross 63 distinct time series with missing values, each sampled from widely\ndifferent dynamical systems. Likewise, we demonstrate that it can perform\nzero-shot imputation of missing high-dimensional data in 10 vastly different\nsettings, spanning human motion, air quality, traffic and electricity studies,\nas well as Navier-Stokes simulations -- without requiring any fine-tuning. What\nis more, our proposal often outperforms state-of-the-art methods, which are\ntrained on the target datasets.\n  Our pretrained model, repository and tutorials are available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical systems governed by ordinary differential equations (ODEs) serve as\nmodels for a vast number of natural and social phenomena. In this work, we\noffer a fresh perspective on the classical problem of imputing missing time\nseries data, whose underlying dynamics are assumed to be determined by ODEs.\nSpecifically, we revisit ideas from amortized inference and neural operators,\nand propose a novel supervised learning framework for zero-shot time series\nimputation, through parametric functions satisfying some (hidden) ODEs. Our\nproposal consists of two components. First, a broad probability distribution\nover the space of ODE solutions, observation times and noise mechanisms, with\nwhich we generate a large, synthetic dataset of (hidden) ODE solutions, along\nwith their noisy and sparse observations. Second, a neural recognition model\nthat is trained offline, to map the generated time series onto the spaces of\ninitial conditions and time derivatives of the (hidden) ODE solutions, which we\nthen integrate to impute the missing data. We empirically demonstrate that one\nand the same (pretrained) recognition model can perform zero-shot imputation\nacross 63 distinct time series with missing values, each sampled from widely\ndifferent dynamical systems. Likewise, we demonstrate that it can perform\nzero-shot imputation of missing high-dimensional data in 10 vastly different\nsettings, spanning human motion, air quality, traffic and electricity studies,\nas well as Navier-Stokes simulations -- without requiring any fine-tuning. What\nis more, our proposal often outperforms state-of-the-art methods, which are\ntrained on the target datasets.\n  Our pretrained model, repository and tutorials are available online."
                },
                "authors": [
                    {
                        "name": "Patrick Seifner"
                    },
                    {
                        "name": "Kostadin Cvejoski"
                    },
                    {
                        "name": "Antonia Krner"
                    },
                    {
                        "name": "Ramss J. Snchez"
                    }
                ],
                "author_detail": {
                    "name": "Ramss J. Snchez"
                },
                "author": "Ramss J. Snchez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18934v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18934v3",
                "updated": "2025-02-28T14:23:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    23,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-26T08:36:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    36,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Kanana: Compute-efficient Bilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kanana: Compute-efficient Bilingual Language Models"
                },
                "summary": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models."
                },
                "authors": [
                    {
                        "name": "Kanana LLM Team"
                    },
                    {
                        "name": "Yunju Bak"
                    },
                    {
                        "name": "Hojin Lee"
                    },
                    {
                        "name": "Minho Ryu"
                    },
                    {
                        "name": "Jiyeon Ham"
                    },
                    {
                        "name": "Seungjae Jung"
                    },
                    {
                        "name": "Daniel Wontae Nam"
                    },
                    {
                        "name": "Taegyeong Eo"
                    },
                    {
                        "name": "Donghun Lee"
                    },
                    {
                        "name": "Doohae Jung"
                    },
                    {
                        "name": "Boseop Kim"
                    },
                    {
                        "name": "Nayeon Kim"
                    },
                    {
                        "name": "Jaesun Park"
                    },
                    {
                        "name": "Hyunho Kim"
                    },
                    {
                        "name": "Hyunwoong Ko"
                    },
                    {
                        "name": "Changmin Lee"
                    },
                    {
                        "name": "Kyoung-Woon On"
                    },
                    {
                        "name": "Seulye Baeg"
                    },
                    {
                        "name": "Junrae Cho"
                    },
                    {
                        "name": "Sunghee Jung"
                    },
                    {
                        "name": "Jieun Kang"
                    },
                    {
                        "name": "EungGyun Kim"
                    },
                    {
                        "name": "Eunhwa Kim"
                    },
                    {
                        "name": "Byeongil Ko"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Minchul Lee"
                    },
                    {
                        "name": "Miok Lee"
                    },
                    {
                        "name": "Shinbok Lee"
                    },
                    {
                        "name": "Gaeun Seo"
                    }
                ],
                "author_detail": {
                    "name": "Gaeun Seo"
                },
                "author": "Gaeun Seo",
                "arxiv_comment": "40 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18934v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18934v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21075v1",
                "updated": "2025-02-28T14:08:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    8,
                    30,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:08:30Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    8,
                    30,
                    4,
                    59,
                    0
                ],
                "title": "Spatial Reasoning with Denoising Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Reasoning with Denoising Models"
                },
                "summary": "We introduce Spatial Reasoning Models (SRMs), a framework to perform\nreasoning over sets of continuous variables via denoising generative models.\nSRMs infer continuous representations on a set of unobserved variables, given\nobservations on observed variables. Current generative models on spatial\ndomains, such as diffusion and flow matching models, often collapse to\nhallucination in case of complex distributions. To measure this, we introduce a\nset of benchmark tasks that test the quality of complex reasoning in generative\nmodels and can quantify hallucination. The SRM framework allows to report key\nfindings about importance of sequentialization in generation, the associated\norder, as well as the sampling strategies during training. It demonstrates, for\nthe first time, that order of generation can successfully be predicted by the\ndenoising network itself. Using these findings, we can increase the accuracy of\nspecific reasoning tasks from <1% to >50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Spatial Reasoning Models (SRMs), a framework to perform\nreasoning over sets of continuous variables via denoising generative models.\nSRMs infer continuous representations on a set of unobserved variables, given\nobservations on observed variables. Current generative models on spatial\ndomains, such as diffusion and flow matching models, often collapse to\nhallucination in case of complex distributions. To measure this, we introduce a\nset of benchmark tasks that test the quality of complex reasoning in generative\nmodels and can quantify hallucination. The SRM framework allows to report key\nfindings about importance of sequentialization in generation, the associated\norder, as well as the sampling strategies during training. It demonstrates, for\nthe first time, that order of generation can successfully be predicted by the\ndenoising network itself. Using these findings, we can increase the accuracy of\nspecific reasoning tasks from <1% to >50%."
                },
                "authors": [
                    {
                        "name": "Christopher Wewer"
                    },
                    {
                        "name": "Bart Pogodzinski"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Jan Eric Lenssen"
                    }
                ],
                "author_detail": {
                    "name": "Jan Eric Lenssen"
                },
                "author": "Jan Eric Lenssen",
                "arxiv_comment": "Project website: https://geometric-rl.mpi-inf.mpg.de/srm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21074v1",
                "updated": "2025-02-28T14:07:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    7,
                    48,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:07:48Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    7,
                    48,
                    4,
                    59,
                    0
                ],
                "title": "CODI: Compressing Chain-of-Thought into Continuous Space via\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CODI: Compressing Chain-of-Thought into Continuous Space via\n  Self-Distillation"
                },
                "summary": "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT."
                },
                "authors": [
                    {
                        "name": "Zhenyi Shen"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21068v1",
                "updated": "2025-02-28T14:03:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    3,
                    53,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:03:53Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    3,
                    53,
                    4,
                    59,
                    0
                ],
                "title": "GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping"
                },
                "summary": "GUI prototyping serves as one of the most valuable techniques for enhancing\nthe elicitation of requirements and facilitating the visualization and\nrefinement of customer needs. While GUI prototyping has a positive impact on\nthe software development process, it simultaneously demands significant effort\nand resources. The emergence of Large Language Models (LLMs) with their\nimpressive code generation capabilities offers a promising approach for\nautomating GUI prototyping. Despite their potential, there is a gap between\ncurrent LLM-based prototyping solutions and traditional user-based GUI\nprototyping approaches which provide visual representations of the GUI\nprototypes and direct editing functionality. In contrast, LLMs and related\ngenerative approaches merely produce text sequences or non-editable image\noutput, which lacks both mentioned aspects and therefore impede supporting GUI\nprototyping. Moreover, minor changes requested by the user typically lead to an\ninefficient regeneration of the entire GUI prototype when using LLMs directly.\nIn this work, we propose GUIDE, a novel LLM-driven GUI generation decomposition\napproach seamlessly integrated into the popular prototyping framework Figma.\nOur approach initially decomposes high-level GUI descriptions into\nfine-granular GUI requirements, which are subsequently translated into Material\nDesign GUI prototypes, enabling higher controllability and more efficient\nadaption of changes. To efficiently conduct prompting-based generation of\nMaterial Design GUI prototypes, we propose a retrieval-augmented generation\napproach to integrate the component library. Our preliminary evaluation\ndemonstrates the effectiveness of GUIDE in bridging the gap between LLM\ngeneration capabilities and traditional GUI prototyping workflows, offering a\nmore effective and controlled user-based approach to LLM-driven GUI\nprototyping. Video: https://youtu.be/C9RbhMxqpTU",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI prototyping serves as one of the most valuable techniques for enhancing\nthe elicitation of requirements and facilitating the visualization and\nrefinement of customer needs. While GUI prototyping has a positive impact on\nthe software development process, it simultaneously demands significant effort\nand resources. The emergence of Large Language Models (LLMs) with their\nimpressive code generation capabilities offers a promising approach for\nautomating GUI prototyping. Despite their potential, there is a gap between\ncurrent LLM-based prototyping solutions and traditional user-based GUI\nprototyping approaches which provide visual representations of the GUI\nprototypes and direct editing functionality. In contrast, LLMs and related\ngenerative approaches merely produce text sequences or non-editable image\noutput, which lacks both mentioned aspects and therefore impede supporting GUI\nprototyping. Moreover, minor changes requested by the user typically lead to an\ninefficient regeneration of the entire GUI prototype when using LLMs directly.\nIn this work, we propose GUIDE, a novel LLM-driven GUI generation decomposition\napproach seamlessly integrated into the popular prototyping framework Figma.\nOur approach initially decomposes high-level GUI descriptions into\nfine-granular GUI requirements, which are subsequently translated into Material\nDesign GUI prototypes, enabling higher controllability and more efficient\nadaption of changes. To efficiently conduct prompting-based generation of\nMaterial Design GUI prototypes, we propose a retrieval-augmented generation\napproach to integrate the component library. Our preliminary evaluation\ndemonstrates the effectiveness of GUIDE in bridging the gap between LLM\ngeneration capabilities and traditional GUI prototyping workflows, offering a\nmore effective and controlled user-based approach to LLM-driven GUI\nprototyping. Video: https://youtu.be/C9RbhMxqpTU"
                },
                "authors": [
                    {
                        "name": "Kristian Kolthoff"
                    },
                    {
                        "name": "Felix Kretzer"
                    },
                    {
                        "name": "Christian Bartelt"
                    },
                    {
                        "name": "Alexander Maedche"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Paolo Ponzetto"
                },
                "author": "Simone Paolo Ponzetto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21059v1",
                "updated": "2025-02-28T13:59:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    59,
                    11,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:59:11Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    59,
                    11,
                    4,
                    59,
                    0
                ],
                "title": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated\n  Flowcharts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated\n  Flowcharts"
                },
                "summary": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted\nin some practical applications. However, recent research has revealed their\nvulnerability to multimodal jailbreak attacks, whereby the model can be induced\nto generate harmful content, leading to safety risks. Although most LVLMs have\nundergone safety alignment, recent research shows that the visual modality is\nstill vulnerable to jailbreak attacks. In our work, we discover that by using\nflowcharts with partially harmful information, LVLMs can be induced to provide\nadditional harmful details. Based on this, we propose a jailbreak attack method\nbased on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first\nfine-tunes a pre-trained LLM to create a step-description generator based on\nbenign datasets. The generator is then used to produce step descriptions\ncorresponding to a harmful query, which are transformed into flowcharts in 3\ndifferent shapes (vertical, horizontal, and S-shaped) as visual prompts. These\nflowcharts are then combined with a benign textual prompt to execute a\njailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that\nFC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next,\nQwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak\nmethods. Additionally, we investigate factors affecting the attack performance,\nincluding the number of steps and the font styles in the flowcharts. Our\nevaluation shows that FC-Attack can improve the jailbreak performance from 4%\nto 28% in Claude-3.5 by changing the font style. To mitigate the attack, we\nexplore several defenses and find that AdaShield can largely reduce the\njailbreak performance but with the cost of utility drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted\nin some practical applications. However, recent research has revealed their\nvulnerability to multimodal jailbreak attacks, whereby the model can be induced\nto generate harmful content, leading to safety risks. Although most LVLMs have\nundergone safety alignment, recent research shows that the visual modality is\nstill vulnerable to jailbreak attacks. In our work, we discover that by using\nflowcharts with partially harmful information, LVLMs can be induced to provide\nadditional harmful details. Based on this, we propose a jailbreak attack method\nbased on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first\nfine-tunes a pre-trained LLM to create a step-description generator based on\nbenign datasets. The generator is then used to produce step descriptions\ncorresponding to a harmful query, which are transformed into flowcharts in 3\ndifferent shapes (vertical, horizontal, and S-shaped) as visual prompts. These\nflowcharts are then combined with a benign textual prompt to execute a\njailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that\nFC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next,\nQwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak\nmethods. Additionally, we investigate factors affecting the attack performance,\nincluding the number of steps and the font styles in the flowcharts. Our\nevaluation shows that FC-Attack can improve the jailbreak performance from 4%\nto 28% in Claude-3.5 by changing the font style. To mitigate the attack, we\nexplore several defenses and find that AdaShield can largely reduce the\njailbreak performance but with the cost of utility drop."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Zongmin Zhang"
                    },
                    {
                        "name": "Jihui Guo"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11431v3",
                "updated": "2025-02-28T13:43:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    43,
                    17,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-17T11:36:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    36,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in\n  Weak-to-Strong Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in\n  Weak-to-Strong Generalization"
                },
                "summary": "Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Shiqi Shen"
                    },
                    {
                        "name": "Guangyao Shen"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Zhi Gong"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted at ICLR 2025, camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15835v2",
                "updated": "2025-02-28T13:40:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    40,
                    42,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-20T12:44:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    44,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "Pragmatic Reasoning improves LLM Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatic Reasoning improves LLM Code Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuchen Cao"
                    },
                    {
                        "name": "Sven Apel"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21037v1",
                "updated": "2025-02-28T13:29:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    29,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:29:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    29,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "The amplifier effect of artificial agents in social contagion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The amplifier effect of artificial agents in social contagion"
                },
                "summary": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways."
                },
                "authors": [
                    {
                        "name": "Eric Hitz"
                    },
                    {
                        "name": "Mingmin Feng"
                    },
                    {
                        "name": "Radu Tanase"
                    },
                    {
                        "name": "Ren Algesheimer"
                    },
                    {
                        "name": "Manuel S. Mariani"
                    }
                ],
                "author_detail": {
                    "name": "Manuel S. Mariani"
                },
                "author": "Manuel S. Mariani",
                "arxiv_comment": "Main text pp. 1-5; Supplementary Material pp. 6-10",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21030v1",
                "updated": "2025-02-28T13:22:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    22,
                    29,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:22:29Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    22,
                    29,
                    4,
                    59,
                    0
                ],
                "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs"
                },
                "summary": "Recent advances in large language models (LLMs) have popularized the\nchain-of-thought (CoT) paradigm, in which models produce explicit reasoning\nsteps in natural language. Although this approach improves interpretability and\nfacilitates external auditing, it may not represent the most computationally\nefficient method for internal reasoning. In contrast, human cognition relies on\nimplicit mental representations that recall past sensory and episodic\ninformation without requiring complete verbalization. In this paper, we propose\na framework that integrates implicit mental representations into the internal\nreasoning processes of LLMs. Preliminary experiments indicate that\nincorporating an Implicit Memory Module (IMM) into a simple GPT model yields a\nreduction of between 35% and 57% in final training loss compared to a regular\nGPT baseline. The addition of an explicit interpretability channel (e.g., a\nchain-of-thought decoder) is straightforward to implement within this approach.\nWe outline theoretical foundations, propose technical mechanisms to scale the\nmemory module, and discuss how these ideas may lead to more efficient and\nrobust reasoning, with optional future extensions for explicit auditability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have popularized the\nchain-of-thought (CoT) paradigm, in which models produce explicit reasoning\nsteps in natural language. Although this approach improves interpretability and\nfacilitates external auditing, it may not represent the most computationally\nefficient method for internal reasoning. In contrast, human cognition relies on\nimplicit mental representations that recall past sensory and episodic\ninformation without requiring complete verbalization. In this paper, we propose\na framework that integrates implicit mental representations into the internal\nreasoning processes of LLMs. Preliminary experiments indicate that\nincorporating an Implicit Memory Module (IMM) into a simple GPT model yields a\nreduction of between 35% and 57% in final training loss compared to a regular\nGPT baseline. The addition of an explicit interpretability channel (e.g., a\nchain-of-thought decoder) is straightforward to implement within this approach.\nWe outline theoretical foundations, propose technical mechanisms to scale the\nmemory module, and discuss how these ideas may lead to more efficient and\nrobust reasoning, with optional future extensions for explicit auditability."
                },
                "authors": [
                    {
                        "name": "Jos I. Orlicki"
                    }
                ],
                "author_detail": {
                    "name": "Jos I. Orlicki"
                },
                "author": "Jos I. Orlicki",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21028v1",
                "updated": "2025-02-28T13:16:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    16,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    16,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Measuring and identifying factors of individuals' trust in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and identifying factors of individuals' trust in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Edoardo Sebastiano De Duro"
                    },
                    {
                        "name": "Giuseppe Alessandro Veltri"
                    },
                    {
                        "name": "Hudson Golino"
                    },
                    {
                        "name": "Massimo Stella"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Stella"
                },
                "author": "Massimo Stella",
                "arxiv_comment": "24 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21026v1",
                "updated": "2025-02-28T13:14:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    14,
                    58,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    14,
                    58,
                    4,
                    59,
                    0
                ],
                "title": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries\n  through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries\n  through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis"
                },
                "summary": "Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web\napplications. Existing static tools in detecting vulnerabilities in PHP web\napplications neither contain SSRF-related features to enhance detection\naccuracy nor consider PHP's dynamic type features. In this paper, we present\nArtemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP\nweb applications. First, Artemis extracts both PHP built-in and third-party\nfunctions as candidate source and sink functions. Second, Artemis constructs\nboth explicit and implicit call graphs to infer functions' relationships.Third,\nArtemis performs taint analysis based on a set of rules that prevent\nover-tainting and pauses when SSRF exploitation is impossible.Fourth, Artemis\nanalyzes the compatibility of path conditions to prune false positives.We have\nimplemented a prototype of Artemis and evaluated it on 250 PHP web\napplications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with\n15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported\nto developers, with 24 confirmed and assigned CVE IDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web\napplications. Existing static tools in detecting vulnerabilities in PHP web\napplications neither contain SSRF-related features to enhance detection\naccuracy nor consider PHP's dynamic type features. In this paper, we present\nArtemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP\nweb applications. First, Artemis extracts both PHP built-in and third-party\nfunctions as candidate source and sink functions. Second, Artemis constructs\nboth explicit and implicit call graphs to infer functions' relationships.Third,\nArtemis performs taint analysis based on a set of rules that prevent\nover-tainting and pauses when SSRF exploitation is impossible.Fourth, Artemis\nanalyzes the compatibility of path conditions to prune false positives.We have\nimplemented a prototype of Artemis and evaluated it on 250 PHP web\napplications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with\n15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported\nto developers, with 24 confirmed and assigned CVE IDs."
                },
                "authors": [
                    {
                        "name": "Yuchen Ji"
                    },
                    {
                        "name": "Ting Dai"
                    },
                    {
                        "name": "Zhichao Zhou"
                    },
                    {
                        "name": "Yutian Tang"
                    },
                    {
                        "name": "Jingzhu He"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhu He"
                },
                "author": "Jingzhu He",
                "arxiv_doi": "10.1145/3720488",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3720488",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full version of paper submitted to OOPSLA '25",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20372v2",
                "updated": "2025-02-28T13:06:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    6,
                    2,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-29T06:32:36Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    6,
                    32,
                    36,
                    6,
                    364,
                    0
                ],
                "title": "LLM2: Let Large Language Models Harness System 2 Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2: Let Large Language Models Harness System 2 Reasoning"
                },
                "summary": "Large language models (LLMs) have exhibited impressive capabilities across a\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\nthese limitations are rooted in the foundational autoregressive architecture of\nLLMs, which inherently lacks mechanisms for differentiating between desirable\nand undesirable results. Drawing inspiration from the dual-process theory of\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\nresponsible for generating plausible candidates, while the verifier provides\ntimely process-based feedback to distinguish desirable and undesirable outputs.\nThe verifier is trained with a pairwise comparison loss on synthetic\nprocess-supervision data generated through our token quality exploration\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\nself-consistency, LLM2 achieves additional improvements, boosting major@20\naccuracy from 56.2 to 70.2 (+14.0).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive capabilities across a\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\nthese limitations are rooted in the foundational autoregressive architecture of\nLLMs, which inherently lacks mechanisms for differentiating between desirable\nand undesirable results. Drawing inspiration from the dual-process theory of\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\nresponsible for generating plausible candidates, while the verifier provides\ntimely process-based feedback to distinguish desirable and undesirable outputs.\nThe verifier is trained with a pairwise comparison loss on synthetic\nprocess-supervision data generated through our token quality exploration\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\nself-consistency, LLM2 achieves additional improvements, boosting major@20\naccuracy from 56.2 to 70.2 (+14.0)."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Bo Shui"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21017v1",
                "updated": "2025-02-28T13:04:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    4,
                    4,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:04:04Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    4,
                    4,
                    4,
                    59,
                    0
                ],
                "title": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in\n  Persuasive Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in\n  Persuasive Dialogues"
                },
                "summary": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM."
                },
                "authors": [
                    {
                        "name": "Fangxu Yu"
                    },
                    {
                        "name": "Lai Jiang"
                    },
                    {
                        "name": "Shenyi Huang"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Xinyu Dai"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Dai"
                },
                "author": "Xinyu Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04682v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04682v3",
                "updated": "2025-02-28T13:02:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    2,
                    23,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-07T01:29:19Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    29,
                    19,
                    0,
                    281,
                    0
                ],
                "title": "On the Adversarial Risk of Test Time Adaptation: An Investigation into\n  Realistic Test-Time Data Poisoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Adversarial Risk of Test Time Adaptation: An Investigation into\n  Realistic Test-Time Data Poisoning"
                },
                "summary": "Test-time adaptation (TTA) updates the model weights during the inference\nstage using testing data to enhance generalization. However, this practice\nexposes TTA to adversarial risks. Existing studies have shown that when TTA is\nupdated with crafted adversarial test samples, also known as test-time poisoned\ndata, the performance on benign samples can deteriorate. Nonetheless, the\nperceived adversarial risk may be overstated if the poisoned data is generated\nunder overly strong assumptions. In this work, we first review realistic\nassumptions for test-time data poisoning, including white-box versus grey-box\nattacks, access to benign data, attack order, and more. We then propose an\neffective and realistic attack method that better produces poisoned samples\nwithout access to benign samples, and derive an effective in-distribution\nattack objective. We also design two TTA-aware attack objectives. Our\nbenchmarks of existing attack methods reveal that the TTA methods are more\nrobust than previously believed. In addition, we analyze effective defense\nstrategies to help develop adversarially robust TTA methods. The source code is\navailable at https://github.com/Gorilla-Lab-SCUT/RTTDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) updates the model weights during the inference\nstage using testing data to enhance generalization. However, this practice\nexposes TTA to adversarial risks. Existing studies have shown that when TTA is\nupdated with crafted adversarial test samples, also known as test-time poisoned\ndata, the performance on benign samples can deteriorate. Nonetheless, the\nperceived adversarial risk may be overstated if the poisoned data is generated\nunder overly strong assumptions. In this work, we first review realistic\nassumptions for test-time data poisoning, including white-box versus grey-box\nattacks, access to benign data, attack order, and more. We then propose an\neffective and realistic attack method that better produces poisoned samples\nwithout access to benign samples, and derive an effective in-distribution\nattack objective. We also design two TTA-aware attack objectives. Our\nbenchmarks of existing attack methods reveal that the TTA methods are more\nrobust than previously believed. In addition, we analyze effective defense\nstrategies to help develop adversarially robust TTA methods. The source code is\navailable at https://github.com/Gorilla-Lab-SCUT/RTTDP."
                },
                "authors": [
                    {
                        "name": "Yongyi Su"
                    },
                    {
                        "name": "Yushu Li"
                    },
                    {
                        "name": "Nanqing Liu"
                    },
                    {
                        "name": "Kui Jia"
                    },
                    {
                        "name": "Xulei Yang"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "Xun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xun Xu"
                },
                "author": "Xun Xu",
                "arxiv_comment": "Accepted by ICLR 2025. 25 pages, 4 figures and 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04682v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04682v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19883v2",
                "updated": "2025-02-28T12:59:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    59,
                    26,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-27T08:44:04Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    44,
                    4,
                    3,
                    58,
                    0
                ],
                "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models"
                },
                "summary": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs."
                },
                "authors": [
                    {
                        "name": "Sibo Yi"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Jiaxing Song"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Song"
                },
                "author": "Jiaxing Song",
                "arxiv_comment": "12 pages. 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21014v1",
                "updated": "2025-02-28T12:58:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    58,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T12:58:57Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    58,
                    57,
                    4,
                    59,
                    0
                ],
                "title": "Explainable Biomedical Claim Verification with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Biomedical Claim Verification with Large Language Models"
                },
                "summary": "Verification of biomedical claims is critical for healthcare decision-making,\npublic health policy and scientific research. We present an interactive\nbiomedical claim verification system by integrating LLMs, transparent model\nexplanations, and user-guided justification. In the system, users first\nretrieve relevant scientific studies from a persistent medical literature\ncorpus and explore how different LLMs perform natural language inference (NLI)\nwithin task-adaptive reasoning framework to classify each study as \"Support,\"\n\"Contradict,\" or \"Not Enough Information\" regarding the claim. Users can\nexamine the model's reasoning process with additional insights provided by SHAP\nvalues that highlight word-level contributions to the final result. This\ncombination enables a more transparent and interpretable evaluation of the\nmodel's decision-making process. A summary stage allows users to consolidate\nthe results by selecting a result with narrative justification generated by\nLLMs. As a result, a consensus-based final decision is summarized for each\nretrieved study, aiming safe and accountable AI-assisted decision-making in\nbiomedical contexts. We aim to integrate this explainable verification system\nas a component within a broader evidence synthesis framework to support\nhuman-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification of biomedical claims is critical for healthcare decision-making,\npublic health policy and scientific research. We present an interactive\nbiomedical claim verification system by integrating LLMs, transparent model\nexplanations, and user-guided justification. In the system, users first\nretrieve relevant scientific studies from a persistent medical literature\ncorpus and explore how different LLMs perform natural language inference (NLI)\nwithin task-adaptive reasoning framework to classify each study as \"Support,\"\n\"Contradict,\" or \"Not Enough Information\" regarding the claim. Users can\nexamine the model's reasoning process with additional insights provided by SHAP\nvalues that highlight word-level contributions to the final result. This\ncombination enables a more transparent and interpretable evaluation of the\nmodel's decision-making process. A summary stage allows users to consolidate\nthe results by selecting a result with narrative justification generated by\nLLMs. As a result, a consensus-based final decision is summarized for each\nretrieved study, aiming safe and accountable AI-assisted decision-making in\nbiomedical contexts. We aim to integrate this explainable verification system\nas a component within a broader evidence synthesis framework to support\nhuman-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Siting Liang"
                    },
                    {
                        "name": "Daniel Sonntag"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sonntag"
                },
                "author": "Daniel Sonntag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17723v2",
                "updated": "2025-02-28T12:56:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    56,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2024-01-31T10:35:53Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    10,
                    35,
                    53,
                    2,
                    31,
                    0
                ],
                "title": "LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks"
                },
                "summary": "Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems."
                },
                "authors": [
                    {
                        "name": "Kaike Zhang"
                    },
                    {
                        "name": "Qi Cao"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21011v1",
                "updated": "2025-02-28T12:55:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    55,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T12:55:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    55,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "MagNet: Multi-Level Attention Graph Network for Predicting\n  High-Resolution Spatial Transcriptomics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagNet: Multi-Level Attention Graph Network for Predicting\n  High-Resolution Spatial Transcriptomics"
                },
                "summary": "The rapid development of spatial transcriptomics (ST) offers new\nopportunities to explore the gene expression patterns within the spatial\nmicroenvironment. Current research integrates pathological images to infer gene\nexpression, addressing the high costs and time-consuming processes to generate\nspatial transcriptomics data. However, as spatial transcriptomics resolution\ncontinues to improve, existing methods remain primarily focused on gene\nexpression prediction at low-resolution spot levels. These methods face\nsignificant challenges, especially the information bottleneck, when they are\napplied to high-resolution HD data. To bridge this gap, this paper introduces\nMagNet, a multi-level attention graph network designed for accurate prediction\nof high-resolution HD data. MagNet employs cross-attention layers to integrate\nfeatures from multi-resolution image patches hierarchically and utilizes a\nGAT-Transformer module to aggregate neighborhood information. By integrating\nmultilevel features, MagNet overcomes the limitations posed by low-resolution\ninputs in predicting high-resolution gene expression. We systematically\nevaluated MagNet and existing ST prediction models on both a private spatial\ntranscriptomics dataset and a public dataset at three different resolution\nlevels. The results demonstrate that MagNet achieves state-of-the-art\nperformance at both spot level and high-resolution bin levels, providing a\nnovel methodology and benchmark for future research and applications in\nhigh-resolution HD-level spatial transcriptomics. Code is available at\nhttps://github.com/Junchao-Zhu/MagNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of spatial transcriptomics (ST) offers new\nopportunities to explore the gene expression patterns within the spatial\nmicroenvironment. Current research integrates pathological images to infer gene\nexpression, addressing the high costs and time-consuming processes to generate\nspatial transcriptomics data. However, as spatial transcriptomics resolution\ncontinues to improve, existing methods remain primarily focused on gene\nexpression prediction at low-resolution spot levels. These methods face\nsignificant challenges, especially the information bottleneck, when they are\napplied to high-resolution HD data. To bridge this gap, this paper introduces\nMagNet, a multi-level attention graph network designed for accurate prediction\nof high-resolution HD data. MagNet employs cross-attention layers to integrate\nfeatures from multi-resolution image patches hierarchically and utilizes a\nGAT-Transformer module to aggregate neighborhood information. By integrating\nmultilevel features, MagNet overcomes the limitations posed by low-resolution\ninputs in predicting high-resolution gene expression. We systematically\nevaluated MagNet and existing ST prediction models on both a private spatial\ntranscriptomics dataset and a public dataset at three different resolution\nlevels. The results demonstrate that MagNet achieves state-of-the-art\nperformance at both spot level and high-resolution bin levels, providing a\nnovel methodology and benchmark for future research and applications in\nhigh-resolution HD-level spatial transcriptomics. Code is available at\nhttps://github.com/Junchao-Zhu/MagNet."
                },
                "authors": [
                    {
                        "name": "Junchao Zhu"
                    },
                    {
                        "name": "Ruining Deng"
                    },
                    {
                        "name": "Tianyuan Yao"
                    },
                    {
                        "name": "Juming Xiong"
                    },
                    {
                        "name": "Chongyu Qu"
                    },
                    {
                        "name": "Junlin Guo"
                    },
                    {
                        "name": "Siqi Lu"
                    },
                    {
                        "name": "Yucheng Tang"
                    },
                    {
                        "name": "Daguang Xu"
                    },
                    {
                        "name": "Mengmeng Yin"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Shilin Zhao"
                    },
                    {
                        "name": "Yaohong Wang"
                    },
                    {
                        "name": "Haichun Yang"
                    },
                    {
                        "name": "Yuankai Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yuankai Huo"
                },
                "author": "Yuankai Huo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17812v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17812v4",
                "updated": "2025-02-28T12:53:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    53,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2024-02-27T14:51:11Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    14,
                    51,
                    11,
                    1,
                    58,
                    0
                ],
                "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp."
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Baeseong Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Minjung Jo"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsuk Jeon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17812v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17812v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v5",
                "updated": "2025-02-28T12:35:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    35,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM ($p$-value\n$<0.001$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM ($p$-value\n$<0.001$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20988v1",
                "updated": "2025-02-28T12:00:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    0,
                    51,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T12:00:51Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    0,
                    51,
                    4,
                    59,
                    0
                ],
                "title": "Merging Clinical Knowledge into Large Language Models for Medical\n  Research and Applications: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging Clinical Knowledge into Large Language Models for Medical\n  Research and Applications: A Survey"
                },
                "summary": "Clinical knowledge is the collection of information learned from studies on\nthe causes, prognosis, diagnosis, and treatment of diseases. This type of\nknowledge can improve curing performances, and promote physical health. With\nthe emergence of large language models (LLMs), medical artificial intelligence\n(medical AI), which aims to apply academic medical AI systems to real-world\nmedical scenarios, has entered a new age of development, resulting in excellent\nworks such as DoctorGPT and Pangu-Drug from academic and industrial researches.\nHowever, the field lacks a comprehensive compendium and comparison of building\nmedical AI systems from academia and industry. Therefore, this survey focuses\non the building paradigms of medical AI systems including the use of clinical\ndatabases, datasets, training pipelines, integrating medical knowledge graphs,\nsystem applications, and evaluation systems. We hope that this survey can help\nrelevant practical researchers understand the current performance of academic\nmodels in various fields of healthcare, as well as the potential problems and\nfuture directions for implementing these scientific achievements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical knowledge is the collection of information learned from studies on\nthe causes, prognosis, diagnosis, and treatment of diseases. This type of\nknowledge can improve curing performances, and promote physical health. With\nthe emergence of large language models (LLMs), medical artificial intelligence\n(medical AI), which aims to apply academic medical AI systems to real-world\nmedical scenarios, has entered a new age of development, resulting in excellent\nworks such as DoctorGPT and Pangu-Drug from academic and industrial researches.\nHowever, the field lacks a comprehensive compendium and comparison of building\nmedical AI systems from academia and industry. Therefore, this survey focuses\non the building paradigms of medical AI systems including the use of clinical\ndatabases, datasets, training pipelines, integrating medical knowledge graphs,\nsystem applications, and evaluation systems. We hope that this survey can help\nrelevant practical researchers understand the current performance of academic\nmodels in various fields of healthcare, as well as the potential problems and\nfuture directions for implementing these scientific achievements."
                },
                "authors": [
                    {
                        "name": "Qiyuan Li"
                    },
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Caicai Guo"
                    },
                    {
                        "name": "Deyu Chen"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Jinguang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinguang Gu"
                },
                "author": "Jinguang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06655v2",
                "updated": "2025-02-28T11:58:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    58,
                    28,
                    4,
                    59,
                    0
                ],
                "published": "2024-11-11T01:42:56Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    42,
                    56,
                    0,
                    316,
                    0
                ],
                "title": "Explore the Reasoning Capability of LLMs in the Chess Testbed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore the Reasoning Capability of LLMs in the Chess Testbed"
                },
                "summary": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Lei Ji"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Wenxiao Zhao"
                    },
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Yifan Hou"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "arxiv_comment": "NAACL2025 Main Conference. Data and models are available:\n  https://mate-chess.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20984v1",
                "updated": "2025-02-28T11:52:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    52,
                    2,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:52:02Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    52,
                    2,
                    4,
                    59,
                    0
                ],
                "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation"
                },
                "summary": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL."
                },
                "authors": [
                    {
                        "name": "Thanet Markchom"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Liting Huang"
                    },
                    {
                        "name": "Huizhi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huizhi Liang"
                },
                "author": "Huizhi Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20975v1",
                "updated": "2025-02-28T11:40:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    40,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:40:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    40,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Set-Theoretic Compositionality of Sentence Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set-Theoretic Compositionality of Sentence Embeddings"
                },
                "summary": "Sentence encoders play a pivotal role in various NLP tasks; hence, an\naccurate evaluation of their compositional properties is paramount. However,\nexisting evaluation methods predominantly focus on goal task-specific\nperformance. This leaves a significant gap in understanding how well sentence\nembeddings demonstrate fundamental compositional properties in a\ntask-independent context. Leveraging classical set theory, we address this gap\nby proposing six criteria based on three core \"set-like\"\ncompositions/operations: \\textit{TextOverlap}, \\textit{TextDifference}, and\n\\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large\nLanguage Model (LLM)-based sentence encoders to assess their alignment with\nthese criteria. Our findings show that SBERT consistently demonstrates set-like\ncompositional properties, surpassing even the latest LLMs. Additionally, we\nintroduce a new dataset of ~$192$K samples designed to facilitate future\nbenchmarking efforts on set-like compositionality of sentence embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence encoders play a pivotal role in various NLP tasks; hence, an\naccurate evaluation of their compositional properties is paramount. However,\nexisting evaluation methods predominantly focus on goal task-specific\nperformance. This leaves a significant gap in understanding how well sentence\nembeddings demonstrate fundamental compositional properties in a\ntask-independent context. Leveraging classical set theory, we address this gap\nby proposing six criteria based on three core \"set-like\"\ncompositions/operations: \\textit{TextOverlap}, \\textit{TextDifference}, and\n\\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large\nLanguage Model (LLM)-based sentence encoders to assess their alignment with\nthese criteria. Our findings show that SBERT consistently demonstrates set-like\ncompositional properties, surpassing even the latest LLMs. Additionally, we\nintroduce a new dataset of ~$192$K samples designed to facilitate future\nbenchmarking efforts on set-like compositionality of sentence embeddings."
                },
                "authors": [
                    {
                        "name": "Naman Bansal"
                    },
                    {
                        "name": "Yash mahajan"
                    },
                    {
                        "name": "Sanjeev Sinha"
                    },
                    {
                        "name": "Santu Karmaker"
                    }
                ],
                "author_detail": {
                    "name": "Santu Karmaker"
                },
                "author": "Santu Karmaker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20973v1",
                "updated": "2025-02-28T11:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    37,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    37,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?"
                },
                "summary": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic."
                },
                "authors": [
                    {
                        "name": "Perla Al Almaoui"
                    },
                    {
                        "name": "Pierrette Bouillon"
                    },
                    {
                        "name": "Simon Hengchen"
                    }
                ],
                "author_detail": {
                    "name": "Simon Hengchen"
                },
                "author": "Simon Hengchen",
                "arxiv_comment": "Submitted to MT Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20969v1",
                "updated": "2025-02-28T11:32:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with\n  Lookahead Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with\n  Lookahead Retrieval"
                },
                "summary": "Retrieval-augmented generation (RAG) extends large language models (LLMs)\nwith external data sources to enhance factual correctness and domain coverage.\nModern RAG pipelines rely on large datastores, leading to system challenges in\nlatency-sensitive deployments, especially when limited GPU memory is available.\nTo address these challenges, we propose TeleRAG, an efficient inference system\nthat reduces RAG latency with minimal GPU memory requirements. The core\ninnovation of TeleRAG is lookahead retrieval, a prefetching mechanism that\nanticipates required data and transfers it from CPU to GPU in parallel with LLM\ngeneration. By leveraging the modularity of RAG pipelines, the inverted file\nindex (IVF) search algorithm and similarities between queries, TeleRAG\noptimally overlaps data movement and computation. Experimental results show\nthat TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average\ncompared to state-of-the-art systems, enabling faster, more memory-efficient\ndeployments of advanced RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) extends large language models (LLMs)\nwith external data sources to enhance factual correctness and domain coverage.\nModern RAG pipelines rely on large datastores, leading to system challenges in\nlatency-sensitive deployments, especially when limited GPU memory is available.\nTo address these challenges, we propose TeleRAG, an efficient inference system\nthat reduces RAG latency with minimal GPU memory requirements. The core\ninnovation of TeleRAG is lookahead retrieval, a prefetching mechanism that\nanticipates required data and transfers it from CPU to GPU in parallel with LLM\ngeneration. By leveraging the modularity of RAG pipelines, the inverted file\nindex (IVF) search algorithm and similarities between queries, TeleRAG\noptimally overlaps data movement and computation. Experimental results show\nthat TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average\ncompared to state-of-the-art systems, enabling faster, more memory-efficient\ndeployments of advanced RAG applications."
                },
                "authors": [
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Keisuke Kamahori"
                    },
                    {
                        "name": "Yiyu Liu"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Madhav Kashyap"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20968v1",
                "updated": "2025-02-28T11:31:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    31,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:31:27Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    31,
                    27,
                    4,
                    59,
                    0
                ],
                "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play\n  Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play\n  Fine-Tuning of LLMs"
                },
                "summary": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Xingyu Sui"
                    },
                    {
                        "name": "Xinyang Han"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "25 pages, 10 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16757v2",
                "updated": "2025-02-28T11:29:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    29,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2023-10-25T16:45:02Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    16,
                    45,
                    2,
                    2,
                    298,
                    0
                ],
                "title": "All-rounder: A Flexible AI Accelerator with Diverse Data Format Support\n  and Morphable Structure for Multi-DNN Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-rounder: A Flexible AI Accelerator with Diverse Data Format Support\n  and Morphable Structure for Multi-DNN Processing"
                },
                "summary": "Recognizing the explosive increase in the use of AI-based applications,\nseveral industrial companies developed custom ASICs (e.g., Google TPU, IBM\nRaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure\nwith them. These ASICs perform operations of the inference or training process\nof AI models which are requested by users. Since the AI models have different\ndata formats and types of operations, the ASICs need to support diverse data\nformats and various operation shapes. However, the previous ASIC solutions do\nnot or less fulfill these requirements. To overcome these limitations, we first\npresent an area-efficient multiplier, named all-in-one multiplier, that\nsupports multiple bit-widths for both integer and floating point data types.\nThen, we build a MAC array equipped with these multipliers with multi-format\nsupport. In addition, the MAC array can be partitioned into multiple blocks\nthat can be flexibly fused to support various DNN operation types. We evaluate\nthe practical effectiveness of the proposed MAC array by making an accelerator\nout of it, named All-rounder. According to our evaluation, the proposed\nall-in-one multiplier occupies 1.49x smaller area compared to the baselines\nwith dedicated multipliers for each data format. Then, we compare the\nperformance and energy efficiency of the proposed All-rounder with three\ndifferent accelerators showing consistent speedup and higher efficiency across\nvarious AI benchmarks from vision to LLM-based language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing the explosive increase in the use of AI-based applications,\nseveral industrial companies developed custom ASICs (e.g., Google TPU, IBM\nRaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure\nwith them. These ASICs perform operations of the inference or training process\nof AI models which are requested by users. Since the AI models have different\ndata formats and types of operations, the ASICs need to support diverse data\nformats and various operation shapes. However, the previous ASIC solutions do\nnot or less fulfill these requirements. To overcome these limitations, we first\npresent an area-efficient multiplier, named all-in-one multiplier, that\nsupports multiple bit-widths for both integer and floating point data types.\nThen, we build a MAC array equipped with these multipliers with multi-format\nsupport. In addition, the MAC array can be partitioned into multiple blocks\nthat can be flexibly fused to support various DNN operation types. We evaluate\nthe practical effectiveness of the proposed MAC array by making an accelerator\nout of it, named All-rounder. According to our evaluation, the proposed\nall-in-one multiplier occupies 1.49x smaller area compared to the baselines\nwith dedicated multipliers for each data format. Then, we compare the\nperformance and energy efficiency of the proposed All-rounder with three\ndifferent accelerators showing consistent speedup and higher efficiency across\nvarious AI benchmarks from vision to LLM-based language tasks."
                },
                "authors": [
                    {
                        "name": "Seock-Hwan Noh"
                    },
                    {
                        "name": "Seungpyo Lee"
                    },
                    {
                        "name": "Banseok Shin"
                    },
                    {
                        "name": "Sehun Park"
                    },
                    {
                        "name": "Yongjoo Jang"
                    },
                    {
                        "name": "Jaeha Kung"
                    }
                ],
                "author_detail": {
                    "name": "Jaeha Kung"
                },
                "author": "Jaeha Kung",
                "arxiv_comment": "A paper accepted in the 2025 IEEE Transactions on Very Large Scale\n  Integration (VLSI) Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20966v1",
                "updated": "2025-02-28T11:29:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    29,
                    6,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:29:06Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    29,
                    6,
                    4,
                    59,
                    0
                ],
                "title": "Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via\n  Activation-Level Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via\n  Activation-Level Gaussian Processes"
                },
                "summary": "Uncertainty quantification in neural networks through methods such as\nDropout, Bayesian neural networks and Laplace approximations is either prone to\nunderfitting or computationally demanding, rendering these approaches\nimpractical for large-scale datasets. In this work, we address these\nshortcomings by shifting the focus from uncertainty in the weight space to\nuncertainty at the activation level, via Gaussian processes. More specifically,\nwe introduce the Gaussian Process Activation function (GAPA) to capture\nneuron-level uncertainties. Our approach operates in a post-hoc manner,\npreserving the original mean predictions of the pre-trained neural network and\nthereby avoiding the underfitting issues commonly encountered in previous\nmethods. We propose two methods. The first, GAPA-Free, employs empirical kernel\nlearning from the training data for the hyperparameters and is highly efficient\nduring training. The second, GAPA-Variational, learns the hyperparameters via\ngradient descent on the kernels, thus affording greater flexibility. Empirical\nresults demonstrate that GAPA-Variational outperforms the Laplace approximation\non most datasets in at least one of the uncertainty quantification metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification in neural networks through methods such as\nDropout, Bayesian neural networks and Laplace approximations is either prone to\nunderfitting or computationally demanding, rendering these approaches\nimpractical for large-scale datasets. In this work, we address these\nshortcomings by shifting the focus from uncertainty in the weight space to\nuncertainty at the activation level, via Gaussian processes. More specifically,\nwe introduce the Gaussian Process Activation function (GAPA) to capture\nneuron-level uncertainties. Our approach operates in a post-hoc manner,\npreserving the original mean predictions of the pre-trained neural network and\nthereby avoiding the underfitting issues commonly encountered in previous\nmethods. We propose two methods. The first, GAPA-Free, employs empirical kernel\nlearning from the training data for the hyperparameters and is highly efficient\nduring training. The second, GAPA-Variational, learns the hyperparameters via\ngradient descent on the kernels, thus affording greater flexibility. Empirical\nresults demonstrate that GAPA-Variational outperforms the Laplace approximation\non most datasets in at least one of the uncertainty quantification metrics."
                },
                "authors": [
                    {
                        "name": "Richard Bergna"
                    },
                    {
                        "name": "Stefan Depeweg"
                    },
                    {
                        "name": "Sergio Calvo Ordonez"
                    },
                    {
                        "name": "Jonathan Plenk"
                    },
                    {
                        "name": "Alvaro Cartea"
                    },
                    {
                        "name": "Jose Miguel Hernandez-Lobato"
                    }
                ],
                "author_detail": {
                    "name": "Jose Miguel Hernandez-Lobato"
                },
                "author": "Jose Miguel Hernandez-Lobato",
                "arxiv_comment": "10 pages, 8 figures, 7th Symposium on Advances in Approximate\n  Bayesian Inference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09955v3",
                "updated": "2025-02-28T11:28:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    28,
                    41,
                    4,
                    59,
                    0
                ],
                "published": "2023-10-15T21:26:50Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    21,
                    26,
                    50,
                    6,
                    288,
                    0
                ],
                "title": "Statistical Inference for Random Unknowns via Modifications of Extended\n  Likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Random Unknowns via Modifications of Extended\n  Likelihood"
                },
                "summary": "Fisher's likelihood is widely used for statistical inference for fixed\nunknowns. This paper aims to extend two important likelihood-based methods,\nnamely the maximum likelihood procedure for point estimation and the confidence\nprocedure for interval estimation, to embrace a broader class of statistical\nmodels with additional random unknowns. We propose the new h-likelihood and the\nh-confidence by modifying extended likelihoods. Maximization of the\nh-likelihood yields both maximum likelihood estimators of fixed unknowns and\nasymptotically optimal predictors for random unknowns, achieving the\ngeneralized Cram\\'er-Rao lower bound. The h-likelihood further offers\nadvantages in scalability for large datasets and complex models. The\nh-confidence could yield a valid interval estimation and prediction by\nmaintaining the coverage probability for both fixed and random unknowns in\nsmall samples. We study approximate methods for the h-likelihood and\nh-confidence, which can be applied to a general class of models with additional\nrandom unknowns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fisher's likelihood is widely used for statistical inference for fixed\nunknowns. This paper aims to extend two important likelihood-based methods,\nnamely the maximum likelihood procedure for point estimation and the confidence\nprocedure for interval estimation, to embrace a broader class of statistical\nmodels with additional random unknowns. We propose the new h-likelihood and the\nh-confidence by modifying extended likelihoods. Maximization of the\nh-likelihood yields both maximum likelihood estimators of fixed unknowns and\nasymptotically optimal predictors for random unknowns, achieving the\ngeneralized Cram\\'er-Rao lower bound. The h-likelihood further offers\nadvantages in scalability for large datasets and complex models. The\nh-confidence could yield a valid interval estimation and prediction by\nmaintaining the coverage probability for both fixed and random unknowns in\nsmall samples. We study approximate methods for the h-likelihood and\nh-confidence, which can be applied to a general class of models with additional\nrandom unknowns."
                },
                "authors": [
                    {
                        "name": "Hangbin Lee"
                    },
                    {
                        "name": "Youngjo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Youngjo Lee"
                },
                "author": "Youngjo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20963v1",
                "updated": "2025-02-28T11:25:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    25,
                    11,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    25,
                    11,
                    4,
                    59,
                    0
                ],
                "title": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration"
                },
                "summary": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research."
                },
                "authors": [
                    {
                        "name": "Gerion Spielberger"
                    },
                    {
                        "name": "Florian Artinger"
                    },
                    {
                        "name": "Jochen Reb"
                    },
                    {
                        "name": "Rudolf Kerschreiter"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Kerschreiter"
                },
                "author": "Rudolf Kerschreiter",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20959v1",
                "updated": "2025-02-28T11:19:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    19,
                    5,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:19:05Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    19,
                    5,
                    4,
                    59,
                    0
                ],
                "title": "Cicada: A Pipeline-Efficient Approach to Serverless Inference with\n  Decoupled Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cicada: A Pipeline-Efficient Approach to Serverless Inference with\n  Decoupled Management"
                },
                "summary": "Serverless computing has emerged as a pivotal paradigm for deploying Deep\nLearning (DL) models, offering automatic scaling and cost efficiency. However,\nthe inherent cold start problem in serverless ML inference systems,\nparticularly the time-consuming model loading process, remains a significant\nbottleneck. Utilizing pipelined model loading improves efficiency but still\nsuffer from pipeline stalls due to sequential layer construction and monolithic\nweight loading. In this paper, we propose \\textit{Cicada}, a novel pipeline\noptimization framework that coordinates computational, storage, and scheduling\nresources through three key mechanisms: (1) \\textit{MiniLoader}: which reduces\nlayer construction overhead by opportunistically optimizing parameter\ninitialization; (2) \\textit{WeightDecoupler}: decoupling weight file processing\nfrom layer construction, enabling asynchronous weight retrieval and\nout-of-order weight application; (3) \\textit{Priority-Aware Scheduler}:\ndynamically allocating resources to ensure high-priority inference tasks are\nexecuted promptly. Our experimental results demonstrate that Cicada achieves\nsignificant performance improvements over the state-of-the-art PISeL framework.\nSpecifically, Cicada reduces end-to-end inference latency by an average of\n61.59\\%, with the MiniLoader component contributing the majority of this\noptimization (53.41\\%), and the WeightDecoupler achieves up to 26.17\\%\nimprovement. Additionally, Cicada achieves up to 2.52x speedup in the inference\npipeline utlization compared to PISeL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has emerged as a pivotal paradigm for deploying Deep\nLearning (DL) models, offering automatic scaling and cost efficiency. However,\nthe inherent cold start problem in serverless ML inference systems,\nparticularly the time-consuming model loading process, remains a significant\nbottleneck. Utilizing pipelined model loading improves efficiency but still\nsuffer from pipeline stalls due to sequential layer construction and monolithic\nweight loading. In this paper, we propose \\textit{Cicada}, a novel pipeline\noptimization framework that coordinates computational, storage, and scheduling\nresources through three key mechanisms: (1) \\textit{MiniLoader}: which reduces\nlayer construction overhead by opportunistically optimizing parameter\ninitialization; (2) \\textit{WeightDecoupler}: decoupling weight file processing\nfrom layer construction, enabling asynchronous weight retrieval and\nout-of-order weight application; (3) \\textit{Priority-Aware Scheduler}:\ndynamically allocating resources to ensure high-priority inference tasks are\nexecuted promptly. Our experimental results demonstrate that Cicada achieves\nsignificant performance improvements over the state-of-the-art PISeL framework.\nSpecifically, Cicada reduces end-to-end inference latency by an average of\n61.59\\%, with the MiniLoader component contributing the majority of this\noptimization (53.41\\%), and the WeightDecoupler achieves up to 26.17\\%\nimprovement. Additionally, Cicada achieves up to 2.52x speedup in the inference\npipeline utlization compared to PISeL."
                },
                "authors": [
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "Y. Deng"
                    },
                    {
                        "name": "J. Hu"
                    },
                    {
                        "name": "L. Cui"
                    },
                    {
                        "name": "Z. Zhang"
                    },
                    {
                        "name": "L. Zeng"
                    },
                    {
                        "name": "G. Min"
                    }
                ],
                "author_detail": {
                    "name": "G. Min"
                },
                "author": "G. Min",
                "arxiv_comment": "13pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20952v1",
                "updated": "2025-02-28T11:07:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    7,
                    41,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:07:41Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    7,
                    41,
                    4,
                    59,
                    0
                ],
                "title": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers\n  Exhibit Greater Sensitivity to Harmful Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers\n  Exhibit Greater Sensitivity to Harmful Content"
                },
                "summary": "With the widespread application of Large Language Models across various\ndomains, their security issues have increasingly garnered significant attention\nfrom both academic and industrial communities. This study conducts sampling and\nnormalization of the parameters of the LLM to generate visual representations\nand heatmaps of parameter distributions, revealing notable discrepancies in\nparameter distributions among certain layers within the hidden layers. Further\nanalysis involves calculating statistical metrics for each layer, followed by\nthe computation of a Comprehensive Sensitivity Score based on these metrics,\nwhich identifies the lower layers as being particularly sensitive to the\ngeneration of harmful content. Based on this finding, we employ a Freeze\ntraining strategy, selectively performing Supervised Fine-Tuning only on the\nlower layers. Experimental results demonstrate that this method significantly\nreduces training duration and GPU memory consumption while maintaining a high\njailbreak success rate and a high harm score, outperforming the results\nachieved by applying the LoRA method for SFT across all layers. Additionally,\nthe method has been successfully extended to other open-source large models,\nvalidating its generality and effectiveness across different model\narchitectures. Furthermore, we compare our method with ohter jailbreak method,\ndemonstrating the superior performance of our approach. By innovatively\nproposing a method to statistically analyze and compare large model parameters\nlayer by layer, this study provides new insights into the interpretability of\nlarge models. These discoveries emphasize the necessity of continuous research\nand the implementation of adaptive security measures in the rapidly evolving\nfield of LLMs to prevent potential jailbreak attack risks, thereby promoting\nthe development of more robust and secure LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models across various\ndomains, their security issues have increasingly garnered significant attention\nfrom both academic and industrial communities. This study conducts sampling and\nnormalization of the parameters of the LLM to generate visual representations\nand heatmaps of parameter distributions, revealing notable discrepancies in\nparameter distributions among certain layers within the hidden layers. Further\nanalysis involves calculating statistical metrics for each layer, followed by\nthe computation of a Comprehensive Sensitivity Score based on these metrics,\nwhich identifies the lower layers as being particularly sensitive to the\ngeneration of harmful content. Based on this finding, we employ a Freeze\ntraining strategy, selectively performing Supervised Fine-Tuning only on the\nlower layers. Experimental results demonstrate that this method significantly\nreduces training duration and GPU memory consumption while maintaining a high\njailbreak success rate and a high harm score, outperforming the results\nachieved by applying the LoRA method for SFT across all layers. Additionally,\nthe method has been successfully extended to other open-source large models,\nvalidating its generality and effectiveness across different model\narchitectures. Furthermore, we compare our method with ohter jailbreak method,\ndemonstrating the superior performance of our approach. By innovatively\nproposing a method to statistically analyze and compare large model parameters\nlayer by layer, this study provides new insights into the interpretability of\nlarge models. These discoveries emphasize the necessity of continuous research\nand the implementation of adaptive security measures in the rapidly evolving\nfield of LLMs to prevent potential jailbreak attack risks, thereby promoting\nthe development of more robust and secure LLMs."
                },
                "authors": [
                    {
                        "name": "Hongyuan Shen"
                    },
                    {
                        "name": "Min Zheng"
                    },
                    {
                        "name": "Jincheng Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06617v3",
                "updated": "2025-02-28T10:57:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    57,
                    49,
                    4,
                    59,
                    0
                ],
                "published": "2024-04-09T21:07:41Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    21,
                    7,
                    41,
                    1,
                    100,
                    0
                ],
                "title": "No evidence for anisotropy in galaxy spin directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No evidence for anisotropy in galaxy spin directions"
                },
                "summary": "Modern cosmology rests on the cosmological principle, that on large enough\nscales the Universe is both homogeneous and isotropic. A corollary is that\ngalaxies' spin vectors should be isotropically distributed on the sky. This has\nbeen challenged by multiple authors for over a decade, with claims to have\ndetected a statistically significant dipole pattern of spins. We collect all\npublicly available datasets with spin classifications (binary Z-wise/S-wise),\nand analyse them for large-angle anisotropies ($\\ell \\le 2$). We perform each\ninference in both a Bayesian and frequentist fashion, the former establishing\nposterior probabilities on the multipole parameters and the latter calculating\n$p$-values for rejection of the null hypothesis of isotropy (i.e. no power at\n$\\ell>0$). All analysis indicate consistency with isotropy to within $3\\sigma$.\nWe similarly identify no evidence for a \"hemisphere anisotropy\" that neglects\nthe angular dependence of the dipole. We isolate the differences with contrary\nclaims in the ad hoc or biased statistics that they employ. Our code is\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern cosmology rests on the cosmological principle, that on large enough\nscales the Universe is both homogeneous and isotropic. A corollary is that\ngalaxies' spin vectors should be isotropically distributed on the sky. This has\nbeen challenged by multiple authors for over a decade, with claims to have\ndetected a statistically significant dipole pattern of spins. We collect all\npublicly available datasets with spin classifications (binary Z-wise/S-wise),\nand analyse them for large-angle anisotropies ($\\ell \\le 2$). We perform each\ninference in both a Bayesian and frequentist fashion, the former establishing\nposterior probabilities on the multipole parameters and the latter calculating\n$p$-values for rejection of the null hypothesis of isotropy (i.e. no power at\n$\\ell>0$). All analysis indicate consistency with isotropy to within $3\\sigma$.\nWe similarly identify no evidence for a \"hemisphere anisotropy\" that neglects\nthe angular dependence of the dipole. We isolate the differences with contrary\nclaims in the ad hoc or biased statistics that they employ. Our code is\npublicly available."
                },
                "authors": [
                    {
                        "name": "Dhruva Patel"
                    },
                    {
                        "name": "Harry Desmond"
                    }
                ],
                "author_detail": {
                    "name": "Harry Desmond"
                },
                "author": "Harry Desmond",
                "arxiv_doi": "10.1093/mnras/stae2158",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2158",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.06617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 5 figures; minor revision, matches MNRAS published version",
                "arxiv_journal_ref": "MNRAS 2024 534 (2): 1553-1560",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20946v1",
                "updated": "2025-02-28T10:56:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    56,
                    39,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:56:39Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    56,
                    39,
                    4,
                    59,
                    0
                ],
                "title": "Generative Uncertainty in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Uncertainty in Diffusion Models"
                },
                "summary": "Diffusion models have recently driven significant breakthroughs in generative\nmodeling. While state-of-the-art models produce high-quality samples on\naverage, individual samples can still be low quality. Detecting such samples\nwithout human inspection remains a challenging task. To address this, we\npropose a Bayesian framework for estimating generative uncertainty of synthetic\nsamples. We outline how to make Bayesian inference practical for large, modern\ngenerative models and introduce a new semantic likelihood (evaluated in the\nlatent space of a feature extractor) to address the challenges posed by\nhigh-dimensional sample spaces. Through our experiments, we demonstrate that\nthe proposed generative uncertainty effectively identifies poor-quality samples\nand significantly outperforms existing uncertainty-based methods. Notably, our\nBayesian framework can be applied post-hoc to any pretrained diffusion or flow\nmatching model (via the Laplace approximation), and we propose simple yet\neffective techniques to minimize its computational overhead during sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently driven significant breakthroughs in generative\nmodeling. While state-of-the-art models produce high-quality samples on\naverage, individual samples can still be low quality. Detecting such samples\nwithout human inspection remains a challenging task. To address this, we\npropose a Bayesian framework for estimating generative uncertainty of synthetic\nsamples. We outline how to make Bayesian inference practical for large, modern\ngenerative models and introduce a new semantic likelihood (evaluated in the\nlatent space of a feature extractor) to address the challenges posed by\nhigh-dimensional sample spaces. Through our experiments, we demonstrate that\nthe proposed generative uncertainty effectively identifies poor-quality samples\nand significantly outperforms existing uncertainty-based methods. Notably, our\nBayesian framework can be applied post-hoc to any pretrained diffusion or flow\nmatching model (via the Laplace approximation), and we propose simple yet\neffective techniques to minimize its computational overhead during sampling."
                },
                "authors": [
                    {
                        "name": "Metod Jazbec"
                    },
                    {
                        "name": "Eliot Wong-Toi"
                    },
                    {
                        "name": "Guoxuan Xia"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Eric Nalisnick"
                    },
                    {
                        "name": "Stephan Mandt"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Mandt"
                },
                "author": "Stephan Mandt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17146v2",
                "updated": "2025-02-28T10:53:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    53,
                    12,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-22T16:26:05Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    26,
                    5,
                    1,
                    296,
                    0
                ],
                "title": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging"
                },
                "summary": "Fine-tuning pre-trained models has become the standard approach to endow them\nwith specialized knowledge, but it poses fundamental challenges. In particular,\n\\textit{(i)} fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks, and\n\\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to\nsignificant performance loss. To address these challenges, we introduce LiNeS,\nLayer-increasing Network Scaling, a post-training editing technique designed to\npreserve pre-trained generalization while enhancing fine-tuned task\nperformance. LiNeS scales parameter updates linearly based on their layer depth\nwithin the network, maintaining shallow layers close to their pre-trained\nvalues to preserve general features while allowing deeper layers to retain\ntask-specific representations. In multi-task model merging scenarios,\nlayer-wise scaling of merged parameters reduces negative task interference.\nLiNeS demonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Our method is simple to implement, computationally efficient and\ncomplementary to many existing techniques. Our source code is available at\nhttps://github.com/wang-kee/LiNeS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained models has become the standard approach to endow them\nwith specialized knowledge, but it poses fundamental challenges. In particular,\n\\textit{(i)} fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks, and\n\\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to\nsignificant performance loss. To address these challenges, we introduce LiNeS,\nLayer-increasing Network Scaling, a post-training editing technique designed to\npreserve pre-trained generalization while enhancing fine-tuned task\nperformance. LiNeS scales parameter updates linearly based on their layer depth\nwithin the network, maintaining shallow layers close to their pre-trained\nvalues to preserve general features while allowing deeper layers to retain\ntask-specific representations. In multi-task model merging scenarios,\nlayer-wise scaling of merged parameters reduces negative task interference.\nLiNeS demonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Our method is simple to implement, computationally efficient and\ncomplementary to many existing techniques. Our source code is available at\nhttps://github.com/wang-kee/LiNeS"
                },
                "authors": [
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Nikolaos Dimitriadis"
                    },
                    {
                        "name": "Alessandro Favero"
                    },
                    {
                        "name": "Guillermo Ortiz-Jimenez"
                    },
                    {
                        "name": "Francois Fleuret"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "arxiv_comment": "The first two authors contributed equally to this work. Accepted at\n  ICLR 2025. Project website: https://lines-merging.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20940v1",
                "updated": "2025-02-28T10:51:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    51,
                    22,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:51:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    51,
                    22,
                    4,
                    59,
                    0
                ],
                "title": "Probing the interstellar medium toward GRB 221009A through X-ray dust\n  scattering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the interstellar medium toward GRB 221009A through X-ray dust\n  scattering"
                },
                "summary": "The observation of 21 X-ray dust-scattering rings around the extraordinarily\nbright gamma-ray burst (GRB) 221009A provides a unique opportunity to study the\ninterstellar medium (ISM) through which the X-ray radiation traveled in our\nGalaxy and, by difference, in the host galaxy as well. In particular, since the\nring intensity and radius at a given time depend on the amount of dust and on\nits distance, respectively XMM-Newton and Swift images allowed us to map the\nISM around the direction of the GRB with better resolution than in the existing\noptical and infrared-based 3D dust maps, both in the plane of the sky (few\narcminutes) and along the line of sight (from $\\simeq 1$ pc for dust clouds\nwithin 1 kpc to $\\simeq 100$ pc for structures at distances larger than 10\nkpc). As a consequence, we can revise prior estimates of the GRB soft X-ray\nfluence, obtaining a $\\sim$35\\% lower value, which, however, still indicates a\nsubstantial excess with respect to the extrapolation of the spectral models\nconstrained by hard X-ray observations.\n  Additionally, we detect significant spectral variability in two azimuthal\nsectors of the X-ray rings, which can be fully attributed to different Galactic\nabsorption in these two directions. The comparison of the total hydrogen column\ndensity inferred from spectral fitting, with the Galactic contribution derived\nfrom the intensity of the X-ray rings, in the same sectors, allowed us to more\nrobustly constrain the absorption in the host galaxy to $N_{\\rm{H,z=0.151}}=\n(3.7\\pm0.3)\\,\\times\\,10^{21}\\,\\rm{cm^{-2}}$. This result is relevant not only\nto characterize the ISM of the host galaxy and to understand how the GRB\nradiation might have affected it, but also to model the broad-band spectrum of\nthe GRB afterglow and to constrain the properties of a possible underlying\nsupernova.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observation of 21 X-ray dust-scattering rings around the extraordinarily\nbright gamma-ray burst (GRB) 221009A provides a unique opportunity to study the\ninterstellar medium (ISM) through which the X-ray radiation traveled in our\nGalaxy and, by difference, in the host galaxy as well. In particular, since the\nring intensity and radius at a given time depend on the amount of dust and on\nits distance, respectively XMM-Newton and Swift images allowed us to map the\nISM around the direction of the GRB with better resolution than in the existing\noptical and infrared-based 3D dust maps, both in the plane of the sky (few\narcminutes) and along the line of sight (from $\\simeq 1$ pc for dust clouds\nwithin 1 kpc to $\\simeq 100$ pc for structures at distances larger than 10\nkpc). As a consequence, we can revise prior estimates of the GRB soft X-ray\nfluence, obtaining a $\\sim$35\\% lower value, which, however, still indicates a\nsubstantial excess with respect to the extrapolation of the spectral models\nconstrained by hard X-ray observations.\n  Additionally, we detect significant spectral variability in two azimuthal\nsectors of the X-ray rings, which can be fully attributed to different Galactic\nabsorption in these two directions. The comparison of the total hydrogen column\ndensity inferred from spectral fitting, with the Galactic contribution derived\nfrom the intensity of the X-ray rings, in the same sectors, allowed us to more\nrobustly constrain the absorption in the host galaxy to $N_{\\rm{H,z=0.151}}=\n(3.7\\pm0.3)\\,\\times\\,10^{21}\\,\\rm{cm^{-2}}$. This result is relevant not only\nto characterize the ISM of the host galaxy and to understand how the GRB\nradiation might have affected it, but also to model the broad-band spectrum of\nthe GRB afterglow and to constrain the properties of a possible underlying\nsupernova."
                },
                "authors": [
                    {
                        "name": "B. Vaia"
                    },
                    {
                        "name": ". Bonjak"
                    },
                    {
                        "name": "A. Bracco"
                    },
                    {
                        "name": "S. Campana"
                    },
                    {
                        "name": "P. Esposito"
                    },
                    {
                        "name": "V. Jeli"
                    },
                    {
                        "name": "A. Sacchi"
                    },
                    {
                        "name": "A. Tiengo"
                    }
                ],
                "author_detail": {
                    "name": "A. Tiengo"
                },
                "author": "A. Tiengo",
                "arxiv_comment": "Accepted for publication by A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20939v1",
                "updated": "2025-02-28T10:50:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    50,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:50:27Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    50,
                    27,
                    4,
                    59,
                    0
                ],
                "title": "Modeling cell differentiation in neuroblastoma: insights into\n  development, malignancy, and treatment relapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling cell differentiation in neuroblastoma: insights into\n  development, malignancy, and treatment relapse"
                },
                "summary": "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment."
                },
                "authors": [
                    {
                        "name": "Simon F. Martina-Perez"
                    },
                    {
                        "name": "Luke A. Heirene"
                    },
                    {
                        "name": "Jennifer C. Kasemeier"
                    },
                    {
                        "name": "Paul M. Kulesa"
                    },
                    {
                        "name": "Ruth E. Baker"
                    }
                ],
                "author_detail": {
                    "name": "Ruth E. Baker"
                },
                "author": "Ruth E. Baker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15127v2",
                "updated": "2025-02-28T10:49:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    49,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-09-23T15:33:38Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    33,
                    38,
                    0,
                    267,
                    0
                ],
                "title": "Cost-Effective, High-Performance Open-Source LLMs via Optimized Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective, High-Performance Open-Source LLMs via Optimized Context\n  Retrieval"
                },
                "summary": "Large Language Models (LLMs) in healthcare promise transformation, yet\nadoption is limited by concerns over factual accuracy and the high cost of\nproprietary models. This study demonstrates that optimized context retrieval\nunlocks cost-effective, high-performance healthcare AI using open-source LLMs,\nachieving a significantly improved cost-accuracy Pareto frontier for medical\nquestion answering and showcasing that open models can rival proprietary\nsystems at a fraction of the cost. A key contribution is OpenMedQA, a novel\nbenchmark for open-ended medical question answering that overcomes the\nlimitations of multiple-choice formats - formats that we show lead to\nperformance degradation in open-ended settings and often lack clinical realism.\nFurther contributions include: (1) practical guidelines for implementing\noptimized context retrieval; (2) empirical validation of enhanced\ncost-effectiveness via the improved Pareto frontier; (3) the introduction of\nOpenMedQA for rigorous evaluation of open-ended medical QA; and (4) the release\nof prompt_engine alongside CoT/ToT/Thinking databases as community resources\nfor cost-effective healthcare AI. Advancing optimized retrieval and open-ended\nQA benchmarking, we pave the way for more accessible and impactful LLM-powered\nhealthcare solutions. All the materials have been made public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in healthcare promise transformation, yet\nadoption is limited by concerns over factual accuracy and the high cost of\nproprietary models. This study demonstrates that optimized context retrieval\nunlocks cost-effective, high-performance healthcare AI using open-source LLMs,\nachieving a significantly improved cost-accuracy Pareto frontier for medical\nquestion answering and showcasing that open models can rival proprietary\nsystems at a fraction of the cost. A key contribution is OpenMedQA, a novel\nbenchmark for open-ended medical question answering that overcomes the\nlimitations of multiple-choice formats - formats that we show lead to\nperformance degradation in open-ended settings and often lack clinical realism.\nFurther contributions include: (1) practical guidelines for implementing\noptimized context retrieval; (2) empirical validation of enhanced\ncost-effectiveness via the improved Pareto frontier; (3) the introduction of\nOpenMedQA for rigorous evaluation of open-ended medical QA; and (4) the release\nof prompt_engine alongside CoT/ToT/Thinking databases as community resources\nfor cost-effective healthcare AI. Advancing optimized retrieval and open-ended\nQA benchmarking, we pave the way for more accessible and impactful LLM-powered\nhealthcare solutions. All the materials have been made public."
                },
                "authors": [
                    {
                        "name": "Jordi Bayarri-Planas"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "arxiv_comment": "14 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20938v1",
                "updated": "2025-02-28T10:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    48,
                    14,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    48,
                    14,
                    4,
                    59,
                    0
                ],
                "title": "A Deep User Interface for Exploring LLaMa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep User Interface for Exploring LLaMa"
                },
                "summary": "The growing popularity and widespread adoption of large language models\n(LLMs) necessitates the development of tools that enhance the effectiveness of\nuser interactions with these models. Understanding the structures and functions\nof these models poses a significant challenge for users. Visual\nanalytics-driven tools enables users to explore and compare, facilitating\nbetter decision-making. This paper presents a visual analytics-driven tool\nequipped with interactive controls for key hyperparameters, including top-p,\nfrequency and presence penalty, enabling users to explore, examine and compare\nthe outputs of LLMs. In a user study, we assessed the tool's effectiveness,\nwhich received favorable feedback for its visual design, with particular\ncommendation for the interface layout and ease of navigation. Additionally, the\nfeedback provided valuable insights for enhancing the effectiveness of\nHuman-LLM interaction tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity and widespread adoption of large language models\n(LLMs) necessitates the development of tools that enhance the effectiveness of\nuser interactions with these models. Understanding the structures and functions\nof these models poses a significant challenge for users. Visual\nanalytics-driven tools enables users to explore and compare, facilitating\nbetter decision-making. This paper presents a visual analytics-driven tool\nequipped with interactive controls for key hyperparameters, including top-p,\nfrequency and presence penalty, enabling users to explore, examine and compare\nthe outputs of LLMs. In a user study, we assessed the tool's effectiveness,\nwhich received favorable feedback for its visual design, with particular\ncommendation for the interface layout and ease of navigation. Additionally, the\nfeedback provided valuable insights for enhancing the effectiveness of\nHuman-LLM interaction tools."
                },
                "authors": [
                    {
                        "name": "Divya Perumal"
                    },
                    {
                        "name": "Swaroop Panda"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Panda"
                },
                "author": "Swaroop Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.21321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21321v1",
                "updated": "2025-02-28T18:59:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    59,
                    54,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:59:54Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    59,
                    54,
                    4,
                    59,
                    0
                ],
                "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training."
                },
                "authors": [
                    {
                        "name": "Komal Kumar"
                    },
                    {
                        "name": "Tajamul Ashraf"
                    },
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Phillip H. S. Torr"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "31 pages, 7 figures, 3 tables, 375 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10776v2",
                "updated": "2025-02-28T18:56:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    56,
                    33,
                    4,
                    59,
                    0
                ],
                "published": "2024-04-16T17:59:55Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    17,
                    59,
                    55,
                    1,
                    107,
                    0
                ],
                "title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from\n  Adversarial Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearly Optimal Algorithms for Contextual Dueling Bandits from\n  Adversarial Feedback"
                },
                "summary": "Learning from human feedback plays an important role in aligning generative\nmodels, such as large language models (LLM). However, the effectiveness of this\napproach can be influenced by adversaries, who may intentionally provide\nmisleading preferences to manipulate the output in an undesirable or harmful\ndirection. To tackle this challenge, we study a specific model within this\nproblem domain--contextual dueling bandits with adversarial feedback, where the\ntrue preference label can be flipped by an adversary. We propose an algorithm\nnamely robust contextual dueling bandits (RCDB), which is based on\nuncertainty-weighted maximum likelihood estimation. Our algorithm achieves an\n$\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of\nrounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the\nderivative of the link function, and $ 0 \\le C \\le T$ is the total number of\nadversarial feedback. We also prove a lower bound to show that our regret bound\nis nearly optimal, both in scenarios with and without ($C=0$) adversarial\nfeedback. Our work is the first to achieve nearly minimax optimal regret for\ndueling bandits in the presence of adversarial preference feedback.\nAdditionally, for the sigmoid link function, we develop a novel algorithm that\ntakes into account the effect of local derivatives into maximum likelihood\nestimation (MLE) analysis through a refined method for estimating the link\nfunction's derivative. This method helps us to eliminate the $\\kappa$\ndependence in the leading term with respect to $T$, which reduces the\nexponential dependence on the parameter radius $B$ to a polynomial dependence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human feedback plays an important role in aligning generative\nmodels, such as large language models (LLM). However, the effectiveness of this\napproach can be influenced by adversaries, who may intentionally provide\nmisleading preferences to manipulate the output in an undesirable or harmful\ndirection. To tackle this challenge, we study a specific model within this\nproblem domain--contextual dueling bandits with adversarial feedback, where the\ntrue preference label can be flipped by an adversary. We propose an algorithm\nnamely robust contextual dueling bandits (RCDB), which is based on\nuncertainty-weighted maximum likelihood estimation. Our algorithm achieves an\n$\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of\nrounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the\nderivative of the link function, and $ 0 \\le C \\le T$ is the total number of\nadversarial feedback. We also prove a lower bound to show that our regret bound\nis nearly optimal, both in scenarios with and without ($C=0$) adversarial\nfeedback. Our work is the first to achieve nearly minimax optimal regret for\ndueling bandits in the presence of adversarial preference feedback.\nAdditionally, for the sigmoid link function, we develop a novel algorithm that\ntakes into account the effect of local derivatives into maximum likelihood\nestimation (MLE) analysis through a refined method for estimating the link\nfunction's derivative. This method helps us to eliminate the $\\kappa$\ndependence in the leading term with respect to $T$, which reduces the\nexponential dependence on the parameter radius $B$ to a polynomial dependence."
                },
                "authors": [
                    {
                        "name": "Qiwei Di"
                    },
                    {
                        "name": "Jiafan He"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "44pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21309v1",
                "updated": "2025-02-28T18:52:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    52,
                    24,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:52:24Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    52,
                    24,
                    4,
                    59,
                    0
                ],
                "title": "FANformer: Improving Large Language Models Through Effective Periodicity\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FANformer: Improving Large Language Models Through Effective Periodicity\n  Modeling"
                },
                "summary": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yongding Tao"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jinliang Deng"
                    },
                    {
                        "name": "Hong Mei"
                    }
                ],
                "author_detail": {
                    "name": "Hong Mei"
                },
                "author": "Hong Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09768v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09768v3",
                "updated": "2025-02-28T18:27:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    27,
                    21,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-15T11:32:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    32,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Can Large Language Models Predict the Outcome of Judicial Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict the Outcome of Judicial Decisions?"
                },
                "summary": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a\ncomprehensive evaluation framework that integrates both quantitative metrics\n(such as BLEU, ROUGE, and BERT) and qualitative assessments (including\nCoherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate\nthat fine-tuned smaller models achieve comparable performance to larger models\nin task-specific contexts while offering significant resource efficiency.\nFurthermore, we investigate the impact of fine-tuning the model on a diverse\nset of instructions, offering valuable insights into the development of a more\nhuman-centric and adaptable LLM. We have made the dataset, code, and models\npublicly available to provide a solid foundation for future research in Arabic\nlegal NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a\ncomprehensive evaluation framework that integrates both quantitative metrics\n(such as BLEU, ROUGE, and BERT) and qualitative assessments (including\nCoherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate\nthat fine-tuned smaller models achieve comparable performance to larger models\nin task-specific contexts while offering significant resource efficiency.\nFurthermore, we investigate the impact of fine-tuning the model on a diverse\nset of instructions, offering valuable insights into the development of a more\nhuman-centric and adaptable LLM. We have made the dataset, code, and models\npublicly available to provide a solid foundation for future research in Arabic\nlegal NLP."
                },
                "authors": [
                    {
                        "name": "Mohamed Bayan Kmainasi"
                    },
                    {
                        "name": "Ali Ezzat Shahroor"
                    },
                    {
                        "name": "Amani Al-Ghraibah"
                    }
                ],
                "author_detail": {
                    "name": "Amani Al-Ghraibah"
                },
                "author": "Amani Al-Ghraibah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09768v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09768v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21290v1",
                "updated": "2025-02-28T18:15:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    15,
                    31,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T18:15:31Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    15,
                    31,
                    4,
                    59,
                    0
                ],
                "title": "Contextualizing biological perturbation experiments through language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing biological perturbation experiments through language"
                },
                "summary": "High-content perturbation experiments allow scientists to probe biomolecular\nsystems at unprecedented resolution, but experimental and analysis costs pose\nsignificant barriers to widespread adoption. Machine learning has the potential\nto guide efficient exploration of the perturbation space and extract novel\ninsights from these data. However, current approaches neglect the semantic\nrichness of the relevant biology, and their objectives are misaligned with\ndownstream biological analyses. In this paper, we hypothesize that large\nlanguage models (LLMs) present a natural medium for representing complex\nbiological relationships and rationalizing experimental outcomes. We propose\nPerturbQA, a benchmark for structured reasoning over perturbation experiments.\nUnlike current benchmarks that primarily interrogate existing knowledge,\nPerturbQA is inspired by open problems in perturbation modeling: prediction of\ndifferential expression and change of direction for unseen perturbations, and\ngene set enrichment. We evaluate state-of-the-art machine learning and\nstatistical approaches for modeling perturbations, as well as standard LLM\nreasoning strategies, and we find that current methods perform poorly on\nPerturbQA. As a proof of feasibility, we introduce Summer (SUMMarize, retrievE,\nand answeR, a simple, domain-informed LLM framework that matches or exceeds the\ncurrent state-of-the-art. Our code and data are publicly available at\nhttps://github.com/genentech/PerturbQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-content perturbation experiments allow scientists to probe biomolecular\nsystems at unprecedented resolution, but experimental and analysis costs pose\nsignificant barriers to widespread adoption. Machine learning has the potential\nto guide efficient exploration of the perturbation space and extract novel\ninsights from these data. However, current approaches neglect the semantic\nrichness of the relevant biology, and their objectives are misaligned with\ndownstream biological analyses. In this paper, we hypothesize that large\nlanguage models (LLMs) present a natural medium for representing complex\nbiological relationships and rationalizing experimental outcomes. We propose\nPerturbQA, a benchmark for structured reasoning over perturbation experiments.\nUnlike current benchmarks that primarily interrogate existing knowledge,\nPerturbQA is inspired by open problems in perturbation modeling: prediction of\ndifferential expression and change of direction for unseen perturbations, and\ngene set enrichment. We evaluate state-of-the-art machine learning and\nstatistical approaches for modeling perturbations, as well as standard LLM\nreasoning strategies, and we find that current methods perform poorly on\nPerturbQA. As a proof of feasibility, we introduce Summer (SUMMarize, retrievE,\nand answeR, a simple, domain-informed LLM framework that matches or exceeds the\ncurrent state-of-the-art. Our code and data are publicly available at\nhttps://github.com/genentech/PerturbQA."
                },
                "authors": [
                    {
                        "name": "Menghua Wu"
                    },
                    {
                        "name": "Russell Littman"
                    },
                    {
                        "name": "Jacob Levine"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "David Richmond"
                    },
                    {
                        "name": "Jan-Christian Huetter"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Christian Huetter"
                },
                "author": "Jan-Christian Huetter",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00075v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00075v5",
                "updated": "2025-02-28T17:50:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    50,
                    12,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-21T19:18:16Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    19,
                    18,
                    16,
                    4,
                    173,
                    0
                ],
                "title": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference"
                },
                "summary": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks."
                },
                "authors": [
                    {
                        "name": "Anton Xue"
                    },
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Surbhi Goel"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00075v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00075v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21271v1",
                "updated": "2025-02-28T17:46:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    46,
                    29,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:46:29Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    46,
                    29,
                    4,
                    59,
                    0
                ],
                "title": "Adaptive Keyframe Sampling for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Keyframe Sampling for Long Video Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have enabled open-world visual\nunderstanding by injecting visual input as extra tokens into large language\nmodels (LLMs) as contexts. However, when the visual input changes from a single\nimage to a long video, the above paradigm encounters difficulty because the\nvast amount of video tokens has significantly exceeded the maximal capacity of\nMLLMs. Therefore, existing video-based MLLMs are mostly established upon\nsampling a small portion of tokens from input data, which can cause key\ninformation to be lost and thus produce incorrect answers. This paper presents\na simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It\ninserts a plug-and-play module known as keyframe selection, which aims to\nmaximize the useful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving (1) the relevance\nbetween the keyframes and the prompt, and (2) the coverage of the keyframes\nover the video, and present an adaptive algorithm to approximate the best\nsolution. Experiments on two long video understanding benchmarks validate that\nAdaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines)\nupon selecting informative keyframes. Our study reveals the importance of\ninformation pre-filtering in video-based MLLMs. Code is available at\nhttps://github.com/ncTimTang/AKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enabled open-world visual\nunderstanding by injecting visual input as extra tokens into large language\nmodels (LLMs) as contexts. However, when the visual input changes from a single\nimage to a long video, the above paradigm encounters difficulty because the\nvast amount of video tokens has significantly exceeded the maximal capacity of\nMLLMs. Therefore, existing video-based MLLMs are mostly established upon\nsampling a small portion of tokens from input data, which can cause key\ninformation to be lost and thus produce incorrect answers. This paper presents\na simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It\ninserts a plug-and-play module known as keyframe selection, which aims to\nmaximize the useful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving (1) the relevance\nbetween the keyframes and the prompt, and (2) the coverage of the keyframes\nover the video, and present an adaptive algorithm to approximate the best\nsolution. Experiments on two long video understanding benchmarks validate that\nAdaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines)\nupon selecting informative keyframes. Our study reveals the importance of\ninformation pre-filtering in video-based MLLMs. Code is available at\nhttps://github.com/ncTimTang/AKS."
                },
                "authors": [
                    {
                        "name": "Xi Tang"
                    },
                    {
                        "name": "Jihao Qiu"
                    },
                    {
                        "name": "Lingxi Xie"
                    },
                    {
                        "name": "Yunjie Tian"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Qixiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Qixiang Ye"
                },
                "author": "Qixiang Ye",
                "arxiv_comment": "CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21265v1",
                "updated": "2025-02-28T17:41:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    41,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:41:27Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    41,
                    27,
                    4,
                    59,
                    0
                ],
                "title": "Token-level Ensembling of Models with Different Vocabularies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-level Ensembling of Models with Different Vocabularies"
                },
                "summary": "Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually."
                },
                "authors": [
                    {
                        "name": "Rachel Wicks"
                    },
                    {
                        "name": "Kartik Ravisankar"
                    },
                    {
                        "name": "Xinchen Yang"
                    },
                    {
                        "name": "Philipp Koehn"
                    },
                    {
                        "name": "Matt Post"
                    }
                ],
                "author_detail": {
                    "name": "Matt Post"
                },
                "author": "Matt Post",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21248v1",
                "updated": "2025-02-28T17:18:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    18,
                    38,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:18:38Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    18,
                    38,
                    4,
                    59,
                    0
                ],
                "title": "Digital Doppelgangers: Ethical and Societal Implications of Pre-Mortem\n  AI Clones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Doppelgangers: Ethical and Societal Implications of Pre-Mortem\n  AI Clones"
                },
                "summary": "The rapid advancement of generative AI has enabled the creation of pre-mortem\ndigital twins, AI-driven replicas that mimic the behavior, personality, and\nknowledge of living individuals. These digital doppelgangers serve various\nfunctions, including enhancing productivity, enabling creative collaboration,\nand preserving personal legacies. However, their development raises critical\nethical, legal, and societal concerns. Issues such as identity fragmentation,\npsychological effects on individuals and their social circles, and the risks of\nunauthorized cloning and data exploitation demand careful examination.\nAdditionally, as these AI clones evolve into more autonomous entities, concerns\nabout consent, ownership, and accountability become increasingly complex.\n  This paper differentiates pre-mortem AI clones from post-mortem generative\nghosts, examining their unique ethical and legal implications. We explore key\nchallenges, including the erosion of personal identity, the implications of AI\nagency, and the regulatory gaps in digital rights and privacy laws. Through a\nresearch-driven approach, we propose a framework for responsible AI governance,\nemphasizing identity preservation, consent mechanisms, and autonomy safeguards.\nBy aligning technological advancements with societal values, this study\ncontributes to the growing discourse on AI ethics and provides policy\nrecommendations for the ethical deployment of pre-mortem AI clones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of generative AI has enabled the creation of pre-mortem\ndigital twins, AI-driven replicas that mimic the behavior, personality, and\nknowledge of living individuals. These digital doppelgangers serve various\nfunctions, including enhancing productivity, enabling creative collaboration,\nand preserving personal legacies. However, their development raises critical\nethical, legal, and societal concerns. Issues such as identity fragmentation,\npsychological effects on individuals and their social circles, and the risks of\nunauthorized cloning and data exploitation demand careful examination.\nAdditionally, as these AI clones evolve into more autonomous entities, concerns\nabout consent, ownership, and accountability become increasingly complex.\n  This paper differentiates pre-mortem AI clones from post-mortem generative\nghosts, examining their unique ethical and legal implications. We explore key\nchallenges, including the erosion of personal identity, the implications of AI\nagency, and the regulatory gaps in digital rights and privacy laws. Through a\nresearch-driven approach, we propose a framework for responsible AI governance,\nemphasizing identity preservation, consent mechanisms, and autonomy safeguards.\nBy aligning technological advancements with societal values, this study\ncontributes to the growing discourse on AI ethics and provides policy\nrecommendations for the ethical deployment of pre-mortem AI clones."
                },
                "authors": [
                    {
                        "name": "Vijayalaxmi Methuku"
                    },
                    {
                        "name": "Praveen Kumar Myakala"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Kumar Myakala"
                },
                "author": "Praveen Kumar Myakala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21239v1",
                "updated": "2025-02-28T17:09:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:09:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Swair Shah"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16100v2",
                "updated": "2025-02-28T17:02:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    2,
                    23,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-20T17:42:25Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    42,
                    25,
                    4,
                    355,
                    0
                ],
                "title": "Logical Consistency of Large Language Models in Fact-checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical Consistency of Large Language Models in Fact-checking"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses - a meaning-preserving change in the input query results\nin an inconsistent response and attributes to vulnerabilities of LLMs such as\nhallucination. Consequently, existing research focuses on simple\nparaphrasing-based consistency assessment of LLMs, and ignores complex queries\nthat necessitate an even better understanding of logical reasoning by an LLM.\nOur work therefore addresses the logical inconsistency of LLMs under complex\nlogical queries with primitive logical operators, e.g., negation, conjunction,\nand disjunction. As a test bed, we consider retrieval-augmented LLMs on a\nfact-checking task involving propositional logic queries from knowledge graphs\n(KGs). Our contributions are threefold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries and demonstrate that existing LLMs lack logical\nconsistency, especially on complex queries. Improvement: We employ supervised\nfine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts. We have made our source code and\nbenchmarks available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses - a meaning-preserving change in the input query results\nin an inconsistent response and attributes to vulnerabilities of LLMs such as\nhallucination. Consequently, existing research focuses on simple\nparaphrasing-based consistency assessment of LLMs, and ignores complex queries\nthat necessitate an even better understanding of logical reasoning by an LLM.\nOur work therefore addresses the logical inconsistency of LLMs under complex\nlogical queries with primitive logical operators, e.g., negation, conjunction,\nand disjunction. As a test bed, we consider retrieval-augmented LLMs on a\nfact-checking task involving propositional logic queries from knowledge graphs\n(KGs). Our contributions are threefold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries and demonstrate that existing LLMs lack logical\nconsistency, especially on complex queries. Improvement: We employ supervised\nfine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts. We have made our source code and\nbenchmarks available."
                },
                "authors": [
                    {
                        "name": "Bishwamittra Ghosh"
                    },
                    {
                        "name": "Sarah Hasan"
                    },
                    {
                        "name": "Naheed Anjum Arafat"
                    },
                    {
                        "name": "Arijit Khan"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Khan"
                },
                "author": "Arijit Khan",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21231v1",
                "updated": "2025-02-28T17:01:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    1,
                    3,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T17:01:03Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    1,
                    3,
                    4,
                    59,
                    0
                ],
                "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length\n  on More Than 12,000 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length\n  on More Than 12,000 GPUs"
                },
                "summary": "Scaling long-context ability is essential for Large Language Models (LLMs).\nTo amortize the memory consumption across multiple devices in long-context\ntraining, inter-data partitioning (a.k.a. Data Parallelism) and intra-data\npartitioning (a.k.a. Context Parallelism) are commonly used. Current training\nframeworks predominantly treat the two techniques as orthogonal, and establish\nstatic communication groups to organize the devices as a static mesh (e.g., a\n2D mesh). However, the sequences for LLM training typically vary in lengths, no\nmatter for texts, multi-modalities or reinforcement learning. The mismatch\nbetween data heterogeneity and static mesh causes redundant communication and\nimbalanced computation, degrading the training efficiency.\n  In this work, we introduce ByteScale, an efficient, flexible, and scalable\nLLM training framework for large-scale mixed training of long and short\nsequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid\nData Parallelism (HDP), which unifies the inter- and intra-data partitioning\nwith a dynamic mesh design. In particular, we build a communication optimizer,\nwhich eliminates the redundant communication for short sequences by data-aware\nsharding and dynamic communication, and further compresses the communication\ncost for long sequences by selective offloading. Besides, we also develop a\nbalance scheduler to mitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model sizes ranging from 7B to\n141B, context lengths from 256K to 2048K, on a production cluster with more\nthan 12,000 GPUs. Experiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling long-context ability is essential for Large Language Models (LLMs).\nTo amortize the memory consumption across multiple devices in long-context\ntraining, inter-data partitioning (a.k.a. Data Parallelism) and intra-data\npartitioning (a.k.a. Context Parallelism) are commonly used. Current training\nframeworks predominantly treat the two techniques as orthogonal, and establish\nstatic communication groups to organize the devices as a static mesh (e.g., a\n2D mesh). However, the sequences for LLM training typically vary in lengths, no\nmatter for texts, multi-modalities or reinforcement learning. The mismatch\nbetween data heterogeneity and static mesh causes redundant communication and\nimbalanced computation, degrading the training efficiency.\n  In this work, we introduce ByteScale, an efficient, flexible, and scalable\nLLM training framework for large-scale mixed training of long and short\nsequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid\nData Parallelism (HDP), which unifies the inter- and intra-data partitioning\nwith a dynamic mesh design. In particular, we build a communication optimizer,\nwhich eliminates the redundant communication for short sequences by data-aware\nsharding and dynamic communication, and further compresses the communication\ncost for long sequences by selective offloading. Besides, we also develop a\nbalance scheduler to mitigate the imbalanced computation by parallelism-aware\ndata assignment. We evaluate ByteScale with the model sizes ranging from 7B to\n141B, context lengths from 256K to 2048K, on a production cluster with more\nthan 12,000 GPUs. Experiment results show that ByteScale outperforms the\nstate-of-the-art training system by up to 7.89x."
                },
                "authors": [
                    {
                        "name": "Hao Ge"
                    },
                    {
                        "name": "Junda Feng"
                    },
                    {
                        "name": "Qi Huang"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Lei Zuo"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "arxiv_comment": "12 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21228v1",
                "updated": "2025-02-28T16:59:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    59,
                    30,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:59:30Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    59,
                    30,
                    4,
                    59,
                    0
                ],
                "title": "ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer"
                },
                "summary": "To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in."
                },
                "authors": [
                    {
                        "name": "Omer Goldman"
                    },
                    {
                        "name": "Uri Shaham"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Joshua Maynez"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Jason Riesa"
                    },
                    {
                        "name": "Shruti Rijhwani"
                    },
                    {
                        "name": "Laura Rimell"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Reut Tsarfaty"
                    },
                    {
                        "name": "Matan Eyal"
                    }
                ],
                "author_detail": {
                    "name": "Matan Eyal"
                },
                "author": "Matan Eyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20672v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20672v3",
                "updated": "2025-02-28T16:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    44,
                    24,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-28T02:15:45Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    2,
                    15,
                    45,
                    0,
                    302,
                    0
                ],
                "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA"
                },
                "summary": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Seungyeon Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Tal Schuster"
                },
                "author": "Tal Schuster",
                "arxiv_comment": "ICLR 2025; 49 pages, 17 figures, 19 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20672v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20672v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21212v1",
                "updated": "2025-02-28T16:40:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    40,
                    38,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:40:38Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    40,
                    38,
                    4,
                    59,
                    0
                ],
                "title": "Transformers Learn to Implement Multi-step Gradient Descent with Chain\n  of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Learn to Implement Multi-step Gradient Descent with Chain\n  of Thought"
                },
                "summary": "Chain of Thought (CoT) prompting has been shown to significantly improve the\nperformance of large language models (LLMs), particularly in arithmetic and\nreasoning tasks, by instructing the model to produce intermediate reasoning\nsteps. Despite the remarkable empirical success of CoT and its theoretical\nadvantages in enhancing expressivity, the mechanisms underlying CoT training\nremain largely unexplored. In this paper, we study the training dynamics of\ntransformers over a CoT objective on an in-context weight prediction task for\nlinear regression. We prove that while a one-layer linear transformer without\nCoT can only implement a single step of gradient descent (GD) and fails to\nrecover the ground-truth weight vector, a transformer with CoT prompting can\nlearn to perform multi-step GD autoregressively, achieving near-exact recovery.\nFurthermore, we show that the trained transformer effectively generalizes on\nthe unseen data. With our technique, we also show that looped transformers\nsignificantly improve final performance compared to transformers without\nlooping in the in-context learning of linear regression. Empirically, we\ndemonstrate that CoT prompting yields substantial performance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) prompting has been shown to significantly improve the\nperformance of large language models (LLMs), particularly in arithmetic and\nreasoning tasks, by instructing the model to produce intermediate reasoning\nsteps. Despite the remarkable empirical success of CoT and its theoretical\nadvantages in enhancing expressivity, the mechanisms underlying CoT training\nremain largely unexplored. In this paper, we study the training dynamics of\ntransformers over a CoT objective on an in-context weight prediction task for\nlinear regression. We prove that while a one-layer linear transformer without\nCoT can only implement a single step of gradient descent (GD) and fails to\nrecover the ground-truth weight vector, a transformer with CoT prompting can\nlearn to perform multi-step GD autoregressively, achieving near-exact recovery.\nFurthermore, we show that the trained transformer effectively generalizes on\nthe unseen data. With our technique, we also show that looped transformers\nsignificantly improve final performance compared to transformers without\nlooping in the in-context learning of linear regression. Empirically, we\ndemonstrate that CoT prompting yields substantial performance improvements."
                },
                "authors": [
                    {
                        "name": "Jianhao Huang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jason D. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. Lee"
                },
                "author": "Jason D. Lee",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21209v1",
                "updated": "2025-02-28T16:28:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    28,
                    15,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:28:15Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    28,
                    15,
                    4,
                    59,
                    0
                ],
                "title": "End-to-End Deep Learning in Phase Noisy Coherent Optical Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Deep Learning in Phase Noisy Coherent Optical Link"
                },
                "summary": "In coherent optical orthogonal frequency-division multiplexing (CO-OFDM)\nfiber communications, a novel end-to-end learning framework to mitigate Laser\nPhase Noise (LPN) impairments is proposed in this paper. Inspired by\nAutoencoder (AE) principles, the proposed approach trains a model to learn\nrobust symbol sequences capable of combat LPN, even from low-cost distributed\nfeedback (DFB) lasers with linewidths up to 2 MHz. This allows for the use of\nhigh-level modulation formats and large-scale Fast Fourier Transform (FFT)\nprocessing, maximizing spectral efficiency in CO-OFDM systems. By eliminating\nthe need for complex traditional techniques, this approach offers a potentially\nmore efficient and streamlined solution for CO-OFDM systems. The most\nsignificant achievement of this study is the demonstration that the proposed\nAE-based model can enhance system performance by reducing the bit error rate\n(BER) to below the threshold of forward error correction (FEC), even under\nsevere phase noise conditions, thus proving its effectiveness and efficiency in\npractical deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In coherent optical orthogonal frequency-division multiplexing (CO-OFDM)\nfiber communications, a novel end-to-end learning framework to mitigate Laser\nPhase Noise (LPN) impairments is proposed in this paper. Inspired by\nAutoencoder (AE) principles, the proposed approach trains a model to learn\nrobust symbol sequences capable of combat LPN, even from low-cost distributed\nfeedback (DFB) lasers with linewidths up to 2 MHz. This allows for the use of\nhigh-level modulation formats and large-scale Fast Fourier Transform (FFT)\nprocessing, maximizing spectral efficiency in CO-OFDM systems. By eliminating\nthe need for complex traditional techniques, this approach offers a potentially\nmore efficient and streamlined solution for CO-OFDM systems. The most\nsignificant achievement of this study is the demonstration that the proposed\nAE-based model can enhance system performance by reducing the bit error rate\n(BER) to below the threshold of forward error correction (FEC), even under\nsevere phase noise conditions, thus proving its effectiveness and efficiency in\npractical deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Omar Alnaseri"
                    },
                    {
                        "name": "Yassine Himeur"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Himeur"
                },
                "author": "Yassine Himeur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21208v1",
                "updated": "2025-02-28T16:28:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    28,
                    13,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:28:13Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    28,
                    13,
                    4,
                    59,
                    0
                ],
                "title": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARIES: Autonomous Reasoning with LLMs on Interactive Thought Graph\n  Environments"
                },
                "summary": "Recent research has shown that LLM performance on reasoning tasks can be\nenhanced by scaling test-time compute. One promising approach, particularly\nwith decomposable problems, involves arranging intermediate solutions as a\ngraph on which transformations are performed to explore the solution space.\nHowever, prior works rely on pre-determined, task-specific transformation\nschedules which are subject to a set of searched hyperparameters. In this work,\nwe view thought graph transformations as actions in a Markov decision process,\nand implement policy agents to drive effective action policies for the\nunderlying reasoning LLM agent. In particular, we investigate the ability for\nanother LLM to act as a policy agent on thought graph environments and\nintroduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES,\nreasoning LLM agents solve decomposed subproblems, while policy LLM agents\nmaintain visibility of the thought graph states, and dynamically adapt the\nproblem-solving strategy. Through extensive experiments, we observe that using\noff-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can\nyield up to $29\\%$ higher accuracy on HumanEval relative to static\ntransformation schedules, as well as reducing inference costs by $35\\%$ and\navoid any search requirements. We also conduct a thorough analysis of observed\nfailure modes, highlighting that limitations on LLM sizes and the depth of\nproblem decomposition can be seen as challenges to scaling LLM-guided\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that LLM performance on reasoning tasks can be\nenhanced by scaling test-time compute. One promising approach, particularly\nwith decomposable problems, involves arranging intermediate solutions as a\ngraph on which transformations are performed to explore the solution space.\nHowever, prior works rely on pre-determined, task-specific transformation\nschedules which are subject to a set of searched hyperparameters. In this work,\nwe view thought graph transformations as actions in a Markov decision process,\nand implement policy agents to drive effective action policies for the\nunderlying reasoning LLM agent. In particular, we investigate the ability for\nanother LLM to act as a policy agent on thought graph environments and\nintroduce ARIES, a multi-agent architecture for reasoning with LLMs. In ARIES,\nreasoning LLM agents solve decomposed subproblems, while policy LLM agents\nmaintain visibility of the thought graph states, and dynamically adapt the\nproblem-solving strategy. Through extensive experiments, we observe that using\noff-the-shelf LLMs as policy agents with no supervised fine-tuning (SFT) can\nyield up to $29\\%$ higher accuracy on HumanEval relative to static\ntransformation schedules, as well as reducing inference costs by $35\\%$ and\navoid any search requirements. We also conduct a thorough analysis of observed\nfailure modes, highlighting that limitations on LLM sizes and the depth of\nproblem decomposition can be seen as challenges to scaling LLM-guided\nreasoning."
                },
                "authors": [
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Jeffrey Wong"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21198v1",
                "updated": "2025-02-28T16:15:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    15,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T16:15:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    15,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "AI-Enhanced Self-Triggering for Extensive Air Showers: Performance and\n  FPGA Feasibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Enhanced Self-Triggering for Extensive Air Showers: Performance and\n  FPGA Feasibility"
                },
                "summary": "Cosmic-ray detection with radio antennas has traditionally depended on\nexternal triggers from particle detectors, constraining sensitivity and\nincreasing complexity. Previous attempts at fully standalone, radio-only\ntriggers have often failed under intense radio frequency interference, making\ngenuine air-shower signals difficult to isolate. We present a\nproof-of-principle artificial intelligence-based self-triggering system that\novercomes these limitations. By training a deep learning model on both real\nnoise data and injected cosmic-ray-like pulses, we achieve an exceptionally low\nfalse-positive rate alongside high detection efficiency. Configurable operating\npoints can suppress false positives below 0.01\\% while retaining more than 88\\%\nof genuine signals, and can even eliminate false positives entirely at a modest\nreduction in signal efficiency. This flexibility makes single-station\ncosmic-ray detection feasible without requiring external trigger inputs.\nApplying our approach to real-world noise conditions reduces the initial\nfalse-positive event rate by several orders of magnitude, supporting\nlarge-scale deployments. Extrapolation to dedicated hardware implementations,\nsuch as FPGAs, indicates that sub-\\SI{}{\\micro\\second} inference times are\nachievable, enabling real-time autonomous triggering. These results highlight\nthe transformative potential of artificial intelligence for enhancing radio\ndetection sensitivity and inaugurate a new generation of fully self-triggered\ncosmic-ray observatories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic-ray detection with radio antennas has traditionally depended on\nexternal triggers from particle detectors, constraining sensitivity and\nincreasing complexity. Previous attempts at fully standalone, radio-only\ntriggers have often failed under intense radio frequency interference, making\ngenuine air-shower signals difficult to isolate. We present a\nproof-of-principle artificial intelligence-based self-triggering system that\novercomes these limitations. By training a deep learning model on both real\nnoise data and injected cosmic-ray-like pulses, we achieve an exceptionally low\nfalse-positive rate alongside high detection efficiency. Configurable operating\npoints can suppress false positives below 0.01\\% while retaining more than 88\\%\nof genuine signals, and can even eliminate false positives entirely at a modest\nreduction in signal efficiency. This flexibility makes single-station\ncosmic-ray detection feasible without requiring external trigger inputs.\nApplying our approach to real-world noise conditions reduces the initial\nfalse-positive event rate by several orders of magnitude, supporting\nlarge-scale deployments. Extrapolation to dedicated hardware implementations,\nsuch as FPGAs, indicates that sub-\\SI{}{\\micro\\second} inference times are\nachievable, enabling real-time autonomous triggering. These results highlight\nthe transformative potential of artificial intelligence for enhancing radio\ndetection sensitivity and inaugurate a new generation of fully self-triggered\ncosmic-ray observatories."
                },
                "authors": [
                    {
                        "name": "Qader Dorosti"
                    }
                ],
                "author_detail": {
                    "name": "Qader Dorosti"
                },
                "author": "Qader Dorosti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05467v4",
                "updated": "2025-02-28T16:02:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    16,
                    2,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-06T23:43:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    23,
                    43,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "The BrowserGym Ecosystem for Web Agent Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym Ecosystem for Web Agent Research"
                },
                "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs). Many existing benchmarks suffer from\nfragmentation and inconsistent evaluation methodologies, making it challenging\nto achieve reliable comparisons and reproducible results. In an earlier work,\nDrouin et al. (2024) introduced BrowserGym which aims to solve this by\nproviding a unified, gym-like environment with well-defined observation and\naction spaces, facilitating standardized evaluation across diverse benchmarks.\nWe propose an extended BrowserGym-based ecosystem for web agent research, which\nunifies existing benchmarks from the literature and includes AgentLab, a\ncomplementary framework that aids in agent creation, testing, and analysis. Our\nproposed ecosystem offers flexibility for integrating new benchmarks while\nensuring consistent evaluation and comprehensive experiment management. As a\nsupporting evidence, we conduct the first large-scale, multi-benchmark web\nagent experiment and compare the performance of 6 state-of-the-art LLMs across\n6 popular web agent benchmarks made available in BrowserGym. Among other\nfindings, our results highlight a large discrepancy between OpenAI and\nAnthropic's latests models, with Claude-3.5-Sonnet leading the way on almost\nall benchmarks, except on vision-related tasks where GPT-4o is superior.\nDespite these advancements, our results emphasize that building robust and\nefficient web agents remains a significant challenge, due to the inherent\ncomplexity of real-world web environments and the limitations of current\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs). Many existing benchmarks suffer from\nfragmentation and inconsistent evaluation methodologies, making it challenging\nto achieve reliable comparisons and reproducible results. In an earlier work,\nDrouin et al. (2024) introduced BrowserGym which aims to solve this by\nproviding a unified, gym-like environment with well-defined observation and\naction spaces, facilitating standardized evaluation across diverse benchmarks.\nWe propose an extended BrowserGym-based ecosystem for web agent research, which\nunifies existing benchmarks from the literature and includes AgentLab, a\ncomplementary framework that aids in agent creation, testing, and analysis. Our\nproposed ecosystem offers flexibility for integrating new benchmarks while\nensuring consistent evaluation and comprehensive experiment management. As a\nsupporting evidence, we conduct the first large-scale, multi-benchmark web\nagent experiment and compare the performance of 6 state-of-the-art LLMs across\n6 popular web agent benchmarks made available in BrowserGym. Among other\nfindings, our results highlight a large discrepancy between OpenAI and\nAnthropic's latests models, with Claude-3.5-Sonnet leading the way on almost\nall benchmarks, except on vision-related tasks where GPT-4o is superior.\nDespite these advancements, our results emphasize that building robust and\nefficient web agents remains a significant challenge, due to the inherent\ncomplexity of real-world web environments and the limitations of current\nmodels."
                },
                "authors": [
                    {
                        "name": "Thibault Le Sellier De Chezelles"
                    },
                    {
                        "name": "Maxime Gasse"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "Lo Boisvert"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Rim Assouel"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Lawrence Keunho Jang"
                    },
                    {
                        "name": "Xing Han L"
                    },
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11843v3",
                "updated": "2025-02-28T15:41:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    41,
                    0,
                    4,
                    59,
                    0
                ],
                "published": "2024-09-23T08:39:16Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    39,
                    16,
                    0,
                    267,
                    0
                ],
                "title": "From Commands to Prompts: LLM-based Semantic File System for AIOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Commands to Prompts: LLM-based Semantic File System for AIOS"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Chaoji Zuo"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yujie Ren"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Dong Deng"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03664v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03664v4",
                "updated": "2025-02-28T15:33:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    33,
                    10,
                    4,
                    59,
                    0
                ],
                "published": "2024-02-16T10:56:15Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    10,
                    56,
                    15,
                    4,
                    47,
                    0
                ],
                "title": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in the Heart of Differential Testing: A Case Study on a Medical\n  Rule Engine"
                },
                "summary": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cancer Registry of Norway (CRN) uses an automated cancer registration\nsupport system (CaReSS) to support core cancer registry activities, i.e, data\ncapture, data curation, and producing data products and statistics for various\nstakeholders. GURI is a core component of CaReSS, which is responsible for\nvalidating incoming data with medical rules. Such medical rules are manually\nimplemented by medical experts based on medical standards, regulations, and\nresearch. Since large language models (LLMs) have been trained on a large\namount of public information, including these documents, they can be employed\nto generate tests for GURI. Thus, we propose an LLM-based test generation and\ndifferential testing approach (LLMeDiff) to test GURI. We experimented with\nfour different LLMs, two medical rule engine implementations, and 58 real\nmedical rules to investigate the hallucination, success, time efficiency, and\nrobustness of the LLMs to generate tests, and these tests' ability to find\npotential issues in GURI. Our results showed that GPT-3.5 hallucinates the\nleast, is the most successful, and is generally the most robust; however, it\nhas the worst time efficiency. Our differential testing revealed 22 medical\nrules where implementation inconsistencies were discovered (e.g., regarding\nhandling rule versions). Finally, we provide insights for practitioners and\nresearchers based on the results."
                },
                "authors": [
                    {
                        "name": "Erblin Isaku"
                    },
                    {
                        "name": "Christoph Laaber"
                    },
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Thomas Schwitalla"
                    },
                    {
                        "name": "Jan F. Nygrd"
                    }
                ],
                "author_detail": {
                    "name": "Jan F. Nygrd"
                },
                "author": "Jan F. Nygrd",
                "arxiv_comment": "12 pages, 6 figures, 4 tables, 1 listing, revised arguments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03664v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03664v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15296v3",
                "updated": "2025-02-28T15:23:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    23,
                    40,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-25T18:26:39Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    18,
                    26,
                    39,
                    5,
                    25,
                    0
                ],
                "title": "You Only Prune Once: Designing Calibration-Free Model Compression With\n  Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Prune Once: Designing Calibration-Free Model Compression With\n  Policy Learning"
                },
                "summary": "The ever-increasing size of large language models (LLMs) presents significant\nchallenges for deployment due to their heavy computational and memory\nrequirements. Current model pruning techniques attempt to alleviate these\nissues by relying heavily on external calibration datasets to determine which\nparameters to prune or compress, thus limiting their flexibility and\nscalability across different compression ratios. Moreover, these methods often\ncause severe performance degradation, particularly in downstream tasks, when\nsubjected to higher compression rates. In this paper, we propose PruneNet, a\nnovel model compression method that addresses these limitations by\nreformulating model pruning as a policy learning process. PruneNet decouples\nthe pruning process from the model architecture, eliminating the need for\ncalibration datasets. It learns a stochastic pruning policy to assess parameter\nimportance solely based on intrinsic model properties while preserving the\nspectral structure to minimize information loss. PruneNet can compress the\nLLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its\nzero-shot performance with a 30% compression ratio, outperforming existing\nmethods that retain only 75% performance. Furthermore, on complex multitask\nlanguage understanding tasks, PruneNet demonstrates its robustness by\npreserving up to 80% performance of the original model, proving itself a\nsuperior alternative to conventional structured compression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing size of large language models (LLMs) presents significant\nchallenges for deployment due to their heavy computational and memory\nrequirements. Current model pruning techniques attempt to alleviate these\nissues by relying heavily on external calibration datasets to determine which\nparameters to prune or compress, thus limiting their flexibility and\nscalability across different compression ratios. Moreover, these methods often\ncause severe performance degradation, particularly in downstream tasks, when\nsubjected to higher compression rates. In this paper, we propose PruneNet, a\nnovel model compression method that addresses these limitations by\nreformulating model pruning as a policy learning process. PruneNet decouples\nthe pruning process from the model architecture, eliminating the need for\ncalibration datasets. It learns a stochastic pruning policy to assess parameter\nimportance solely based on intrinsic model properties while preserving the\nspectral structure to minimize information loss. PruneNet can compress the\nLLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its\nzero-shot performance with a 30% compression ratio, outperforming existing\nmethods that retain only 75% performance. Furthermore, on complex multitask\nlanguage understanding tasks, PruneNet demonstrates its robustness by\npreserving up to 80% performance of the original model, proving itself a\nsuperior alternative to conventional structured compression techniques."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08587v2",
                "updated": "2025-02-28T15:16:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    16,
                    4,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-12T18:47:28Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    18,
                    47,
                    28,
                    2,
                    164,
                    0
                ],
                "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nadvancing various fields of research and society. However, the current\ncommunity of LLMs overly focuses on benchmarks for analyzing specific\nfoundational skills (e.g. mathematics and code generation), neglecting an\nall-round evaluation of the computer science field. To bridge this gap, we\nintroduce CS-Bench, the first multilingual (English, Chinese, French, German)\nbenchmark dedicated to evaluating the performance of LLMs in computer science.\nCS-Bench comprises approximately 10K meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing\nvarious task forms and divisions of knowledge and reasoning. Utilizing\nCS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,\nrevealing the relationship between CS performance and model scales. We also\nquantitatively analyze the reasons for failures in existing LLMs and highlight\ndirections for improvements, including knowledge supplementation and\nCS-specific reasoning. Further cross-capability experiments show a high\ncorrelation between LLMs' capabilities in computer science and their abilities\nin mathematics and coding. Moreover, expert LLMs specialized in mathematics and\ncoding also demonstrate strong performances in several CS subfields. Looking\nahead, we envision CS-Bench serving as a cornerstone for LLM applications in\nthe CS field and paving new avenues in assessing LLMs' diverse reasoning\ncapabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nadvancing various fields of research and society. However, the current\ncommunity of LLMs overly focuses on benchmarks for analyzing specific\nfoundational skills (e.g. mathematics and code generation), neglecting an\nall-round evaluation of the computer science field. To bridge this gap, we\nintroduce CS-Bench, the first multilingual (English, Chinese, French, German)\nbenchmark dedicated to evaluating the performance of LLMs in computer science.\nCS-Bench comprises approximately 10K meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing\nvarious task forms and divisions of knowledge and reasoning. Utilizing\nCS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,\nrevealing the relationship between CS performance and model scales. We also\nquantitatively analyze the reasons for failures in existing LLMs and highlight\ndirections for improvements, including knowledge supplementation and\nCS-specific reasoning. Further cross-capability experiments show a high\ncorrelation between LLMs' capabilities in computer science and their abilities\nin mathematics and coding. Moreover, expert LLMs specialized in mathematics and\ncoding also demonstrate strong performances in several CS subfields. Looking\nahead, we envision CS-Bench serving as a cornerstone for LLM applications in\nthe CS field and paving new avenues in assessing LLMs' diverse reasoning\ncapabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench."
                },
                "authors": [
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Muxi Diao"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Runqi Qiao"
                    },
                    {
                        "name": "Zhexu Wang"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Huangxuan Wu"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Zhuoma GongQue"
                    },
                    {
                        "name": "Jianing Yu"
                    },
                    {
                        "name": "Qiuna Tan"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06842v2",
                "updated": "2025-02-28T15:15:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    15,
                    31,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-12T15:21:22Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    21,
                    22,
                    6,
                    12,
                    0
                ],
                "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git"
                },
                "authors": [
                    {
                        "name": "Tianjin Huang"
                    },
                    {
                        "name": "Ziquan Zhu"
                    },
                    {
                        "name": "Gaojie Jin"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15282v2",
                "updated": "2025-02-28T15:11:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    11,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-25T17:31:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    17,
                    31,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AutoG: Towards automatic graph construction from tabular data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoG: Towards automatic graph construction from tabular data"
                },
                "summary": "Recent years have witnessed significant advancements in graph machine\nlearning (GML), with its applications spanning numerous domains. However, the\nfocus of GML has predominantly been on developing powerful models, often\noverlooking a crucial initial step: constructing suitable graphs from common\ndata formats, such as tabular data. This construction process is fundamental to\napplying graph-based models, yet it remains largely understudied and lacks\nformalization. Our research aims to address this gap by formalizing the graph\nconstruction problem and proposing an effective solution. We identify two\ncritical challenges to achieve this goal: 1. The absence of dedicated datasets\nto formalize and evaluate the effectiveness of graph construction methods, and\n2. Existing automatic construction methods can only be applied to some specific\ncases, while tedious human engineering is required to generate high-quality\ngraphs. To tackle these challenges, we present a two-fold contribution. First,\nwe introduce a set of datasets to formalize and evaluate graph construction\nmethods. Second, we propose an LLM-based solution, AutoG, automatically\ngenerating high-quality graph schemas without human intervention. The\nexperimental results demonstrate that the quality of constructed graphs is\ncritical to downstream task performance, and AutoG can generate high-quality\ngraphs that rival those produced by human experts. Our code can be accessible\nfrom https://github.com/amazon-science/Automatic-Table-to-Graph-Generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed significant advancements in graph machine\nlearning (GML), with its applications spanning numerous domains. However, the\nfocus of GML has predominantly been on developing powerful models, often\noverlooking a crucial initial step: constructing suitable graphs from common\ndata formats, such as tabular data. This construction process is fundamental to\napplying graph-based models, yet it remains largely understudied and lacks\nformalization. Our research aims to address this gap by formalizing the graph\nconstruction problem and proposing an effective solution. We identify two\ncritical challenges to achieve this goal: 1. The absence of dedicated datasets\nto formalize and evaluate the effectiveness of graph construction methods, and\n2. Existing automatic construction methods can only be applied to some specific\ncases, while tedious human engineering is required to generate high-quality\ngraphs. To tackle these challenges, we present a two-fold contribution. First,\nwe introduce a set of datasets to formalize and evaluate graph construction\nmethods. Second, we propose an LLM-based solution, AutoG, automatically\ngenerating high-quality graph schemas without human intervention. The\nexperimental results demonstrate that the quality of constructed graphs is\ncritical to downstream task performance, and AutoG can generate high-quality\ngraphs that rival those produced by human experts. Our code can be accessible\nfrom https://github.com/amazon-science/Automatic-Table-to-Graph-Generation."
                },
                "authors": [
                    {
                        "name": "Zhikai Chen"
                    },
                    {
                        "name": "Han Xie"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Xiang song"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    }
                ],
                "author_detail": {
                    "name": "George Karypis"
                },
                "author": "George Karypis",
                "arxiv_comment": "camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13983v2",
                "updated": "2025-02-28T15:07:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    7,
                    55,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-23T06:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    57,
                    24,
                    3,
                    23,
                    0
                ],
                "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. AdEval extracts key knowledge points and main ideas to\nalign dynamically generated questions with static data's core concepts. It also\nleverages online search to provide detailed explanations of related knowledge\npoints, thereby creating high-quality evaluation samples with robust knowledge\nsupport. Furthermore, AdEval incorporates mechanisms to control the number and\ncomplexity of questions, enabling dynamic alignment and flexible adjustment.\nThis ensures that the generated questions align with the complexity of static\ndata while supporting varied complexity levels. Based on Bloom's taxonomy,\nAdEval conducts a multi-dimensional evaluation of LLMs across six cognitive\nlevels: remembering, understanding, applying, analyzing, evaluating, and\ncreating. Experimental results on multiple datasets demonstrate that AdEval\neffectively reduces the impact of data contamination on evaluation outcomes,\nenhancing both the fairness and reliability of the evaluation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. AdEval extracts key knowledge points and main ideas to\nalign dynamically generated questions with static data's core concepts. It also\nleverages online search to provide detailed explanations of related knowledge\npoints, thereby creating high-quality evaluation samples with robust knowledge\nsupport. Furthermore, AdEval incorporates mechanisms to control the number and\ncomplexity of questions, enabling dynamic alignment and flexible adjustment.\nThis ensures that the generated questions align with the complexity of static\ndata while supporting varied complexity levels. Based on Bloom's taxonomy,\nAdEval conducts a multi-dimensional evaluation of LLMs across six cognitive\nlevels: remembering, understanding, applying, analyzing, evaluating, and\ncreating. Experimental results on multiple datasets demonstrate that AdEval\neffectively reduces the impact of data contamination on evaluation outcomes,\nenhancing both the fairness and reliability of the evaluation process."
                },
                "authors": [
                    {
                        "name": "Yang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yang Fan"
                },
                "author": "Yang Fan",
                "arxiv_comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19104v2",
                "updated": "2025-02-28T15:00:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    15,
                    0,
                    1,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-26T12:46:59Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    12,
                    46,
                    59,
                    2,
                    57,
                    0
                ],
                "title": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine\n  Translation"
                },
                "summary": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german."
                },
                "authors": [
                    {
                        "name": "Michelle Kappl"
                    }
                ],
                "author_detail": {
                    "name": "Michelle Kappl"
                },
                "author": "Michelle Kappl",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21112v1",
                "updated": "2025-02-28T14:52:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    52,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    52,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts"
                },
                "summary": "The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques."
                },
                "authors": [
                    {
                        "name": "Mattia Birti"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Andrea Maurino"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Maurino"
                },
                "author": "Andrea Maurino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18540v2",
                "updated": "2025-02-28T14:49:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    49,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2024-05-28T19:16:17Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    19,
                    16,
                    17,
                    1,
                    149,
                    0
                ],
                "title": "Learning diverse attacks on large language models for robust red-teaming\n  and safety tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning diverse attacks on large language models for robust red-teaming\n  and safety tuning"
                },
                "summary": "Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches."
                },
                "authors": [
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Lynn Cherif"
                    },
                    {
                        "name": "David Dobre"
                    },
                    {
                        "name": "Juho Lee"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Moksh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Moksh Jain"
                },
                "author": "Moksh Jain",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21110v1",
                "updated": "2025-02-28T14:47:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    47,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:47:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    47,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "Rare event modeling with self-regularized normalizing flows: what can we\n  learn from a single failure?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare event modeling with self-regularized normalizing flows: what can we\n  learn from a single failure?"
                },
                "summary": "Increased deployment of autonomous systems in fields like transportation and\nrobotics have seen a corresponding increase in safety-critical failures. These\nfailures can be difficult to model and debug due to the relative lack of data:\ncompared to tens of thousands of examples from normal operations, we may have\nonly seconds of data leading up to the failure. This scarcity makes it\nchallenging to train generative models of rare failure events, as existing\nmethods risk either overfitting to noise in the limited failure dataset or\nunderfitting due to an overly strong prior. We address this challenge with\nCalNF, or calibrated normalizing flows, a self-regularized framework for\nposterior learning from limited data. CalNF achieves state-of-the-art\nperformance on data-limited failure modeling and inverse problems and enables a\nfirst-of-a-kind case study into the root causes of the 2022 Southwest Airlines\nscheduling crisis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increased deployment of autonomous systems in fields like transportation and\nrobotics have seen a corresponding increase in safety-critical failures. These\nfailures can be difficult to model and debug due to the relative lack of data:\ncompared to tens of thousands of examples from normal operations, we may have\nonly seconds of data leading up to the failure. This scarcity makes it\nchallenging to train generative models of rare failure events, as existing\nmethods risk either overfitting to noise in the limited failure dataset or\nunderfitting due to an overly strong prior. We address this challenge with\nCalNF, or calibrated normalizing flows, a self-regularized framework for\nposterior learning from limited data. CalNF achieves state-of-the-art\nperformance on data-limited failure modeling and inverse problems and enables a\nfirst-of-a-kind case study into the root causes of the 2022 Southwest Airlines\nscheduling crisis."
                },
                "authors": [
                    {
                        "name": "Charles Dawson"
                    },
                    {
                        "name": "Van Tran"
                    },
                    {
                        "name": "Max Z. Li"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "Published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21108v1",
                "updated": "2025-02-28T14:46:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    46,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    46,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Large Language Model-Based Benchmarking Experiment Settings for\n  Evolutionary Multi-Objective Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Benchmarking Experiment Settings for\n  Evolutionary Multi-Objective Optimization"
                },
                "summary": "When we manually design an evolutionary optimization algorithm, we implicitly\nor explicitly assume a set of target optimization problems. In the case of\nautomated algorithm design, target optimization problems are usually explicitly\nshown. Recently, the use of large language models (LLMs) for the design of\nevolutionary multi-objective optimization (EMO) algorithms have been examined\nin some studies. In those studies, target multi-objective problems are not\nalways explicitly shown. It is well known in the EMO community that the\nperformance evaluation results of EMO algorithms depend on not only test\nproblems but also many other factors such as performance indicators, reference\npoint, termination condition, and population size. Thus, it is likely that the\ndesigned EMO algorithms by LLMs depends on those factors. In this paper, we try\nto examine the implicit assumption about the performance comparison of EMO\nalgorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking\nexperiment of EMO algorithms. Our experiments show that LLMs often suggest\nclassical benchmark settings: Performance examination of NSGA-II, MOEA/D and\nNSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter\nspecifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When we manually design an evolutionary optimization algorithm, we implicitly\nor explicitly assume a set of target optimization problems. In the case of\nautomated algorithm design, target optimization problems are usually explicitly\nshown. Recently, the use of large language models (LLMs) for the design of\nevolutionary multi-objective optimization (EMO) algorithms have been examined\nin some studies. In those studies, target multi-objective problems are not\nalways explicitly shown. It is well known in the EMO community that the\nperformance evaluation results of EMO algorithms depend on not only test\nproblems but also many other factors such as performance indicators, reference\npoint, termination condition, and population size. Thus, it is likely that the\ndesigned EMO algorithms by LLMs depends on those factors. In this paper, we try\nto examine the implicit assumption about the performance comparison of EMO\nalgorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking\nexperiment of EMO algorithms. Our experiments show that LLMs often suggest\nclassical benchmark settings: Performance examination of NSGA-II, MOEA/D and\nNSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter\nspecifications."
                },
                "authors": [
                    {
                        "name": "Lie Meng Pang"
                    },
                    {
                        "name": "Hisao Ishibuchi"
                    }
                ],
                "author_detail": {
                    "name": "Hisao Ishibuchi"
                },
                "author": "Hisao Ishibuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21106v1",
                "updated": "2025-02-28T14:44:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    44,
                    55,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:44:55Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    44,
                    55,
                    4,
                    59,
                    0
                ],
                "title": "A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma\n  Triage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma\n  Triage"
                },
                "summary": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology."
                },
                "authors": [
                    {
                        "name": "Youngjin Yoo"
                    },
                    {
                        "name": "Bogdan Georgescu"
                    },
                    {
                        "name": "Yanbo Zhang"
                    },
                    {
                        "name": "Sasa Grbic"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Gabriela D. Aldea"
                    },
                    {
                        "name": "Thomas J. Re"
                    },
                    {
                        "name": "Jyotipriya Das"
                    },
                    {
                        "name": "Poikavila Ullaskrishnan"
                    },
                    {
                        "name": "Eva Eibenberger"
                    },
                    {
                        "name": "Andrei Chekkoury"
                    },
                    {
                        "name": "Uttam K. Bodanapally"
                    },
                    {
                        "name": "Savvas Nicolaou"
                    },
                    {
                        "name": "Pina C. Sanelli"
                    },
                    {
                        "name": "Thomas J. Schroeppel"
                    },
                    {
                        "name": "Yvonne W. Lui"
                    },
                    {
                        "name": "Eli Gibson"
                    }
                ],
                "author_detail": {
                    "name": "Eli Gibson"
                },
                "author": "Eli Gibson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04755v4",
                "updated": "2025-02-28T14:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    41,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-07T08:54:55Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    54,
                    55,
                    4,
                    159,
                    0
                ],
                "title": "LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses"
                },
                "summary": "Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing that our\nadversarially perturbed prompts 1) are indistinguishable from unaltered prompts\nby humans, 2) push LLMs to recommend target concepts more often, and 3) make\nusers more likely to notice target concepts, all without arousing suspicion.\nThe practicality of this attack has the potential to undermine user autonomy.\nAmong other measures, we recommend implementing warnings against using prompts\nfrom untrusted parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing that our\nadversarially perturbed prompts 1) are indistinguishable from unaltered prompts\nby humans, 2) push LLMs to recommend target concepts more often, and 3) make\nusers more likely to notice target concepts, all without arousing suspicion.\nThe practicality of this attack has the potential to undermine user autonomy.\nAmong other measures, we recommend implementing warnings against using prompts\nfrom untrusted parties."
                },
                "authors": [
                    {
                        "name": "Weiran Lin"
                    },
                    {
                        "name": "Anna Gerchanovsky"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Lujo Bauer"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Zifan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zifan Wang"
                },
                "author": "Zifan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21098v1",
                "updated": "2025-02-28T14:36:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    36,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:36:57Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    36,
                    57,
                    4,
                    59,
                    0
                ],
                "title": "Re-evaluating Theory of Mind evaluation in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-evaluating Theory of Mind evaluation in large language models"
                },
                "summary": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition."
                },
                "authors": [
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Felix Sosa"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11540v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11540v2",
                "updated": "2025-02-28T14:35:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    35,
                    58,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-15T12:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    57,
                    1,
                    289,
                    0
                ],
                "title": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models"
                },
                "summary": "Federated Learning (FL) enables privacy-preserving collaborative instruction\ntuning of large language models (LLMs) by leveraging massively distributed\ndata. However, the decentralized nature of FL exacerbates data quality\nchallenges, as local clients lack global visibility to filter noisy or\nlow-quality samples before training. To resolve this issue, we propose FedDQC,\na novel federated instruction tuning framework with dynamic data quality\ncontrol. Our approach introduces two key innovations. First, we propose\ninstruction-response alignment (IRA), an efficient client-side metric for\nquality evaluation requiring only low-cost inference. We validate that\nhigher-IRA data corresponds to more relevant and easier-to-learn\nquestion-answer pairs. Second, mirroring the human easy-to-hard knowledge\nacquisition process, we design a quality-aware hierarchical FL training\nframework, where the LLM is progressively fine-tuned from high- to low-IRA data\nin a collaborative manner. The framework also supports adaptive data quality\nassessment at each hierarchy, enabling dynamic adjustments throughout the\ntraining process. Extensive experiments on synthetic and real-world datasets\nshow that our method significantly improves LLM performance on mixed-quality\ndata in FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables privacy-preserving collaborative instruction\ntuning of large language models (LLMs) by leveraging massively distributed\ndata. However, the decentralized nature of FL exacerbates data quality\nchallenges, as local clients lack global visibility to filter noisy or\nlow-quality samples before training. To resolve this issue, we propose FedDQC,\na novel federated instruction tuning framework with dynamic data quality\ncontrol. Our approach introduces two key innovations. First, we propose\ninstruction-response alignment (IRA), an efficient client-side metric for\nquality evaluation requiring only low-cost inference. We validate that\nhigher-IRA data corresponds to more relevant and easier-to-learn\nquestion-answer pairs. Second, mirroring the human easy-to-hard knowledge\nacquisition process, we design a quality-aware hierarchical FL training\nframework, where the LLM is progressively fine-tuned from high- to low-IRA data\nin a collaborative manner. The framework also supports adaptive data quality\nassessment at each hierarchy, enabling dynamic adjustments throughout the\ntraining process. Extensive experiments on synthetic and real-world datasets\nshow that our method significantly improves LLM performance on mixed-quality\ndata in FL."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Fengting Yuchi"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11540v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11540v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21092v1",
                "updated": "2025-02-28T14:31:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    31,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:31:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    31,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "An LLM-based Delphi Study to Predict GenAI Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Delphi Study to Predict GenAI Evolution"
                },
                "summary": "Predicting the future trajectory of complex and rapidly evolving systems\nremains a significant challenge, particularly in domains where data is scarce\nor unreliable. This study introduces a novel approach to qualitative\nforecasting by leveraging Large Language Models to conduct Delphi studies. The\nmethodology was applied to explore the future evolution of Generative\nArtificial Intelligence, revealing insights into key factors such as\ngeopolitical tensions, economic disparities, regulatory frameworks, and ethical\nconsiderations. The results highlight how LLM-based Delphi studies can\nfacilitate structured scenario analysis, capturing diverse perspectives while\nmitigating issues such as respondent fatigue. However, limitations emerge in\nterms of knowledge cutoffs, inherent biases, and sensitivity to initial\nconditions. While the approach provides an innovative means for structured\nforesight, this method could be also considered as a novel form of reasoning.\nfurther research is needed to refine its ability to manage heterogeneity,\nimprove reliability, and integrate external data sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the future trajectory of complex and rapidly evolving systems\nremains a significant challenge, particularly in domains where data is scarce\nor unreliable. This study introduces a novel approach to qualitative\nforecasting by leveraging Large Language Models to conduct Delphi studies. The\nmethodology was applied to explore the future evolution of Generative\nArtificial Intelligence, revealing insights into key factors such as\ngeopolitical tensions, economic disparities, regulatory frameworks, and ethical\nconsiderations. The results highlight how LLM-based Delphi studies can\nfacilitate structured scenario analysis, capturing diverse perspectives while\nmitigating issues such as respondent fatigue. However, limitations emerge in\nterms of knowledge cutoffs, inherent biases, and sensitivity to initial\nconditions. While the approach provides an innovative means for structured\nforesight, this method could be also considered as a novel form of reasoning.\nfurther research is needed to refine its ability to manage heterogeneity,\nimprove reliability, and integrate external data sources."
                },
                "authors": [
                    {
                        "name": "Francesco Bertolotti"
                    },
                    {
                        "name": "Luca Mari"
                    }
                ],
                "author_detail": {
                    "name": "Luca Mari"
                },
                "author": "Luca Mari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21087v1",
                "updated": "2025-02-28T14:26:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    26,
                    47,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:26:47Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    26,
                    47,
                    4,
                    59,
                    0
                ],
                "title": "PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured\n  Data with Text and Relational Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured\n  Data with Text and Relational Information"
                },
                "summary": "Large language models (LLMs) have shown impressive abilities in answering\nquestions across various domains, but they often encounter hallucination issues\non questions that require professional and up-to-date knowledge. To address\nthis limitation, retrieval-augmented generation (RAG) techniques have been\nproposed, which retrieve relevant information from external sources to inform\ntheir responses. However, existing RAG methods typically focus on a single type\nof external data, such as vectorized text database or knowledge graphs, and\ncannot well handle real-world questions on semi-structured data containing both\ntext and relational information. To bridge this gap, we introduce PASemiQA, a\nnovel approach that jointly leverages text and relational information in\nsemi-structured data to answer questions. PASemiQA first generates a plan to\nidentify relevant text and relational information to answer the question in\nsemi-structured data, and then uses an LLM agent to traverse the\nsemi-structured data and extract necessary information. Our empirical results\ndemonstrate the effectiveness of PASemiQA across different semi-structured\ndatasets from various domains, showcasing its potential to improve the accuracy\nand reliability of question answering systems on semi-structured data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive abilities in answering\nquestions across various domains, but they often encounter hallucination issues\non questions that require professional and up-to-date knowledge. To address\nthis limitation, retrieval-augmented generation (RAG) techniques have been\nproposed, which retrieve relevant information from external sources to inform\ntheir responses. However, existing RAG methods typically focus on a single type\nof external data, such as vectorized text database or knowledge graphs, and\ncannot well handle real-world questions on semi-structured data containing both\ntext and relational information. To bridge this gap, we introduce PASemiQA, a\nnovel approach that jointly leverages text and relational information in\nsemi-structured data to answer questions. PASemiQA first generates a plan to\nidentify relevant text and relational information to answer the question in\nsemi-structured data, and then uses an LLM agent to traverse the\nsemi-structured data and extract necessary information. Our empirical results\ndemonstrate the effectiveness of PASemiQA across different semi-structured\ndatasets from various domains, showcasing its potential to improve the accuracy\nand reliability of question answering systems on semi-structured data."
                },
                "authors": [
                    {
                        "name": "Hansi Yang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18934v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18934v3",
                "updated": "2025-02-28T14:23:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    23,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-26T08:36:20Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    8,
                    36,
                    20,
                    2,
                    57,
                    0
                ],
                "title": "Kanana: Compute-efficient Bilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kanana: Compute-efficient Bilingual Language Models"
                },
                "summary": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models."
                },
                "authors": [
                    {
                        "name": "Kanana LLM Team"
                    },
                    {
                        "name": "Yunju Bak"
                    },
                    {
                        "name": "Hojin Lee"
                    },
                    {
                        "name": "Minho Ryu"
                    },
                    {
                        "name": "Jiyeon Ham"
                    },
                    {
                        "name": "Seungjae Jung"
                    },
                    {
                        "name": "Daniel Wontae Nam"
                    },
                    {
                        "name": "Taegyeong Eo"
                    },
                    {
                        "name": "Donghun Lee"
                    },
                    {
                        "name": "Doohae Jung"
                    },
                    {
                        "name": "Boseop Kim"
                    },
                    {
                        "name": "Nayeon Kim"
                    },
                    {
                        "name": "Jaesun Park"
                    },
                    {
                        "name": "Hyunho Kim"
                    },
                    {
                        "name": "Hyunwoong Ko"
                    },
                    {
                        "name": "Changmin Lee"
                    },
                    {
                        "name": "Kyoung-Woon On"
                    },
                    {
                        "name": "Seulye Baeg"
                    },
                    {
                        "name": "Junrae Cho"
                    },
                    {
                        "name": "Sunghee Jung"
                    },
                    {
                        "name": "Jieun Kang"
                    },
                    {
                        "name": "EungGyun Kim"
                    },
                    {
                        "name": "Eunhwa Kim"
                    },
                    {
                        "name": "Byeongil Ko"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Minchul Lee"
                    },
                    {
                        "name": "Miok Lee"
                    },
                    {
                        "name": "Shinbok Lee"
                    },
                    {
                        "name": "Gaeun Seo"
                    }
                ],
                "author_detail": {
                    "name": "Gaeun Seo"
                },
                "author": "Gaeun Seo",
                "arxiv_comment": "40 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18934v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18934v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21074v1",
                "updated": "2025-02-28T14:07:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    7,
                    48,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:07:48Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    7,
                    48,
                    4,
                    59,
                    0
                ],
                "title": "CODI: Compressing Chain-of-Thought into Continuous Space via\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CODI: Compressing Chain-of-Thought into Continuous Space via\n  Self-Distillation"
                },
                "summary": "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT."
                },
                "authors": [
                    {
                        "name": "Zhenyi Shen"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21068v1",
                "updated": "2025-02-28T14:03:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    3,
                    53,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:03:53Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    3,
                    53,
                    4,
                    59,
                    0
                ],
                "title": "GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIDE: LLM-Driven GUI Generation Decomposition for Automated Prototyping"
                },
                "summary": "GUI prototyping serves as one of the most valuable techniques for enhancing\nthe elicitation of requirements and facilitating the visualization and\nrefinement of customer needs. While GUI prototyping has a positive impact on\nthe software development process, it simultaneously demands significant effort\nand resources. The emergence of Large Language Models (LLMs) with their\nimpressive code generation capabilities offers a promising approach for\nautomating GUI prototyping. Despite their potential, there is a gap between\ncurrent LLM-based prototyping solutions and traditional user-based GUI\nprototyping approaches which provide visual representations of the GUI\nprototypes and direct editing functionality. In contrast, LLMs and related\ngenerative approaches merely produce text sequences or non-editable image\noutput, which lacks both mentioned aspects and therefore impede supporting GUI\nprototyping. Moreover, minor changes requested by the user typically lead to an\ninefficient regeneration of the entire GUI prototype when using LLMs directly.\nIn this work, we propose GUIDE, a novel LLM-driven GUI generation decomposition\napproach seamlessly integrated into the popular prototyping framework Figma.\nOur approach initially decomposes high-level GUI descriptions into\nfine-granular GUI requirements, which are subsequently translated into Material\nDesign GUI prototypes, enabling higher controllability and more efficient\nadaption of changes. To efficiently conduct prompting-based generation of\nMaterial Design GUI prototypes, we propose a retrieval-augmented generation\napproach to integrate the component library. Our preliminary evaluation\ndemonstrates the effectiveness of GUIDE in bridging the gap between LLM\ngeneration capabilities and traditional GUI prototyping workflows, offering a\nmore effective and controlled user-based approach to LLM-driven GUI\nprototyping. Video: https://youtu.be/C9RbhMxqpTU",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI prototyping serves as one of the most valuable techniques for enhancing\nthe elicitation of requirements and facilitating the visualization and\nrefinement of customer needs. While GUI prototyping has a positive impact on\nthe software development process, it simultaneously demands significant effort\nand resources. The emergence of Large Language Models (LLMs) with their\nimpressive code generation capabilities offers a promising approach for\nautomating GUI prototyping. Despite their potential, there is a gap between\ncurrent LLM-based prototyping solutions and traditional user-based GUI\nprototyping approaches which provide visual representations of the GUI\nprototypes and direct editing functionality. In contrast, LLMs and related\ngenerative approaches merely produce text sequences or non-editable image\noutput, which lacks both mentioned aspects and therefore impede supporting GUI\nprototyping. Moreover, minor changes requested by the user typically lead to an\ninefficient regeneration of the entire GUI prototype when using LLMs directly.\nIn this work, we propose GUIDE, a novel LLM-driven GUI generation decomposition\napproach seamlessly integrated into the popular prototyping framework Figma.\nOur approach initially decomposes high-level GUI descriptions into\nfine-granular GUI requirements, which are subsequently translated into Material\nDesign GUI prototypes, enabling higher controllability and more efficient\nadaption of changes. To efficiently conduct prompting-based generation of\nMaterial Design GUI prototypes, we propose a retrieval-augmented generation\napproach to integrate the component library. Our preliminary evaluation\ndemonstrates the effectiveness of GUIDE in bridging the gap between LLM\ngeneration capabilities and traditional GUI prototyping workflows, offering a\nmore effective and controlled user-based approach to LLM-driven GUI\nprototyping. Video: https://youtu.be/C9RbhMxqpTU"
                },
                "authors": [
                    {
                        "name": "Kristian Kolthoff"
                    },
                    {
                        "name": "Felix Kretzer"
                    },
                    {
                        "name": "Christian Bartelt"
                    },
                    {
                        "name": "Alexander Maedche"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    }
                ],
                "author_detail": {
                    "name": "Simone Paolo Ponzetto"
                },
                "author": "Simone Paolo Ponzetto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21059v1",
                "updated": "2025-02-28T13:59:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    59,
                    11,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:59:11Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    59,
                    11,
                    4,
                    59,
                    0
                ],
                "title": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated\n  Flowcharts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated\n  Flowcharts"
                },
                "summary": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted\nin some practical applications. However, recent research has revealed their\nvulnerability to multimodal jailbreak attacks, whereby the model can be induced\nto generate harmful content, leading to safety risks. Although most LVLMs have\nundergone safety alignment, recent research shows that the visual modality is\nstill vulnerable to jailbreak attacks. In our work, we discover that by using\nflowcharts with partially harmful information, LVLMs can be induced to provide\nadditional harmful details. Based on this, we propose a jailbreak attack method\nbased on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first\nfine-tunes a pre-trained LLM to create a step-description generator based on\nbenign datasets. The generator is then used to produce step descriptions\ncorresponding to a harmful query, which are transformed into flowcharts in 3\ndifferent shapes (vertical, horizontal, and S-shaped) as visual prompts. These\nflowcharts are then combined with a benign textual prompt to execute a\njailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that\nFC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next,\nQwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak\nmethods. Additionally, we investigate factors affecting the attack performance,\nincluding the number of steps and the font styles in the flowcharts. Our\nevaluation shows that FC-Attack can improve the jailbreak performance from 4%\nto 28% in Claude-3.5 by changing the font style. To mitigate the attack, we\nexplore several defenses and find that AdaShield can largely reduce the\njailbreak performance but with the cost of utility drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have become powerful and widely adopted\nin some practical applications. However, recent research has revealed their\nvulnerability to multimodal jailbreak attacks, whereby the model can be induced\nto generate harmful content, leading to safety risks. Although most LVLMs have\nundergone safety alignment, recent research shows that the visual modality is\nstill vulnerable to jailbreak attacks. In our work, we discover that by using\nflowcharts with partially harmful information, LVLMs can be induced to provide\nadditional harmful details. Based on this, we propose a jailbreak attack method\nbased on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first\nfine-tunes a pre-trained LLM to create a step-description generator based on\nbenign datasets. The generator is then used to produce step descriptions\ncorresponding to a harmful query, which are transformed into flowcharts in 3\ndifferent shapes (vertical, horizontal, and S-shaped) as visual prompts. These\nflowcharts are then combined with a benign textual prompt to execute a\njailbreak attack on LVLMs. Our evaluations using the Advbench dataset show that\nFC-Attack achieves over 90% attack success rates on Gemini-1.5, Llaval-Next,\nQwen2-VL, and InternVL-2.5 models, outperforming existing LVLM jailbreak\nmethods. Additionally, we investigate factors affecting the attack performance,\nincluding the number of steps and the font styles in the flowcharts. Our\nevaluation shows that FC-Attack can improve the jailbreak performance from 4%\nto 28% in Claude-3.5 by changing the font style. To mitigate the attack, we\nexplore several defenses and find that AdaShield can largely reduce the\njailbreak performance but with the cost of utility drop."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Zongmin Zhang"
                    },
                    {
                        "name": "Jihui Guo"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11431v3",
                "updated": "2025-02-28T13:43:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    43,
                    17,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-17T11:36:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    36,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in\n  Weak-to-Strong Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in\n  Weak-to-Strong Generalization"
                },
                "summary": "Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Shiqi Shen"
                    },
                    {
                        "name": "Guangyao Shen"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Zhi Gong"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted at ICLR 2025, camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15835v2",
                "updated": "2025-02-28T13:40:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    40,
                    42,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-20T12:44:26Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    44,
                    26,
                    3,
                    51,
                    0
                ],
                "title": "Pragmatic Reasoning improves LLM Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatic Reasoning improves LLM Code Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuchen Cao"
                    },
                    {
                        "name": "Sven Apel"
                    },
                    {
                        "name": "Adish Singla"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21037v1",
                "updated": "2025-02-28T13:29:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    29,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:29:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    29,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "The amplifier effect of artificial agents in social contagion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The amplifier effect of artificial agents in social contagion"
                },
                "summary": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways."
                },
                "authors": [
                    {
                        "name": "Eric Hitz"
                    },
                    {
                        "name": "Mingmin Feng"
                    },
                    {
                        "name": "Radu Tanase"
                    },
                    {
                        "name": "Ren Algesheimer"
                    },
                    {
                        "name": "Manuel S. Mariani"
                    }
                ],
                "author_detail": {
                    "name": "Manuel S. Mariani"
                },
                "author": "Manuel S. Mariani",
                "arxiv_comment": "Main text pp. 1-5; Supplementary Material pp. 6-10",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21035v1",
                "updated": "2025-02-28T13:27:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    27,
                    25,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:27:25Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    27,
                    25,
                    4,
                    59,
                    0
                ],
                "title": "S4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient\n  Sensor Networks in Smart Buildings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S4ConvD: Adaptive Scaling and Frequency Adjustment for Energy-Efficient\n  Sensor Networks in Smart Buildings"
                },
                "summary": "Predicting energy consumption in smart buildings is challenging due to\ndependencies in sensor data and the variability of environmental conditions. We\nintroduce S4ConvD, a novel convolutional variant of Deep State Space Models\n(Deep-SSMs), that minimizes reliance on extensive preprocessing steps. S4ConvD\nis designed to optimize runtime in resource-constrained environments. By\nimplementing adaptive scaling and frequency adjustments, this model shows to\ncapture complex temporal patterns in building energy dynamics. Experiments on\nthe ASHRAE Great Energy Predictor III dataset reveal that S4ConvD outperforms\ncurrent benchmarks. Additionally, S4ConvD benefits from significant\nimprovements in GPU runtime through the use of Block Tiling optimization\ntechniques. Thus, S4ConvD has the potential for practical deployment in\nreal-time energy modeling. Furthermore, the complete codebase and dataset are\naccessible on GitHub, fostering open-source contributions and facilitating\nfurther research. Our method also promotes resource-efficient model execution,\nenhancing both energy forecasting and the potential integration of renewable\nenergy sources into smart grid systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting energy consumption in smart buildings is challenging due to\ndependencies in sensor data and the variability of environmental conditions. We\nintroduce S4ConvD, a novel convolutional variant of Deep State Space Models\n(Deep-SSMs), that minimizes reliance on extensive preprocessing steps. S4ConvD\nis designed to optimize runtime in resource-constrained environments. By\nimplementing adaptive scaling and frequency adjustments, this model shows to\ncapture complex temporal patterns in building energy dynamics. Experiments on\nthe ASHRAE Great Energy Predictor III dataset reveal that S4ConvD outperforms\ncurrent benchmarks. Additionally, S4ConvD benefits from significant\nimprovements in GPU runtime through the use of Block Tiling optimization\ntechniques. Thus, S4ConvD has the potential for practical deployment in\nreal-time energy modeling. Furthermore, the complete codebase and dataset are\naccessible on GitHub, fostering open-source contributions and facilitating\nfurther research. Our method also promotes resource-efficient model execution,\nenhancing both energy forecasting and the potential integration of renewable\nenergy sources into smart grid systems."
                },
                "authors": [
                    {
                        "name": "Melanie Schaller"
                    },
                    {
                        "name": "Bodo Rosenhahn"
                    }
                ],
                "author_detail": {
                    "name": "Bodo Rosenhahn"
                },
                "author": "Bodo Rosenhahn",
                "arxiv_comment": "Submitted to TOSN Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21030v1",
                "updated": "2025-02-28T13:22:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    22,
                    29,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:22:29Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    22,
                    29,
                    4,
                    59,
                    0
                ],
                "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs"
                },
                "summary": "Recent advances in large language models (LLMs) have popularized the\nchain-of-thought (CoT) paradigm, in which models produce explicit reasoning\nsteps in natural language. Although this approach improves interpretability and\nfacilitates external auditing, it may not represent the most computationally\nefficient method for internal reasoning. In contrast, human cognition relies on\nimplicit mental representations that recall past sensory and episodic\ninformation without requiring complete verbalization. In this paper, we propose\na framework that integrates implicit mental representations into the internal\nreasoning processes of LLMs. Preliminary experiments indicate that\nincorporating an Implicit Memory Module (IMM) into a simple GPT model yields a\nreduction of between 35% and 57% in final training loss compared to a regular\nGPT baseline. The addition of an explicit interpretability channel (e.g., a\nchain-of-thought decoder) is straightforward to implement within this approach.\nWe outline theoretical foundations, propose technical mechanisms to scale the\nmemory module, and discuss how these ideas may lead to more efficient and\nrobust reasoning, with optional future extensions for explicit auditability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have popularized the\nchain-of-thought (CoT) paradigm, in which models produce explicit reasoning\nsteps in natural language. Although this approach improves interpretability and\nfacilitates external auditing, it may not represent the most computationally\nefficient method for internal reasoning. In contrast, human cognition relies on\nimplicit mental representations that recall past sensory and episodic\ninformation without requiring complete verbalization. In this paper, we propose\na framework that integrates implicit mental representations into the internal\nreasoning processes of LLMs. Preliminary experiments indicate that\nincorporating an Implicit Memory Module (IMM) into a simple GPT model yields a\nreduction of between 35% and 57% in final training loss compared to a regular\nGPT baseline. The addition of an explicit interpretability channel (e.g., a\nchain-of-thought decoder) is straightforward to implement within this approach.\nWe outline theoretical foundations, propose technical mechanisms to scale the\nmemory module, and discuss how these ideas may lead to more efficient and\nrobust reasoning, with optional future extensions for explicit auditability."
                },
                "authors": [
                    {
                        "name": "Jos I. Orlicki"
                    }
                ],
                "author_detail": {
                    "name": "Jos I. Orlicki"
                },
                "author": "Jos I. Orlicki",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21028v1",
                "updated": "2025-02-28T13:16:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    16,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:16:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    16,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Measuring and identifying factors of individuals' trust in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and identifying factors of individuals' trust in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Edoardo Sebastiano De Duro"
                    },
                    {
                        "name": "Giuseppe Alessandro Veltri"
                    },
                    {
                        "name": "Hudson Golino"
                    },
                    {
                        "name": "Massimo Stella"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Stella"
                },
                "author": "Massimo Stella",
                "arxiv_comment": "24 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21026v1",
                "updated": "2025-02-28T13:14:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    14,
                    58,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:14:58Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    14,
                    58,
                    4,
                    59,
                    0
                ],
                "title": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries\n  through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artemis: Toward Accurate Detection of Server-Side Request Forgeries\n  through LLM-Assisted Inter-Procedural Path-Sensitive Taint Analysis"
                },
                "summary": "Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web\napplications. Existing static tools in detecting vulnerabilities in PHP web\napplications neither contain SSRF-related features to enhance detection\naccuracy nor consider PHP's dynamic type features. In this paper, we present\nArtemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP\nweb applications. First, Artemis extracts both PHP built-in and third-party\nfunctions as candidate source and sink functions. Second, Artemis constructs\nboth explicit and implicit call graphs to infer functions' relationships.Third,\nArtemis performs taint analysis based on a set of rules that prevent\nover-tainting and pauses when SSRF exploitation is impossible.Fourth, Artemis\nanalyzes the compatibility of path conditions to prune false positives.We have\nimplemented a prototype of Artemis and evaluated it on 250 PHP web\napplications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with\n15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported\nto developers, with 24 confirmed and assigned CVE IDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Server-side request forgery (SSRF) vulnerabilities are inevitable in PHP web\napplications. Existing static tools in detecting vulnerabilities in PHP web\napplications neither contain SSRF-related features to enhance detection\naccuracy nor consider PHP's dynamic type features. In this paper, we present\nArtemis, a static taint analysis tool for detecting SSRF vulnerabilities in PHP\nweb applications. First, Artemis extracts both PHP built-in and third-party\nfunctions as candidate source and sink functions. Second, Artemis constructs\nboth explicit and implicit call graphs to infer functions' relationships.Third,\nArtemis performs taint analysis based on a set of rules that prevent\nover-tainting and pauses when SSRF exploitation is impossible.Fourth, Artemis\nanalyzes the compatibility of path conditions to prune false positives.We have\nimplemented a prototype of Artemis and evaluated it on 250 PHP web\napplications. Artemis reports 207 true vulnerable paths (106 true SSRFs) with\n15 false positives. Of the 106 detected SSRFs, 35 are newly found and reported\nto developers, with 24 confirmed and assigned CVE IDs."
                },
                "authors": [
                    {
                        "name": "Yuchen Ji"
                    },
                    {
                        "name": "Ting Dai"
                    },
                    {
                        "name": "Zhichao Zhou"
                    },
                    {
                        "name": "Yutian Tang"
                    },
                    {
                        "name": "Jingzhu He"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhu He"
                },
                "author": "Jingzhu He",
                "arxiv_doi": "10.1145/3720488",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3720488",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full version of paper submitted to OOPSLA '25",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20372v2",
                "updated": "2025-02-28T13:06:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    6,
                    2,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-29T06:32:36Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    6,
                    32,
                    36,
                    6,
                    364,
                    0
                ],
                "title": "LLM2: Let Large Language Models Harness System 2 Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2: Let Large Language Models Harness System 2 Reasoning"
                },
                "summary": "Large language models (LLMs) have exhibited impressive capabilities across a\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\nthese limitations are rooted in the foundational autoregressive architecture of\nLLMs, which inherently lacks mechanisms for differentiating between desirable\nand undesirable results. Drawing inspiration from the dual-process theory of\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\nresponsible for generating plausible candidates, while the verifier provides\ntimely process-based feedback to distinguish desirable and undesirable outputs.\nThe verifier is trained with a pairwise comparison loss on synthetic\nprocess-supervision data generated through our token quality exploration\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\nself-consistency, LLM2 achieves additional improvements, boosting major@20\naccuracy from 56.2 to 70.2 (+14.0).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive capabilities across a\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\nthese limitations are rooted in the foundational autoregressive architecture of\nLLMs, which inherently lacks mechanisms for differentiating between desirable\nand undesirable results. Drawing inspiration from the dual-process theory of\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\nresponsible for generating plausible candidates, while the verifier provides\ntimely process-based feedback to distinguish desirable and undesirable outputs.\nThe verifier is trained with a pairwise comparison loss on synthetic\nprocess-supervision data generated through our token quality exploration\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\nself-consistency, LLM2 achieves additional improvements, boosting major@20\naccuracy from 56.2 to 70.2 (+14.0)."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Bo Shui"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Accepted to NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21017v1",
                "updated": "2025-02-28T13:04:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    4,
                    4,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T13:04:04Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    4,
                    4,
                    4,
                    59,
                    0
                ],
                "title": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in\n  Persuasive Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in\n  Persuasive Dialogues"
                },
                "summary": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM."
                },
                "authors": [
                    {
                        "name": "Fangxu Yu"
                    },
                    {
                        "name": "Lai Jiang"
                    },
                    {
                        "name": "Shenyi Huang"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Xinyu Dai"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Dai"
                },
                "author": "Xinyu Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19883v2",
                "updated": "2025-02-28T12:59:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    59,
                    26,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-27T08:44:04Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    8,
                    44,
                    4,
                    3,
                    58,
                    0
                ],
                "title": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behind the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models"
                },
                "summary": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs."
                },
                "authors": [
                    {
                        "name": "Sibo Yi"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Jiaxing Song"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Song"
                },
                "author": "Jiaxing Song",
                "arxiv_comment": "12 pages. 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21014v1",
                "updated": "2025-02-28T12:58:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    58,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T12:58:57Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    58,
                    57,
                    4,
                    59,
                    0
                ],
                "title": "Explainable Biomedical Claim Verification with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Biomedical Claim Verification with Large Language Models"
                },
                "summary": "Verification of biomedical claims is critical for healthcare decision-making,\npublic health policy and scientific research. We present an interactive\nbiomedical claim verification system by integrating LLMs, transparent model\nexplanations, and user-guided justification. In the system, users first\nretrieve relevant scientific studies from a persistent medical literature\ncorpus and explore how different LLMs perform natural language inference (NLI)\nwithin task-adaptive reasoning framework to classify each study as \"Support,\"\n\"Contradict,\" or \"Not Enough Information\" regarding the claim. Users can\nexamine the model's reasoning process with additional insights provided by SHAP\nvalues that highlight word-level contributions to the final result. This\ncombination enables a more transparent and interpretable evaluation of the\nmodel's decision-making process. A summary stage allows users to consolidate\nthe results by selecting a result with narrative justification generated by\nLLMs. As a result, a consensus-based final decision is summarized for each\nretrieved study, aiming safe and accountable AI-assisted decision-making in\nbiomedical contexts. We aim to integrate this explainable verification system\nas a component within a broader evidence synthesis framework to support\nhuman-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification of biomedical claims is critical for healthcare decision-making,\npublic health policy and scientific research. We present an interactive\nbiomedical claim verification system by integrating LLMs, transparent model\nexplanations, and user-guided justification. In the system, users first\nretrieve relevant scientific studies from a persistent medical literature\ncorpus and explore how different LLMs perform natural language inference (NLI)\nwithin task-adaptive reasoning framework to classify each study as \"Support,\"\n\"Contradict,\" or \"Not Enough Information\" regarding the claim. Users can\nexamine the model's reasoning process with additional insights provided by SHAP\nvalues that highlight word-level contributions to the final result. This\ncombination enables a more transparent and interpretable evaluation of the\nmodel's decision-making process. A summary stage allows users to consolidate\nthe results by selecting a result with narrative justification generated by\nLLMs. As a result, a consensus-based final decision is summarized for each\nretrieved study, aiming safe and accountable AI-assisted decision-making in\nbiomedical contexts. We aim to integrate this explainable verification system\nas a component within a broader evidence synthesis framework to support\nhuman-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Siting Liang"
                    },
                    {
                        "name": "Daniel Sonntag"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sonntag"
                },
                "author": "Daniel Sonntag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17723v2",
                "updated": "2025-02-28T12:56:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    56,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2024-01-31T10:35:53Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    10,
                    35,
                    53,
                    2,
                    31,
                    0
                ],
                "title": "LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks"
                },
                "summary": "Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems."
                },
                "authors": [
                    {
                        "name": "Kaike Zhang"
                    },
                    {
                        "name": "Qi Cao"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17812v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17812v4",
                "updated": "2025-02-28T12:53:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    53,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2024-02-27T14:51:11Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    14,
                    51,
                    11,
                    1,
                    58,
                    0
                ],
                "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp."
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Baeseong Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Minjung Jo"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsuk Jeon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17812v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17812v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19839v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19839v5",
                "updated": "2025-02-28T12:35:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    35,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2024-09-30T00:41:51Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    0,
                    41,
                    51,
                    0,
                    274,
                    0
                ],
                "title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities"
                },
                "summary": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM ($p$-value\n$<0.001$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM ($p$-value\n$<0.001$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org."
                },
                "authors": [
                    {
                        "name": "Ezra Karger"
                    },
                    {
                        "name": "Houtan Bastani"
                    },
                    {
                        "name": "Chen Yueh-Han"
                    },
                    {
                        "name": "Zachary Jacobs"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Fred Zhang"
                    },
                    {
                        "name": "Philip E. Tetlock"
                    }
                ],
                "author_detail": {
                    "name": "Philip E. Tetlock"
                },
                "author": "Philip E. Tetlock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19839v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19839v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20988v1",
                "updated": "2025-02-28T12:00:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    0,
                    51,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T12:00:51Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    12,
                    0,
                    51,
                    4,
                    59,
                    0
                ],
                "title": "Merging Clinical Knowledge into Large Language Models for Medical\n  Research and Applications: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging Clinical Knowledge into Large Language Models for Medical\n  Research and Applications: A Survey"
                },
                "summary": "Clinical knowledge is the collection of information learned from studies on\nthe causes, prognosis, diagnosis, and treatment of diseases. This type of\nknowledge can improve curing performances, and promote physical health. With\nthe emergence of large language models (LLMs), medical artificial intelligence\n(medical AI), which aims to apply academic medical AI systems to real-world\nmedical scenarios, has entered a new age of development, resulting in excellent\nworks such as DoctorGPT and Pangu-Drug from academic and industrial researches.\nHowever, the field lacks a comprehensive compendium and comparison of building\nmedical AI systems from academia and industry. Therefore, this survey focuses\non the building paradigms of medical AI systems including the use of clinical\ndatabases, datasets, training pipelines, integrating medical knowledge graphs,\nsystem applications, and evaluation systems. We hope that this survey can help\nrelevant practical researchers understand the current performance of academic\nmodels in various fields of healthcare, as well as the potential problems and\nfuture directions for implementing these scientific achievements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical knowledge is the collection of information learned from studies on\nthe causes, prognosis, diagnosis, and treatment of diseases. This type of\nknowledge can improve curing performances, and promote physical health. With\nthe emergence of large language models (LLMs), medical artificial intelligence\n(medical AI), which aims to apply academic medical AI systems to real-world\nmedical scenarios, has entered a new age of development, resulting in excellent\nworks such as DoctorGPT and Pangu-Drug from academic and industrial researches.\nHowever, the field lacks a comprehensive compendium and comparison of building\nmedical AI systems from academia and industry. Therefore, this survey focuses\non the building paradigms of medical AI systems including the use of clinical\ndatabases, datasets, training pipelines, integrating medical knowledge graphs,\nsystem applications, and evaluation systems. We hope that this survey can help\nrelevant practical researchers understand the current performance of academic\nmodels in various fields of healthcare, as well as the potential problems and\nfuture directions for implementing these scientific achievements."
                },
                "authors": [
                    {
                        "name": "Qiyuan Li"
                    },
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Caicai Guo"
                    },
                    {
                        "name": "Deyu Chen"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Jinguang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinguang Gu"
                },
                "author": "Jinguang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06655v2",
                "updated": "2025-02-28T11:58:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    58,
                    28,
                    4,
                    59,
                    0
                ],
                "published": "2024-11-11T01:42:56Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    42,
                    56,
                    0,
                    316,
                    0
                ],
                "title": "Explore the Reasoning Capability of LLMs in the Chess Testbed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore the Reasoning Capability of LLMs in the Chess Testbed"
                },
                "summary": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Lei Ji"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Wenxiao Zhao"
                    },
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Yifan Hou"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "arxiv_comment": "NAACL2025 Main Conference. Data and models are available:\n  https://mate-chess.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20984v1",
                "updated": "2025-02-28T11:52:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    52,
                    2,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:52:02Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    52,
                    2,
                    4,
                    59,
                    0
                ],
                "title": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation"
                },
                "summary": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL."
                },
                "authors": [
                    {
                        "name": "Thanet Markchom"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Liting Huang"
                    },
                    {
                        "name": "Huizhi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huizhi Liang"
                },
                "author": "Huizhi Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20979v1",
                "updated": "2025-02-28T11:49:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    49,
                    58,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:49:58Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    49,
                    58,
                    4,
                    59,
                    0
                ],
                "title": "Real-Time Aerial Fire Detection on Resource-Constrained Devices Using\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Aerial Fire Detection on Resource-Constrained Devices Using\n  Knowledge Distillation"
                },
                "summary": "Wildfire catastrophes cause significant environmental degradation, human\nlosses, and financial damage. To mitigate these severe impacts, early fire\ndetection and warning systems are crucial. Current systems rely primarily on\nfixed CCTV cameras with a limited field of view, restricting their\neffectiveness in large outdoor environments. The fusion of intelligent fire\ndetection with remote sensing improves coverage and mobility, enabling\nmonitoring in remote and challenging areas. Existing approaches predominantly\nutilize convolutional neural networks and vision transformer models. While\nthese architectures provide high accuracy in fire detection, their\ncomputational complexity limits real-time performance on edge devices such as\nUAVs. In our work, we present a lightweight fire detection model based on\nMobileViT-S, compressed through the distillation of knowledge from a stronger\nteacher model. The ablation study highlights the impact of a teacher model and\nthe chosen distillation technique on the model's performance improvement. We\ngenerate activation map visualizations using Grad-CAM to confirm the model's\nability to focus on relevant fire regions. The high accuracy and efficiency of\nthe proposed model make it well-suited for deployment on satellites, UAVs, and\nIoT devices for effective fire detection. Experiments on common fire benchmarks\ndemonstrate that our model suppresses the state-of-the-art model by 0.44%,\n2.00% while maintaining a compact model size. Our model delivers the highest\nprocessing speed among existing works, achieving real-time performance on\nresource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wildfire catastrophes cause significant environmental degradation, human\nlosses, and financial damage. To mitigate these severe impacts, early fire\ndetection and warning systems are crucial. Current systems rely primarily on\nfixed CCTV cameras with a limited field of view, restricting their\neffectiveness in large outdoor environments. The fusion of intelligent fire\ndetection with remote sensing improves coverage and mobility, enabling\nmonitoring in remote and challenging areas. Existing approaches predominantly\nutilize convolutional neural networks and vision transformer models. While\nthese architectures provide high accuracy in fire detection, their\ncomputational complexity limits real-time performance on edge devices such as\nUAVs. In our work, we present a lightweight fire detection model based on\nMobileViT-S, compressed through the distillation of knowledge from a stronger\nteacher model. The ablation study highlights the impact of a teacher model and\nthe chosen distillation technique on the model's performance improvement. We\ngenerate activation map visualizations using Grad-CAM to confirm the model's\nability to focus on relevant fire regions. The high accuracy and efficiency of\nthe proposed model make it well-suited for deployment on satellites, UAVs, and\nIoT devices for effective fire detection. Experiments on common fire benchmarks\ndemonstrate that our model suppresses the state-of-the-art model by 0.44%,\n2.00% while maintaining a compact model size. Our model delivers the highest\nprocessing speed among existing works, achieving real-time performance on\nresource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Sabina Jangirova"
                    },
                    {
                        "name": "Branislava Jankovic"
                    },
                    {
                        "name": "Waseem Ullah"
                    },
                    {
                        "name": "Latif U. Khan"
                    },
                    {
                        "name": "Mohsen Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Guizani"
                },
                "author": "Mohsen Guizani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20975v1",
                "updated": "2025-02-28T11:40:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    40,
                    34,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:40:34Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    40,
                    34,
                    4,
                    59,
                    0
                ],
                "title": "Set-Theoretic Compositionality of Sentence Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set-Theoretic Compositionality of Sentence Embeddings"
                },
                "summary": "Sentence encoders play a pivotal role in various NLP tasks; hence, an\naccurate evaluation of their compositional properties is paramount. However,\nexisting evaluation methods predominantly focus on goal task-specific\nperformance. This leaves a significant gap in understanding how well sentence\nembeddings demonstrate fundamental compositional properties in a\ntask-independent context. Leveraging classical set theory, we address this gap\nby proposing six criteria based on three core \"set-like\"\ncompositions/operations: \\textit{TextOverlap}, \\textit{TextDifference}, and\n\\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large\nLanguage Model (LLM)-based sentence encoders to assess their alignment with\nthese criteria. Our findings show that SBERT consistently demonstrates set-like\ncompositional properties, surpassing even the latest LLMs. Additionally, we\nintroduce a new dataset of ~$192$K samples designed to facilitate future\nbenchmarking efforts on set-like compositionality of sentence embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence encoders play a pivotal role in various NLP tasks; hence, an\naccurate evaluation of their compositional properties is paramount. However,\nexisting evaluation methods predominantly focus on goal task-specific\nperformance. This leaves a significant gap in understanding how well sentence\nembeddings demonstrate fundamental compositional properties in a\ntask-independent context. Leveraging classical set theory, we address this gap\nby proposing six criteria based on three core \"set-like\"\ncompositions/operations: \\textit{TextOverlap}, \\textit{TextDifference}, and\n\\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large\nLanguage Model (LLM)-based sentence encoders to assess their alignment with\nthese criteria. Our findings show that SBERT consistently demonstrates set-like\ncompositional properties, surpassing even the latest LLMs. Additionally, we\nintroduce a new dataset of ~$192$K samples designed to facilitate future\nbenchmarking efforts on set-like compositionality of sentence embeddings."
                },
                "authors": [
                    {
                        "name": "Naman Bansal"
                    },
                    {
                        "name": "Yash mahajan"
                    },
                    {
                        "name": "Sanjeev Sinha"
                    },
                    {
                        "name": "Santu Karmaker"
                    }
                ],
                "author_detail": {
                    "name": "Santu Karmaker"
                },
                "author": "Santu Karmaker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20973v1",
                "updated": "2025-02-28T11:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    37,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    37,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?"
                },
                "summary": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic."
                },
                "authors": [
                    {
                        "name": "Perla Al Almaoui"
                    },
                    {
                        "name": "Pierrette Bouillon"
                    },
                    {
                        "name": "Simon Hengchen"
                    }
                ],
                "author_detail": {
                    "name": "Simon Hengchen"
                },
                "author": "Simon Hengchen",
                "arxiv_comment": "Submitted to MT Summit 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20969v1",
                "updated": "2025-02-28T11:32:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:32:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    32,
                    22,
                    4,
                    59,
                    0
                ],
                "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with\n  Lookahead Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with\n  Lookahead Retrieval"
                },
                "summary": "Retrieval-augmented generation (RAG) extends large language models (LLMs)\nwith external data sources to enhance factual correctness and domain coverage.\nModern RAG pipelines rely on large datastores, leading to system challenges in\nlatency-sensitive deployments, especially when limited GPU memory is available.\nTo address these challenges, we propose TeleRAG, an efficient inference system\nthat reduces RAG latency with minimal GPU memory requirements. The core\ninnovation of TeleRAG is lookahead retrieval, a prefetching mechanism that\nanticipates required data and transfers it from CPU to GPU in parallel with LLM\ngeneration. By leveraging the modularity of RAG pipelines, the inverted file\nindex (IVF) search algorithm and similarities between queries, TeleRAG\noptimally overlaps data movement and computation. Experimental results show\nthat TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average\ncompared to state-of-the-art systems, enabling faster, more memory-efficient\ndeployments of advanced RAG applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) extends large language models (LLMs)\nwith external data sources to enhance factual correctness and domain coverage.\nModern RAG pipelines rely on large datastores, leading to system challenges in\nlatency-sensitive deployments, especially when limited GPU memory is available.\nTo address these challenges, we propose TeleRAG, an efficient inference system\nthat reduces RAG latency with minimal GPU memory requirements. The core\ninnovation of TeleRAG is lookahead retrieval, a prefetching mechanism that\nanticipates required data and transfers it from CPU to GPU in parallel with LLM\ngeneration. By leveraging the modularity of RAG pipelines, the inverted file\nindex (IVF) search algorithm and similarities between queries, TeleRAG\noptimally overlaps data movement and computation. Experimental results show\nthat TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average\ncompared to state-of-the-art systems, enabling faster, more memory-efficient\ndeployments of advanced RAG applications."
                },
                "authors": [
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Keisuke Kamahori"
                    },
                    {
                        "name": "Yiyu Liu"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Madhav Kashyap"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20968v1",
                "updated": "2025-02-28T11:31:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    31,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:31:27Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    31,
                    27,
                    4,
                    59,
                    0
                ],
                "title": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play\n  Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play\n  Fine-Tuning of LLMs"
                },
                "summary": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Xingyu Sui"
                    },
                    {
                        "name": "Xinyang Han"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "25 pages, 10 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16757v2",
                "updated": "2025-02-28T11:29:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    29,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2023-10-25T16:45:02Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    16,
                    45,
                    2,
                    2,
                    298,
                    0
                ],
                "title": "All-rounder: A Flexible AI Accelerator with Diverse Data Format Support\n  and Morphable Structure for Multi-DNN Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-rounder: A Flexible AI Accelerator with Diverse Data Format Support\n  and Morphable Structure for Multi-DNN Processing"
                },
                "summary": "Recognizing the explosive increase in the use of AI-based applications,\nseveral industrial companies developed custom ASICs (e.g., Google TPU, IBM\nRaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure\nwith them. These ASICs perform operations of the inference or training process\nof AI models which are requested by users. Since the AI models have different\ndata formats and types of operations, the ASICs need to support diverse data\nformats and various operation shapes. However, the previous ASIC solutions do\nnot or less fulfill these requirements. To overcome these limitations, we first\npresent an area-efficient multiplier, named all-in-one multiplier, that\nsupports multiple bit-widths for both integer and floating point data types.\nThen, we build a MAC array equipped with these multipliers with multi-format\nsupport. In addition, the MAC array can be partitioned into multiple blocks\nthat can be flexibly fused to support various DNN operation types. We evaluate\nthe practical effectiveness of the proposed MAC array by making an accelerator\nout of it, named All-rounder. According to our evaluation, the proposed\nall-in-one multiplier occupies 1.49x smaller area compared to the baselines\nwith dedicated multipliers for each data format. Then, we compare the\nperformance and energy efficiency of the proposed All-rounder with three\ndifferent accelerators showing consistent speedup and higher efficiency across\nvarious AI benchmarks from vision to LLM-based language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing the explosive increase in the use of AI-based applications,\nseveral industrial companies developed custom ASICs (e.g., Google TPU, IBM\nRaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure\nwith them. These ASICs perform operations of the inference or training process\nof AI models which are requested by users. Since the AI models have different\ndata formats and types of operations, the ASICs need to support diverse data\nformats and various operation shapes. However, the previous ASIC solutions do\nnot or less fulfill these requirements. To overcome these limitations, we first\npresent an area-efficient multiplier, named all-in-one multiplier, that\nsupports multiple bit-widths for both integer and floating point data types.\nThen, we build a MAC array equipped with these multipliers with multi-format\nsupport. In addition, the MAC array can be partitioned into multiple blocks\nthat can be flexibly fused to support various DNN operation types. We evaluate\nthe practical effectiveness of the proposed MAC array by making an accelerator\nout of it, named All-rounder. According to our evaluation, the proposed\nall-in-one multiplier occupies 1.49x smaller area compared to the baselines\nwith dedicated multipliers for each data format. Then, we compare the\nperformance and energy efficiency of the proposed All-rounder with three\ndifferent accelerators showing consistent speedup and higher efficiency across\nvarious AI benchmarks from vision to LLM-based language tasks."
                },
                "authors": [
                    {
                        "name": "Seock-Hwan Noh"
                    },
                    {
                        "name": "Seungpyo Lee"
                    },
                    {
                        "name": "Banseok Shin"
                    },
                    {
                        "name": "Sehun Park"
                    },
                    {
                        "name": "Yongjoo Jang"
                    },
                    {
                        "name": "Jaeha Kung"
                    }
                ],
                "author_detail": {
                    "name": "Jaeha Kung"
                },
                "author": "Jaeha Kung",
                "arxiv_comment": "A paper accepted in the 2025 IEEE Transactions on Very Large Scale\n  Integration (VLSI) Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20963v1",
                "updated": "2025-02-28T11:25:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    25,
                    11,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:25:11Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    25,
                    11,
                    4,
                    59,
                    0
                ],
                "title": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration"
                },
                "summary": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research."
                },
                "authors": [
                    {
                        "name": "Gerion Spielberger"
                    },
                    {
                        "name": "Florian Artinger"
                    },
                    {
                        "name": "Jochen Reb"
                    },
                    {
                        "name": "Rudolf Kerschreiter"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Kerschreiter"
                },
                "author": "Rudolf Kerschreiter",
                "arxiv_comment": "30 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20952v1",
                "updated": "2025-02-28T11:07:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    7,
                    41,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T11:07:41Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    11,
                    7,
                    41,
                    4,
                    59,
                    0
                ],
                "title": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers\n  Exhibit Greater Sensitivity to Harmful Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Jailbreaking of Large Models by Freeze Training: Lower Layers\n  Exhibit Greater Sensitivity to Harmful Content"
                },
                "summary": "With the widespread application of Large Language Models across various\ndomains, their security issues have increasingly garnered significant attention\nfrom both academic and industrial communities. This study conducts sampling and\nnormalization of the parameters of the LLM to generate visual representations\nand heatmaps of parameter distributions, revealing notable discrepancies in\nparameter distributions among certain layers within the hidden layers. Further\nanalysis involves calculating statistical metrics for each layer, followed by\nthe computation of a Comprehensive Sensitivity Score based on these metrics,\nwhich identifies the lower layers as being particularly sensitive to the\ngeneration of harmful content. Based on this finding, we employ a Freeze\ntraining strategy, selectively performing Supervised Fine-Tuning only on the\nlower layers. Experimental results demonstrate that this method significantly\nreduces training duration and GPU memory consumption while maintaining a high\njailbreak success rate and a high harm score, outperforming the results\nachieved by applying the LoRA method for SFT across all layers. Additionally,\nthe method has been successfully extended to other open-source large models,\nvalidating its generality and effectiveness across different model\narchitectures. Furthermore, we compare our method with ohter jailbreak method,\ndemonstrating the superior performance of our approach. By innovatively\nproposing a method to statistically analyze and compare large model parameters\nlayer by layer, this study provides new insights into the interpretability of\nlarge models. These discoveries emphasize the necessity of continuous research\nand the implementation of adaptive security measures in the rapidly evolving\nfield of LLMs to prevent potential jailbreak attack risks, thereby promoting\nthe development of more robust and secure LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of Large Language Models across various\ndomains, their security issues have increasingly garnered significant attention\nfrom both academic and industrial communities. This study conducts sampling and\nnormalization of the parameters of the LLM to generate visual representations\nand heatmaps of parameter distributions, revealing notable discrepancies in\nparameter distributions among certain layers within the hidden layers. Further\nanalysis involves calculating statistical metrics for each layer, followed by\nthe computation of a Comprehensive Sensitivity Score based on these metrics,\nwhich identifies the lower layers as being particularly sensitive to the\ngeneration of harmful content. Based on this finding, we employ a Freeze\ntraining strategy, selectively performing Supervised Fine-Tuning only on the\nlower layers. Experimental results demonstrate that this method significantly\nreduces training duration and GPU memory consumption while maintaining a high\njailbreak success rate and a high harm score, outperforming the results\nachieved by applying the LoRA method for SFT across all layers. Additionally,\nthe method has been successfully extended to other open-source large models,\nvalidating its generality and effectiveness across different model\narchitectures. Furthermore, we compare our method with ohter jailbreak method,\ndemonstrating the superior performance of our approach. By innovatively\nproposing a method to statistically analyze and compare large model parameters\nlayer by layer, this study provides new insights into the interpretability of\nlarge models. These discoveries emphasize the necessity of continuous research\nand the implementation of adaptive security measures in the rapidly evolving\nfield of LLMs to prevent potential jailbreak attack risks, thereby promoting\nthe development of more robust and secure LLMs."
                },
                "authors": [
                    {
                        "name": "Hongyuan Shen"
                    },
                    {
                        "name": "Min Zheng"
                    },
                    {
                        "name": "Jincheng Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17146v2",
                "updated": "2025-02-28T10:53:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    53,
                    12,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-22T16:26:05Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    26,
                    5,
                    1,
                    296,
                    0
                ],
                "title": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances\n  Model Merging"
                },
                "summary": "Fine-tuning pre-trained models has become the standard approach to endow them\nwith specialized knowledge, but it poses fundamental challenges. In particular,\n\\textit{(i)} fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks, and\n\\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to\nsignificant performance loss. To address these challenges, we introduce LiNeS,\nLayer-increasing Network Scaling, a post-training editing technique designed to\npreserve pre-trained generalization while enhancing fine-tuned task\nperformance. LiNeS scales parameter updates linearly based on their layer depth\nwithin the network, maintaining shallow layers close to their pre-trained\nvalues to preserve general features while allowing deeper layers to retain\ntask-specific representations. In multi-task model merging scenarios,\nlayer-wise scaling of merged parameters reduces negative task interference.\nLiNeS demonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Our method is simple to implement, computationally efficient and\ncomplementary to many existing techniques. Our source code is available at\nhttps://github.com/wang-kee/LiNeS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pre-trained models has become the standard approach to endow them\nwith specialized knowledge, but it poses fundamental challenges. In particular,\n\\textit{(i)} fine-tuning often leads to catastrophic forgetting, where\nimprovements on a target domain degrade generalization on other tasks, and\n\\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to\nsignificant performance loss. To address these challenges, we introduce LiNeS,\nLayer-increasing Network Scaling, a post-training editing technique designed to\npreserve pre-trained generalization while enhancing fine-tuned task\nperformance. LiNeS scales parameter updates linearly based on their layer depth\nwithin the network, maintaining shallow layers close to their pre-trained\nvalues to preserve general features while allowing deeper layers to retain\ntask-specific representations. In multi-task model merging scenarios,\nlayer-wise scaling of merged parameters reduces negative task interference.\nLiNeS demonstrates significant improvements in both single-task and multi-task\nsettings across various benchmarks in vision and natural language processing.\nIt mitigates forgetting, enhances out-of-distribution generalization,\nintegrates seamlessly with existing multi-task model merging baselines\nimproving their performance across benchmarks and model sizes, and can boost\ngeneralization when merging LLM policies aligned with different rewards via\nRLHF. Our method is simple to implement, computationally efficient and\ncomplementary to many existing techniques. Our source code is available at\nhttps://github.com/wang-kee/LiNeS"
                },
                "authors": [
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Nikolaos Dimitriadis"
                    },
                    {
                        "name": "Alessandro Favero"
                    },
                    {
                        "name": "Guillermo Ortiz-Jimenez"
                    },
                    {
                        "name": "Francois Fleuret"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "arxiv_comment": "The first two authors contributed equally to this work. Accepted at\n  ICLR 2025. Project website: https://lines-merging.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15127v2",
                "updated": "2025-02-28T10:49:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    49,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-09-23T15:33:38Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    33,
                    38,
                    0,
                    267,
                    0
                ],
                "title": "Cost-Effective, High-Performance Open-Source LLMs via Optimized Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Effective, High-Performance Open-Source LLMs via Optimized Context\n  Retrieval"
                },
                "summary": "Large Language Models (LLMs) in healthcare promise transformation, yet\nadoption is limited by concerns over factual accuracy and the high cost of\nproprietary models. This study demonstrates that optimized context retrieval\nunlocks cost-effective, high-performance healthcare AI using open-source LLMs,\nachieving a significantly improved cost-accuracy Pareto frontier for medical\nquestion answering and showcasing that open models can rival proprietary\nsystems at a fraction of the cost. A key contribution is OpenMedQA, a novel\nbenchmark for open-ended medical question answering that overcomes the\nlimitations of multiple-choice formats - formats that we show lead to\nperformance degradation in open-ended settings and often lack clinical realism.\nFurther contributions include: (1) practical guidelines for implementing\noptimized context retrieval; (2) empirical validation of enhanced\ncost-effectiveness via the improved Pareto frontier; (3) the introduction of\nOpenMedQA for rigorous evaluation of open-ended medical QA; and (4) the release\nof prompt_engine alongside CoT/ToT/Thinking databases as community resources\nfor cost-effective healthcare AI. Advancing optimized retrieval and open-ended\nQA benchmarking, we pave the way for more accessible and impactful LLM-powered\nhealthcare solutions. All the materials have been made public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in healthcare promise transformation, yet\nadoption is limited by concerns over factual accuracy and the high cost of\nproprietary models. This study demonstrates that optimized context retrieval\nunlocks cost-effective, high-performance healthcare AI using open-source LLMs,\nachieving a significantly improved cost-accuracy Pareto frontier for medical\nquestion answering and showcasing that open models can rival proprietary\nsystems at a fraction of the cost. A key contribution is OpenMedQA, a novel\nbenchmark for open-ended medical question answering that overcomes the\nlimitations of multiple-choice formats - formats that we show lead to\nperformance degradation in open-ended settings and often lack clinical realism.\nFurther contributions include: (1) practical guidelines for implementing\noptimized context retrieval; (2) empirical validation of enhanced\ncost-effectiveness via the improved Pareto frontier; (3) the introduction of\nOpenMedQA for rigorous evaluation of open-ended medical QA; and (4) the release\nof prompt_engine alongside CoT/ToT/Thinking databases as community resources\nfor cost-effective healthcare AI. Advancing optimized retrieval and open-ended\nQA benchmarking, we pave the way for more accessible and impactful LLM-powered\nhealthcare solutions. All the materials have been made public."
                },
                "authors": [
                    {
                        "name": "Jordi Bayarri-Planas"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Dario Garcia-Gasulla"
                    }
                ],
                "author_detail": {
                    "name": "Dario Garcia-Gasulla"
                },
                "author": "Dario Garcia-Gasulla",
                "arxiv_comment": "14 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20938v1",
                "updated": "2025-02-28T10:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    48,
                    14,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    48,
                    14,
                    4,
                    59,
                    0
                ],
                "title": "A Deep User Interface for Exploring LLaMa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep User Interface for Exploring LLaMa"
                },
                "summary": "The growing popularity and widespread adoption of large language models\n(LLMs) necessitates the development of tools that enhance the effectiveness of\nuser interactions with these models. Understanding the structures and functions\nof these models poses a significant challenge for users. Visual\nanalytics-driven tools enables users to explore and compare, facilitating\nbetter decision-making. This paper presents a visual analytics-driven tool\nequipped with interactive controls for key hyperparameters, including top-p,\nfrequency and presence penalty, enabling users to explore, examine and compare\nthe outputs of LLMs. In a user study, we assessed the tool's effectiveness,\nwhich received favorable feedback for its visual design, with particular\ncommendation for the interface layout and ease of navigation. Additionally, the\nfeedback provided valuable insights for enhancing the effectiveness of\nHuman-LLM interaction tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity and widespread adoption of large language models\n(LLMs) necessitates the development of tools that enhance the effectiveness of\nuser interactions with these models. Understanding the structures and functions\nof these models poses a significant challenge for users. Visual\nanalytics-driven tools enables users to explore and compare, facilitating\nbetter decision-making. This paper presents a visual analytics-driven tool\nequipped with interactive controls for key hyperparameters, including top-p,\nfrequency and presence penalty, enabling users to explore, examine and compare\nthe outputs of LLMs. In a user study, we assessed the tool's effectiveness,\nwhich received favorable feedback for its visual design, with particular\ncommendation for the interface layout and ease of navigation. Additionally, the\nfeedback provided valuable insights for enhancing the effectiveness of\nHuman-LLM interaction tools."
                },
                "authors": [
                    {
                        "name": "Divya Perumal"
                    },
                    {
                        "name": "Swaroop Panda"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Panda"
                },
                "author": "Swaroop Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20936v1",
                "updated": "2025-02-28T10:46:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    46,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:46:52Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    46,
                    52,
                    4,
                    59,
                    0
                ],
                "title": "WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense\n  Retrieval"
                },
                "summary": "We present WebFAQ, a large-scale collection of open-domain question answering\ndatasets derived from FAQ-style schema.org annotations. In total, the data\ncollection consists of 96 million natural question-answer (QA) pairs across 75\nlanguages, including 47 million (49%) non-English samples. WebFAQ further\nserves as the foundation for 20 monolingual retrieval benchmarks with a total\nsize of 11.2 million QA pairs (5.9 million non-English). These datasets are\ncarefully curated through refined filtering and near-duplicate detection,\nyielding high-quality resources for training and evaluating multilingual dense\nretrieval models. To empirically confirm WebFAQ's efficacy, we use the\ncollected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through\nthis process of dataset-specific fine-tuning, the model achieves significant\nretrieval performance gains, which generalize - beyond WebFAQ - to other\nmultilingual retrieval benchmarks evaluated in zero-shot setting. Last but not\nleast, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora\nspanning over 1000 language pairs using state-of-the-art bitext mining and\nautomated LLM-assessed translation evaluation. Due to our advanced, automated\nmethod of bitext dataset generation, the resulting bilingual corpora\ndemonstrate higher translation quality compared to similar datasets. WebFAQ and\nall associated resources are publicly available on GitHub and HuggingFace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present WebFAQ, a large-scale collection of open-domain question answering\ndatasets derived from FAQ-style schema.org annotations. In total, the data\ncollection consists of 96 million natural question-answer (QA) pairs across 75\nlanguages, including 47 million (49%) non-English samples. WebFAQ further\nserves as the foundation for 20 monolingual retrieval benchmarks with a total\nsize of 11.2 million QA pairs (5.9 million non-English). These datasets are\ncarefully curated through refined filtering and near-duplicate detection,\nyielding high-quality resources for training and evaluating multilingual dense\nretrieval models. To empirically confirm WebFAQ's efficacy, we use the\ncollected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through\nthis process of dataset-specific fine-tuning, the model achieves significant\nretrieval performance gains, which generalize - beyond WebFAQ - to other\nmultilingual retrieval benchmarks evaluated in zero-shot setting. Last but not\nleast, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora\nspanning over 1000 language pairs using state-of-the-art bitext mining and\nautomated LLM-assessed translation evaluation. Due to our advanced, automated\nmethod of bitext dataset generation, the resulting bilingual corpora\ndemonstrate higher translation quality compared to similar datasets. WebFAQ and\nall associated resources are publicly available on GitHub and HuggingFace."
                },
                "authors": [
                    {
                        "name": "Michael Dinzinger"
                    },
                    {
                        "name": "Laura Caspari"
                    },
                    {
                        "name": "Kanishka Ghosh Dastidar"
                    },
                    {
                        "name": "Jelena Mitrovi"
                    },
                    {
                        "name": "Michael Granitzer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Granitzer"
                },
                "author": "Michael Granitzer",
                "arxiv_comment": "10 pages, 3 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20933v1",
                "updated": "2025-02-28T10:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    41,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    41,
                    16,
                    4,
                    59,
                    0
                ],
                "title": "Large Language Models Are Innate Crystal Structure Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Innate Crystal Structure Generators"
                },
                "summary": "Crystal structure generation is fundamental to materials discovery, enabling\nthe prediction of novel materials with desired properties. While existing\napproaches leverage Large Language Models (LLMs) through extensive fine-tuning\non materials databases, we show that pre-trained LLMs can inherently generate\nstable crystal structures without additional training. Our novel framework\nMatLLMSearch integrates pre-trained LLMs with evolutionary search algorithms,\nachieving a 78.38% metastable rate validated by machine learning interatomic\npotentials and 31.7% DFT-verified stability via quantum mechanical\ncalculations, outperforming specialized models such as CrystalTextLLM. Beyond\ncrystal structure generation, we further demonstrate that our framework can be\nreadily adapted to diverse materials design tasks, including crystal structure\nprediction and multi-objective optimization of properties such as deformation\nenergy and bulk modulus, all without fine-tuning. These results establish\npre-trained LLMs as versatile and effective tools for materials discovery,\nopening up new venues for crystal structure generation with reduced\ncomputational overhead and broader accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crystal structure generation is fundamental to materials discovery, enabling\nthe prediction of novel materials with desired properties. While existing\napproaches leverage Large Language Models (LLMs) through extensive fine-tuning\non materials databases, we show that pre-trained LLMs can inherently generate\nstable crystal structures without additional training. Our novel framework\nMatLLMSearch integrates pre-trained LLMs with evolutionary search algorithms,\nachieving a 78.38% metastable rate validated by machine learning interatomic\npotentials and 31.7% DFT-verified stability via quantum mechanical\ncalculations, outperforming specialized models such as CrystalTextLLM. Beyond\ncrystal structure generation, we further demonstrate that our framework can be\nreadily adapted to diverse materials design tasks, including crystal structure\nprediction and multi-objective optimization of properties such as deformation\nenergy and bulk modulus, all without fine-tuning. These results establish\npre-trained LLMs as versatile and effective tools for materials discovery,\nopening up new venues for crystal structure generation with reduced\ncomputational overhead and broader accessibility."
                },
                "authors": [
                    {
                        "name": "Jingru Gan"
                    },
                    {
                        "name": "Peichen Zhong"
                    },
                    {
                        "name": "Yuanqi Du"
                    },
                    {
                        "name": "Yanqiao Zhu"
                    },
                    {
                        "name": "Chenru Duan"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Carla P. Gomes"
                    },
                    {
                        "name": "Kristin A. Persson"
                    },
                    {
                        "name": "Daniel Schwalbe-Koda"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Preprint, 18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20916v1",
                "updated": "2025-02-28T10:17:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    17,
                    7,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T10:17:07Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    10,
                    17,
                    7,
                    4,
                    59,
                    0
                ],
                "title": "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays"
                },
                "summary": "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by up to two orders of\nmagnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by up to two orders of\nmagnitude."
                },
                "authors": [
                    {
                        "name": "Stefano Roberto Soleti"
                    },
                    {
                        "name": "Juan Jos Gomez-Cadenas"
                    }
                ],
                "author_detail": {
                    "name": "Juan Jos Gomez-Cadenas"
                },
                "author": "Juan Jos Gomez-Cadenas",
                "arxiv_comment": "12 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20898v1",
                "updated": "2025-02-28T09:54:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    54,
                    13,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T09:54:13Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    54,
                    13,
                    4,
                    59,
                    0
                ],
                "title": "A database to support the evaluation of gender biases in GPT-4o output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A database to support the evaluation of gender biases in GPT-4o output"
                },
                "summary": "The widespread application of Large Language Models (LLMs) involves ethical\nrisks for users and societies. A prominent ethical risk of LLMs is the\ngeneration of unfair language output that reinforces or exacerbates harm for\nmembers of disadvantaged social groups through gender biases (Weidinger et al.,\n2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the\nfairness of LLM outputs with respect to such biases is a topic of rising\ninterest. To advance research in this field, promote discourse on suitable\nnormative bases and evaluation methodologies, and enhance the reproducibility\nof related studies, we propose a novel approach to database construction. This\napproach enables the assessment of gender-related biases in LLM-generated\nlanguage beyond merely evaluating their degree of neutralization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread application of Large Language Models (LLMs) involves ethical\nrisks for users and societies. A prominent ethical risk of LLMs is the\ngeneration of unfair language output that reinforces or exacerbates harm for\nmembers of disadvantaged social groups through gender biases (Weidinger et al.,\n2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the\nfairness of LLM outputs with respect to such biases is a topic of rising\ninterest. To advance research in this field, promote discourse on suitable\nnormative bases and evaluation methodologies, and enhance the reproducibility\nof related studies, we propose a novel approach to database construction. This\napproach enables the assessment of gender-related biases in LLM-generated\nlanguage beyond merely evaluating their degree of neutralization."
                },
                "authors": [
                    {
                        "name": "Luise Mehner"
                    },
                    {
                        "name": "Lena Alicija Philine Fiedler"
                    },
                    {
                        "name": "Sabine Ammon"
                    },
                    {
                        "name": "Dorothea Kolossa"
                    }
                ],
                "author_detail": {
                    "name": "Dorothea Kolossa"
                },
                "author": "Dorothea Kolossa",
                "arxiv_comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20897v1",
                "updated": "2025-02-28T09:53:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    53,
                    42,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T09:53:42Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    53,
                    42,
                    4,
                    59,
                    0
                ],
                "title": "Beyond Demographics: Fine-tuning Large Language Models to Predict\n  Individuals' Subjective Text Perceptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Demographics: Fine-tuning Large Language Models to Predict\n  Individuals' Subjective Text Perceptions"
                },
                "summary": "People naturally vary in their annotations for subjective questions and some\nof this variation is thought to be due to the person's sociodemographic\ncharacteristics. LLMs have also been used to label data, but recent work has\nshown that models perform poorly when prompted with sociodemographic\nattributes, suggesting limited inherent sociodemographic knowledge. Here, we\nask whether LLMs can be trained to be accurate sociodemographic models of\nannotator variation. Using a curated dataset of five tasks with standardized\nsociodemographics, we show that models do improve in sociodemographic prompting\nwhen trained but that this performance gain is largely due to models learning\nannotator-specific behaviour rather than sociodemographic patterns. Across all\ntasks, our results suggest that models learn little meaningful connection\nbetween sociodemographics and annotation, raising doubts about the current use\nof LLMs for simulating sociodemographic variation and behaviour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People naturally vary in their annotations for subjective questions and some\nof this variation is thought to be due to the person's sociodemographic\ncharacteristics. LLMs have also been used to label data, but recent work has\nshown that models perform poorly when prompted with sociodemographic\nattributes, suggesting limited inherent sociodemographic knowledge. Here, we\nask whether LLMs can be trained to be accurate sociodemographic models of\nannotator variation. Using a curated dataset of five tasks with standardized\nsociodemographics, we show that models do improve in sociodemographic prompting\nwhen trained but that this performance gain is largely due to models learning\nannotator-specific behaviour rather than sociodemographic patterns. Across all\ntasks, our results suggest that models learn little meaningful connection\nbetween sociodemographics and annotation, raising doubts about the current use\nof LLMs for simulating sociodemographic variation and behaviour."
                },
                "authors": [
                    {
                        "name": "Matthias Orlikowski"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Philipp Cimiano"
                    },
                    {
                        "name": "David Jurgens"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "Reviewed ARR December 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10636v2",
                "updated": "2025-02-28T09:38:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    38,
                    19,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-15T02:25:49Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    2,
                    25,
                    49,
                    5,
                    46,
                    0
                ],
                "title": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning\n  for Social Human-Robot Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning\n  for Social Human-Robot Interactions"
                },
                "summary": "The integration of vision-language models into robotic systems constitutes a\nsignificant advancement in enabling machines to interact with their\nsurroundings in a more intuitive manner. While VLMs offer rich multimodal\nreasoning, existing approaches lack user-specific adaptability, often relying\non generic interaction paradigms that fail to account for individual\nbehavioral, contextual, or socio-emotional nuances. When customization is\nattempted, ethical concerns arise from unmitigated biases in user data, risking\nexclusion or unfair treatment. To address these dual challenges, we propose\nUser-VLM 360{\\deg}, a holistic framework integrating multimodal user modeling\nwith bias-aware optimization. Our approach features: (1) user-aware tuning that\nadapts interactions in real time using visual-linguistic signals; (2) bias\nmitigation via preference optimization; and (3) curated 360{\\deg} socio-emotive\ninteraction datasets annotated with demographic, emotion, and relational\nmetadata. Evaluations across eight benchmarks demonstrate state-of-the-art\nresults: +35.3% F1 in personalized VQA, +47.5% F1 in facial features\nunderstanding, 15% bias reduction, and 30X speedup over baselines. Ablation\nstudies confirm component efficacy, and deployment on the Pepper robot\nvalidates real-time adaptability across diverse users. We open-source\nparameter-efficient 3B/10B models and an ethical verification framework for\nresponsible adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of vision-language models into robotic systems constitutes a\nsignificant advancement in enabling machines to interact with their\nsurroundings in a more intuitive manner. While VLMs offer rich multimodal\nreasoning, existing approaches lack user-specific adaptability, often relying\non generic interaction paradigms that fail to account for individual\nbehavioral, contextual, or socio-emotional nuances. When customization is\nattempted, ethical concerns arise from unmitigated biases in user data, risking\nexclusion or unfair treatment. To address these dual challenges, we propose\nUser-VLM 360{\\deg}, a holistic framework integrating multimodal user modeling\nwith bias-aware optimization. Our approach features: (1) user-aware tuning that\nadapts interactions in real time using visual-linguistic signals; (2) bias\nmitigation via preference optimization; and (3) curated 360{\\deg} socio-emotive\ninteraction datasets annotated with demographic, emotion, and relational\nmetadata. Evaluations across eight benchmarks demonstrate state-of-the-art\nresults: +35.3% F1 in personalized VQA, +47.5% F1 in facial features\nunderstanding, 15% bias reduction, and 30X speedup over baselines. Ablation\nstudies confirm component efficacy, and deployment on the Pepper robot\nvalidates real-time adaptability across diverse users. We open-source\nparameter-efficient 3B/10B models and an ethical verification framework for\nresponsible adaptation."
                },
                "authors": [
                    {
                        "name": "Hamed Rahimi"
                    },
                    {
                        "name": "Adil Bahaj"
                    },
                    {
                        "name": "Mouad Abrini"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Mounir Ghogho"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chetouani"
                },
                "author": "Mohamed Chetouani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01570v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01570v3",
                "updated": "2025-02-28T09:23:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    23,
                    4,
                    4,
                    59,
                    0
                ],
                "published": "2024-03-03T17:35:52Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    17,
                    35,
                    52,
                    6,
                    63,
                    0
                ],
                "title": "Small Models are LLM Knowledge Triggers on Medical Tabular Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Models are LLM Knowledge Triggers on Medical Tabular Prediction"
                },
                "summary": "Recent development in large language models (LLMs) has demonstrated\nimpressive domain proficiency on unstructured textual or multi-modal tasks.\nHowever, despite with intrinsic world knowledge, their application on\nstructured tabular data prediction still lags behind, primarily due to the\nnumerical insensitivity and modality discrepancy that brings a gap between LLM\nreasoning and statistical tabular learning. Unlike textual or vision data\n(e.g., electronic clinical notes or medical imaging data), tabular data is\noften presented in heterogeneous numerical values (e.g., CBC reports). This\nubiquitous data format requires intensive expert annotation, and its numerical\nnature limits LLMs' capability to effectively transfer untapped domain\nexpertise. In this paper, we propose SERSAL, a general self-prompting method by\nsynergy learning with small models to enhance LLM tabular prediction in an\nunsupervised manner. Specifically, SERSAL utilizes the LLM's prior outcomes as\noriginal soft noisy annotations, which are dynamically leveraged to teach a\nbetter small student model. Reversely, the outcomes from the trained small\nmodel are used to teach the LLM to further refine its real capability. This\nprocess can be repeatedly applied to gradually distill refined knowledge for\ncontinuous progress. Comprehensive experiments on widely used medical domain\ntabular datasets show that, without access to gold labels, applying SERSAL to\nOpenAI GPT reasoning process attains substantial improvement compared to\nlinguistic prompting methods, which serves as an orthogonal direction for\ntabular LLM, and increasing prompting bonus is observed as more powerful LLMs\nappear.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent development in large language models (LLMs) has demonstrated\nimpressive domain proficiency on unstructured textual or multi-modal tasks.\nHowever, despite with intrinsic world knowledge, their application on\nstructured tabular data prediction still lags behind, primarily due to the\nnumerical insensitivity and modality discrepancy that brings a gap between LLM\nreasoning and statistical tabular learning. Unlike textual or vision data\n(e.g., electronic clinical notes or medical imaging data), tabular data is\noften presented in heterogeneous numerical values (e.g., CBC reports). This\nubiquitous data format requires intensive expert annotation, and its numerical\nnature limits LLMs' capability to effectively transfer untapped domain\nexpertise. In this paper, we propose SERSAL, a general self-prompting method by\nsynergy learning with small models to enhance LLM tabular prediction in an\nunsupervised manner. Specifically, SERSAL utilizes the LLM's prior outcomes as\noriginal soft noisy annotations, which are dynamically leveraged to teach a\nbetter small student model. Reversely, the outcomes from the trained small\nmodel are used to teach the LLM to further refine its real capability. This\nprocess can be repeatedly applied to gradually distill refined knowledge for\ncontinuous progress. Comprehensive experiments on widely used medical domain\ntabular datasets show that, without access to gold labels, applying SERSAL to\nOpenAI GPT reasoning process attains substantial improvement compared to\nlinguistic prompting methods, which serves as an orthogonal direction for\ntabular LLM, and increasing prompting bonus is observed as more powerful LLMs\nappear."
                },
                "authors": [
                    {
                        "name": "Jiahuan Yan"
                    },
                    {
                        "name": "Jintai Chen"
                    },
                    {
                        "name": "Chaowen Hu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Yaojun Hu"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Jian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wu"
                },
                "author": "Jian Wu",
                "arxiv_comment": "Accepted to ICLR 2025. Codes will be available at\n  https://github.com/jyansir/sersal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01570v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01570v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20869v1",
                "updated": "2025-02-28T09:13:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    13,
                    1,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T09:13:01Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    13,
                    1,
                    4,
                    59,
                    0
                ],
                "title": "PathVG: A New Benchmark and Dataset for Pathology Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PathVG: A New Benchmark and Dataset for Pathology Visual Grounding"
                },
                "summary": "With the rapid development of computational pathology, many AI-assisted\ndiagnostic tasks have emerged. Cellular nuclei segmentation can segment various\ntypes of cells for downstream analysis, but it relies on predefined categories\nand lacks flexibility. Moreover, pathology visual question answering can\nperform image-level understanding but lacks region-level detection capability.\nTo address this, we propose a new benchmark called Pathology Visual Grounding\n(PathVG), which aims to detect regions based on expressions with different\nattributes. To evaluate PathVG, we create a new dataset named RefPath which\ncontains 27,610 images with 33,500 language-grounded boxes. Compared to visual\ngrounding in other domains, PathVG presents pathological images at multi-scale\nand contains expressions with pathological knowledge. In the experimental\nstudy, we found that the biggest challenge was the implicit information\nunderlying the pathological expressions. Based on this, we proposed Pathology\nKnowledge-enhanced Network (PKNet) as the baseline model for PathVG. PKNet\nleverages the knowledge-enhancement capabilities of Large Language Models\n(LLMs) to convert pathological terms with implicit information into explicit\nvisual features, and fuses knowledge features with expression features through\nthe designed Knowledge Fusion Module (KFM). The proposed method achieves\nstate-of-the-art performance on the PathVG benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of computational pathology, many AI-assisted\ndiagnostic tasks have emerged. Cellular nuclei segmentation can segment various\ntypes of cells for downstream analysis, but it relies on predefined categories\nand lacks flexibility. Moreover, pathology visual question answering can\nperform image-level understanding but lacks region-level detection capability.\nTo address this, we propose a new benchmark called Pathology Visual Grounding\n(PathVG), which aims to detect regions based on expressions with different\nattributes. To evaluate PathVG, we create a new dataset named RefPath which\ncontains 27,610 images with 33,500 language-grounded boxes. Compared to visual\ngrounding in other domains, PathVG presents pathological images at multi-scale\nand contains expressions with pathological knowledge. In the experimental\nstudy, we found that the biggest challenge was the implicit information\nunderlying the pathological expressions. Based on this, we proposed Pathology\nKnowledge-enhanced Network (PKNet) as the baseline model for PathVG. PKNet\nleverages the knowledge-enhancement capabilities of Large Language Models\n(LLMs) to convert pathological terms with implicit information into explicit\nvisual features, and fuses knowledge features with expression features through\nthe designed Knowledge Fusion Module (KFM). The proposed method achieves\nstate-of-the-art performance on the PathVG benchmark."
                },
                "authors": [
                    {
                        "name": "Chunlin Zhong"
                    },
                    {
                        "name": "Shuang Hao"
                    },
                    {
                        "name": "Junhua Wu"
                    },
                    {
                        "name": "Xiaona Chang"
                    },
                    {
                        "name": "Jiwei Jiang"
                    },
                    {
                        "name": "Xiu Nie"
                    },
                    {
                        "name": "He Tang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "10pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20868v1",
                "updated": "2025-02-28T09:12:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    12,
                    42,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T09:12:42Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    12,
                    42,
                    4,
                    59,
                    0
                ],
                "title": "ProBench: Benchmarking Large Language Models in Competitive Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProBench: Benchmarking Large Language Models in Competitive Programming"
                },
                "summary": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging,\nlarge language models (LLMs) have entered a new phase of development. However,\nexisting benchmarks for coding evaluation are gradually inadequate to assess\nthe capability of advanced LLMs in code reasoning. To bridge the gap for\nhigh-level code reasoning assessment, we propose ProBench to benchmark LLMs in\ncompetitive programming, drawing inspiration from the International Collegiate\nProgramming Contest. ProBench collects a comprehensive set of competitive\nprogramming problems from Codeforces, Luogu, and Nowcoder platforms during the\nperiod from July to December 2024, obtaining real test results through online\nsubmissions to ensure the fairness and accuracy of the evaluation. We establish\na unified problem attribute system, including difficulty grading and algorithm\ntagging. With carefully collected and annotated data in ProBench, we\nsystematically assess 9 latest LLMs in competitive programming across multiple\ndimensions, including thought chain analysis, error type diagnosis, and\nreasoning depth evaluation. Experimental results show that QwQ-32B-Preview\nachieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38,\nsuggesting that models trained with specialized reasoning tasks significantly\noutperform general-purpose models (even larger than reasoning-oriented models)\nin programming. Further analysis also reveals key areas for programming\ncapability enhancement, e.g., algorithm adaptability and reasoning sufficiency,\nproviding important insights for the future development of reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging,\nlarge language models (LLMs) have entered a new phase of development. However,\nexisting benchmarks for coding evaluation are gradually inadequate to assess\nthe capability of advanced LLMs in code reasoning. To bridge the gap for\nhigh-level code reasoning assessment, we propose ProBench to benchmark LLMs in\ncompetitive programming, drawing inspiration from the International Collegiate\nProgramming Contest. ProBench collects a comprehensive set of competitive\nprogramming problems from Codeforces, Luogu, and Nowcoder platforms during the\nperiod from July to December 2024, obtaining real test results through online\nsubmissions to ensure the fairness and accuracy of the evaluation. We establish\na unified problem attribute system, including difficulty grading and algorithm\ntagging. With carefully collected and annotated data in ProBench, we\nsystematically assess 9 latest LLMs in competitive programming across multiple\ndimensions, including thought chain analysis, error type diagnosis, and\nreasoning depth evaluation. Experimental results show that QwQ-32B-Preview\nachieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38,\nsuggesting that models trained with specialized reasoning tasks significantly\noutperform general-purpose models (even larger than reasoning-oriented models)\nin programming. Further analysis also reveals key areas for programming\ncapability enhancement, e.g., algorithm adaptability and reasoning sufficiency,\nproviding important insights for the future development of reasoning models."
                },
                "authors": [
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Jianxiang Peng"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20866v1",
                "updated": "2025-02-28T09:08:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    8,
                    57,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T09:08:57Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    8,
                    57,
                    4,
                    59,
                    0
                ],
                "title": "Better Benchmarking LLMs for Zero-Shot Dependency Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Benchmarking LLMs for Zero-Shot Dependency Parsing"
                },
                "summary": "While LLMs excel in zero-shot tasks, their performance in linguistic\nchallenges like syntactic parsing has been less scrutinized. This paper studies\nstate-of-the-art open-weight LLMs on the task by comparing them to baselines\nthat do not have access to the input sentence, including baselines that have\nnot been used in this context such as random projective trees or optimal linear\narrangements. The results show that most of the tested LLMs cannot outperform\nthe best uninformed baselines, with only the newest and largest versions of\nLLaMA doing so for most languages, and still achieving rather low performance.\nThus, accurate zero-shot syntactic parsing is not forthcoming with open LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel in zero-shot tasks, their performance in linguistic\nchallenges like syntactic parsing has been less scrutinized. This paper studies\nstate-of-the-art open-weight LLMs on the task by comparing them to baselines\nthat do not have access to the input sentence, including baselines that have\nnot been used in this context such as random projective trees or optimal linear\narrangements. The results show that most of the tested LLMs cannot outperform\nthe best uninformed baselines, with only the newest and largest versions of\nLLaMA doing so for most languages, and still achieving rather low performance.\nThus, accurate zero-shot syntactic parsing is not forthcoming with open LLMs."
                },
                "authors": [
                    {
                        "name": "Ana Ezquerro"
                    },
                    {
                        "name": "Carlos Gmez-Rodrguez"
                    },
                    {
                        "name": "David Vilares"
                    }
                ],
                "author_detail": {
                    "name": "David Vilares"
                },
                "author": "David Vilares",
                "arxiv_comment": "Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20859v1",
                "updated": "2025-02-28T09:01:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    1,
                    39,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T09:01:39Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    9,
                    1,
                    39,
                    4,
                    59,
                    0
                ],
                "title": "The Power of Personality: A Human Simulation Perspective to Investigate\n  Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Personality: A Human Simulation Perspective to Investigate\n  Large Language Model Agents"
                },
                "summary": "Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) How do personality traits affect\nproblem-solving in closed tasks? (2) How do traits shape creativity in open\ntasks? (3) How does single-agent performance influence multi-agent\ncollaboration? By assigning Big Five personality traits to LLM agents and\nevaluating their performance in single- and multi-agent settings, we reveal\nthat specific traits significantly influence reasoning accuracy (closed tasks)\nand creative output (open tasks). Furthermore, multi-agent systems exhibit\ncollective intelligence distinct from individual capabilities, driven by\ndistinguishing combinations of personalities. We demonstrate that LLMs\ninherently simulate human behavior through next-token prediction, mirroring\nhuman language, decision-making, and collaborative dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) How do personality traits affect\nproblem-solving in closed tasks? (2) How do traits shape creativity in open\ntasks? (3) How does single-agent performance influence multi-agent\ncollaboration? By assigning Big Five personality traits to LLM agents and\nevaluating their performance in single- and multi-agent settings, we reveal\nthat specific traits significantly influence reasoning accuracy (closed tasks)\nand creative output (open tasks). Furthermore, multi-agent systems exhibit\ncollective intelligence distinct from individual capabilities, driven by\ndistinguishing combinations of personalities. We demonstrate that LLMs\ninherently simulate human behavior through next-token prediction, mirroring\nhuman language, decision-making, and collaborative dynamics."
                },
                "authors": [
                    {
                        "name": "Yifan Duan"
                    },
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20854v1",
                "updated": "2025-02-28T08:53:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    53,
                    8,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T08:53:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    53,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "A Pilot Empirical Study on When and How to Use Knowledge Graphs as\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Empirical Study on When and How to Use Knowledge Graphs as\n  Retrieval Augmented Generation"
                },
                "summary": "The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 7 datasets in diverse scenarios, analyzing the impact of 9\nKG-RAG configurations in combination with 17 LLMs. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 7 datasets in diverse scenarios, analyzing the impact of 9\nKG-RAG configurations in combination with 17 LLMs. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components."
                },
                "authors": [
                    {
                        "name": "Xujie Yuan"
                    },
                    {
                        "name": "Yongxu Liu"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Shiwen Wu"
                    },
                    {
                        "name": "Libin Zheng"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Jian Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yin"
                },
                "author": "Jian Yin",
                "arxiv_comment": "8 pages, 2 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15601v2",
                "updated": "2025-02-28T08:49:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    49,
                    29,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-21T17:18:30Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    17,
                    18,
                    30,
                    4,
                    52,
                    0
                ],
                "title": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldCraft: Photo-Realistic 3D World Creation and Customization via LLM\n  Agents"
                },
                "summary": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing photorealistic virtual worlds has applications across various\nfields, but it often requires the extensive labor of highly trained\nprofessionals to operate conventional 3D modeling software. To democratize this\nprocess, we introduce WorldCraft, a system where large language model (LLM)\nagents leverage procedural generation to create indoor and outdoor scenes\npopulated with objects, allowing users to control individual object attributes\nand the scene layout using intuitive natural language commands. In our\nframework, a coordinator agent manages the overall process and works with two\nspecialized LLM agents to complete the scene creation: ForgeIt, which\nintegrates an ever-growing manual through auto-verification to enable precise\ncustomization of individual objects, and ArrangeIt, which formulates\nhierarchical optimization problems to achieve a layout that balances ergonomic\nand aesthetic considerations. Additionally, our pipeline incorporates a\ntrajectory control agent, allowing users to animate the scene and operate the\ncamera through natural language interactions. Our system is also compatible\nwith off-the-shelf deep 3D generators to enrich scene assets. Through\nevaluations and comparisons with state-of-the-art methods, we demonstrate the\nversatility of WorldCraft, ranging from single-object customization to\nintricate, large-scale interior and exterior scene designs. This system\nempowers non-professionals to bring their creative visions to life."
                },
                "authors": [
                    {
                        "name": "Xinhang Liu"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Wing Tai"
                },
                "author": "Yu-Wing Tai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20246v2",
                "updated": "2025-02-28T08:39:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    39,
                    27,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-27T16:30:00Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    16,
                    30,
                    0,
                    3,
                    58,
                    0
                ],
                "title": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets"
                },
                "summary": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets."
                },
                "authors": [
                    {
                        "name": "Chi-Chien Tsai"
                    },
                    {
                        "name": "Chia-Mu Yu"
                    },
                    {
                        "name": "Ying-Dar Lin"
                    },
                    {
                        "name": "Yu-Sung Wu"
                    },
                    {
                        "name": "Wei-Bin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Bin Lee"
                },
                "author": "Wei-Bin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20144v2",
                "updated": "2025-02-28T08:39:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    39,
                    21,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-27T14:35:47Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    35,
                    47,
                    3,
                    58,
                    0
                ],
                "title": "Robust sensitivity control in digital pathology via tile score\n  distribution matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust sensitivity control in digital pathology via tile score\n  distribution matching"
                },
                "summary": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems."
                },
                "authors": [
                    {
                        "name": "Arthur Pignet"
                    },
                    {
                        "name": "John Klein"
                    },
                    {
                        "name": "Genevieve Robin"
                    },
                    {
                        "name": "Antoine Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Olivier"
                },
                "author": "Antoine Olivier",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20834v1",
                "updated": "2025-02-28T08:30:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    30,
                    47,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T08:30:47Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    30,
                    47,
                    4,
                    59,
                    0
                ],
                "title": "Learning to Substitute Components for Compositional Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Substitute Components for Compositional Generalization"
                },
                "summary": "Despite the rising prevalence of neural language models, recent empirical\nevidence suggests their deficiency in compositional generalization. One of the\ncurrent de-facto solutions to this problem is compositional data augmentation,\nwhich aims to introduce additional compositional inductive bias. However,\nexisting handcrafted augmentation strategies offer limited improvement when\nsystematic generalization of neural language models requires multi-grained\ncompositional bias (i.e., not limited to either lexical or structural biases\nalone) or when training sentences have an imbalanced difficulty distribution.\nTo address these challenges, we first propose a novel compositional\naugmentation strategy called Component Substitution (CompSub), which enables\nmulti-grained composition of substantial substructures across the entire\ntraining set. Furthermore, we introduce the Learning Component Substitution\n(LCS) framework. This framework empowers the learning of component substitution\nprobabilities in CompSub in an end-to-end manner by maximizing the loss of\nneural language models, thereby prioritizing challenging compositions with\nelusive concepts and novel contexts. We extend the key ideas of CompSub and LCS\nto the recently emerging in-context learning scenarios of pre-trained large\nlanguage models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot\ncompositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we\nprovide insights into why applying our algorithms to language models can\nimprove compositional generalization performance. Empirically, our results on\nfour standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and\nCOGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with\nimprovements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rising prevalence of neural language models, recent empirical\nevidence suggests their deficiency in compositional generalization. One of the\ncurrent de-facto solutions to this problem is compositional data augmentation,\nwhich aims to introduce additional compositional inductive bias. However,\nexisting handcrafted augmentation strategies offer limited improvement when\nsystematic generalization of neural language models requires multi-grained\ncompositional bias (i.e., not limited to either lexical or structural biases\nalone) or when training sentences have an imbalanced difficulty distribution.\nTo address these challenges, we first propose a novel compositional\naugmentation strategy called Component Substitution (CompSub), which enables\nmulti-grained composition of substantial substructures across the entire\ntraining set. Furthermore, we introduce the Learning Component Substitution\n(LCS) framework. This framework empowers the learning of component substitution\nprobabilities in CompSub in an end-to-end manner by maximizing the loss of\nneural language models, thereby prioritizing challenging compositions with\nelusive concepts and novel contexts. We extend the key ideas of CompSub and LCS\nto the recently emerging in-context learning scenarios of pre-trained large\nlanguage models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot\ncompositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we\nprovide insights into why applying our algorithms to language models can\nimprove compositional generalization performance. Empirically, our results on\nfour standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and\nCOGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with\nimprovements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Gangwei Jiang"
                    },
                    {
                        "name": "Chenwang Wu"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "23 pages, 9 figures, preprint, the extension paper of the paper\n  (arXiv:2306.02840)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20826v1",
                "updated": "2025-02-28T08:12:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    12,
                    23,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T08:12:23Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    12,
                    23,
                    4,
                    59,
                    0
                ],
                "title": "CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free\n  Zero-Shot Composed Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free\n  Zero-Shot Composed Image Retrieval"
                },
                "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by\nintegrating information from a composed query (reference image and modification\ntext) without training samples. Existing methods primarily combine caption\nmodels and large language models (LLMs) to generate target captions based on\ncomposed queries but face various issues such as incompatibility, visual\ninformation loss, and insufficient reasoning. In this work, we propose CoTMR, a\ntraining-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT)\nand Multi-scale Reasoning. Instead of relying on caption models for modality\ntransformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve\nunified understanding and reasoning for composed queries. To enhance the\nreasoning reliability, we devise CIRCoT, which guides the LVLM through a\nstep-by-step inference process using predefined subtasks. Considering that\nexisting approaches focus solely on global-level reasoning, our CoTMR\nincorporates multi-scale reasoning to achieve more comprehensive inference via\nfine-grained predictions about the presence or absence of key elements at the\nobject scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which\nintegrates CLIP similarity scores of the above reasoning outputs with candidate\nimages to realize precise retrieval. Extensive experiments demonstrate that our\nCoTMR not only drastically outperforms previous methods across four prominent\nbenchmarks but also offers appealing interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by\nintegrating information from a composed query (reference image and modification\ntext) without training samples. Existing methods primarily combine caption\nmodels and large language models (LLMs) to generate target captions based on\ncomposed queries but face various issues such as incompatibility, visual\ninformation loss, and insufficient reasoning. In this work, we propose CoTMR, a\ntraining-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT)\nand Multi-scale Reasoning. Instead of relying on caption models for modality\ntransformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve\nunified understanding and reasoning for composed queries. To enhance the\nreasoning reliability, we devise CIRCoT, which guides the LVLM through a\nstep-by-step inference process using predefined subtasks. Considering that\nexisting approaches focus solely on global-level reasoning, our CoTMR\nincorporates multi-scale reasoning to achieve more comprehensive inference via\nfine-grained predictions about the presence or absence of key elements at the\nobject scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which\nintegrates CLIP similarity scores of the above reasoning outputs with candidate\nimages to realize precise retrieval. Extensive experiments demonstrate that our\nCoTMR not only drastically outperforms previous methods across four prominent\nbenchmarks but also offers appealing interpretability."
                },
                "authors": [
                    {
                        "name": "Zelong Sun"
                    },
                    {
                        "name": "Dong Jing"
                    },
                    {
                        "name": "Zhiwu Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwu Lu"
                },
                "author": "Zhiwu Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20122v2",
                "updated": "2025-02-28T08:12:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    12,
                    10,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-27T14:14:50Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    14,
                    50,
                    3,
                    58,
                    0
                ],
                "title": "Self-Training Elicits Concise Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Elicits Concise Reasoning in Large Language Models"
                },
                "summary": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning"
                },
                "authors": [
                    {
                        "name": "Tergel Munkhbat"
                    },
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Seo Hyun Kim"
                    },
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "23 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20825v1",
                "updated": "2025-02-28T08:12:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    12,
                    8,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T08:12:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    12,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "LADs: Leveraging LLMs for AI-Driven DevOps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADs: Leveraging LLMs for AI-Driven DevOps"
                },
                "summary": "Automating cloud configuration and deployment remains a critical challenge\ndue to evolving infrastructures, heterogeneous hardware, and fluctuating\nworkloads. Existing solutions lack adaptability and require extensive manual\ntuning, leading to inefficiencies and misconfigurations. We introduce LADs, the\nfirst LLM-driven framework designed to tackle these challenges by ensuring\nrobustness, adaptability, and efficiency in automated cloud management. Instead\nof merely applying existing techniques, LADs provides a principled approach to\nconfiguration optimization through in-depth analysis of what optimization works\nunder which conditions. By leveraging Retrieval-Augmented Generation, Few-Shot\nLearning, Chain-of-Thought, and Feedback-Based Prompt Chaining, LADs generates\naccurate configurations and learns from deployment failures to iteratively\nrefine system settings. Our findings reveal key insights into the trade-offs\nbetween performance, cost, and scalability, helping practitioners determine the\nright strategies for different deployment scenarios. For instance, we\ndemonstrate how prompt chaining-based adaptive feedback loops enhance fault\ntolerance in multi-tenant environments and how structured log analysis with\nexample shots improves configuration accuracy. Through extensive evaluations,\nLADs reduces manual effort, optimizes resource utilization, and improves system\nreliability. By open-sourcing LADs, we aim to drive further innovation in\nAI-powered DevOps automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating cloud configuration and deployment remains a critical challenge\ndue to evolving infrastructures, heterogeneous hardware, and fluctuating\nworkloads. Existing solutions lack adaptability and require extensive manual\ntuning, leading to inefficiencies and misconfigurations. We introduce LADs, the\nfirst LLM-driven framework designed to tackle these challenges by ensuring\nrobustness, adaptability, and efficiency in automated cloud management. Instead\nof merely applying existing techniques, LADs provides a principled approach to\nconfiguration optimization through in-depth analysis of what optimization works\nunder which conditions. By leveraging Retrieval-Augmented Generation, Few-Shot\nLearning, Chain-of-Thought, and Feedback-Based Prompt Chaining, LADs generates\naccurate configurations and learns from deployment failures to iteratively\nrefine system settings. Our findings reveal key insights into the trade-offs\nbetween performance, cost, and scalability, helping practitioners determine the\nright strategies for different deployment scenarios. For instance, we\ndemonstrate how prompt chaining-based adaptive feedback loops enhance fault\ntolerance in multi-tenant environments and how structured log analysis with\nexample shots improves configuration accuracy. Through extensive evaluations,\nLADs reduces manual effort, optimizes resource utilization, and improves system\nreliability. By open-sourcing LADs, we aim to drive further innovation in\nAI-powered DevOps automation."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Azal Ahmad Khan"
                    },
                    {
                        "name": "Anas Mohamed"
                    },
                    {
                        "name": "Haider Ali"
                    },
                    {
                        "name": "Suchithra Moolinti"
                    },
                    {
                        "name": "Sabaat Haroon"
                    },
                    {
                        "name": "Usman Tahir"
                    },
                    {
                        "name": "Mattia Fazzini"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "17 pages with Appendix, 8 figures, and 7 tables. This paper is\n  currently Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17749v2",
                "updated": "2025-02-28T08:06:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    6,
                    0,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-25T00:58:06Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    0,
                    58,
                    6,
                    1,
                    56,
                    0
                ],
                "title": "Detection of LLM-Paraphrased Code and Identification of the Responsible\n  LLM Using Coding Style Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of LLM-Paraphrased Code and Identification of the Responsible\n  LLM Using Coding Style Features"
                },
                "summary": "Recent progress in large language models (LLMs) for code generation has\nraised serious concerns about intellectual property protection. Malicious users\ncan exploit LLMs to produce paraphrased versions of proprietary code that\nclosely resemble the original. While the potential for LLM-assisted code\nparaphrasing continues to grow, research on detecting it remains limited,\nunderscoring an urgent need for detection system. We respond to this need by\nproposing two tasks. The first task is to detect whether code generated by an\nLLM is a paraphrased version of original human-written code. The second task is\nto identify which LLM is used to paraphrase the original code. For these tasks,\nwe construct a dataset LPcode consisting of pairs of human-written code and\nLLM-paraphrased code using various LLMs.\n  We statistically confirm significant differences in the coding styles of\nhuman-written and LLM-paraphrased code, particularly in terms of naming\nconsistency, code structure, and readability. Based on these findings, we\ndevelop LPcodedec, a detection method that identifies paraphrase relationships\nbetween human-written and LLM-generated code, and discover which LLM is used\nfor the paraphrasing. LPcodedec outperforms the best baselines in two tasks,\nimproving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and\n213x, respectively. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) for code generation has\nraised serious concerns about intellectual property protection. Malicious users\ncan exploit LLMs to produce paraphrased versions of proprietary code that\nclosely resemble the original. While the potential for LLM-assisted code\nparaphrasing continues to grow, research on detecting it remains limited,\nunderscoring an urgent need for detection system. We respond to this need by\nproposing two tasks. The first task is to detect whether code generated by an\nLLM is a paraphrased version of original human-written code. The second task is\nto identify which LLM is used to paraphrase the original code. For these tasks,\nwe construct a dataset LPcode consisting of pairs of human-written code and\nLLM-paraphrased code using various LLMs.\n  We statistically confirm significant differences in the coding styles of\nhuman-written and LLM-paraphrased code, particularly in terms of naming\nconsistency, code structure, and readability. Based on these findings, we\ndevelop LPcodedec, a detection method that identifies paraphrase relationships\nbetween human-written and LLM-generated code, and discover which LLM is used\nfor the paraphrasing. LPcodedec outperforms the best baselines in two tasks,\nimproving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and\n213x, respectively. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features."
                },
                "authors": [
                    {
                        "name": "Shinwoo Park"
                    },
                    {
                        "name": "Hyundong Jin"
                    },
                    {
                        "name": "Jeong-won Cha"
                    },
                    {
                        "name": "Yo-Sub Han"
                    }
                ],
                "author_detail": {
                    "name": "Yo-Sub Han"
                },
                "author": "Yo-Sub Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20817v1",
                "updated": "2025-02-28T08:03:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    3,
                    36,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T08:03:36Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    3,
                    36,
                    4,
                    59,
                    0
                ],
                "title": "Learning-Based Leader Localization for Underwater Vehicles With\n  Optical-Acoustic-Pressure Sensor Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Based Leader Localization for Underwater Vehicles With\n  Optical-Acoustic-Pressure Sensor Fusion"
                },
                "summary": "Underwater vehicles have emerged as a critical technology for exploring and\nmonitoring aquatic environments. The deployment of multi-vehicle systems has\ngained substantial interest due to their capability to perform collaborative\ntasks with improved efficiency. However, achieving precise localization of a\nleader underwater vehicle within a multi-vehicle configuration remains a\nsignificant challenge, particularly in dynamic and complex underwater\nconditions. To address this issue, this paper presents a novel tri-modal sensor\nfusion neural network approach that integrates optical, acoustic, and pressure\nsensors to localize the leader vehicle. The proposed method leverages the\nunique strengths of each sensor modality to improve localization accuracy and\nrobustness. Specifically, optical sensors provide high-resolution imaging for\nprecise relative positioning, acoustic sensors enable long-range detection and\nranging, and pressure sensors offer environmental context awareness. The fusion\nof these sensor modalities is implemented using a deep learning architecture\ndesigned to extract and combine complementary features from raw sensor data.\nThe effectiveness of the proposed method is validated through a custom-designed\ntesting platform. Extensive data collection and experimental evaluations\ndemonstrate that the tri-modal approach significantly improves the accuracy and\nrobustness of leader localization, outperforming both single-modal and\ndual-modal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater vehicles have emerged as a critical technology for exploring and\nmonitoring aquatic environments. The deployment of multi-vehicle systems has\ngained substantial interest due to their capability to perform collaborative\ntasks with improved efficiency. However, achieving precise localization of a\nleader underwater vehicle within a multi-vehicle configuration remains a\nsignificant challenge, particularly in dynamic and complex underwater\nconditions. To address this issue, this paper presents a novel tri-modal sensor\nfusion neural network approach that integrates optical, acoustic, and pressure\nsensors to localize the leader vehicle. The proposed method leverages the\nunique strengths of each sensor modality to improve localization accuracy and\nrobustness. Specifically, optical sensors provide high-resolution imaging for\nprecise relative positioning, acoustic sensors enable long-range detection and\nranging, and pressure sensors offer environmental context awareness. The fusion\nof these sensor modalities is implemented using a deep learning architecture\ndesigned to extract and combine complementary features from raw sensor data.\nThe effectiveness of the proposed method is validated through a custom-designed\ntesting platform. Extensive data collection and experimental evaluations\ndemonstrate that the tri-modal approach significantly improves the accuracy and\nrobustness of leader localization, outperforming both single-modal and\ndual-modal methods."
                },
                "authors": [
                    {
                        "name": "Mingyang Yang"
                    },
                    {
                        "name": "Zeyu Sha"
                    },
                    {
                        "name": "Feitian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feitian Zhang"
                },
                "author": "Feitian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09870v3",
                "updated": "2025-02-28T08:02:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    2,
                    31,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-13T15:08:49Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    15,
                    8,
                    49,
                    6,
                    287,
                    0
                ],
                "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains"
                },
                "summary": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply our\nChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by\ntraversing step-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply our\nChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by\ntraversing step-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats."
                },
                "authors": [
                    {
                        "name": "Yein Park"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Donghyeon Lee"
                    },
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "arxiv_comment": "ICLR 2025, 40 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10812v2",
                "updated": "2025-02-28T08:01:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    1,
                    48,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-15T14:34:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    14,
                    34,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "ResiComp: Loss-Resilient Image Compression via Dual-Functional Masked\n  Visual Token Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResiComp: Loss-Resilient Image Compression via Dual-Functional Masked\n  Visual Token Modeling"
                },
                "summary": "Recent advancements in neural image codecs (NICs) are of significant\ncompression performance, but limited attention has been paid to their error\nresilience.\n  These resulting NICs tend to be sensitive to packet losses, which are\nprevalent in real-time communications.\n  In this paper, we investigate how to elevate the resilience ability of NICs\nto combat packet losses.\n  We propose ResiComp, a pioneering neural image compression framework with\nfeature-domain packet loss concealment (PLC).\n  Motivated by the inherent consistency between generation and compression, we\nadvocate merging the tasks of entropy modeling and PLC into a unified framework\nfocused on latent space context modeling.\n  To this end, we take inspiration from the impressive generative capabilities\nof large language models (LLMs), particularly the recent advances of masked\nvisual token modeling (MVTM).\n  During training, we integrate MVTM to mirror the effects of packet loss,\nenabling a dual-functional Transformer to restore the masked latents by\npredicting their missing values and conditional probability mass functions.\n  Our ResiComp jointly optimizes compression efficiency and loss resilience.\n  Moreover, ResiComp provides flexible coding modes, allowing for explicitly\nadjusting the efficiency-resilience trade-off in response to varying Internet\nor wireless network conditions.\n  Extensive experiments demonstrate that ResiComp can significantly enhance the\nNIC's resilience against packet losses, while exhibits a worthy trade-off\nbetween compression efficiency and packet loss resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in neural image codecs (NICs) are of significant\ncompression performance, but limited attention has been paid to their error\nresilience.\n  These resulting NICs tend to be sensitive to packet losses, which are\nprevalent in real-time communications.\n  In this paper, we investigate how to elevate the resilience ability of NICs\nto combat packet losses.\n  We propose ResiComp, a pioneering neural image compression framework with\nfeature-domain packet loss concealment (PLC).\n  Motivated by the inherent consistency between generation and compression, we\nadvocate merging the tasks of entropy modeling and PLC into a unified framework\nfocused on latent space context modeling.\n  To this end, we take inspiration from the impressive generative capabilities\nof large language models (LLMs), particularly the recent advances of masked\nvisual token modeling (MVTM).\n  During training, we integrate MVTM to mirror the effects of packet loss,\nenabling a dual-functional Transformer to restore the masked latents by\npredicting their missing values and conditional probability mass functions.\n  Our ResiComp jointly optimizes compression efficiency and loss resilience.\n  Moreover, ResiComp provides flexible coding modes, allowing for explicitly\nadjusting the efficiency-resilience trade-off in response to varying Internet\nor wireless network conditions.\n  Extensive experiments demonstrate that ResiComp can significantly enhance the\nNIC's resilience against packet losses, while exhibits a worthy trade-off\nbetween compression efficiency and packet loss resilience."
                },
                "authors": [
                    {
                        "name": "Sixian Wang"
                    },
                    {
                        "name": "Jincheng Dai"
                    },
                    {
                        "name": "Xiaoqi Qin"
                    },
                    {
                        "name": "Ke Yang"
                    },
                    {
                        "name": "Kai Niu"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "arxiv_comment": "Accepted by IEEE TCSVT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09542v2",
                "updated": "2025-02-28T08:01:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    8,
                    1,
                    32,
                    4,
                    59,
                    0
                ],
                "published": "2024-10-12T14:12:36Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    14,
                    12,
                    36,
                    5,
                    286,
                    0
                ],
                "title": "MIRAGE: Evaluating and Explaining Inductive Reasoning Process in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Evaluating and Explaining Inductive Reasoning Process in\n  Language Models"
                },
                "summary": "Inductive reasoning is an essential capability for large language models\n(LLMs) to achieve higher intelligence, which requires the model to generalize\nrules from observed facts and then apply them to unseen examples. We present\nMIRAGE, a synthetic dataset that addresses the limitations of previous work,\nspecifically the lack of comprehensive evaluation and flexible test data. In\nit, we evaluate LLMs' capabilities in both the inductive and deductive stages,\nallowing for flexible variation in input distribution, task scenario, and task\ndifficulty to analyze the factors influencing LLMs' inductive reasoning. Based\non these multi-faceted evaluations, we demonstrate that the LLM is a poor\nrule-based reasoner. In many cases, when conducting inductive reasoning, they\ndo not rely on a correct rule to answer the unseen case. From the perspectives\nof different prompting methods, observation numbers, and task forms, models\ntend to consistently conduct correct deduction without correct inductive rules.\nBesides, we find that LLMs are good neighbor-based reasoners. In the inductive\nreasoning process, the model tends to focus on observed facts that are close to\nthe current test example in feature space. By leveraging these similar\nexamples, the model maintains strong inductive capabilities within a localized\nregion, significantly improving its deductive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive reasoning is an essential capability for large language models\n(LLMs) to achieve higher intelligence, which requires the model to generalize\nrules from observed facts and then apply them to unseen examples. We present\nMIRAGE, a synthetic dataset that addresses the limitations of previous work,\nspecifically the lack of comprehensive evaluation and flexible test data. In\nit, we evaluate LLMs' capabilities in both the inductive and deductive stages,\nallowing for flexible variation in input distribution, task scenario, and task\ndifficulty to analyze the factors influencing LLMs' inductive reasoning. Based\non these multi-faceted evaluations, we demonstrate that the LLM is a poor\nrule-based reasoner. In many cases, when conducting inductive reasoning, they\ndo not rely on a correct rule to answer the unseen case. From the perspectives\nof different prompting methods, observation numbers, and task forms, models\ntend to consistently conduct correct deduction without correct inductive rules.\nBesides, we find that LLMs are good neighbor-based reasoners. In the inductive\nreasoning process, the model tends to focus on observed facts that are close to\nthe current test example in feature space. By leveraging these similar\nexamples, the model maintains strong inductive capabilities within a localized\nregion, significantly improving its deductive performance."
                },
                "authors": [
                    {
                        "name": "Jiachun Li"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted as ICLR 2025 conference paper (26 pages, 16 tables, 9\n  figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06287v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06287v3",
                "updated": "2025-02-28T07:54:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    54,
                    16,
                    4,
                    59,
                    0
                ],
                "published": "2024-12-09T08:19:28Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    8,
                    19,
                    28,
                    0,
                    344,
                    0
                ],
                "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models"
                },
                "summary": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,117 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,117 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench."
                },
                "authors": [
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Panfeng Chen"
                    },
                    {
                        "name": "Jiali Li"
                    },
                    {
                        "name": "Linkun Feng"
                    },
                    {
                        "name": "Shuyu Liu"
                    },
                    {
                        "name": "Heng Zhao"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Yanhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanhao Wang"
                },
                "author": "Yanhao Wang",
                "arxiv_comment": "21 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06287v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06287v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15089v2",
                "updated": "2025-02-28T07:53:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    53,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-25T05:32:14Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    5,
                    32,
                    14,
                    5,
                    25,
                    0
                ],
                "title": "LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps://huggingface.co/datasets/lz1bytedance/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps://huggingface.co/datasets/lz1bytedance/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Zhan Ling"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Kai Yan"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Weijian Lin"
                    },
                    {
                        "name": "Ting-Han Fan"
                    },
                    {
                        "name": "Lingfeng Shen"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]