[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v1",
                "updated": "2025-04-29T00:58:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v1",
                "updated": "2025-04-28T09:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camgöz"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.20997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20997v1",
                "updated": "2025-04-29T17:59:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    48,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:59:48Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    48,
                    1,
                    119,
                    0
                ],
                "title": "Toward Efficient Exploration by Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient Exploration by Large Language Model Agents"
                },
                "summary": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration."
                },
                "authors": [
                    {
                        "name": "Dilip Arumugam"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20996v1",
                "updated": "2025-04-29T17:59:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    45,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:59:45Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    45,
                    1,
                    119,
                    0
                ],
                "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Fusion: Introducing New Modality to Frozen Large Language Models"
                },
                "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels."
                },
                "authors": [
                    {
                        "name": "Sicheng Mo"
                    },
                    {
                        "name": "Thao Nguyen"
                    },
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Siddharth Srinivasan Iyer"
                    },
                    {
                        "name": "Yijun Li"
                    },
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Abhishek Tandon"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Bolei Zhou"
                    },
                    {
                        "name": "Yuheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Li"
                },
                "author": "Yuheng Li",
                "arxiv_comment": "Project Page: https://sichengmo.github.io/XFusion/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20984v1",
                "updated": "2025-04-29T17:55:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    55,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    55,
                    52,
                    1,
                    119,
                    0
                ],
                "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE: A Security Architecture for LLM-Integrated App Systems"
                },
                "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."
                },
                "authors": [
                    {
                        "name": "Evan Li"
                    },
                    {
                        "name": "Tushin Mallick"
                    },
                    {
                        "name": "Evan Rose"
                    },
                    {
                        "name": "William Robertson"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Nita-Rotaru"
                },
                "author": "Cristina Nita-Rotaru",
                "arxiv_comment": "21 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20980v1",
                "updated": "2025-04-29T17:50:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    50,
                    29,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:50:29Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    50,
                    29,
                    1,
                    119,
                    0
                ],
                "title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jekyll-and-Hyde Tipping Point in an AI's Behavior"
                },
                "summary": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''"
                },
                "authors": [
                    {
                        "name": "Neil F. Johnson"
                    },
                    {
                        "name": "Frank Yingjie Huo"
                    }
                ],
                "author_detail": {
                    "name": "Frank Yingjie Huo"
                },
                "author": "Frank Yingjie Huo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20976v1",
                "updated": "2025-04-29T17:45:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    45,
                    4,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:45:04Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    45,
                    4,
                    1,
                    119,
                    0
                ],
                "title": "Real-Time Wayfinding Assistant for Blind and Low-Vision Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Wayfinding Assistant for Blind and Low-Vision Users"
                },
                "summary": "Navigating unfamiliar places continues to be one of the most persistent and\nessential everyday obstacles for those who are blind or have limited vision\n(BLV). Existing assistive technologies, such as GPS-based navigation systems,\nAI-powered smart glasses, and sonar-equipped canes, often face limitations in\nreal-time obstacle avoidance, precise localization, and adaptability to dynamic\nsurroundings. To investigate potential solutions, we introduced PathFinder, a\nnovel map-less navigation system that explores different models for\nunderstanding 2D images, including Vision Language Models (VLMs), Large\nLanguage Models (LLMs), and employs monocular depth estimation for free-path\ndetection. Our approach integrates a Depth-First Search (DFS) algorithm on\ndepth images to determine the longest obstacle-free path, ensuring optimal\nroute selection while maintaining computational efficiency. We conducted\ncomparative evaluations against existing AI-powered navigation methods and\nperformed a usability study with BLV participants. The results demonstrate that\nPathFinder achieves a favorable balance between accuracy, computational\nefficiency, and real-time responsiveness. Notably, it reduces mean absolute\nerror (MAE) and improves decision-making speed in outdoor navigation compared\nto AI-based alternatives. Participant feedback emphasizes the system's\nusability and effectiveness in outside situations, but also identifies issues\nin complicated indoor locations and low-light conditions. Usability testing\nrevealed that 73% of participants understood how to use the app in about a\nminute, and 80% praised its balance of accuracy, quick response, and overall\nconvenience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating unfamiliar places continues to be one of the most persistent and\nessential everyday obstacles for those who are blind or have limited vision\n(BLV). Existing assistive technologies, such as GPS-based navigation systems,\nAI-powered smart glasses, and sonar-equipped canes, often face limitations in\nreal-time obstacle avoidance, precise localization, and adaptability to dynamic\nsurroundings. To investigate potential solutions, we introduced PathFinder, a\nnovel map-less navigation system that explores different models for\nunderstanding 2D images, including Vision Language Models (VLMs), Large\nLanguage Models (LLMs), and employs monocular depth estimation for free-path\ndetection. Our approach integrates a Depth-First Search (DFS) algorithm on\ndepth images to determine the longest obstacle-free path, ensuring optimal\nroute selection while maintaining computational efficiency. We conducted\ncomparative evaluations against existing AI-powered navigation methods and\nperformed a usability study with BLV participants. The results demonstrate that\nPathFinder achieves a favorable balance between accuracy, computational\nefficiency, and real-time responsiveness. Notably, it reduces mean absolute\nerror (MAE) and improves decision-making speed in outdoor navigation compared\nto AI-based alternatives. Participant feedback emphasizes the system's\nusability and effectiveness in outside situations, but also identifies issues\nin complicated indoor locations and low-light conditions. Usability testing\nrevealed that 73% of participants understood how to use the app in about a\nminute, and 80% praised its balance of accuracy, quick response, and overall\nconvenience."
                },
                "authors": [
                    {
                        "name": "Dabbrata Das"
                    },
                    {
                        "name": "Argho Deb Das"
                    },
                    {
                        "name": "Farhan Sadaf"
                    }
                ],
                "author_detail": {
                    "name": "Farhan Sadaf"
                },
                "author": "Farhan Sadaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04992v2",
                "updated": "2025-04-29T17:42:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    42,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-06T21:42:35Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    42,
                    35,
                    3,
                    65,
                    0
                ],
                "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wanda++: Pruning Large Language Models via Regional Gradients"
                },
                "summary": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Kai Zhen"
                    },
                    {
                        "name": "Bhavana Ganesh"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Goeric Huybrechts"
                    },
                    {
                        "name": "Markus Müller"
                    },
                    {
                        "name": "Jonas M. Kübler"
                    },
                    {
                        "name": "Rupak Vignesh Swaminathan"
                    },
                    {
                        "name": "Athanasios Mouchtaris"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    },
                    {
                        "name": "Nathan Susanj"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Jack FitzGerald"
                    },
                    {
                        "name": "Abhishek Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Kumar"
                },
                "author": "Abhishek Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20972v1",
                "updated": "2025-04-29T17:40:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    40,
                    29,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:40:29Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    40,
                    29,
                    1,
                    119,
                    0
                ],
                "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SetKE: Knowledge Editing for Knowledge Elements Overlap"
                },
                "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Yifan Wei"
                    },
                    {
                        "name": "Xiaoyan Yu"
                    },
                    {
                        "name": "Ran Song"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Angsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Angsheng Li"
                },
                "author": "Angsheng Li",
                "arxiv_comment": "The CR version will be updated subsequently",
                "arxiv_journal_ref": "IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20965v1",
                "updated": "2025-04-29T17:36:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    36,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:36:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    36,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM\n  Security"
                },
                "summary": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm"
                },
                "authors": [
                    {
                        "name": "Zikui Cai"
                    },
                    {
                        "name": "Shayan Shabihi"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Zora Che"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "ICLR 2025 Workshop BuildingTrust",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20964v1",
                "updated": "2025-04-29T17:34:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    34,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:34:49Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    34,
                    49,
                    1,
                    119,
                    0
                ],
                "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for\n  Operating System Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for\n  Operating System Verification"
                },
                "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench."
                },
                "authors": [
                    {
                        "name": "Shangyu Li"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Jiasi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jiasi Shen"
                },
                "author": "Jiasi Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.11627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.11627v2",
                "updated": "2025-04-29T17:31:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    31,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2022-03-22T11:26:18Z",
                "published_parsed": [
                    2022,
                    3,
                    22,
                    11,
                    26,
                    18,
                    1,
                    81,
                    0
                ],
                "title": "Centered plug-in estimation of Wasserstein distances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centered plug-in estimation of Wasserstein distances"
                },
                "summary": "The plug-in estimator of the squared Euclidean 2-Wasserstein distance is\nconservative, however due to its large positive bias it is often uninformative.\nWe eliminate most of this bias using a simple centering procedure based on\nlinear combinations. We construct a pair of centered plug-in estimators that\ndecrease with the true Wasserstein distance, and are therefore guaranteed to be\ninformative, for any finite sample size. Crucially, we demonstrate that these\nestimators can often be viewed as complementary upper and lower bounds on the\nsquared Wasserstein distance. Finally, we apply the estimators to Bayesian\ncomputation, developing methods for estimating (i) the bias of approximate\ninference methods and (ii) the convergence of MCMC algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The plug-in estimator of the squared Euclidean 2-Wasserstein distance is\nconservative, however due to its large positive bias it is often uninformative.\nWe eliminate most of this bias using a simple centering procedure based on\nlinear combinations. We construct a pair of centered plug-in estimators that\ndecrease with the true Wasserstein distance, and are therefore guaranteed to be\ninformative, for any finite sample size. Crucially, we demonstrate that these\nestimators can often be viewed as complementary upper and lower bounds on the\nsquared Wasserstein distance. Finally, we apply the estimators to Bayesian\ncomputation, developing methods for estimating (i) the bias of approximate\ninference methods and (ii) the convergence of MCMC algorithms."
                },
                "authors": [
                    {
                        "name": "Tamás P. Papp"
                    },
                    {
                        "name": "Chris Sherlock"
                    }
                ],
                "author_detail": {
                    "name": "Chris Sherlock"
                },
                "author": "Chris Sherlock",
                "arxiv_comment": "major revision; 41 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.11627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.11627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19521v2",
                "updated": "2025-04-29T17:22:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    22,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T06:40:01Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    1,
                    0,
                    118,
                    0
                ],
                "title": "Security Steerability is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Steerability is All You Need"
                },
                "summary": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it. Thus, in this work we adopt an application-centric\napproach to GenAI security, and show that while LLMs cannot protect against\nad-hoc application specific threats, they can provide the framework for\napplications to protect themselves against such threats. Our first contribution\nis defining Security Steerability - a novel security measure for LLMs,\nassessing the model's capability to adhere to strict guardrails that are\ndefined in the system prompt ('Refrain from discussing about politics'). These\nguardrails, in case effective, can stop threats in the presence of malicious\nusers who attempt to circumvent the application and cause harm to its\nproviders. Our second contribution is a methodology to measure the security\nsteerability of LLMs, utilizing two newly-developed datasets: VeganRibs\nassesses the LLM behavior in forcing specific guardrails that are not security\nper se in the presence of malicious user that uses attack boosters (jailbreaks\nand perturbations), and ReverseText takes this approach further and measures\nthe LLM ability to force specific treatment of the user input as plain text\nwhile do user try to give it additional meanings...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it. Thus, in this work we adopt an application-centric\napproach to GenAI security, and show that while LLMs cannot protect against\nad-hoc application specific threats, they can provide the framework for\napplications to protect themselves against such threats. Our first contribution\nis defining Security Steerability - a novel security measure for LLMs,\nassessing the model's capability to adhere to strict guardrails that are\ndefined in the system prompt ('Refrain from discussing about politics'). These\nguardrails, in case effective, can stop threats in the presence of malicious\nusers who attempt to circumvent the application and cause harm to its\nproviders. Our second contribution is a methodology to measure the security\nsteerability of LLMs, utilizing two newly-developed datasets: VeganRibs\nassesses the LLM behavior in forcing specific guardrails that are not security\nper se in the presence of malicious user that uses attack boosters (jailbreaks\nand perturbations), and ReverseText takes this approach further and measures\nthe LLM ability to force specific treatment of the user input as plain text\nwhile do user try to give it additional meanings..."
                },
                "authors": [
                    {
                        "name": "Itay Hazan"
                    },
                    {
                        "name": "Idan Habler"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Itsik Mantin"
                    }
                ],
                "author_detail": {
                    "name": "Itsik Mantin"
                },
                "author": "Itsik Mantin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20951v1",
                "updated": "2025-04-29T17:21:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    21,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:21:20Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    21,
                    20,
                    1,
                    119,
                    0
                ],
                "title": "Information Gravity: A Field-Theoretic Model for Token Selection in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Gravity: A Field-Theoretic Model for Token Selection in\n  Large Language Models"
                },
                "summary": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity."
                },
                "authors": [
                    {
                        "name": "Maryna Vyshnyvetska"
                    }
                ],
                "author_detail": {
                    "name": "Maryna Vyshnyvetska"
                },
                "author": "Maryna Vyshnyvetska",
                "arxiv_doi": "10.5281/zenodo.15289890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.15289890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20946v1",
                "updated": "2025-04-29T17:14:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    14,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:14:54Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    14,
                    54,
                    1,
                    119,
                    0
                ],
                "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning\n  Distillation From Large to Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning\n  Distillation From Large to Small Language Models"
                },
                "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications."
                },
                "authors": [
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12836v2",
                "updated": "2025-04-29T17:14:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    14,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-02-18T13:09:59Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    9,
                    59,
                    1,
                    49,
                    0
                ],
                "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation"
                },
                "summary": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Mohammad Feli"
                    },
                    {
                        "name": "Iman Azimi"
                    },
                    {
                        "name": "Pasi Liljeberg"
                    },
                    {
                        "name": "Amir M. Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir M. Rahmani"
                },
                "author": "Amir M. Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08727v2",
                "updated": "2025-04-29T17:11:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    11,
                    44,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-11T01:07:57Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    1,
                    7,
                    57,
                    1,
                    70,
                    0
                ],
                "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation"
                },
                "summary": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG."
                },
                "authors": [
                    {
                        "name": "Lucas Caccia"
                    },
                    {
                        "name": "Alan Ansell"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Ivan Vulić"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sordoni"
                },
                "author": "Alessandro Sordoni",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20930v1",
                "updated": "2025-04-29T16:48:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    48,
                    23,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:48:23Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    48,
                    23,
                    1,
                    119,
                    0
                ],
                "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\n  through Step-by-Step Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\n  through Step-by-Step Verification"
                },
                "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs."
                },
                "authors": [
                    {
                        "name": "Ziqing Fan"
                    },
                    {
                        "name": "Cheng Liang"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20922v1",
                "updated": "2025-04-29T16:38:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    38,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:38:15Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    38,
                    15,
                    1,
                    119,
                    0
                ],
                "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based\n  architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DYNAMAX: Dynamic computing for Transformers and Mamba based\n  architectures"
                },
                "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs."
                },
                "authors": [
                    {
                        "name": "Miguel Nogales"
                    },
                    {
                        "name": "Matteo Gambella"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary), 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20914v1",
                "updated": "2025-04-29T16:33:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    33,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:33:54Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    33,
                    54,
                    1,
                    119,
                    0
                ],
                "title": "Relativistic ejecta from stellar mass black holes: insights from\n  simulations and synthetic radio images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relativistic ejecta from stellar mass black holes: insights from\n  simulations and synthetic radio images"
                },
                "summary": "We present numerical simulations of discrete relativistic ejecta from an\nX-ray binary (XRB) with initial conditions directly informed by observations.\nXRBs have been observed to launch powerful discrete plasma ejecta during state\ntransitions, which can propagate up to parsec distances. Understanding these\nejection events unveils new understanding of jet-launching, jet power, and\njet-ISM interaction among other implications. Multi-frequency\nquasi-simultaneous radio observations of ejecta from the black hole XRB MAXI\nJ1820+070 produced both size and calorimetry constraints, which we use as\ninitial conditions of a relativistic hydrodynamic simulation. We qualitatively\nreproduce the observed deceleration of the ejecta in a homogeneous interstellar\nmedium (ISM). Our simulations demonstrate that the ejecta must be denser than\nthe ISM, the ISM be significantly low-density, and the launch be extremely\npowerful, in order to propagate to the observed distances. The blob propagates\nand clears out a high-pressure low-density cavity in its wake, providing an\nexplanation for this pre-existing low-density environment, as well as\n'bubble-like' environments in the vicinity of XRBs inferred from other studies.\nAs the blob decelerates, we observe the onset of instabilities and a long-lived\nreverse shock -- these mechanisms convert kinetic to internal energy in the\nblob, responsible for in-situ particle acceleration. We transform the outputs\nof our simulation into pseudo-radio images, incorporating the u,v coverage of\nthe MeerKAT and e-MERLIN telescopes from the original observations with\nreal-sky background. Through this, we maximize the interpretability of the\nresults and provide direct comparison to current data, as well as provide\nprediction capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present numerical simulations of discrete relativistic ejecta from an\nX-ray binary (XRB) with initial conditions directly informed by observations.\nXRBs have been observed to launch powerful discrete plasma ejecta during state\ntransitions, which can propagate up to parsec distances. Understanding these\nejection events unveils new understanding of jet-launching, jet power, and\njet-ISM interaction among other implications. Multi-frequency\nquasi-simultaneous radio observations of ejecta from the black hole XRB MAXI\nJ1820+070 produced both size and calorimetry constraints, which we use as\ninitial conditions of a relativistic hydrodynamic simulation. We qualitatively\nreproduce the observed deceleration of the ejecta in a homogeneous interstellar\nmedium (ISM). Our simulations demonstrate that the ejecta must be denser than\nthe ISM, the ISM be significantly low-density, and the launch be extremely\npowerful, in order to propagate to the observed distances. The blob propagates\nand clears out a high-pressure low-density cavity in its wake, providing an\nexplanation for this pre-existing low-density environment, as well as\n'bubble-like' environments in the vicinity of XRBs inferred from other studies.\nAs the blob decelerates, we observe the onset of instabilities and a long-lived\nreverse shock -- these mechanisms convert kinetic to internal energy in the\nblob, responsible for in-situ particle acceleration. We transform the outputs\nof our simulation into pseudo-radio images, incorporating the u,v coverage of\nthe MeerKAT and e-MERLIN telescopes from the original observations with\nreal-sky background. Through this, we maximize the interpretability of the\nresults and provide direct comparison to current data, as well as provide\nprediction capabilities."
                },
                "authors": [
                    {
                        "name": "Katie Savard"
                    },
                    {
                        "name": "James H. Matthews"
                    },
                    {
                        "name": "Rob Fender"
                    },
                    {
                        "name": "Ian Heywood"
                    }
                ],
                "author_detail": {
                    "name": "Ian Heywood"
                },
                "author": "Ian Heywood",
                "arxiv_comment": "Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20911v1",
                "updated": "2025-04-29T16:29:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    29,
                    12,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:29:12Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    29,
                    12,
                    1,
                    119,
                    0
                ],
                "title": "An Empirical Study on the Capability of LLMs in Decomposing Bug Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Capability of LLMs in Decomposing Bug Reports"
                },
                "summary": "Background: Bug reports are essential to the software development life cycle.\nThey help developers track and resolve issues, but are often difficult to\nprocess due to their complexity, which can delay resolution and affect software\nquality. Aims: This study investigates whether large language models (LLMs) can\nassist developers in automatically decomposing complex bug reports into\nsmaller, self-contained units, making them easier to understand and address.\nMethod: We conducted an empirical study on 127 resolved privacy-related bug\nreports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using\ndifferent prompting strategies. We first tested both LLMs with zero-shot\nprompts, then applied improved prompts with demonstrations (using few-shot\nprompting) to measure their abilities in bug decomposition. Results: Our\nfindings show that LLMs are capable of decomposing bug reports, but their\noverall performance still requires further improvement and strongly depends on\nthe quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT\nand DeepSeek) performed poorly. After prompt tuning, ChatGPT's true\ndecomposition rate increased by 140\\% and DeepSeek's by 163.64\\%. Conclusions:\nLLMs show potential in helping developers analyze and decompose complex bug\nreports, but they still need improvement in terms of accuracy and bug\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Bug reports are essential to the software development life cycle.\nThey help developers track and resolve issues, but are often difficult to\nprocess due to their complexity, which can delay resolution and affect software\nquality. Aims: This study investigates whether large language models (LLMs) can\nassist developers in automatically decomposing complex bug reports into\nsmaller, self-contained units, making them easier to understand and address.\nMethod: We conducted an empirical study on 127 resolved privacy-related bug\nreports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using\ndifferent prompting strategies. We first tested both LLMs with zero-shot\nprompts, then applied improved prompts with demonstrations (using few-shot\nprompting) to measure their abilities in bug decomposition. Results: Our\nfindings show that LLMs are capable of decomposing bug reports, but their\noverall performance still requires further improvement and strongly depends on\nthe quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT\nand DeepSeek) performed poorly. After prompt tuning, ChatGPT's true\ndecomposition rate increased by 140\\% and DeepSeek's by 163.64\\%. Conclusions:\nLLMs show potential in helping developers analyze and decompose complex bug\nreports, but they still need improvement in terms of accuracy and bug\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Vanessa Nava-Camal"
                    },
                    {
                        "name": "Ahmad Suleiman"
                    },
                    {
                        "name": "Yiming Tang"
                    },
                    {
                        "name": "Daqing Hou"
                    },
                    {
                        "name": "Weiyi Shang"
                    }
                ],
                "author_detail": {
                    "name": "Weiyi Shang"
                },
                "author": "Weiyi Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20896v1",
                "updated": "2025-04-29T16:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    13,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:13:49Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    13,
                    49,
                    1,
                    119,
                    0
                ],
                "title": "LELANTE: LEveraging LLM for Automated ANdroid TEsting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LELANTE: LEveraging LLM for Automated ANdroid TEsting"
                },
                "summary": "Given natural language test case description for an Android application,\nexisting testing approaches require developers to manually write scripts using\ntools such as Appium and Espresso to execute the corresponding test case. This\nprocess is labor-intensive and demands significant effort to maintain as UI\ninterfaces evolve throughout development. In this work, we introduce LELANTE, a\nnovel framework that utilizes large language models (LLMs) to automate test\ncase execution without requiring pre-written scripts. LELANTE interprets\nnatural language test case descriptions, iteratively generate action plans, and\nperform the actions directly on the Android screen using its GUI. LELANTE\nemploys a screen refinement process to enhance LLM interpretability, constructs\na structured prompt for LLMs, and implements an action generation mechanism\nbased on chain-of-thought reasoning of LLMs. To further reduce computational\ncost and enhance scalability, LELANTE utilizes model distillation using a\nfoundational LLM. In experiments across 390 test cases spanning 10 popular\nAndroid applications, LELANTE achieved a 73% test execution success rate. Our\nresults demonstrate that LLMs can effectively bridge the gap between natural\nlanguage test case description and automated execution, making mobile testing\nmore scalable and adaptable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given natural language test case description for an Android application,\nexisting testing approaches require developers to manually write scripts using\ntools such as Appium and Espresso to execute the corresponding test case. This\nprocess is labor-intensive and demands significant effort to maintain as UI\ninterfaces evolve throughout development. In this work, we introduce LELANTE, a\nnovel framework that utilizes large language models (LLMs) to automate test\ncase execution without requiring pre-written scripts. LELANTE interprets\nnatural language test case descriptions, iteratively generate action plans, and\nperform the actions directly on the Android screen using its GUI. LELANTE\nemploys a screen refinement process to enhance LLM interpretability, constructs\na structured prompt for LLMs, and implements an action generation mechanism\nbased on chain-of-thought reasoning of LLMs. To further reduce computational\ncost and enhance scalability, LELANTE utilizes model distillation using a\nfoundational LLM. In experiments across 390 test cases spanning 10 popular\nAndroid applications, LELANTE achieved a 73% test execution success rate. Our\nresults demonstrate that LLMs can effectively bridge the gap between natural\nlanguage test case description and automated execution, making mobile testing\nmore scalable and adaptable."
                },
                "authors": [
                    {
                        "name": "Shamit Fatin"
                    },
                    {
                        "name": "Mehbubul Hasan Al-Quvi"
                    },
                    {
                        "name": "Haz Sameen Shahgir"
                    },
                    {
                        "name": "Sukarna Barua"
                    },
                    {
                        "name": "Anindya Iqbal"
                    },
                    {
                        "name": "Sadia Sharmin"
                    },
                    {
                        "name": "Md. Mostofa Akbar"
                    },
                    {
                        "name": "Kallol Kumar Pal"
                    },
                    {
                        "name": "A. Asif Al Rashid"
                    }
                ],
                "author_detail": {
                    "name": "A. Asif Al Rashid"
                },
                "author": "A. Asif Al Rashid",
                "arxiv_comment": "6 pages, 4 figures, 29th International Conference on Evaluation and\n  Assessment in Software Engineering (EASE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04907v2",
                "updated": "2025-04-29T15:56:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    56,
                    46,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-07T10:32:42Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    32,
                    42,
                    0,
                    97,
                    0
                ],
                "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-Bench: Human-Aligned Video Generation Benchmark"
                },
                "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment."
                },
                "authors": [
                    {
                        "name": "Hui Han"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yiwen Yuan"
                    },
                    {
                        "name": "Yuling Wu"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Junchen Fu"
                    },
                    {
                        "name": "Youhua Li"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Li-jia Li"
                    },
                    {
                        "name": "Yongxin Ni"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Ni"
                },
                "author": "Yongxin Ni",
                "arxiv_comment": "Accepted by CVPR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23236v2",
                "updated": "2025-04-29T15:51:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    51,
                    44,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-29T22:17:36Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    22,
                    17,
                    36,
                    5,
                    88,
                    0
                ],
                "title": "UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order\n  Model, application to unsteady flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order\n  Model, application to unsteady flows"
                },
                "summary": "Reduced order models (ROMs) play a critical role in fluid mechanics by\nproviding low-cost predictions, making them an attractive tool for engineering\napplications. However, for ROMs to be widely applicable, they must not only\ngeneralise well across different regimes, but also provide a measure of\nconfidence in their predictions. While recent data-driven approaches have begun\nto address nonlinear reduction techniques to improve predictions in transient\nenvironments, challenges remain in terms of robustness and parametrisation. In\nthis work, we present a nonlinear reduction strategy specifically designed for\ntransient flows that incorporates parametrisation and uncertainty\nquantification. Our reduction strategy features a variational auto-encoder\n(VAE) that uses variational inference for confidence measurement. We use a\nlatent space transformer that incorporates recent advances in attention\nmechanisms to predict dynamical systems. Attention's versatility in learning\nsequences and capturing their dependence on external parameters enhances\ngeneralisation across a wide range of dynamics. Prediction, coupled with\nconfidence, enables more informed decision making and addresses the need for\nmore robust models. In addition, this confidence is used to cost-effectively\nsample the parameter space, improving model performance a priori across the\nentire parameter space without requiring evaluation data for the entire domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reduced order models (ROMs) play a critical role in fluid mechanics by\nproviding low-cost predictions, making them an attractive tool for engineering\napplications. However, for ROMs to be widely applicable, they must not only\ngeneralise well across different regimes, but also provide a measure of\nconfidence in their predictions. While recent data-driven approaches have begun\nto address nonlinear reduction techniques to improve predictions in transient\nenvironments, challenges remain in terms of robustness and parametrisation. In\nthis work, we present a nonlinear reduction strategy specifically designed for\ntransient flows that incorporates parametrisation and uncertainty\nquantification. Our reduction strategy features a variational auto-encoder\n(VAE) that uses variational inference for confidence measurement. We use a\nlatent space transformer that incorporates recent advances in attention\nmechanisms to predict dynamical systems. Attention's versatility in learning\nsequences and capturing their dependence on external parameters enhances\ngeneralisation across a wide range of dynamics. Prediction, coupled with\nconfidence, enables more informed decision making and addresses the need for\nmore robust models. In addition, this confidence is used to cost-effectively\nsample the parameter space, improving model performance a priori across the\nentire parameter space without requiring evaluation data for the entire domain."
                },
                "authors": [
                    {
                        "name": "Ismaël Zighed"
                    },
                    {
                        "name": "Nicolas Thome"
                    },
                    {
                        "name": "Patrick Gallinari"
                    },
                    {
                        "name": "Taraneh Sayadi"
                    }
                ],
                "author_detail": {
                    "name": "Taraneh Sayadi"
                },
                "author": "Taraneh Sayadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20879v1",
                "updated": "2025-04-29T15:48:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    48,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T15:48:49Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    48,
                    49,
                    1,
                    119,
                    0
                ],
                "title": "The Leaderboard Illusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Leaderboard Illusion"
                },
                "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"
                },
                "authors": [
                    {
                        "name": "Shivalika Singh"
                    },
                    {
                        "name": "Yiyang Nan"
                    },
                    {
                        "name": "Alex Wang"
                    },
                    {
                        "name": "Daniel D'Souza"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Noah Smith"
                    },
                    {
                        "name": "Beyza Ermis"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "68 pages, 18 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20863v1",
                "updated": "2025-04-29T15:39:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    39,
                    10,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T15:39:10Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    39,
                    10,
                    1,
                    119,
                    0
                ],
                "title": "Bayesian Optimization-based Tire Parameter and Uncertainty Estimation\n  for Real-World Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Optimization-based Tire Parameter and Uncertainty Estimation\n  for Real-World Data"
                },
                "summary": "This work presents a methodology to estimate tire parameters and their\nuncertainty using a Bayesian optimization approach. The literature mainly\nconsiders the estimation of tire parameters but lacks an evaluation of the\nparameter identification quality and the required slip ratios for an adequate\nmodel fit. Therefore, we examine the use of Stochastical Variational Inference\nas a methodology to estimate both - the parameters and their uncertainties. We\nevaluate the method compared to a state-of-the-art Nelder-Mead algorithm for\ntheoretical and real-world application. The theoretical study considers\nparameter fitting at different slip ratios to evaluate the required excitation\nfor an adequate fitting of each parameter. The results are compared to a\nsensitivity analysis for a Pacejka Magic Formula tire model. We show the\napplication of the algorithm on real-world data acquired during the Abu Dhabi\nAutonomous Racing League and highlight the uncertainties in identifying the\ncurvature and shape parameters due to insufficient excitation. The gathered\ninsights can help assess the acquired data's limitations and instead utilize\nstandardized parameters until higher slip ratios are captured. We show that our\nproposed method can be used to assess the mean values and the uncertainties of\ntire model parameters in real-world conditions and derive actions for the tire\nmodeling based on our simulative study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a methodology to estimate tire parameters and their\nuncertainty using a Bayesian optimization approach. The literature mainly\nconsiders the estimation of tire parameters but lacks an evaluation of the\nparameter identification quality and the required slip ratios for an adequate\nmodel fit. Therefore, we examine the use of Stochastical Variational Inference\nas a methodology to estimate both - the parameters and their uncertainties. We\nevaluate the method compared to a state-of-the-art Nelder-Mead algorithm for\ntheoretical and real-world application. The theoretical study considers\nparameter fitting at different slip ratios to evaluate the required excitation\nfor an adequate fitting of each parameter. The results are compared to a\nsensitivity analysis for a Pacejka Magic Formula tire model. We show the\napplication of the algorithm on real-world data acquired during the Abu Dhabi\nAutonomous Racing League and highlight the uncertainties in identifying the\ncurvature and shape parameters due to insufficient excitation. The gathered\ninsights can help assess the acquired data's limitations and instead utilize\nstandardized parameters until higher slip ratios are captured. We show that our\nproposed method can be used to assess the mean values and the uncertainties of\ntire model parameters in real-world conditions and derive actions for the tire\nmodeling based on our simulative study."
                },
                "authors": [
                    {
                        "name": "Sven Goblirsch"
                    },
                    {
                        "name": "Benedikt Ruhland"
                    },
                    {
                        "name": "Johannes Betz"
                    },
                    {
                        "name": "Markus Lienkamp"
                    }
                ],
                "author_detail": {
                    "name": "Markus Lienkamp"
                },
                "author": "Markus Lienkamp",
                "arxiv_comment": "This paper has been accepted at IV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24175v2",
                "updated": "2025-04-29T15:38:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    38,
                    34,
                    1,
                    119,
                    0
                ],
                "published": "2024-10-31T17:42:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07062v3",
                "updated": "2025-04-29T15:33:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    33,
                    0,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-10T00:10:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    0,
                    10,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning"
                },
                "summary": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 5.40\\% (up to 14.29\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 5.40\\% (up to 14.29\\%)."
                },
                "authors": [
                    {
                        "name": "Weihang Chen"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zhiqiang Li"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14562v2",
                "updated": "2025-04-29T15:24:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    24,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2024-03-21T17:06:17Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    17,
                    6,
                    17,
                    3,
                    81,
                    0
                ],
                "title": "Agentic AI: The Era of Semantic Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI: The Era of Semantic Decoding"
                },
                "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation."
                },
                "authors": [
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20849v1",
                "updated": "2025-04-29T15:19:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    19,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T15:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    19,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated\n  Marketing Text in the Music Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated\n  Marketing Text in the Music Industry"
                },
                "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent."
                },
                "authors": [
                    {
                        "name": "Anum Afzal"
                    },
                    {
                        "name": "Alexandre Mercier"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16137v2",
                "updated": "2025-04-29T15:14:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    14,
                    35,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-21T21:04:01Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    4,
                    1,
                    0,
                    111,
                    0
                ],
                "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark"
                },
                "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences."
                },
                "authors": [
                    {
                        "name": "Jasper Götting"
                    },
                    {
                        "name": "Pedro Medeiros"
                    },
                    {
                        "name": "Jon G Sanders"
                    },
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Karam Elabd"
                    },
                    {
                        "name": "Lennart Justen"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Seth Donoughe"
                    }
                ],
                "author_detail": {
                    "name": "Seth Donoughe"
                },
                "author": "Seth Donoughe",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20837v1",
                "updated": "2025-04-29T15:00:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    0,
                    25,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T15:00:25Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    0,
                    25,
                    1,
                    119,
                    0
                ],
                "title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadSAM: Segmenting 3D radiological images with a 2D promptable model"
                },
                "summary": "Medical image segmentation is a crucial and time-consuming task in clinical\ncare, where mask precision is extremely important. The Segment Anything Model\n(SAM) offers a promising approach, as it provides an interactive interface\nbased on visual prompting and edition to refine an initial segmentation. This\nmodel has strong generalization capabilities, does not rely on predefined\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\nimages and lacks the ability to process medical data effectively. In addition,\nthis model is built for 2D images, whereas a whole medical domain is based on\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\nare based on 2D models, thus requiring one prompt per slice to segment 3D\nobjects, making the segmentation process tedious. They also lack important\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\nboxes and points. We then use this novel prompt type with an iterative\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\nfrom a single prompt and evaluate the models' out-of-domain transfer and\nedition capabilities. We demonstrate the effectiveness of our approach against\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\nsegmentation dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation is a crucial and time-consuming task in clinical\ncare, where mask precision is extremely important. The Segment Anything Model\n(SAM) offers a promising approach, as it provides an interactive interface\nbased on visual prompting and edition to refine an initial segmentation. This\nmodel has strong generalization capabilities, does not rely on predefined\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\nimages and lacks the ability to process medical data effectively. In addition,\nthis model is built for 2D images, whereas a whole medical domain is based on\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\nare based on 2D models, thus requiring one prompt per slice to segment 3D\nobjects, making the segmentation process tedious. They also lack important\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\nboxes and points. We then use this novel prompt type with an iterative\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\nfrom a single prompt and evaluate the models' out-of-domain transfer and\nedition capabilities. We demonstrate the effectiveness of our approach against\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\nsegmentation dataset."
                },
                "authors": [
                    {
                        "name": "Julien Khlaut"
                    },
                    {
                        "name": "Elodie Ferreres"
                    },
                    {
                        "name": "Daniel Tordjman"
                    },
                    {
                        "name": "Hélène Philippe"
                    },
                    {
                        "name": "Tom Boeken"
                    },
                    {
                        "name": "Pierre Manceron"
                    },
                    {
                        "name": "Corentin Dancette"
                    }
                ],
                "author_detail": {
                    "name": "Corentin Dancette"
                },
                "author": "Corentin Dancette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20835v1",
                "updated": "2025-04-29T14:59:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    59,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:59:42Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    59,
                    42,
                    1,
                    119,
                    0
                ],
                "title": "Enhancing Non-Core Language Instruction-Following in Speech LLMs via\n  Semi-Implicit Cross-Lingual CoT Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Non-Core Language Instruction-Following in Speech LLMs via\n  Semi-Implicit Cross-Lingual CoT Reasoning"
                },
                "summary": "Large language models have been extended to the speech domain, leading to the\ndevelopment of speech large language models (SLLMs). While existing SLLMs\ndemonstrate strong performance in speech instruction-following for core\nlanguages (e.g., English), they often struggle with non-core languages due to\nthe scarcity of paired speech-text data and limited multilingual semantic\nreasoning capabilities. To address this, we propose the semi-implicit\nCross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates\nspeech-to-text translation into the reasoning process of SLLMs. The XS-CoT\ngenerates four types of tokens: instruction and response tokens in both core\nand non-core languages, enabling cross-lingual transfer of reasoning\ncapabilities. To mitigate inference latency in generating target non-core\nresponse tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which\nprogressively compresses the first three types of intermediate reasoning tokens\nwhile retaining global reasoning logic during training. By leveraging the\nrobust reasoning capabilities of the core language, XS-CoT improves responses\nfor non-core languages by up to 45\\% in GPT-4 score when compared to direct\nsupervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.\nMoreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a\nslight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount\nof high-quality training data for non-core languages by leveraging the\nreasoning capabilities of core languages. To support training, we also develop\na data pipeline and open-source speech instruction-following datasets in\nJapanese, German, and French.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been extended to the speech domain, leading to the\ndevelopment of speech large language models (SLLMs). While existing SLLMs\ndemonstrate strong performance in speech instruction-following for core\nlanguages (e.g., English), they often struggle with non-core languages due to\nthe scarcity of paired speech-text data and limited multilingual semantic\nreasoning capabilities. To address this, we propose the semi-implicit\nCross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates\nspeech-to-text translation into the reasoning process of SLLMs. The XS-CoT\ngenerates four types of tokens: instruction and response tokens in both core\nand non-core languages, enabling cross-lingual transfer of reasoning\ncapabilities. To mitigate inference latency in generating target non-core\nresponse tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which\nprogressively compresses the first three types of intermediate reasoning tokens\nwhile retaining global reasoning logic during training. By leveraging the\nrobust reasoning capabilities of the core language, XS-CoT improves responses\nfor non-core languages by up to 45\\% in GPT-4 score when compared to direct\nsupervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.\nMoreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a\nslight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount\nof high-quality training data for non-core languages by leveraging the\nreasoning capabilities of core languages. To support training, we also develop\na data pipeline and open-source speech instruction-following datasets in\nJapanese, German, and French."
                },
                "authors": [
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Yufeng Tang"
                    },
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "10 pages, 6 figures, Submitted to ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20834v1",
                "updated": "2025-04-29T14:58:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    58,
                    43,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:58:43Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    58,
                    43,
                    1,
                    119,
                    0
                ],
                "title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for LLM Reasoning Under Memory Constraints"
                },
                "summary": "We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated."
                },
                "authors": [
                    {
                        "name": "Alan Lee"
                    },
                    {
                        "name": "Harry Tong"
                    }
                ],
                "author_detail": {
                    "name": "Harry Tong"
                },
                "author": "Harry Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20829v1",
                "updated": "2025-04-29T14:52:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    52,
                    14,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:52:14Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    52,
                    14,
                    1,
                    119,
                    0
                ],
                "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for\n  Targeted Scene Confusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for\n  Targeted Scene Confusion"
                },
                "summary": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability."
                },
                "authors": [
                    {
                        "name": "Jiaxin Hong"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shuoyang Sun"
                    },
                    {
                        "name": "Hongyao Yu"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Yuqi Tan"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Shuhan Qi"
                    },
                    {
                        "name": "Jiawei Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Li"
                },
                "author": "Jiawei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20828v1",
                "updated": "2025-04-29T14:51:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    51,
                    26,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:51:26Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    51,
                    26,
                    1,
                    119,
                    0
                ],
                "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs."
                },
                "authors": [
                    {
                        "name": "Azam Ikram"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Sameh Elnikety"
                    },
                    {
                        "name": "Saurabh Bagchi"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Bagchi"
                },
                "author": "Saurabh Bagchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08295v2",
                "updated": "2025-04-29T14:47:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    47,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2024-08-30T12:04:51Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    12,
                    4,
                    51,
                    4,
                    243,
                    0
                ],
                "title": "Higher order definition of causality by optimally conditioned transfer\n  entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Higher order definition of causality by optimally conditioned transfer\n  entropy"
                },
                "summary": "The description of the dynamics of complex systems, in particular the capture\nof the interaction structure and causal relationships between elements of the\nsystem, is one of the central questions of interdisciplinary research. While\nthe characterization of pairwise causal interactions is a relatively ripe field\nwith established theoretical concepts and the current focus is on technical\nissues of their efficient estimation, it turns out that the standard concepts\nsuch as Granger causality or transfer entropy may not faithfully reflect\npossible synergies or interactions of higher orders, phenomena highly relevant\nfor many real-world complex systems. In this paper, we propose a generalization\nand refinement of the information-theoretic approach to causal inference,\nenabling the description of truly multivariate, rather than multiple pairwise,\ncausal interactions, and moving thus from causal networks to causal\nhypernetworks. In particular, while keeping the ability to control for\nmediating variables or common causes, in case of purely synergetic interactions\nsuch as the exclusive disjunction, it ascribes the causal role to the\nmultivariate causal set but \\emph{not} to individual inputs, distinguishing it\nthus from the case of e.g. two additive univariate causes. We demonstrate this\nconcept by application to illustrative theoretical examples as well as a\nbiophysically realistic simulation of biological neuronal dynamics recently\nreported to employ synergetic computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The description of the dynamics of complex systems, in particular the capture\nof the interaction structure and causal relationships between elements of the\nsystem, is one of the central questions of interdisciplinary research. While\nthe characterization of pairwise causal interactions is a relatively ripe field\nwith established theoretical concepts and the current focus is on technical\nissues of their efficient estimation, it turns out that the standard concepts\nsuch as Granger causality or transfer entropy may not faithfully reflect\npossible synergies or interactions of higher orders, phenomena highly relevant\nfor many real-world complex systems. In this paper, we propose a generalization\nand refinement of the information-theoretic approach to causal inference,\nenabling the description of truly multivariate, rather than multiple pairwise,\ncausal interactions, and moving thus from causal networks to causal\nhypernetworks. In particular, while keeping the ability to control for\nmediating variables or common causes, in case of purely synergetic interactions\nsuch as the exclusive disjunction, it ascribes the causal role to the\nmultivariate causal set but \\emph{not} to individual inputs, distinguishing it\nthus from the case of e.g. two additive univariate causes. We demonstrate this\nconcept by application to illustrative theoretical examples as well as a\nbiophysically realistic simulation of biological neuronal dynamics recently\nreported to employ synergetic computations."
                },
                "authors": [
                    {
                        "name": "Jakub Kořenek"
                    },
                    {
                        "name": "Pavel Sanda"
                    },
                    {
                        "name": "Jaroslav Hlinka"
                    }
                ],
                "author_detail": {
                    "name": "Jaroslav Hlinka"
                },
                "author": "Jaroslav Hlinka",
                "arxiv_doi": "10.1103/PhysRevE.111.L042302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevE.111.L042302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. E 111, L042302 (2025)",
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09089v2",
                "updated": "2025-04-29T14:37:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    37,
                    43,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-12T05:55:01Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    55,
                    1,
                    2,
                    71,
                    0
                ],
                "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocAgent: Graph-Guided LLM Agents for Code Localization"
                },
                "summary": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent."
                },
                "authors": [
                    {
                        "name": "Zhaoling Chen"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Viktor Prasanna"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xingyao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingyao Wang"
                },
                "author": "Xingyao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20808v1",
                "updated": "2025-04-29T14:21:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    21,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:21:08Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    21,
                    8,
                    1,
                    119,
                    0
                ],
                "title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from\n  Gameplay Recordings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from\n  Gameplay Recordings"
                },
                "summary": "This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion"
                },
                "authors": [
                    {
                        "name": "Florian Vahl"
                    },
                    {
                        "name": "Jörn Griepenburg"
                    },
                    {
                        "name": "Jan Gutsche"
                    },
                    {
                        "name": "Jasper Güldenstein"
                    },
                    {
                        "name": "Jianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Zhang"
                },
                "author": "Jianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20799v1",
                "updated": "2025-04-29T14:13:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    13,
                    57,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    13,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges"
                },
                "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs."
                },
                "authors": [
                    {
                        "name": "Yunseo Lee"
                    },
                    {
                        "name": "John Youngeun Song"
                    },
                    {
                        "name": "Dongsun Kim"
                    },
                    {
                        "name": "Jindae Kim"
                    },
                    {
                        "name": "Mijung Kim"
                    },
                    {
                        "name": "Jaechang Nam"
                    }
                ],
                "author_detail": {
                    "name": "Jaechang Nam"
                },
                "author": "Jaechang Nam",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20794v1",
                "updated": "2025-04-29T14:10:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    10,
                    10,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:10:10Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    10,
                    10,
                    1,
                    119,
                    0
                ],
                "title": "Q-Fusion: Diffusing Quantum Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Fusion: Diffusing Quantum Circuits"
                },
                "summary": "Quantum computing holds great potential for solving socially relevant and\ncomputationally complex problems. Furthermore, quantum machine learning (QML)\npromises to rapidly improve our current machine learning capabilities. However,\ncurrent noisy intermediate-scale quantum (NISQ) devices are constrained by\nlimitations in the number of qubits and gate counts, which hinder their full\ncapabilities. Furthermore, the design of quantum algorithms remains a laborious\ntask, requiring significant domain expertise and time. Quantum Architecture\nSearch (QAS) aims to streamline this process by automatically generating novel\nquantum circuits, reducing the need for manual intervention. In this paper, we\npropose a diffusion-based algorithm leveraging the LayerDAG framework to\ngenerate new quantum circuits. This method contrasts with other approaches that\nutilize large language models (LLMs), reinforcement learning (RL), variational\nautoencoders (VAE), and similar techniques. Our results demonstrate that the\nproposed model consistently generates 100% valid quantum circuit outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing holds great potential for solving socially relevant and\ncomputationally complex problems. Furthermore, quantum machine learning (QML)\npromises to rapidly improve our current machine learning capabilities. However,\ncurrent noisy intermediate-scale quantum (NISQ) devices are constrained by\nlimitations in the number of qubits and gate counts, which hinder their full\ncapabilities. Furthermore, the design of quantum algorithms remains a laborious\ntask, requiring significant domain expertise and time. Quantum Architecture\nSearch (QAS) aims to streamline this process by automatically generating novel\nquantum circuits, reducing the need for manual intervention. In this paper, we\npropose a diffusion-based algorithm leveraging the LayerDAG framework to\ngenerate new quantum circuits. This method contrasts with other approaches that\nutilize large language models (LLMs), reinforcement learning (RL), variational\nautoencoders (VAE), and similar techniques. Our results demonstrate that the\nproposed model consistently generates 100% valid quantum circuit outputs."
                },
                "authors": [
                    {
                        "name": "Collin Beaudoin"
                    },
                    {
                        "name": "Swaroop Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Ghosh"
                },
                "author": "Swaroop Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20784v1",
                "updated": "2025-04-29T14:01:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    1,
                    10,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:01:10Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    1,
                    10,
                    1,
                    119,
                    0
                ],
                "title": "Approximate Lifted Model Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Lifted Model Construction"
                },
                "summary": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice."
                },
                "authors": [
                    {
                        "name": "Malte Luttermann"
                    },
                    {
                        "name": "Jan Speller"
                    },
                    {
                        "name": "Marcel Gehrke"
                    },
                    {
                        "name": "Tanya Braun"
                    },
                    {
                        "name": "Ralf Möller"
                    },
                    {
                        "name": "Mattis Hartwig"
                    }
                ],
                "author_detail": {
                    "name": "Mattis Hartwig"
                },
                "author": "Mattis Hartwig",
                "arxiv_comment": "Extended version of paper accepted to the Proceedings of the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20781v1",
                "updated": "2025-04-29T14:00:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:00:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "title": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions"
                },
                "summary": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading.\nBased on the results, we further discussed the pros and cons of the three\nprompting strategies and the strengths and limitations of the DR generated by\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading.\nBased on the results, we further discussed the pros and cons of the three\nprompting strategies and the strengths and limitations of the DR generated by\nLLMs."
                },
                "authors": [
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang",
                "arxiv_comment": "28 pages, 5 images, 7 tables, Manuscript submitted to a journal\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08243v2",
                "updated": "2025-04-29T13:58:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    58,
                    44,
                    1,
                    119,
                    0
                ],
                "published": "2024-11-12T23:43:20Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    23,
                    43,
                    20,
                    1,
                    317,
                    0
                ],
                "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset"
                },
                "summary": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs."
                },
                "authors": [
                    {
                        "name": "Khaoula Chehbouni"
                    },
                    {
                        "name": "Jonathan Colaço Carr"
                    },
                    {
                        "name": "Yash More"
                    },
                    {
                        "name": "Jackie CK Cheung"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "Prepared for conference submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11330v2",
                "updated": "2025-04-29T13:55:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    55,
                    4,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-15T16:04:02Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    4,
                    2,
                    1,
                    105,
                    0
                ],
                "title": "Decorrelation in Complex Wave Scattering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decorrelation in Complex Wave Scattering"
                },
                "summary": "Phenomena involving multiple scattering, despite having attracted\nconsiderable attention in physics for decades, continue to generate unexpected\nand counterintuitive behaviours prompting further studies. For optical\nscattering, the memory effect well predicts fourth order statistics, i.e. the\nintensity correlation, as long as the scattering strength and depth are within\ncertain bounds. The memory effect has found a wide range of applications, where\nits limitations also become apparent: for example, in imaging through turbid\nmedia, decorrelation due to multiscattering in thick samples has been shown to\nrestrict the field of view. However, to our knowledge, no comprehensive\nmechanism exists to date that can account for decorrelation precisely. In this\npaper, we quantify how the scatterer's own statistics determine such\nlimitations. We show that the ensemble statistics of the backscattered field\nmay be decomposed into two terms: one expresses surface scattering, where\nstatistical distributions of multiscale structure features may be inferred from\nour previous works; while the second term originates from the underlying\nscattering volume and is diffusive. The new framework agrees well with\nexperiments, including the prediction of a new quasipower law for fluctuations\ninduced by the single realization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phenomena involving multiple scattering, despite having attracted\nconsiderable attention in physics for decades, continue to generate unexpected\nand counterintuitive behaviours prompting further studies. For optical\nscattering, the memory effect well predicts fourth order statistics, i.e. the\nintensity correlation, as long as the scattering strength and depth are within\ncertain bounds. The memory effect has found a wide range of applications, where\nits limitations also become apparent: for example, in imaging through turbid\nmedia, decorrelation due to multiscattering in thick samples has been shown to\nrestrict the field of view. However, to our knowledge, no comprehensive\nmechanism exists to date that can account for decorrelation precisely. In this\npaper, we quantify how the scatterer's own statistics determine such\nlimitations. We show that the ensemble statistics of the backscattered field\nmay be decomposed into two terms: one expresses surface scattering, where\nstatistical distributions of multiscale structure features may be inferred from\nour previous works; while the second term originates from the underlying\nscattering volume and is diffusive. The new framework agrees well with\nexperiments, including the prediction of a new quasipower law for fluctuations\ninduced by the single realization."
                },
                "authors": [
                    {
                        "name": "Qihang Zhang"
                    },
                    {
                        "name": "Haoyu Yue"
                    },
                    {
                        "name": "Ninghe Liu"
                    },
                    {
                        "name": "Danlin Xu"
                    },
                    {
                        "name": "Renjie Zhou"
                    },
                    {
                        "name": "Liangcai Cao"
                    },
                    {
                        "name": "George Barbastathis"
                    }
                ],
                "author_detail": {
                    "name": "George Barbastathis"
                },
                "author": "George Barbastathis",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20776v1",
                "updated": "2025-04-29T13:53:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    53,
                    33,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:53:33Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    53,
                    33,
                    1,
                    119,
                    0
                ],
                "title": "ECOSoundSet: a finely annotated dataset for the automated acoustic\n  identification of Orthoptera and Cicadidae in North, Central and temperate\n  Western Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECOSoundSet: a finely annotated dataset for the automated acoustic\n  identification of Orthoptera and Cicadidae in North, Central and temperate\n  Western Europe"
                },
                "summary": "Currently available tools for the automated acoustic recognition of European\ninsects in natural soundscapes are limited in scope. Large and ecologically\nheterogeneous acoustic datasets are currently needed for these algorithms to\ncross-contextually recognize the subtle and complex acoustic signatures\nproduced by each species, thus making the availability of such datasets a key\nrequisite for their development. Here we present ECOSoundSet (European\nCicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings\nof 200 orthopteran and 24 cicada species (217 and 26 respective taxa when\nincluding subspecies) present in North, Central, and temperate Western Europe\n(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,\nLuxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly\nthrough targeted fieldwork in South France and Catalonia and partly through\ncontributions from various European entomologists. The dataset is composed of a\ncombination of coarsely labeled recordings, for which we can only infer the\npresence, at some point, of their target species (weak labeling), and finely\nannotated recordings, for which we know the specific time and frequency range\nof each insect sound present in the recording (strong labeling). We also\nprovide a train/validation/test split of the strongly labeled recordings, with\nrespective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate\ntheir incorporation in the training and evaluation of deep learning algorithms.\nThis dataset could serve as a meaningful complement to recordings already\navailable online for the training of deep learning algorithms for the acoustic\nclassification of orthopterans and cicadas in North, Central, and temperate\nWestern Europe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently available tools for the automated acoustic recognition of European\ninsects in natural soundscapes are limited in scope. Large and ecologically\nheterogeneous acoustic datasets are currently needed for these algorithms to\ncross-contextually recognize the subtle and complex acoustic signatures\nproduced by each species, thus making the availability of such datasets a key\nrequisite for their development. Here we present ECOSoundSet (European\nCicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings\nof 200 orthopteran and 24 cicada species (217 and 26 respective taxa when\nincluding subspecies) present in North, Central, and temperate Western Europe\n(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,\nLuxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly\nthrough targeted fieldwork in South France and Catalonia and partly through\ncontributions from various European entomologists. The dataset is composed of a\ncombination of coarsely labeled recordings, for which we can only infer the\npresence, at some point, of their target species (weak labeling), and finely\nannotated recordings, for which we know the specific time and frequency range\nof each insect sound present in the recording (strong labeling). We also\nprovide a train/validation/test split of the strongly labeled recordings, with\nrespective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate\ntheir incorporation in the training and evaluation of deep learning algorithms.\nThis dataset could serve as a meaningful complement to recordings already\navailable online for the training of deep learning algorithms for the acoustic\nclassification of orthopterans and cicadas in North, Central, and temperate\nWestern Europe."
                },
                "authors": [
                    {
                        "name": "David Funosas"
                    },
                    {
                        "name": "Elodie Massol"
                    },
                    {
                        "name": "Yves Bas"
                    },
                    {
                        "name": "Svenja Schmidt"
                    },
                    {
                        "name": "Dominik Arend"
                    },
                    {
                        "name": "Alexander Gebhard"
                    },
                    {
                        "name": "Luc Barbaro"
                    },
                    {
                        "name": "Sebastian König"
                    },
                    {
                        "name": "Rafael Carbonell Font"
                    },
                    {
                        "name": "David Sannier"
                    },
                    {
                        "name": "Fernand Deroussen"
                    },
                    {
                        "name": "Jérôme Sueur"
                    },
                    {
                        "name": "Christian Roesti"
                    },
                    {
                        "name": "Tomi Trilar"
                    },
                    {
                        "name": "Wolfgang Forstmeier"
                    },
                    {
                        "name": "Lucas Roger"
                    },
                    {
                        "name": "Eloïsa Matheu"
                    },
                    {
                        "name": "Piotr Guzik"
                    },
                    {
                        "name": "Julien Barataud"
                    },
                    {
                        "name": "Laurent Pelozuelo"
                    },
                    {
                        "name": "Stéphane Puissant"
                    },
                    {
                        "name": "Sandra Mueller"
                    },
                    {
                        "name": "Björn Schuller"
                    },
                    {
                        "name": "Jose M. Montoya"
                    },
                    {
                        "name": "Andreas Triantafyllopoulos"
                    },
                    {
                        "name": "Maxime Cauchoix"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Cauchoix"
                },
                "author": "Maxime Cauchoix",
                "arxiv_comment": "3 Figures + 2 Supplementary Figures, 2 Tables + 3 Supplementary\n  Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20771v1",
                "updated": "2025-04-29T13:52:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    52,
                    47,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:52:47Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    52,
                    47,
                    1,
                    119,
                    0
                ],
                "title": "Turing Machine Evaluation for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turing Machine Evaluation for Large Language Model"
                },
                "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench."
                },
                "authors": [
                    {
                        "name": "Haitao Wu"
                    },
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Huaxi Huang"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20763v1",
                "updated": "2025-04-29T13:44:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    44,
                    1,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:44:01Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    44,
                    1,
                    1,
                    119,
                    0
                ],
                "title": "Understanding Large Language Model Supply Chain: Structure, Domain, and\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Model Supply Chain: Structure, Domain, and\n  Vulnerabilities"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence\n(AI), driving breakthroughs in natural language understanding, text generation,\nand autonomous systems. However, the rapid growth of LLMs presents significant\nchallenges in the security and reliability of the Large Language Model Supply\nChain (LLMSC), a complex network of open-source components, libraries, and\ntools essential for LLM development and deployment. Despite its critical\nimportance, the LLMSC remains underexplored, particularly regarding its\nstructural characteristics, domain composition, and security vulnerabilities.\nTo address this gap, we conduct the first empirical study of the LLMSC,\nanalyzing a curated dataset of open-source packages from PyPI and NPM across 14\nfunctional domains. We construct a directed dependency graph comprising 15,725\nnodes, 10,402 edges, and 180 unique vulnerabilities to investigate the\nstructural characteristics of the LLMSC and analyze how security risks\npropagate through its dependency network. Our findings reveal that the LLMSC\nexhibits a ``locally dense, globally sparse'' topology, with 79.7% of\ndependency trees containing fewer than 5 nodes, while a few large trees\ndominate the ecosystem, accounting for 77.66% of all nodes. The graph is\ncharacterized by high-degree hubs, with the top 5 most connected nodes\naveraging 1,282 dependents each. Security analysis shows that critical\nvulnerabilities propagate to an average of 142.1 nodes at the second layer of\ndependency trees and peak at 237.8 affected nodes at the third layer. Notably,\ncascading risks are concentrated in critical hub nodes such as transformers,\nwhich directly or indirectly affect over 1,300 downstream packages. These\nfindings provide quantitative insights into the structural and security\ndynamics of the LLMSC and emphasize the need for targeted mitigation strategies\nto enhance ecosystem resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence\n(AI), driving breakthroughs in natural language understanding, text generation,\nand autonomous systems. However, the rapid growth of LLMs presents significant\nchallenges in the security and reliability of the Large Language Model Supply\nChain (LLMSC), a complex network of open-source components, libraries, and\ntools essential for LLM development and deployment. Despite its critical\nimportance, the LLMSC remains underexplored, particularly regarding its\nstructural characteristics, domain composition, and security vulnerabilities.\nTo address this gap, we conduct the first empirical study of the LLMSC,\nanalyzing a curated dataset of open-source packages from PyPI and NPM across 14\nfunctional domains. We construct a directed dependency graph comprising 15,725\nnodes, 10,402 edges, and 180 unique vulnerabilities to investigate the\nstructural characteristics of the LLMSC and analyze how security risks\npropagate through its dependency network. Our findings reveal that the LLMSC\nexhibits a ``locally dense, globally sparse'' topology, with 79.7% of\ndependency trees containing fewer than 5 nodes, while a few large trees\ndominate the ecosystem, accounting for 77.66% of all nodes. The graph is\ncharacterized by high-degree hubs, with the top 5 most connected nodes\naveraging 1,282 dependents each. Security analysis shows that critical\nvulnerabilities propagate to an average of 142.1 nodes at the second layer of\ndependency trees and peak at 237.8 affected nodes at the third layer. Notably,\ncascading risks are concentrated in critical hub nodes such as transformers,\nwhich directly or indirectly affect over 1,300 downstream packages. These\nfindings provide quantitative insights into the structural and security\ndynamics of the LLMSC and emphasize the need for targeted mitigation strategies\nto enhance ecosystem resilience."
                },
                "authors": [
                    {
                        "name": "Yanzhe Hu"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Tianyuan Nie"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11978v2",
                "updated": "2025-04-29T13:43:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    43,
                    16,
                    1,
                    119,
                    0
                ],
                "published": "2025-02-17T16:22:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    22,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Multi-mode Pulsations in AGB Stars: Insights from 3D RHD CO5BOLD\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-mode Pulsations in AGB Stars: Insights from 3D RHD CO5BOLD\n  Simulations"
                },
                "summary": "Stars on the AGB can exhibit acoustic pulsation modes of different radial\norders, along with non-radial modes. These pulsations are essential to the\nmass-loss process and influence the evolutionary pathways of AGB stars. P-L\nrelations serve as a valuable diagnostic for understanding stellar evolution\nalong the AGB. 3D RHD simulations provide a powerful tool for investigating\npulsation phenomena driven by convective processes and their non-linear\ncoupling with stellar oscillations. We investigate multi-mode pulsations in AGB\nstars using advanced 3D 'star-in-a-box' simulations with the CO5BOLD code.\nSignatures of these multi-mode pulsations were weak in our previous 3D models.\nOur focus is on identifying and characterising the various pulsation modes,\nexamining their persistence and transitions, and comparing the results with 1D\nmodel predictions and observational data where applicable. We produced a new\nmodel grid comprising AGB stars with current masses of $0.7$, $0.8$, and\n$1\\,\\mathrm{M}_{\\odot}$. Fourier analysis was applied to dynamic,\ntime-dependent quantities to extract dominant pulsation modes and their\ncorresponding periods. Additionally, wavelet transforms were employed to\nidentify mode-switching behaviour over time. The models successfully reproduce\nthe P-L sequences found in AGB stars. Mode-switching phenomena are found in\nboth the models and wavelet analyses of observational data, allowing us to\ninfer similarities in the underlying pulsation dynamics. These 3D simulations\nhighlight the natural emergence of multi-mode pulsations, including both radial\nand non-radial modes, driven by the self-consistent interplay of convection and\noscillations. Our findings underscore the value of 3D RHD models in capturing\nthe non-linear behaviour of AGB pulsations, providing insights into mode\nswitching, envelope structures, and potential links to episodic mass-loss\nevents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stars on the AGB can exhibit acoustic pulsation modes of different radial\norders, along with non-radial modes. These pulsations are essential to the\nmass-loss process and influence the evolutionary pathways of AGB stars. P-L\nrelations serve as a valuable diagnostic for understanding stellar evolution\nalong the AGB. 3D RHD simulations provide a powerful tool for investigating\npulsation phenomena driven by convective processes and their non-linear\ncoupling with stellar oscillations. We investigate multi-mode pulsations in AGB\nstars using advanced 3D 'star-in-a-box' simulations with the CO5BOLD code.\nSignatures of these multi-mode pulsations were weak in our previous 3D models.\nOur focus is on identifying and characterising the various pulsation modes,\nexamining their persistence and transitions, and comparing the results with 1D\nmodel predictions and observational data where applicable. We produced a new\nmodel grid comprising AGB stars with current masses of $0.7$, $0.8$, and\n$1\\,\\mathrm{M}_{\\odot}$. Fourier analysis was applied to dynamic,\ntime-dependent quantities to extract dominant pulsation modes and their\ncorresponding periods. Additionally, wavelet transforms were employed to\nidentify mode-switching behaviour over time. The models successfully reproduce\nthe P-L sequences found in AGB stars. Mode-switching phenomena are found in\nboth the models and wavelet analyses of observational data, allowing us to\ninfer similarities in the underlying pulsation dynamics. These 3D simulations\nhighlight the natural emergence of multi-mode pulsations, including both radial\nand non-radial modes, driven by the self-consistent interplay of convection and\noscillations. Our findings underscore the value of 3D RHD models in capturing\nthe non-linear behaviour of AGB pulsations, providing insights into mode\nswitching, envelope structures, and potential links to episodic mass-loss\nevents."
                },
                "authors": [
                    {
                        "name": "Arief Ahmad"
                    },
                    {
                        "name": "Bernd Freytag"
                    },
                    {
                        "name": "Susanne Höfner"
                    }
                ],
                "author_detail": {
                    "name": "Susanne Höfner"
                },
                "author": "Susanne Höfner",
                "arxiv_comment": "14 pages and 14 figures. Submitted to Astronomy and Astrophysics.\n  (This version has been revised following the first referee report.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20013v2",
                "updated": "2025-04-29T13:41:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    41,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T17:32:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    32,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation"
                },
                "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."
                },
                "authors": [
                    {
                        "name": "Beizhe Hu"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Juan Cao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Danding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Danding Wang"
                },
                "author": "Danding Wang",
                "arxiv_doi": "10.1145/3726302.3730027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM SIGIR 2025 Full Paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20758v1",
                "updated": "2025-04-29T13:37:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    37,
                    36,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:37:36Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    37,
                    36,
                    1,
                    119,
                    0
                ],
                "title": "Influence network reconstruction from discrete time-series of count data\n  modelled by multidimensional Hawkes processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence network reconstruction from discrete time-series of count data\n  modelled by multidimensional Hawkes processes"
                },
                "summary": "Identifying key influencers from time series data without a known prior\nnetwork structure is a challenging problem in various applications, from crime\nanalysis to social media. While much work has focused on event-based time\nseries (timestamp) data, fewer methods address count data, where event counts\nare recorded in fixed intervals. We develop network inference methods for both\nbatched and sequential count data. Here the strong network connection\nrepresents the key influences among the nodes. We introduce an ensemble-based\nalgorithm, rooted in the expectation-maximization (EM) framework, and\ndemonstrate its utility to identify node dynamics and connections through a\ndiscrete-time Cox or Hawkes process. For the linear multidimensional Hawkes\nmodel, we employ a minimization-majorization (MM) approach, allowing for\nparallelized inference of networks. For sequential inference, we use a\nsecond-order approximation of the Bayesian inference problem. Under certain\nassumptions, a rank-1 update for the covariance matrix reduces computational\ncosts. We validate our methods on synthetic data and real-world datasets,\nincluding email communications within European academic communities. Our\napproach effectively reconstructs underlying networks, accounting for both\nexcitation and diffusion influences. This work advances network reconstruction\nfrom count data in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying key influencers from time series data without a known prior\nnetwork structure is a challenging problem in various applications, from crime\nanalysis to social media. While much work has focused on event-based time\nseries (timestamp) data, fewer methods address count data, where event counts\nare recorded in fixed intervals. We develop network inference methods for both\nbatched and sequential count data. Here the strong network connection\nrepresents the key influences among the nodes. We introduce an ensemble-based\nalgorithm, rooted in the expectation-maximization (EM) framework, and\ndemonstrate its utility to identify node dynamics and connections through a\ndiscrete-time Cox or Hawkes process. For the linear multidimensional Hawkes\nmodel, we employ a minimization-majorization (MM) approach, allowing for\nparallelized inference of networks. For sequential inference, we use a\nsecond-order approximation of the Bayesian inference problem. Under certain\nassumptions, a rank-1 update for the covariance matrix reduces computational\ncosts. We validate our methods on synthetic data and real-world datasets,\nincluding email communications within European academic communities. Our\napproach effectively reconstructs underlying networks, accounting for both\nexcitation and diffusion influences. This work advances network reconstruction\nfrom count data in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Naratip Santitissadeekorn"
                    },
                    {
                        "name": "Martin Short"
                    },
                    {
                        "name": "David J. B. Lloyd"
                    }
                ],
                "author_detail": {
                    "name": "David J. B. Lloyd"
                },
                "author": "David J. B. Lloyd",
                "arxiv_comment": "28 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62F30, 62M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20754v1",
                "updated": "2025-04-29T13:34:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    34,
                    17,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:34:17Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    34,
                    17,
                    1,
                    119,
                    0
                ],
                "title": "DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs"
                },
                "summary": "Diffusion models form an important class of generative models today,\naccounting for much of the state of the art in cutting edge AI research. While\nnumerous extensions beyond image and video generation exist, few of such\napproaches address the issue of explicit constraints in the samples generated.\nIn this paper, we study the problem of generating paths in a layered graph (a\nvariant of a directed acyclic graph) using discrete diffusion models, while\nguaranteeing that our generated samples are indeed paths. Our approach utilizes\na simple yet effective representation for paths which we call the padded\nadjacency-list matrix (PALM). In addition, we show how to effectively perform\nclassifier guidance, which helps steer the sampled paths to specific preferred\nedges without any retraining of the diffusion model. Our preliminary results\nshow that empirically, our method outperforms alternatives which do not\nexplicitly account for path constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models form an important class of generative models today,\naccounting for much of the state of the art in cutting edge AI research. While\nnumerous extensions beyond image and video generation exist, few of such\napproaches address the issue of explicit constraints in the samples generated.\nIn this paper, we study the problem of generating paths in a layered graph (a\nvariant of a directed acyclic graph) using discrete diffusion models, while\nguaranteeing that our generated samples are indeed paths. Our approach utilizes\na simple yet effective representation for paths which we call the padded\nadjacency-list matrix (PALM). In addition, we show how to effectively perform\nclassifier guidance, which helps steer the sampled paths to specific preferred\nedges without any retraining of the diffusion model. Our preliminary results\nshow that empirically, our method outperforms alternatives which do not\nexplicitly account for path constraints."
                },
                "authors": [
                    {
                        "name": "Hao Luan"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Chun Kai Ling"
                    }
                ],
                "author_detail": {
                    "name": "Chun Kai Ling"
                },
                "author": "Chun Kai Ling",
                "arxiv_comment": "To appear at Frontiers in Probabilistic Inference: Sampling meets\n  Learning (FPI) workshop at ICLR 2025.\n  https://openreview.net/forum?id=DBdkU0Ikzy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20752v1",
                "updated": "2025-04-29T13:33:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    33,
                    29,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:33:29Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    33,
                    29,
                    1,
                    119,
                    0
                ],
                "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers"
                },
                "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models."
                },
                "authors": [
                    {
                        "name": "Roman Abramov"
                    },
                    {
                        "name": "Felix Steinbauer"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.3; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16563v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16563v3",
                "updated": "2025-04-29T13:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    30,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-23T09:43:40Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    43,
                    40,
                    2,
                    113,
                    0
                ],
                "title": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution"
                },
                "summary": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Jingli Yang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16563v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16563v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20744v1",
                "updated": "2025-04-29T13:24:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    24,
                    37,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:24:37Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    24,
                    37,
                    1,
                    119,
                    0
                ],
                "title": "DB-GNN: Dual-Branch Graph Neural Network with Multi-Level Contrastive\n  Learning for Jointly Identifying Within- and Cross-Frequency Coupled Brain\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DB-GNN: Dual-Branch Graph Neural Network with Multi-Level Contrastive\n  Learning for Jointly Identifying Within- and Cross-Frequency Coupled Brain\n  Networks"
                },
                "summary": "Within-frequency coupling (WFC) and cross-frequency coupling (CFC) in brain\nnetworks reflect neural synchronization within the same frequency band and\ncross-band oscillatory interactions, respectively. Their synergy provides a\ncomprehensive understanding of neural mechanisms underlying cognitive states\nsuch as emotion. However, existing multi-channel EEG studies often analyze WFC\nor CFC separately, failing to fully leverage their complementary properties.\nThis study proposes a dual-branch graph neural network (DB-GNN) to jointly\nidentify within- and cross-frequency coupled brain networks. Firstly, DBGNN\nleverages its unique dual-branch learning architecture to efficiently mine\nglobal collaborative information and local cross-frequency and within-frequency\ncoupling information. Secondly, to more fully perceive the global information\nof cross-frequency and within-frequency coupling, the global perception branch\nof DB-GNN adopts a Transformer architecture. To prevent overfitting of the\nTransformer architecture, this study integrates prior within- and\ncross-frequency coupling information into the Transformer inference process,\nthereby enhancing the generalization capability of DB-GNN. Finally, a\nmulti-scale graph contrastive learning regularization term is introduced to\nconstrain the global and local perception branches of DB-GNN at both\ngraph-level and node-level, enhancing its joint perception ability and further\nimproving its generalization performance. Experimental validation on the\nemotion recognition dataset shows that DB-GNN achieves a testing accuracy of\n97.88% and an F1- score of 97.87%, reaching the state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within-frequency coupling (WFC) and cross-frequency coupling (CFC) in brain\nnetworks reflect neural synchronization within the same frequency band and\ncross-band oscillatory interactions, respectively. Their synergy provides a\ncomprehensive understanding of neural mechanisms underlying cognitive states\nsuch as emotion. However, existing multi-channel EEG studies often analyze WFC\nor CFC separately, failing to fully leverage their complementary properties.\nThis study proposes a dual-branch graph neural network (DB-GNN) to jointly\nidentify within- and cross-frequency coupled brain networks. Firstly, DBGNN\nleverages its unique dual-branch learning architecture to efficiently mine\nglobal collaborative information and local cross-frequency and within-frequency\ncoupling information. Secondly, to more fully perceive the global information\nof cross-frequency and within-frequency coupling, the global perception branch\nof DB-GNN adopts a Transformer architecture. To prevent overfitting of the\nTransformer architecture, this study integrates prior within- and\ncross-frequency coupling information into the Transformer inference process,\nthereby enhancing the generalization capability of DB-GNN. Finally, a\nmulti-scale graph contrastive learning regularization term is introduced to\nconstrain the global and local perception branches of DB-GNN at both\ngraph-level and node-level, enhancing its joint perception ability and further\nimproving its generalization performance. Experimental validation on the\nemotion recognition dataset shows that DB-GNN achieves a testing accuracy of\n97.88% and an F1- score of 97.87%, reaching the state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Hui Xu"
                    },
                    {
                        "name": "Jing Cai"
                    },
                    {
                        "name": "Ta Zhou"
                    },
                    {
                        "name": "Xibei Yang"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20732v1",
                "updated": "2025-04-29T13:15:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    15,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:15:54Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    15,
                    54,
                    1,
                    119,
                    0
                ],
                "title": "Bayesian Inference in Quantum Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference in Quantum Programs"
                },
                "summary": "Conditioning is a key feature in probabilistic programming to enable modeling\nthe influence of data (also known as observations) to the probability\ndistribution described by such programs. Determining the posterior distribution\nis also known as Bayesian inference. This paper equips a quantum while-language\nwith conditioning, defines its denotational and operational semantics over\ninfinite-dimensional Hilbert spaces, and shows their equivalence. We provide\nsufficient conditions for the existence of weakest (liberal)\nprecondition-transformers and derive inductive characterizations of these\ntransformers. It is shown how w(l)p-transformers can be used to assess the\neffect of Bayesian inference on (possibly diverging) quantum programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditioning is a key feature in probabilistic programming to enable modeling\nthe influence of data (also known as observations) to the probability\ndistribution described by such programs. Determining the posterior distribution\nis also known as Bayesian inference. This paper equips a quantum while-language\nwith conditioning, defines its denotational and operational semantics over\ninfinite-dimensional Hilbert spaces, and shows their equivalence. We provide\nsufficient conditions for the existence of weakest (liberal)\nprecondition-transformers and derive inductive characterizations of these\ntransformers. It is shown how w(l)p-transformers can be used to assess the\neffect of Bayesian inference on (possibly diverging) quantum programs."
                },
                "authors": [
                    {
                        "name": "Christina Gehnen"
                    },
                    {
                        "name": "Dominique Unruh"
                    },
                    {
                        "name": "Joost-Pieter Katoen"
                    }
                ],
                "author_detail": {
                    "name": "Joost-Pieter Katoen"
                },
                "author": "Joost-Pieter Katoen",
                "arxiv_comment": "This is the full version of the paper \"Bayesian Inference in Quantum\n  Programs\" appearing at ICALP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19218v2",
                "updated": "2025-04-29T12:59:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    59,
                    17,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-27T12:51:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    12,
                    51,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null\n  Space of Language Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null\n  Space of Language Embeddings"
                },
                "summary": "Recent advancements in sequential recommendation have underscored the\npotential of Large Language Models (LLMs) for enhancing item embeddings.\nHowever, existing approaches face three key limitations: 1) the degradation of\nthe semantic space when high-dimensional language embeddings are mapped to\nlower-dimensional ID embeddings, 2) the underutilization of language\nembeddings, and 3) the reliance on additional trainable parameters, such as an\nadapter, to bridge the gap between the semantic and behavior spaces. In this\npaper, we introduce AlphaFuse, a simple but effective language-guided learning\nstrategy that addresses these challenges by learning ID embeddings within the\nnull space of language embeddings. Specifically, we decompose the semantic\nspace of language embeddings via Singular Value Decomposition (SVD),\ndistinguishing it into a semantic-rich row space and a semantic-sparse null\nspace. Collaborative signals are then injected into the null space, while\npreserving the rich semantics of the row space. AlphaFuse prevents degradation\nof the semantic space, integrates the retained language embeddings into the\nfinal item embeddings, and eliminates the need for auxiliary trainable modules,\nenabling seamless adaptation to any sequential recommendation framework. We\nvalidate the effectiveness and flexibility of AlphaFuse through extensive\nexperiments on three benchmark datasets, including cold-start user and\nlong-tail settings, showcasing significant improvements in both discriminative\nand diffusion-based generative sequential recommenders. Our codes and datasets\nare available at https://github.com/Hugo-Chinn/AlphaFuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in sequential recommendation have underscored the\npotential of Large Language Models (LLMs) for enhancing item embeddings.\nHowever, existing approaches face three key limitations: 1) the degradation of\nthe semantic space when high-dimensional language embeddings are mapped to\nlower-dimensional ID embeddings, 2) the underutilization of language\nembeddings, and 3) the reliance on additional trainable parameters, such as an\nadapter, to bridge the gap between the semantic and behavior spaces. In this\npaper, we introduce AlphaFuse, a simple but effective language-guided learning\nstrategy that addresses these challenges by learning ID embeddings within the\nnull space of language embeddings. Specifically, we decompose the semantic\nspace of language embeddings via Singular Value Decomposition (SVD),\ndistinguishing it into a semantic-rich row space and a semantic-sparse null\nspace. Collaborative signals are then injected into the null space, while\npreserving the rich semantics of the row space. AlphaFuse prevents degradation\nof the semantic space, integrates the retained language embeddings into the\nfinal item embeddings, and eliminates the need for auxiliary trainable modules,\nenabling seamless adaptation to any sequential recommendation framework. We\nvalidate the effectiveness and flexibility of AlphaFuse through extensive\nexperiments on three benchmark datasets, including cold-start user and\nlong-tail settings, showcasing significant improvements in both discriminative\nand diffusion-based generative sequential recommenders. Our codes and datasets\nare available at https://github.com/Hugo-Chinn/AlphaFuse."
                },
                "authors": [
                    {
                        "name": "Guoqing Hu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Zhibo Cai"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_doi": "10.1145/3726302.3729894",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729894",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00505v2",
                "updated": "2025-04-29T12:58:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    58,
                    23,
                    1,
                    119,
                    0
                ],
                "published": "2024-02-01T11:17:15Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    11,
                    17,
                    15,
                    3,
                    32,
                    0
                ],
                "title": "FLIMFLAM DR1: The First Constraints on the Cosmic Baryon Distribution\n  from 8 FRB sightlines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLIMFLAM DR1: The First Constraints on the Cosmic Baryon Distribution\n  from 8 FRB sightlines"
                },
                "summary": "The dispersion measure of fast radio bursts (FRBs), arising from the\ninteractions of the pulses with free electrons along the propagation path,\nconstitutes a unique probe of the cosmic baryon distribution. Their\nconstraining power is further enhanced in combination with observations of the\nforeground large-scale structure and intervening galaxies. In this work, we\npresent the first constraints on the partition of the cosmic baryons between\nthe intergalactic medium (IGM) and circumgalactic medium (CGM), inferred from\nthe FLIMFLAM spectroscopic survey. In its first data release, the FLIMFLAM\nsurvey targeted galaxies in the foreground of 8 localized FRBs. Using Bayesian\ntechniques, we reconstruct the underlying ~Mpc-scale matter density field that\nis traced by the IGM gas. Simultaneously, deeper spectroscopy of intervening\nforeground galaxies (at impact parameters $b_\\perp \\lesssim r_{200}$) and the\nFRB host galaxies constrains the contribution from the CGM. Applying Bayesian\nparameter inference to our data and assuming a fiducial set of priors, we infer\nthe IGM cosmic baryon fraction to be $f_{\\rm igm}=0.59^{+0.11}_{-0.10}$, and a\nCGM gas fraction of $f_{\\rm gas} = 0.55^{+0.26}_{-0.29}$ for $10^{10}\\,M_\\odot\n\\lesssim M_{\\rm halo}\\lesssim 10^{13}\\,M_\\odot$ halos. The mean FRB host\ndispersion measure (rest-frame) in our sample is $\\langle \\rm{DM_{host}}\\rangle\n= 90^{+29}_{-19}\\rm{pc~cm^{-3}}$, of which $\\langle{\\rm DM_{host}^{unk}}\\rangle\n=69^{+28}_{-19}~\\rm{pc~cm^{-3}}$ arises from the host galaxy ISM and/or the FRB\nprogenitor environment. While our current $f_{\\rm igm}$ and $f_{\\rm gas}$\nuncertainties are too broad to constrain most galactic feedback models, this\nresult marks the first measurement of the IGM and CGM baryon fractions, as well\nas the first systematic separation of the FRB host dispersion measure into two\ncomponents: arising from the halo and from the inner ISM/FRB engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dispersion measure of fast radio bursts (FRBs), arising from the\ninteractions of the pulses with free electrons along the propagation path,\nconstitutes a unique probe of the cosmic baryon distribution. Their\nconstraining power is further enhanced in combination with observations of the\nforeground large-scale structure and intervening galaxies. In this work, we\npresent the first constraints on the partition of the cosmic baryons between\nthe intergalactic medium (IGM) and circumgalactic medium (CGM), inferred from\nthe FLIMFLAM spectroscopic survey. In its first data release, the FLIMFLAM\nsurvey targeted galaxies in the foreground of 8 localized FRBs. Using Bayesian\ntechniques, we reconstruct the underlying ~Mpc-scale matter density field that\nis traced by the IGM gas. Simultaneously, deeper spectroscopy of intervening\nforeground galaxies (at impact parameters $b_\\perp \\lesssim r_{200}$) and the\nFRB host galaxies constrains the contribution from the CGM. Applying Bayesian\nparameter inference to our data and assuming a fiducial set of priors, we infer\nthe IGM cosmic baryon fraction to be $f_{\\rm igm}=0.59^{+0.11}_{-0.10}$, and a\nCGM gas fraction of $f_{\\rm gas} = 0.55^{+0.26}_{-0.29}$ for $10^{10}\\,M_\\odot\n\\lesssim M_{\\rm halo}\\lesssim 10^{13}\\,M_\\odot$ halos. The mean FRB host\ndispersion measure (rest-frame) in our sample is $\\langle \\rm{DM_{host}}\\rangle\n= 90^{+29}_{-19}\\rm{pc~cm^{-3}}$, of which $\\langle{\\rm DM_{host}^{unk}}\\rangle\n=69^{+28}_{-19}~\\rm{pc~cm^{-3}}$ arises from the host galaxy ISM and/or the FRB\nprogenitor environment. While our current $f_{\\rm igm}$ and $f_{\\rm gas}$\nuncertainties are too broad to constrain most galactic feedback models, this\nresult marks the first measurement of the IGM and CGM baryon fractions, as well\nas the first systematic separation of the FRB host dispersion measure into two\ncomponents: arising from the halo and from the inner ISM/FRB engine."
                },
                "authors": [
                    {
                        "name": "Ilya S. Khrykin"
                    },
                    {
                        "name": "Metin Ata"
                    },
                    {
                        "name": "Khee-Gan Lee"
                    },
                    {
                        "name": "Sunil Simha"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "J. Xavier Prochaska"
                    },
                    {
                        "name": "Nicolas Tejos"
                    },
                    {
                        "name": "Keith W. Bannister"
                    },
                    {
                        "name": "Jeff Cooke"
                    },
                    {
                        "name": "Cherie K. Day"
                    },
                    {
                        "name": "Adam Deller"
                    },
                    {
                        "name": "Marcin Glowacki"
                    },
                    {
                        "name": "Alexa C. Gordon"
                    },
                    {
                        "name": "Clancy W. James"
                    },
                    {
                        "name": "Lachlan Marnoch"
                    },
                    {
                        "name": "Ryan. M. Shannon"
                    },
                    {
                        "name": "Jielai Zhang"
                    },
                    {
                        "name": "Lucas Bernales-Cortes"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Bernales-Cortes"
                },
                "author": "Lucas Bernales-Cortes",
                "arxiv_doi": "10.3847/1538-4357/ad6567",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad6567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.00505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 9 figures, 5 tables; Accepted and Published by ApJ",
                "arxiv_journal_ref": "The Astrophysical Journal, 2024, Volume 973, Number 2, page 151",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15358v2",
                "updated": "2025-04-29T12:42:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    42,
                    33,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-19T15:58:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation"
                },
                "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."
                },
                "authors": [
                    {
                        "name": "Thomas Pickard"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Maggie Mi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Dylan Phelps"
                    },
                    {
                        "name": "Marco Idiart"
                    }
                ],
                "author_detail": {
                    "name": "Marco Idiart"
                },
                "author": "Marco Idiart",
                "arxiv_comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20708v1",
                "updated": "2025-04-29T12:39:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    39,
                    7,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:39:07Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    39,
                    7,
                    1,
                    119,
                    0
                ],
                "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think"
                },
                "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner."
                },
                "authors": [
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Hani Itani"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21491v3",
                "updated": "2025-04-29T12:30:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    30,
                    32,
                    1,
                    119,
                    0
                ],
                "published": "2024-10-28T20:02:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    20,
                    2,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning"
                },
                "summary": "Distributed learning (DL) uses multiple nodes to accelerate training,\nenabling efficient optimization of large-scale models. Stochastic Gradient\nDescent (SGD), a key optimization algorithm, plays a central role in this\nprocess. However, communication bottlenecks often limit scalability and\nefficiency, leading to increasing adoption of compressed SGD techniques to\nalleviate these challenges. Despite addressing communication overheads,\ncompressed SGD introduces trustworthiness concerns, as gradient exchanges among\nnodes are vulnerable to attacks like gradient inversion (GradInv) and\nmembership inference attacks (MIA). The trustworthiness of compressed SGD\nremains unexplored, leaving important questions about its reliability\nunanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus\nuncompressed SGD. Specifically, we conducted empirical studies using GradInv\nattacks, revealing that compressed SGD demonstrates significantly higher\nresistance to privacy leakage compared to uncompressed SGD. In addition, our\nfindings suggest that MIA may not be a reliable metric for assessing privacy\nrisks in distributed learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed learning (DL) uses multiple nodes to accelerate training,\nenabling efficient optimization of large-scale models. Stochastic Gradient\nDescent (SGD), a key optimization algorithm, plays a central role in this\nprocess. However, communication bottlenecks often limit scalability and\nefficiency, leading to increasing adoption of compressed SGD techniques to\nalleviate these challenges. Despite addressing communication overheads,\ncompressed SGD introduces trustworthiness concerns, as gradient exchanges among\nnodes are vulnerable to attacks like gradient inversion (GradInv) and\nmembership inference attacks (MIA). The trustworthiness of compressed SGD\nremains unexplored, leaving important questions about its reliability\nunanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus\nuncompressed SGD. Specifically, we conducted empirical studies using GradInv\nattacks, revealing that compressed SGD demonstrates significantly higher\nresistance to privacy leakage compared to uncompressed SGD. In addition, our\nfindings suggest that MIA may not be a reliable metric for assessing privacy\nrisks in distributed learning."
                },
                "authors": [
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Caesar Wu"
                    },
                    {
                        "name": "Mohammed Chadli"
                    },
                    {
                        "name": "Said Mammar"
                    },
                    {
                        "name": "Pascal Bouvry"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Bouvry"
                },
                "author": "Pascal Bouvry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20699v1",
                "updated": "2025-04-29T12:30:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    30,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:30:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    30,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine\n  Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine\n  Translation?"
                },
                "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task."
                },
                "authors": [
                    {
                        "name": "Evangelia Gogoulou"
                    },
                    {
                        "name": "Shorouq Zahra"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Luise Dürlich"
                    },
                    {
                        "name": "Joakim Nivre"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Nivre"
                },
                "author": "Joakim Nivre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00938v2",
                "updated": "2025-04-29T12:16:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    16,
                    58,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-01T19:06:24Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    19,
                    6,
                    24,
                    6,
                    336,
                    0
                ],
                "title": "WR + O binaries as probes of the first phase of mass transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WR + O binaries as probes of the first phase of mass transfer"
                },
                "summary": "Context. Wolf-Rayet (WR) and O-star binaries can be the progenitors of X-ray\nbinaries and double black hole binaries. Their formation is not yet fully\nunderstood, however. For 21 observed WR+O systems, we aim to infer whether the\nmass transfer started on the main sequence (Case A) or later (Case B). We also\ncalculated (limits on) the mass-transfer efficiency {\\beta}, that is, the\nfraction of transferred mass that is accreted, and the parameter {\\gamma},\nwhich denotes the fraction of angular momentum of the binary that is lost per\nunit mass in units of the average angular momentum of the binary per unit mass.\nAims. We inferred the possible values for the initial masses based on the\nobserved WR masses and models for WR from the literature. With these initial\nprimary masses, we created a grid of possible periods and secondary masses for\nwhich we determined the values that {\\beta} and {\\gamma} would have taken for\neither Case A or Case B mass transfer. Based on this, we also determined the\ncase of mass transfer that is most likely for each system. Methods. Taking into\naccount the progenitor distribution of WR+O binaries, we find that highly\nnon-conservative Case A mass transfer seems to be the most likely scenario for\nthe majority of systems as this can explain 14 out of 21 systems. The angular\nmomentum loss is likely relatively high (typically {\\gamma} > 1). Our finding\nthat most systems in our sample experienced Case A mass transfer contradicts\nthe expectation that most massive binaries go through Case B mass transfer.\nThis suggests that post-case-B systems are significantly underrepresented in\nthe observed WR+O binary population, either intrinsically or due to severe\nselection effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Wolf-Rayet (WR) and O-star binaries can be the progenitors of X-ray\nbinaries and double black hole binaries. Their formation is not yet fully\nunderstood, however. For 21 observed WR+O systems, we aim to infer whether the\nmass transfer started on the main sequence (Case A) or later (Case B). We also\ncalculated (limits on) the mass-transfer efficiency {\\beta}, that is, the\nfraction of transferred mass that is accreted, and the parameter {\\gamma},\nwhich denotes the fraction of angular momentum of the binary that is lost per\nunit mass in units of the average angular momentum of the binary per unit mass.\nAims. We inferred the possible values for the initial masses based on the\nobserved WR masses and models for WR from the literature. With these initial\nprimary masses, we created a grid of possible periods and secondary masses for\nwhich we determined the values that {\\beta} and {\\gamma} would have taken for\neither Case A or Case B mass transfer. Based on this, we also determined the\ncase of mass transfer that is most likely for each system. Methods. Taking into\naccount the progenitor distribution of WR+O binaries, we find that highly\nnon-conservative Case A mass transfer seems to be the most likely scenario for\nthe majority of systems as this can explain 14 out of 21 systems. The angular\nmomentum loss is likely relatively high (typically {\\gamma} > 1). Our finding\nthat most systems in our sample experienced Case A mass transfer contradicts\nthe expectation that most massive binaries go through Case B mass transfer.\nThis suggests that post-case-B systems are significantly underrepresented in\nthe observed WR+O binary population, either intrinsically or due to severe\nselection effects."
                },
                "authors": [
                    {
                        "name": "Marit Nuijten"
                    },
                    {
                        "name": "Gijs Nelemans"
                    }
                ],
                "author_detail": {
                    "name": "Gijs Nelemans"
                },
                "arxiv_affiliation": "SRON",
                "author": "Gijs Nelemans",
                "arxiv_doi": "10.1051/0004-6361/202451564",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202451564",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.00938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted version",
                "arxiv_journal_ref": "A&A 695, A117 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20690v1",
                "updated": "2025-04-29T12:14:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    14,
                    47,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:14:47Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    14,
                    47,
                    1,
                    119,
                    0
                ],
                "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Edit: Enabling Instructional Image Editing with In-Context\n  Generation in Large Scale Diffusion Transformer"
                },
                "summary": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/."
                },
                "authors": [
                    {
                        "name": "Zechuan Zhang"
                    },
                    {
                        "name": "Ji Xie"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Zongxin Yang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Project Page: https://river-zhang.github.io/ICEdit-gh-pages/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20686v1",
                "updated": "2025-04-29T12:10:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    10,
                    26,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:10:26Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    10,
                    26,
                    1,
                    119,
                    0
                ],
                "title": "Inference of high-dimensional weak instrumental variable regression\n  models without ridge-regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of high-dimensional weak instrumental variable regression\n  models without ridge-regularization"
                },
                "summary": "Inference of instrumental variable regression models with many weak\ninstruments attracts many attentions recently. To extend the classical\nAnderson-Rubin test to high-dimensional setting, many procedures adopt\nridge-regularization. However, we show that it is not necessary to consider\nridge-regularization. Actually we propose a new quadratic-type test statistic\nwhich does not involve tuning parameters. Our quadratic-type test exhibits high\npower against dense alternatives. While for sparse alternatives, we derive the\nasymptotic distribution of an existing maximum-type test, enabling the use of\nless conservative critical values. To achieve strong performance across a wide\nrange of scenarios, we further introduce a combined test procedure that\nintegrates the strengths of both approaches. This combined procedure is\npowerful without requiring prior knowledge of the underlying sparsity of the\nfirst-stage model. Compared to existing methods, our proposed tests are easy to\nimplement, free of tuning parameters, and robust to arbitrarily weak\ninstruments as well as heteroskedastic errors. Simulation studies and empirical\napplications demonstrate the advantages of our methods over existing\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of instrumental variable regression models with many weak\ninstruments attracts many attentions recently. To extend the classical\nAnderson-Rubin test to high-dimensional setting, many procedures adopt\nridge-regularization. However, we show that it is not necessary to consider\nridge-regularization. Actually we propose a new quadratic-type test statistic\nwhich does not involve tuning parameters. Our quadratic-type test exhibits high\npower against dense alternatives. While for sparse alternatives, we derive the\nasymptotic distribution of an existing maximum-type test, enabling the use of\nless conservative critical values. To achieve strong performance across a wide\nrange of scenarios, we further introduce a combined test procedure that\nintegrates the strengths of both approaches. This combined procedure is\npowerful without requiring prior knowledge of the underlying sparsity of the\nfirst-stage model. Compared to existing methods, our proposed tests are easy to\nimplement, free of tuning parameters, and robust to arbitrarily weak\ninstruments as well as heteroskedastic errors. Simulation studies and empirical\napplications demonstrate the advantages of our methods over existing\napproaches."
                },
                "authors": [
                    {
                        "name": "Jiarong Ding"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Yanmei Shi"
                    },
                    {
                        "name": "Yuxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxin Wang"
                },
                "author": "Yuxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20684v1",
                "updated": "2025-04-29T12:07:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    7,
                    39,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:07:39Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    7,
                    39,
                    1,
                    119,
                    0
                ],
                "title": "Identifying Uncertainty in Self-Adaptive Robotics with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Uncertainty in Self-Adaptive Robotics with Large Language\n  Models"
                },
                "summary": "Future self-adaptive robots are expected to operate in highly dynamic\nenvironments while effectively managing uncertainties. However, identifying the\nsources and impacts of uncertainties in such robotic systems and defining\nappropriate mitigation strategies is challenging due to the inherent complexity\nof self-adaptive robots and the lack of comprehensive knowledge about the\nvarious factors influencing uncertainty. Hence, practitioners often rely on\nintuition and past experiences from similar systems to address uncertainties.\nIn this article, we evaluate the potential of large language models (LLMs) in\nenabling a systematic and automated approach to identify uncertainties in\nself-adaptive robotics throughout the software engineering lifecycle. For this\nevaluation, we analyzed 10 advanced LLMs with varying capabilities across four\nindustrial-sized robotics case studies, gathering the practitioners'\nperspectives on the LLM-generated responses related to uncertainties. Results\nshowed that practitioners agreed with 63-88% of the LLM responses and expressed\nstrong interest in the practicality of LLMs for this purpose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future self-adaptive robots are expected to operate in highly dynamic\nenvironments while effectively managing uncertainties. However, identifying the\nsources and impacts of uncertainties in such robotic systems and defining\nappropriate mitigation strategies is challenging due to the inherent complexity\nof self-adaptive robots and the lack of comprehensive knowledge about the\nvarious factors influencing uncertainty. Hence, practitioners often rely on\nintuition and past experiences from similar systems to address uncertainties.\nIn this article, we evaluate the potential of large language models (LLMs) in\nenabling a systematic and automated approach to identify uncertainties in\nself-adaptive robotics throughout the software engineering lifecycle. For this\nevaluation, we analyzed 10 advanced LLMs with varying capabilities across four\nindustrial-sized robotics case studies, gathering the practitioners'\nperspectives on the LLM-generated responses related to uncertainties. Results\nshowed that practitioners agreed with 63-88% of the LLM responses and expressed\nstrong interest in the practicality of LLMs for this purpose."
                },
                "authors": [
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Jalil Boudjadar"
                    },
                    {
                        "name": "Mirgita Frasheri"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Peter Gorm Larsen"
                    }
                ],
                "author_detail": {
                    "name": "Peter Gorm Larsen"
                },
                "author": "Peter Gorm Larsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19373v2",
                "updated": "2025-04-29T12:00:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    0,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-27T22:26:45Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    26,
                    45,
                    6,
                    117,
                    0
                ],
                "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for\n  Agentic Multi-Modal Large Reasoning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for\n  Agentic Multi-Modal Large Reasoning Model"
                },
                "summary": "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery."
                },
                "authors": [
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Qiming Zhang"
                    },
                    {
                        "name": "Tianyu Lu"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20673v1",
                "updated": "2025-04-29T11:57:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    57,
                    23,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:57:23Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    57,
                    23,
                    1,
                    119,
                    0
                ],
                "title": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language\n  Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language\n  Model Evaluation"
                },
                "summary": "Large language models (LLMs) play a crucial role in software engineering,\nexcelling in tasks like code generation and maintenance. However, existing\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\ncomprehensive evaluation framework that reflects real-world applications. To\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\ncode generation, code modification, and code review. These dimensions capture\nessential developer needs, ensuring a more systematic and representative\nevaluation. CoCo-Bench includes multiple programming languages and varying task\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\nuncovering significant variations in model performance, effectively\nhighlighting strengths and weaknesses. By offering a holistic and objective\nevaluation, CoCo-Bench provides valuable insights to guide future research and\ntechnological advancements in code-oriented LLMs, establishing a reliable\nbenchmark for the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) play a crucial role in software engineering,\nexcelling in tasks like code generation and maintenance. However, existing\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\ncomprehensive evaluation framework that reflects real-world applications. To\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\ncode generation, code modification, and code review. These dimensions capture\nessential developer needs, ensuring a more systematic and representative\nevaluation. CoCo-Bench includes multiple programming languages and varying task\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\nuncovering significant variations in model performance, effectively\nhighlighting strengths and weaknesses. By offering a holistic and objective\nevaluation, CoCo-Bench provides valuable insights to guide future research and\ntechnological advancements in code-oriented LLMs, establishing a reliable\nbenchmark for the field."
                },
                "authors": [
                    {
                        "name": "Wenjing Yin"
                    },
                    {
                        "name": "Tianze Sun"
                    },
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Jiawei Fang"
                    },
                    {
                        "name": "Guangyao Su"
                    },
                    {
                        "name": "Jiancheng Wang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ziyun Dai"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Menghang Dong"
                    },
                    {
                        "name": "Peng Luo"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Da Lei"
                    },
                    {
                        "name": "Yajun Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xiang Ma"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Yuanjian Xu"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "Submitted to ACL 2025. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00816v2",
                "updated": "2025-04-29T11:57:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    57,
                    11,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-01T14:05:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    5,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images"
                },
                "summary": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging."
                },
                "authors": [
                    {
                        "name": "Yeqi Fang"
                    },
                    {
                        "name": "Rong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Rong Zhou"
                },
                "author": "Rong Zhou",
                "arxiv_comment": "20 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20668v1",
                "updated": "2025-04-29T11:49:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    49,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:49:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    49,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and\n  Retrieving Claims from Social Media Platforms in Multiple Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and\n  Retrieving Claims from Social Media Platforms in Multiple Languages"
                },
                "summary": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Martin Hyben"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Michal Gregor"
                    },
                    {
                        "name": "Jakub Simko"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Simko"
                },
                "author": "Jakub Simko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02467v2",
                "updated": "2025-04-29T11:33:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    33,
                    25,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-03T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    10,
                    9,
                    1,
                    338,
                    0
                ],
                "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators"
                },
                "summary": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage."
                },
                "authors": [
                    {
                        "name": "Tejumade Afonja"
                    },
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Raouf Kerkouche"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_journal_ref": "Transactions on Machine Learning Research (03/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; G.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20653v1",
                "updated": "2025-04-29T11:22:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    22,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:22:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    22,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of\n  Complex Verilog Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of\n  Complex Verilog Code"
                },
                "summary": "Recent advances have demonstrated the promising capabilities of large\nlanguage models (LLMs) in generating register-transfer level (RTL) code, such\nas Verilog. However, existing LLM-based frameworks still face significant\nchallenges in accurately handling the complexity of real-world RTL designs,\nparticularly those that are large-scale and involve multi-level module\ninstantiations. To address this issue, we present ComplexVCoder, an open-source\nLLM-driven framework that enhances both the generation quality and efficiency\nof complex Verilog code. Specifically, we introduce a two-stage generation\nmechanism, which leverages an intermediate representation to enable a more\naccurate and structured transition from natural language descriptions to\nintricate Verilog designs. In addition, we introduce a rule-based alignment\nmethod and a domain-specific retrieval-augmented generation (RAG) to further\nimprove the correctness of the synthesized code by incorporating relevant\ndesign knowledge during generation. To evaluate our approach, we construct a\ncomprehensive dataset comprising 55 complex Verilog designs derived from\nreal-world implementations. We also release an open-source benchmark suite for\nsystematically assessing the quality of auto-generated RTL code together with\nthe ComplexVCoder framework. Experimental results show that ComplexVCoder\noutperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%,\nrespectively, in terms of function correctness on complex Verilog benchmarks.\nFurthermore, ComplexVcoder achieves comparable generation performances in terms\nof functionality correctness using a lightweight 32B model (Qwen2.5), rivaling\nlarger-scale models such as GPT-3.5 and DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have demonstrated the promising capabilities of large\nlanguage models (LLMs) in generating register-transfer level (RTL) code, such\nas Verilog. However, existing LLM-based frameworks still face significant\nchallenges in accurately handling the complexity of real-world RTL designs,\nparticularly those that are large-scale and involve multi-level module\ninstantiations. To address this issue, we present ComplexVCoder, an open-source\nLLM-driven framework that enhances both the generation quality and efficiency\nof complex Verilog code. Specifically, we introduce a two-stage generation\nmechanism, which leverages an intermediate representation to enable a more\naccurate and structured transition from natural language descriptions to\nintricate Verilog designs. In addition, we introduce a rule-based alignment\nmethod and a domain-specific retrieval-augmented generation (RAG) to further\nimprove the correctness of the synthesized code by incorporating relevant\ndesign knowledge during generation. To evaluate our approach, we construct a\ncomprehensive dataset comprising 55 complex Verilog designs derived from\nreal-world implementations. We also release an open-source benchmark suite for\nsystematically assessing the quality of auto-generated RTL code together with\nthe ComplexVCoder framework. Experimental results show that ComplexVCoder\noutperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%,\nrespectively, in terms of function correctness on complex Verilog benchmarks.\nFurthermore, ComplexVcoder achieves comparable generation performances in terms\nof functionality correctness using a lightweight 32B model (Qwen2.5), rivaling\nlarger-scale models such as GPT-3.5 and DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "Jian Zuo"
                    },
                    {
                        "name": "Junzhe Liu"
                    },
                    {
                        "name": "Xianyong Wang"
                    },
                    {
                        "name": "Yicheng Liu"
                    },
                    {
                        "name": "Navya Goli"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Umamaheswara Rao Tida"
                    },
                    {
                        "name": "Zhenge Jia"
                    },
                    {
                        "name": "Mengying Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mengying Zhao"
                },
                "author": "Mengying Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20644v1",
                "updated": "2025-04-29T11:13:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    18,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:13:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    18,
                    1,
                    119,
                    0
                ],
                "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified\n  File Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified\n  File Selection"
                },
                "summary": "Selecting high-quality pre-training data for large language models (LLMs) is\ncrucial for enhancing their overall performance under limited computation\nbudget, improving both training and sample efficiency. Recent advancements in\nfile selection primarily rely on using an existing or trained proxy model to\nassess the similarity of samples to a target domain, such as high quality\nsources BookCorpus and Wikipedia. However, upon revisiting these methods, the\ndomain-similarity selection criteria demonstrates a diversity dilemma,\ni.e.dimensional collapse in the feature space, improving performance on the\ndomain-related tasks but causing severe degradation on generic performance. To\nprevent collapse and enhance diversity, we propose a DiverSified File selection\nalgorithm (DiSF), which selects the most decorrelated text files in the feature\nspace. We approach this with a classical greedy algorithm to achieve more\nuniform eigenvalues in the feature covariance matrix of the selected texts,\nanalyzing its approximation to the optimal solution under a formulation of\n$\\gamma$-weakly submodular optimization problem. Empirically, we establish a\nbenchmark and conduct extensive experiments on the TinyLlama architecture with\nmodels from 120M to 1.1B parameters. Evaluating across nine tasks from the\nHarness framework, DiSF demonstrates a significant improvement on overall\nperformance. Specifically, DiSF saves 98.5% of 590M training files in\nSlimPajama, outperforming the full-data pre-training within a 50B training\nbudget, and achieving about 1.5x training efficiency and 5x data efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting high-quality pre-training data for large language models (LLMs) is\ncrucial for enhancing their overall performance under limited computation\nbudget, improving both training and sample efficiency. Recent advancements in\nfile selection primarily rely on using an existing or trained proxy model to\nassess the similarity of samples to a target domain, such as high quality\nsources BookCorpus and Wikipedia. However, upon revisiting these methods, the\ndomain-similarity selection criteria demonstrates a diversity dilemma,\ni.e.dimensional collapse in the feature space, improving performance on the\ndomain-related tasks but causing severe degradation on generic performance. To\nprevent collapse and enhance diversity, we propose a DiverSified File selection\nalgorithm (DiSF), which selects the most decorrelated text files in the feature\nspace. We approach this with a classical greedy algorithm to achieve more\nuniform eigenvalues in the feature covariance matrix of the selected texts,\nanalyzing its approximation to the optimal solution under a formulation of\n$\\gamma$-weakly submodular optimization problem. Empirically, we establish a\nbenchmark and conduct extensive experiments on the TinyLlama architecture with\nmodels from 120M to 1.1B parameters. Evaluating across nine tasks from the\nHarness framework, DiSF demonstrates a significant improvement on overall\nperformance. Specifically, DiSF saves 98.5% of 590M training files in\nSlimPajama, outperforming the full-data pre-training within a 50B training\nbudget, and achieving about 1.5x training efficiency and 5x data efficiency."
                },
                "authors": [
                    {
                        "name": "Ziqing Fan"
                    },
                    {
                        "name": "Siyuan Du"
                    },
                    {
                        "name": "Shengchao Hu"
                    },
                    {
                        "name": "Pingjie Wang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20643v1",
                "updated": "2025-04-29T11:13:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:13:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM\n  Creativity through Structured Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM\n  Creativity through Structured Representations"
                },
                "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI."
                },
                "authors": [
                    {
                        "name": "Moran Mizrahi"
                    },
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Dafna Shahaf"
                    }
                ],
                "author_detail": {
                    "name": "Dafna Shahaf"
                },
                "author": "Dafna Shahaf",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19013v2",
                "updated": "2025-04-29T11:00:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    0,
                    53,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-26T19:58:21Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    19,
                    58,
                    21,
                    5,
                    116,
                    0
                ],
                "title": "\\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed\n  Neural Networks"
                },
                "summary": "Physics-Informed Neural Networks (PINNs) are a novel computational approach\nfor solving partial differential equations (PDEs) with noisy and sparse initial\nand boundary data. Although, efficient quantification of epistemic and\naleatoric uncertainties in big multi-scale problems remains challenging. We\npropose \\$PINN a novel method of computing global uncertainty in PDEs using a\nBayesian framework, by combining local Bayesian Physics-Informed Neural\nNetworks (BPINN) with domain decomposition. The solution continuity across\nsubdomains is obtained by imposing the flux continuity across the interface of\nneighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct\na series of computational experiments on PDEs in 1D and 2D spatial domains.\nAlthough we have adopted conservative PINNs (cPINNs), the method can be\nseamlessly extended to other domain decomposition techniques. The results infer\nthat the proposed method recovers the global uncertainty by computing the local\nuncertainty exactly more efficiently as the uncertainty in each subdomain can\nbe computed concurrently. The robustness of \\$PINN is verified by adding\nuncorrelated random noise to the training data up to 15% and testing for\ndifferent domain sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Neural Networks (PINNs) are a novel computational approach\nfor solving partial differential equations (PDEs) with noisy and sparse initial\nand boundary data. Although, efficient quantification of epistemic and\naleatoric uncertainties in big multi-scale problems remains challenging. We\npropose \\$PINN a novel method of computing global uncertainty in PDEs using a\nBayesian framework, by combining local Bayesian Physics-Informed Neural\nNetworks (BPINN) with domain decomposition. The solution continuity across\nsubdomains is obtained by imposing the flux continuity across the interface of\nneighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct\na series of computational experiments on PDEs in 1D and 2D spatial domains.\nAlthough we have adopted conservative PINNs (cPINNs), the method can be\nseamlessly extended to other domain decomposition techniques. The results infer\nthat the proposed method recovers the global uncertainty by computing the local\nuncertainty exactly more efficiently as the uncertainty in each subdomain can\nbe computed concurrently. The robustness of \\$PINN is verified by adding\nuncorrelated random noise to the training data up to 15% and testing for\ndifferent domain sizes."
                },
                "authors": [
                    {
                        "name": "Júlia Vicens Figueres"
                    },
                    {
                        "name": "Juliette Vanderhaeghen"
                    },
                    {
                        "name": "Federica Bragone"
                    },
                    {
                        "name": "Kateryna Morozovska"
                    },
                    {
                        "name": "Khemraj Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Khemraj Shukla"
                },
                "author": "Khemraj Shukla",
                "arxiv_comment": "37 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04530v3",
                "updated": "2025-04-29T11:00:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    0,
                    46,
                    1,
                    119,
                    0
                ],
                "published": "2024-07-05T14:18:07Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    14,
                    18,
                    7,
                    4,
                    187,
                    0
                ],
                "title": "A spatial-correlated multitask linear mixed-effects model for imaging\n  genetics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A spatial-correlated multitask linear mixed-effects model for imaging\n  genetics"
                },
                "summary": "Imaging genetics aims to uncover the hidden relationship between imaging\nquantitative traits (QTs) and genetic markers (e.g. single nucleotide\npolymorphism (SNP)), and brings valuable insights into the pathogenesis of\ncomplex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's\nDisease). However, most linear models in imaging genetics didn't explicitly\nmodel the inner relationship among QTs, which might miss some potential\nefficiency gains from information borrowing across brain regions. In this work,\nwe developed a novel Bayesian regression framework for identifying significant\nassociations between QTs and genetic markers while explicitly modeling spatial\ndependency between QTs, with the main contributions as follows. Firstly, we\ndeveloped a spatial-correlated multitask linear mixed-effects model (LMM) to\naccount for dependencies between QTs. We incorporated a population-level mixed\neffects term into the model, taking full advantage of the dependent structure\nof brain imaging-derived QTs. Secondly, we implemented the model in the\nBayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to\nachieve the model inference. Further, we incorporated the MCMC samples with the\nCauchy combination test (CCT) to examine the association between SNPs and QTs,\nwhich avoided computationally intractable multi-test issues. The simulation\nstudies indicated improved power of our proposed model compared to classic\nmodels where inner dependencies of QTs were not modeled. We also applied the\nnew spatial model to an imaging dataset obtained from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imaging genetics aims to uncover the hidden relationship between imaging\nquantitative traits (QTs) and genetic markers (e.g. single nucleotide\npolymorphism (SNP)), and brings valuable insights into the pathogenesis of\ncomplex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's\nDisease). However, most linear models in imaging genetics didn't explicitly\nmodel the inner relationship among QTs, which might miss some potential\nefficiency gains from information borrowing across brain regions. In this work,\nwe developed a novel Bayesian regression framework for identifying significant\nassociations between QTs and genetic markers while explicitly modeling spatial\ndependency between QTs, with the main contributions as follows. Firstly, we\ndeveloped a spatial-correlated multitask linear mixed-effects model (LMM) to\naccount for dependencies between QTs. We incorporated a population-level mixed\neffects term into the model, taking full advantage of the dependent structure\nof brain imaging-derived QTs. Secondly, we implemented the model in the\nBayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to\nachieve the model inference. Further, we incorporated the MCMC samples with the\nCauchy combination test (CCT) to examine the association between SNPs and QTs,\nwhich avoided computationally intractable multi-test issues. The simulation\nstudies indicated improved power of our proposed model compared to classic\nmodels where inner dependencies of QTs were not modeled. We also applied the\nnew spatial model to an imaging dataset obtained from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database."
                },
                "authors": [
                    {
                        "name": "Zhibin Pu"
                    },
                    {
                        "name": "Shufei Ge"
                    }
                ],
                "author_detail": {
                    "name": "Shufei Ge"
                },
                "author": "Shufei Ge",
                "arxiv_comment": "32 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08 (Primary) 62J05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20628v1",
                "updated": "2025-04-29T10:55:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    55,
                    40,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:55:40Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    55,
                    40,
                    1,
                    119,
                    0
                ],
                "title": "Cognitive maps are generative programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive maps are generative programs"
                },
                "summary": "Making sense of the world and acting in it relies on building simplified\nmental representations that abstract away aspects of reality. This principle of\ncognitive mapping is universal to agents with limited resources. Living\norganisms, people, and algorithms all face the problem of forming functional\nrepresentations of their world under various computing constraints. In this\nwork, we explore the hypothesis that human resource-efficient planning may\narise from representing the world as predictably structured. Building on the\nmetaphor of concepts as programs, we propose that cognitive maps can take the\nform of generative programs that exploit predictability and redundancy, in\ncontrast to directly encoding spatial layouts. We use a behavioral experiment\nto show that people who navigate in structured spaces rely on modular planning\nstrategies that align with programmatic map representations. We describe a\ncomputational model that predicts human behavior in a variety of structured\nscenarios. This model infers a small distribution over possible programmatic\ncognitive maps conditioned on human prior knowledge of the world, and uses this\ndistribution to generate resource-efficient plans. Our models leverages a Large\nLanguage Model as an embedding of human priors, implicitly learned through\ntraining on a vast corpus of human data. Our model demonstrates improved\ncomputational efficiency, requires drastically less memory, and outperforms\nunstructured planning algorithms with cognitive constraints at predicting human\nbehavior, suggesting that human planning strategies rely on programmatic\ncognitive maps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making sense of the world and acting in it relies on building simplified\nmental representations that abstract away aspects of reality. This principle of\ncognitive mapping is universal to agents with limited resources. Living\norganisms, people, and algorithms all face the problem of forming functional\nrepresentations of their world under various computing constraints. In this\nwork, we explore the hypothesis that human resource-efficient planning may\narise from representing the world as predictably structured. Building on the\nmetaphor of concepts as programs, we propose that cognitive maps can take the\nform of generative programs that exploit predictability and redundancy, in\ncontrast to directly encoding spatial layouts. We use a behavioral experiment\nto show that people who navigate in structured spaces rely on modular planning\nstrategies that align with programmatic map representations. We describe a\ncomputational model that predicts human behavior in a variety of structured\nscenarios. This model infers a small distribution over possible programmatic\ncognitive maps conditioned on human prior knowledge of the world, and uses this\ndistribution to generate resource-efficient plans. Our models leverages a Large\nLanguage Model as an embedding of human priors, implicitly learned through\ntraining on a vast corpus of human data. Our model demonstrates improved\ncomputational efficiency, requires drastically less memory, and outperforms\nunstructured planning algorithms with cognitive constraints at predicting human\nbehavior, suggesting that human planning strategies rely on programmatic\ncognitive maps."
                },
                "authors": [
                    {
                        "name": "Marta Kryven"
                    },
                    {
                        "name": "Cole Wyeth"
                    },
                    {
                        "name": "Aidan Curtis"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "arxiv_comment": "9 pages, 4 figures, to be published in Cognitive Sciences Society\n  proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20624v1",
                "updated": "2025-04-29T10:51:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    51,
                    58,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:51:58Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    51,
                    58,
                    1,
                    119,
                    0
                ],
                "title": "PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time\n  Retrieval"
                },
                "summary": "Social chatbots have become essential intelligent companions in daily\nscenarios ranging from emotional support to personal interaction. However,\nconventional chatbots with passive response mechanisms usually rely on users to\ninitiate or sustain dialogues by bringing up new topics, resulting in\ndiminished engagement and shortened dialogue duration. In this paper, we\npresent PaRT, a novel framework enabling context-aware proactive dialogues for\nsocial chatbots through personalized real-time retrieval and generation.\nSpecifically, PaRT first integrates user profiles and dialogue context into a\nlarge language model (LLM), which is initially prompted to refine user queries\nand recognize their underlying intents for the upcoming conversation. Guided by\nrefined intents, the LLM generates personalized dialogue topics, which then\nserve as targeted queries to retrieve relevant passages from RedNote. Finally,\nwe prompt LLMs with summarized passages to generate knowledge-grounded and\nengagement-optimized responses. Our approach has been running stably in a\nreal-world production environment for more than 30 days, achieving a 21.77\\%\nimprovement in the average duration of dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social chatbots have become essential intelligent companions in daily\nscenarios ranging from emotional support to personal interaction. However,\nconventional chatbots with passive response mechanisms usually rely on users to\ninitiate or sustain dialogues by bringing up new topics, resulting in\ndiminished engagement and shortened dialogue duration. In this paper, we\npresent PaRT, a novel framework enabling context-aware proactive dialogues for\nsocial chatbots through personalized real-time retrieval and generation.\nSpecifically, PaRT first integrates user profiles and dialogue context into a\nlarge language model (LLM), which is initially prompted to refine user queries\nand recognize their underlying intents for the upcoming conversation. Guided by\nrefined intents, the LLM generates personalized dialogue topics, which then\nserve as targeted queries to retrieve relevant passages from RedNote. Finally,\nwe prompt LLMs with summarized passages to generate knowledge-grounded and\nengagement-optimized responses. Our approach has been running stably in a\nreal-world production environment for more than 30 days, achieving a 21.77\\%\nimprovement in the average duration of dialogues."
                },
                "authors": [
                    {
                        "name": "Zihan Niu"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Chonggang Lu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16184v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16184v3",
                "updated": "2025-04-29T10:39:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    39,
                    35,
                    1,
                    119,
                    0
                ],
                "published": "2024-03-24T15:02:24Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    15,
                    2,
                    24,
                    6,
                    84,
                    0
                ],
                "title": "Predicate Debiasing in Vision-Language Models Integration for Scene\n  Graph Generation Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicate Debiasing in Vision-Language Models Integration for Scene\n  Graph Generation Enhancement"
                },
                "summary": "Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between\nobjects. This complexity and diversity in SGG leads to underrepresentation,\nwhere parts of triplet labels are rare or even unseen during training,\nresulting in imprecise predictions. To tackle this, we propose integrating the\npretrained Vision-language Models to enhance representation. However, due to\nthe gap between pretraining and SGG, direct inference of pretrained VLMs on SGG\nleads to severe bias, which stems from the imbalanced predicates distribution\nin the pretraining language set. To alleviate the bias, we introduce a novel LM\nEstimation to approximate the unattainable predicates distribution. Finally, we\nensemble the debiased VLMs with SGG models to enhance the representation, where\nwe design a certainty-aware indicator to score each sample and dynamically\nadjust the ensemble weights. Our training-free method effectively addresses the\npredicates bias in pretrained VLMs, enhances SGG's representation, and\nsignificantly improve the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between\nobjects. This complexity and diversity in SGG leads to underrepresentation,\nwhere parts of triplet labels are rare or even unseen during training,\nresulting in imprecise predictions. To tackle this, we propose integrating the\npretrained Vision-language Models to enhance representation. However, due to\nthe gap between pretraining and SGG, direct inference of pretrained VLMs on SGG\nleads to severe bias, which stems from the imbalanced predicates distribution\nin the pretraining language set. To alleviate the bias, we introduce a novel LM\nEstimation to approximate the unattainable predicates distribution. Finally, we\nensemble the debiased VLMs with SGG models to enhance the representation, where\nwe design a certainty-aware indicator to score each sample and dynamically\nadjust the ensemble weights. Our training-free method effectively addresses the\npredicates bias in pretrained VLMs, enhances SGG's representation, and\nsignificantly improve the performance."
                },
                "authors": [
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyuan Liu"
                },
                "author": "Xiaoyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16184v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16184v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20612v1",
                "updated": "2025-04-29T10:23:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    23,
                    11,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:23:11Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    23,
                    11,
                    1,
                    119,
                    0
                ],
                "title": "The Hidden Risks of LLM-Generated Web Application Code: A\n  Security-Centric Evaluation of Code Generation Capabilities in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Risks of LLM-Generated Web Application Code: A\n  Security-Centric Evaluation of Code Generation Capabilities in Large Language\n  Models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications."
                },
                "authors": [
                    {
                        "name": "Swaroop Dora"
                    },
                    {
                        "name": "Deven Lunkad"
                    },
                    {
                        "name": "Naziya Aslam"
                    },
                    {
                        "name": "S. Venkatesan"
                    },
                    {
                        "name": "Sandeep Kumar Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Kumar Shukla"
                },
                "author": "Sandeep Kumar Shukla",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20610v1",
                "updated": "2025-04-29T10:21:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    21,
                    40,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:21:40Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    21,
                    40,
                    1,
                    119,
                    0
                ],
                "title": "Information Retrieval in the Age of Generative AI: The RGB Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Retrieval in the Age of Generative AI: The RGB Model"
                },
                "summary": "The advent of Large Language Models (LLMs) and generative AI is fundamentally\ntransforming information retrieval and processing on the Internet, bringing\nboth great potential and significant concerns regarding content authenticity\nand reliability. This paper presents a novel quantitative approach to shed\nlight on the complex information dynamics arising from the growing use of\ngenerative AI tools. Despite their significant impact on the digital ecosystem,\nthese dynamics remain largely uncharted and poorly understood. We propose a\nstochastic model to characterize the generation, indexing, and dissemination of\ninformation in response to new topics. This scenario particularly challenges\ncurrent LLMs, which often rely on real-time Retrieval-Augmented Generation\n(RAG) techniques to overcome their static knowledge limitations. Our findings\nsuggest that the rapid pace of generative AI adoption, combined with increasing\nuser reliance, can outpace human verification, escalating the risk of\ninaccurate information proliferation across digital resources. An in-depth\nanalysis of Stack Exchange data confirms that high-quality answers inevitably\nrequire substantial time and human effort to emerge. This underscores the\nconsiderable risks associated with generating persuasive text in response to\nnew questions and highlights the critical need for responsible development and\ndeployment of future generative AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) and generative AI is fundamentally\ntransforming information retrieval and processing on the Internet, bringing\nboth great potential and significant concerns regarding content authenticity\nand reliability. This paper presents a novel quantitative approach to shed\nlight on the complex information dynamics arising from the growing use of\ngenerative AI tools. Despite their significant impact on the digital ecosystem,\nthese dynamics remain largely uncharted and poorly understood. We propose a\nstochastic model to characterize the generation, indexing, and dissemination of\ninformation in response to new topics. This scenario particularly challenges\ncurrent LLMs, which often rely on real-time Retrieval-Augmented Generation\n(RAG) techniques to overcome their static knowledge limitations. Our findings\nsuggest that the rapid pace of generative AI adoption, combined with increasing\nuser reliance, can outpace human verification, escalating the risk of\ninaccurate information proliferation across digital resources. An in-depth\nanalysis of Stack Exchange data confirms that high-quality answers inevitably\nrequire substantial time and human effort to emerge. This underscores the\nconsiderable risks associated with generating persuasive text in response to\nnew questions and highlights the critical need for responsible development and\ndeployment of future generative AI tools."
                },
                "authors": [
                    {
                        "name": "Michele Garetto"
                    },
                    {
                        "name": "Alessandro Cornacchia"
                    },
                    {
                        "name": "Franco Galante"
                    },
                    {
                        "name": "Emilio Leonardi"
                    },
                    {
                        "name": "Alessandro Nordio"
                    },
                    {
                        "name": "Alberto Tarable"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Tarable"
                },
                "author": "Alberto Tarable",
                "arxiv_comment": "To be presented at ACM SIGIR 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20609v1",
                "updated": "2025-04-29T10:19:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    19,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:19:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    19,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WenyanGPT: A Large Language Model for Classical Chinese Tasks"
                },
                "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing."
                },
                "authors": [
                    {
                        "name": "Xinyu Yao"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaobing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobing Zhao"
                },
                "author": "Xiaobing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01070v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01070v3",
                "updated": "2025-04-29T10:17:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    17,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2025-02-03T05:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    26,
                    22,
                    0,
                    34,
                    0
                ],
                "title": "An Inquiry into Datacenter TCO for LLM Inference with FP8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Inquiry into Datacenter TCO for LLM Inference with FP8"
                },
                "summary": "As large language models (LLMs) continue to scale, their inference demands\npresent significant challenges, particularly due to the high power consumption\nof AI accelerators in datacenters. These facilities require specialized cooling\nand power management systems, substantially increasing the total cost of\nownership (TCO) for cloud service providers (CSPs). In this work, we analyze\nthe computational characteristics and constraints of LLM inference from a TCO\nperspective, focusing on two representative accelerators: the Gaudi 2 and\nNVIDIA H100. We present a generalizable framework that enables CSPs to compare\nand select AI accelerators according to diverse operational requirements. Using\nthis model, we analyze the impact of FP8 precision and LLM inference workload\ncharacteristics as key factors influencing TCO. We investigate FP8\nquantization, which is gaining adoption in LLM training, as a technique to\nimprove inference throughput while maintaining cost efficiency. Furthermore,\nour analysis of LLM inference workloads reveals that performance on thin GEMMs,\nwhich dominate the decode phase, can have a greater impact than theoretical\nhardware peak performance. By studying the interaction between power\nconsumption, quantization strategies, and hardware architecture, we offer\ninsights that support informed deployment decisions and guide future\naccelerator designs to improve the TCO of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, their inference demands\npresent significant challenges, particularly due to the high power consumption\nof AI accelerators in datacenters. These facilities require specialized cooling\nand power management systems, substantially increasing the total cost of\nownership (TCO) for cloud service providers (CSPs). In this work, we analyze\nthe computational characteristics and constraints of LLM inference from a TCO\nperspective, focusing on two representative accelerators: the Gaudi 2 and\nNVIDIA H100. We present a generalizable framework that enables CSPs to compare\nand select AI accelerators according to diverse operational requirements. Using\nthis model, we analyze the impact of FP8 precision and LLM inference workload\ncharacteristics as key factors influencing TCO. We investigate FP8\nquantization, which is gaining adoption in LLM training, as a technique to\nimprove inference throughput while maintaining cost efficiency. Furthermore,\nour analysis of LLM inference workloads reveals that performance on thin GEMMs,\nwhich dominate the decode phase, can have a greater impact than theoretical\nhardware peak performance. By studying the interaction between power\nconsumption, quantization strategies, and hardware architecture, we offer\ninsights that support informed deployment decisions and guide future\naccelerator designs to improve the TCO of LLM inference."
                },
                "authors": [
                    {
                        "name": "Jiwoo Kim"
                    },
                    {
                        "name": "Joonhyung Lee"
                    },
                    {
                        "name": "Gunho Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    },
                    {
                        "name": "Youngjoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Lee"
                },
                "author": "Youngjoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01070v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01070v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02166v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02166v5",
                "updated": "2025-04-29T09:54:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    54,
                    32,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-04T01:04:14Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    1,
                    4,
                    14,
                    1,
                    63,
                    0
                ],
                "title": "The impact of local noise recorded at the ET candidate sites on the\n  signal to noise ratio of CBC gravitational wave signals for the ET triangle\n  configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of local noise recorded at the ET candidate sites on the\n  signal to noise ratio of CBC gravitational wave signals for the ET triangle\n  configuration"
                },
                "summary": "We present an evaluation of how site dependent noise can affect the signal to\nnoise ratio (SNR) of compact binary coalescence (CBC) signals in the future 3rd\ngeneration gravitational wave (GW) detector Einstein Telescope (ET). The design\nof ET is currently pushing the scientific community to study its scientific\npotential with respect to known, and possibly unexpected, GW signals using its\ndesign sensitivity curves. However, local ambient noise may have an impact on\nthe ET sensitivity at low frequency and therefore affect the SNR of CBC signals\nat low frequency. Therefore, we study the impact of ambient noise on the ET\nsensitivity curve at the two sites candidate to host ET - Sardinia, in Italy,\nand the Euregio Meuse-Rhine (EMR) at the Netherlands-Belgium border - and infer\nthe impact on the ET sensitivity curve and how the SNR of CBC signals at low\nfrequencies is affected. We find that Sardinia shows results which are on par,\nif not better, than the design case. On the other hand, ambient noise for the\ncurrent EMR sensitivity curve in Terziet causes a higher degradation of the SNR\nperformances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an evaluation of how site dependent noise can affect the signal to\nnoise ratio (SNR) of compact binary coalescence (CBC) signals in the future 3rd\ngeneration gravitational wave (GW) detector Einstein Telescope (ET). The design\nof ET is currently pushing the scientific community to study its scientific\npotential with respect to known, and possibly unexpected, GW signals using its\ndesign sensitivity curves. However, local ambient noise may have an impact on\nthe ET sensitivity at low frequency and therefore affect the SNR of CBC signals\nat low frequency. Therefore, we study the impact of ambient noise on the ET\nsensitivity curve at the two sites candidate to host ET - Sardinia, in Italy,\nand the Euregio Meuse-Rhine (EMR) at the Netherlands-Belgium border - and infer\nthe impact on the ET sensitivity curve and how the SNR of CBC signals at low\nfrequencies is affected. We find that Sardinia shows results which are on par,\nif not better, than the design case. On the other hand, ambient noise for the\ncurrent EMR sensitivity curve in Terziet causes a higher degradation of the SNR\nperformances."
                },
                "authors": [
                    {
                        "name": "Matteo Di Giovanni"
                    },
                    {
                        "name": "Davide Rozza"
                    },
                    {
                        "name": "Rosario De Rosa"
                    },
                    {
                        "name": "Enrico Calloni"
                    },
                    {
                        "name": "Domenico D'Urso"
                    },
                    {
                        "name": "Luca Naticchioni"
                    },
                    {
                        "name": "Annalisa Allocca"
                    },
                    {
                        "name": "Giovanni Luca Cardello"
                    },
                    {
                        "name": "Alessandro Cardini"
                    },
                    {
                        "name": "Andrea Contu"
                    },
                    {
                        "name": "Giovanni Diaferia"
                    },
                    {
                        "name": "Luciano Errico"
                    },
                    {
                        "name": "Carlo Giunchi"
                    },
                    {
                        "name": "Jan Harms"
                    },
                    {
                        "name": "Irene Molinari"
                    },
                    {
                        "name": "Marco Olivieri"
                    },
                    {
                        "name": "Piero Rapagnani"
                    },
                    {
                        "name": "Fulvio Ricci"
                    },
                    {
                        "name": "Valeria Sipala"
                    },
                    {
                        "name": "Lucia Trozzo"
                    }
                ],
                "author_detail": {
                    "name": "Lucia Trozzo"
                },
                "author": "Lucia Trozzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02166v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02166v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20595v1",
                "updated": "2025-04-29T09:49:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    49,
                    28,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:49:28Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    49,
                    28,
                    1,
                    119,
                    0
                ],
                "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonIR: Training Retrievers for Reasoning Tasks"
                },
                "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model."
                },
                "authors": [
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Rui Qiao"
                    },
                    {
                        "name": "Varsha Kishore"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Sewon Min"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    }
                ],
                "author_detail": {
                    "name": "Luke Zettlemoyer"
                },
                "author": "Luke Zettlemoyer",
                "arxiv_comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20571v1",
                "updated": "2025-04-29T09:24:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    24,
                    30,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:24:30Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    24,
                    30,
                    1,
                    119,
                    0
                ],
                "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example"
                },
                "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR"
                },
                "authors": [
                    {
                        "name": "Yiping Wang"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Lucas Liu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Xuehai He"
                    },
                    {
                        "name": "Kuan Wang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Simon Shaolei Du"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13279v2",
                "updated": "2025-04-29T09:23:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    23,
                    40,
                    1,
                    119,
                    0
                ],
                "published": "2024-03-20T03:39:51Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    3,
                    39,
                    51,
                    2,
                    80,
                    0
                ],
                "title": "Specification Mining for Smart Contracts with Trace Slicing and\n  Predicate Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specification Mining for Smart Contracts with Trace Slicing and\n  Predicate Abstraction"
                },
                "summary": "Smart contracts are computer programs running on blockchains to implement\nDecentralized Applications. The absence of contract specifications hinders\nroutine tasks, such as contract understanding and testing. In this work, we\npropose a specification mining approach to infer contract specifications from\npast transaction histories. Our approach derives high-level behavioral automata\nof function invocations, accompanied by program invariants statistically\ninferred from the transaction histories. We implemented our approach as tool\nSMCON and evaluated it on eleven well-studied Azure benchmark smart contracts\nand six popular real-world DApp smart contracts. The experiments show that\nSMCON mines reasonably accurate specifications that can be used to enhance\nsymbolic analysis of smart contracts achieving higher code coverage and up to\n56% speedup, and facilitate DApp developers in maintaining high-quality\ndocumentation and test suites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are computer programs running on blockchains to implement\nDecentralized Applications. The absence of contract specifications hinders\nroutine tasks, such as contract understanding and testing. In this work, we\npropose a specification mining approach to infer contract specifications from\npast transaction histories. Our approach derives high-level behavioral automata\nof function invocations, accompanied by program invariants statistically\ninferred from the transaction histories. We implemented our approach as tool\nSMCON and evaluated it on eleven well-studied Azure benchmark smart contracts\nand six popular real-world DApp smart contracts. The experiments show that\nSMCON mines reasonably accurate specifications that can be used to enhance\nsymbolic analysis of smart contracts achieving higher code coverage and up to\n56% speedup, and facilitate DApp developers in maintaining high-quality\ndocumentation and test suites."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Yixuan Liu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Cyrille Artho"
                    }
                ],
                "author_detail": {
                    "name": "Cyrille Artho"
                },
                "author": "Cyrille Artho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20570v1",
                "updated": "2025-04-29T09:23:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    23,
                    19,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:23:19Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    23,
                    19,
                    1,
                    119,
                    0
                ],
                "title": "ReCIT: Reconstructing Full Private Data from Gradient in\n  Parameter-Efficient Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCIT: Reconstructing Full Private Data from Gradient in\n  Parameter-Efficient Fine-Tuning of Large Language Models"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution\nfor adapting large language models (LLMs) to custom datasets with significantly\nreduced computational cost. When carrying out PEFT under collaborative learning\nscenarios (e.g., federated learning), it is often required to exchange model\nupdates (or gradients) across parties. These gradients, even with limited\ndimensions, can cause severe breach of data privacy. Recent works have shown\nthat both contextual prefixes and personally identifiable information (PII) can\nbe exposed through gradients. However, \\emph{simultaneously} and\n\\emph{accurately} recovering both components from the same training instance\nremains infeasible due to the following challenges: 1) limited number of PEFT\nparameters; 2) high-dimensional token spaces; and 3) large batch sizes. We\npropose ReCIT, a novel privacy attack that addresses all challenges, and\nachieves recovery of \\emph{full} private data from PEFT gradients with high\nfidelity. Specifically, ReCIT proposes to enhance the memorization capability\nof the pre-trained model through malicious fine-tuning with Personal Notes;\nReCIT also proposes a novel filter-based token extraction technique and a token\npairing mechanism, to accurately reconstruct tokens from the training sequences\nwith large batch sizes. Extensive evaluations show that ReCIT consistently\noutperforms state-of-the-art gradient inversion and memorization-based attacks\nacross different PEFT paradigms. It achieves up to 10$\\times$ higher PII\nrecovery rates and remains effective across varying batch sizes, especially in\nsettings where prefix reconstruction is intractable for conventional\napproaches. These findings highlight an urgent need to reassess the privacy\nguarantees of PEFT, especially in decentralized or shared training\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution\nfor adapting large language models (LLMs) to custom datasets with significantly\nreduced computational cost. When carrying out PEFT under collaborative learning\nscenarios (e.g., federated learning), it is often required to exchange model\nupdates (or gradients) across parties. These gradients, even with limited\ndimensions, can cause severe breach of data privacy. Recent works have shown\nthat both contextual prefixes and personally identifiable information (PII) can\nbe exposed through gradients. However, \\emph{simultaneously} and\n\\emph{accurately} recovering both components from the same training instance\nremains infeasible due to the following challenges: 1) limited number of PEFT\nparameters; 2) high-dimensional token spaces; and 3) large batch sizes. We\npropose ReCIT, a novel privacy attack that addresses all challenges, and\nachieves recovery of \\emph{full} private data from PEFT gradients with high\nfidelity. Specifically, ReCIT proposes to enhance the memorization capability\nof the pre-trained model through malicious fine-tuning with Personal Notes;\nReCIT also proposes a novel filter-based token extraction technique and a token\npairing mechanism, to accurately reconstruct tokens from the training sequences\nwith large batch sizes. Extensive evaluations show that ReCIT consistently\noutperforms state-of-the-art gradient inversion and memorization-based attacks\nacross different PEFT paradigms. It achieves up to 10$\\times$ higher PII\nrecovery rates and remains effective across varying batch sizes, especially in\nsettings where prefix reconstruction is intractable for conventional\napproaches. These findings highlight an urgent need to reassess the privacy\nguarantees of PEFT, especially in decentralized or shared training\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Ruishi He"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shouling Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shouling Ji"
                },
                "author": "Shouling Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17593v2",
                "updated": "2025-04-29T09:21:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    21,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-23T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "title": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation"
                },
                "summary": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation."
                },
                "authors": [
                    {
                        "name": "Chengbing Wang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Tianhao Shi"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Accepted by WWW'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06892v2",
                "updated": "2025-04-29T08:57:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    57,
                    4,
                    1,
                    119,
                    0
                ],
                "published": "2024-07-09T14:22:27Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    14,
                    22,
                    27,
                    1,
                    191,
                    0
                ],
                "title": "When Knockoffs fail: diagnosing and fixing non-exchangeability of\n  Knockoffs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Knockoffs fail: diagnosing and fixing non-exchangeability of\n  Knockoffs"
                },
                "summary": "Knockoffs are a popular statistical framework that addresses the challenging\nproblem of conditional variable selection in high-dimensional settings with\nstatistical control. Such statistical control is essential for the reliability\nof inference. However, knockoff guarantees rely on an exchangeability\nassumption that is difficult to test in practice, and there is little\ndiscussion in the literature on how to deal with unfulfilled hypotheses. This\nassumption is related to the ability to generate data similar to the observed\ndata. To maintain reliable inference, we introduce a diagnostic tool based on\nClassifier Two-Sample Tests. Using simulations and real data, we show that\nviolations of this assumption occur in common settings for classical knockoff\ngenerators, especially when the data have a strong dependence structure. As a\nconsequence, knockoff-based inference suffers from a massive inflation of false\npositives. We show that the diagnostic tool correctly detects such behavior. We\nshow that an alternative knockoff construction, based on constructing a\npredictor of each variable based on all others, solves the issue. We also\npropose a computationally-efficient variant of this algorithm and show\nempirically that this approach restores error control on simulated data and\nsemi-simulated experiments based on neuroimaging data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knockoffs are a popular statistical framework that addresses the challenging\nproblem of conditional variable selection in high-dimensional settings with\nstatistical control. Such statistical control is essential for the reliability\nof inference. However, knockoff guarantees rely on an exchangeability\nassumption that is difficult to test in practice, and there is little\ndiscussion in the literature on how to deal with unfulfilled hypotheses. This\nassumption is related to the ability to generate data similar to the observed\ndata. To maintain reliable inference, we introduce a diagnostic tool based on\nClassifier Two-Sample Tests. Using simulations and real data, we show that\nviolations of this assumption occur in common settings for classical knockoff\ngenerators, especially when the data have a strong dependence structure. As a\nconsequence, knockoff-based inference suffers from a massive inflation of false\npositives. We show that the diagnostic tool correctly detects such behavior. We\nshow that an alternative knockoff construction, based on constructing a\npredictor of each variable based on all others, solves the issue. We also\npropose a computationally-efficient variant of this algorithm and show\nempirically that this approach restores error control on simulated data and\nsemi-simulated experiments based on neuroimaging data."
                },
                "authors": [
                    {
                        "name": "Alexandre Blain"
                    },
                    {
                        "name": "Angel Reyero Lobo"
                    },
                    {
                        "name": "Julia Linhart"
                    },
                    {
                        "name": "Bertrand Thirion"
                    },
                    {
                        "name": "Pierre Neuvial"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Neuvial"
                },
                "author": "Pierre Neuvial",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11819v3",
                "updated": "2025-04-29T08:56:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    56,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2024-10-15T17:46:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "title": "Mind the memory: Consistent time reversal removes artefactual scaling of\n  energy dissipation rate and provides more accurate and reliable thermodynamic\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the memory: Consistent time reversal removes artefactual scaling of\n  energy dissipation rate and provides more accurate and reliable thermodynamic\n  inference"
                },
                "summary": "It has been proposed that an observed inverse power-law dependence of the\nMarkovian estimate for the steady-state dissipation rate on the coarse-graining\nscale in self-similar networks reflects a scale-dependent energy dissipation.\nBy explicit examples, it is demonstrated here that there are in general no\nrelations between such an apparent power-law dependence and the actual\ndissipation on different length scales. We construct fractal networks with a\nsingle dissipative scale and networks with a true inverse energy-dissipation\ncascade, and show that they display the same scaling behavior. Moreover, we\nshow that a self-similar network structure does not imply an inverse power-law\nscaling but may be mistaken for one in practice. When no dissipative cycles\nbecome hidden by the coarse graining, any scale dependence of the dissipation\nestimate vanishes if the memory is correctly accounted for in the time-reversal\noperation. A $k$-th order estimator is derived and necessary and sufficient\nconditions are proved for a guaranteed lower bound on dissipation. These\nhigher-order estimators saturated in the order are proved to provide sharper\nlower bounds on dissipation and their scale dependence signifies hidden\ndissipative cycles. It is shown that estimators not saturated in the order may\nerroneously overestimate the microscopic dissipation. Our results underscore\nthe still underappreciated importance of correctly accounting for memory in\nanalyzing coarse observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been proposed that an observed inverse power-law dependence of the\nMarkovian estimate for the steady-state dissipation rate on the coarse-graining\nscale in self-similar networks reflects a scale-dependent energy dissipation.\nBy explicit examples, it is demonstrated here that there are in general no\nrelations between such an apparent power-law dependence and the actual\ndissipation on different length scales. We construct fractal networks with a\nsingle dissipative scale and networks with a true inverse energy-dissipation\ncascade, and show that they display the same scaling behavior. Moreover, we\nshow that a self-similar network structure does not imply an inverse power-law\nscaling but may be mistaken for one in practice. When no dissipative cycles\nbecome hidden by the coarse graining, any scale dependence of the dissipation\nestimate vanishes if the memory is correctly accounted for in the time-reversal\noperation. A $k$-th order estimator is derived and necessary and sufficient\nconditions are proved for a guaranteed lower bound on dissipation. These\nhigher-order estimators saturated in the order are proved to provide sharper\nlower bounds on dissipation and their scale dependence signifies hidden\ndissipative cycles. It is shown that estimators not saturated in the order may\nerroneously overestimate the microscopic dissipation. Our results underscore\nthe still underappreciated importance of correctly accounting for memory in\nanalyzing coarse observations."
                },
                "authors": [
                    {
                        "name": "Tassilo Schwarz"
                    },
                    {
                        "name": "Anatoly B. Kolomeisky"
                    },
                    {
                        "name": "Aljaž Godec"
                    }
                ],
                "author_detail": {
                    "name": "Aljaž Godec"
                },
                "author": "Aljaž Godec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15253v2",
                "updated": "2025-04-29T08:51:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    51,
                    57,
                    1,
                    119,
                    0
                ],
                "published": "2024-08-09T08:09:28Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    8,
                    9,
                    28,
                    4,
                    222,
                    0
                ],
                "title": "A Deep Generative Model for Five-Class Sleep Staging with Arbitrary\n  Sensor Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep Generative Model for Five-Class Sleep Staging with Arbitrary\n  Sensor Input"
                },
                "summary": "Gold-standard sleep scoring is based on epoch-based assignment of sleep\nstages based on a combination of EEG, EOG and EMG signals. However, a\npolysomnographic recording consists of many other signals that could be used\nfor sleep staging, including cardio-respiratory modalities. Leveraging this\nsignal variety would offer important advantages, for example increasing\nreliability, resilience to signal loss, and application to long-term\nnon-obtrusive recordings. We developed a deep generative model for automatic\nsleep staging from a plurality of sensors and any -- arbitrary -- combination\nthereof. We trained a score-based diffusion model using a dataset of 1947\nexpert-labelled overnight recordings with 36 different signals, and achieved\nzero-shot inference on any sensor set by leveraging a novel Bayesian\nfactorization of the score function across the sensors. On single-channel EEG,\nthe model reaches the performance limit in terms of polysomnography inter-rater\nagreement (5-class accuracy 85.6%, Cohen's kappa 0.791). Moreover, the method\noffers full flexibility to use any sensor set, for example finger\nphotoplethysmography, nasal flow and thoracic respiratory movements, (5-class\naccuracy 79.0%, Cohen's kappa of 0.697), or even derivations very\nunconventional for sleep staging, such as tibialis and sternocleidomastoid EMG\n(5-class accuracy 71.0%, kappa 0.575). Additionally, we propose a novel\ninterpretability metric in terms of information gain per sensor and show this\nis linearly correlated with classification performance. Finally, our model\nallows for post-hoc addition of entirely new sensor modalities by merely\ntraining a score estimator on the novel input instead of having to retrain from\nscratch on all inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gold-standard sleep scoring is based on epoch-based assignment of sleep\nstages based on a combination of EEG, EOG and EMG signals. However, a\npolysomnographic recording consists of many other signals that could be used\nfor sleep staging, including cardio-respiratory modalities. Leveraging this\nsignal variety would offer important advantages, for example increasing\nreliability, resilience to signal loss, and application to long-term\nnon-obtrusive recordings. We developed a deep generative model for automatic\nsleep staging from a plurality of sensors and any -- arbitrary -- combination\nthereof. We trained a score-based diffusion model using a dataset of 1947\nexpert-labelled overnight recordings with 36 different signals, and achieved\nzero-shot inference on any sensor set by leveraging a novel Bayesian\nfactorization of the score function across the sensors. On single-channel EEG,\nthe model reaches the performance limit in terms of polysomnography inter-rater\nagreement (5-class accuracy 85.6%, Cohen's kappa 0.791). Moreover, the method\noffers full flexibility to use any sensor set, for example finger\nphotoplethysmography, nasal flow and thoracic respiratory movements, (5-class\naccuracy 79.0%, Cohen's kappa of 0.697), or even derivations very\nunconventional for sleep staging, such as tibialis and sternocleidomastoid EMG\n(5-class accuracy 71.0%, kappa 0.575). Additionally, we propose a novel\ninterpretability metric in terms of information gain per sensor and show this\nis linearly correlated with classification performance. Finally, our model\nallows for post-hoc addition of entirely new sensor modalities by merely\ntraining a score estimator on the novel input instead of having to retrain from\nscratch on all inputs."
                },
                "authors": [
                    {
                        "name": "Hans van Gorp"
                    },
                    {
                        "name": "Merel M. van Gilst"
                    },
                    {
                        "name": "Pedro Fonseca"
                    },
                    {
                        "name": "Fokke B. van Meulen"
                    },
                    {
                        "name": "Johannes P. van Dijk"
                    },
                    {
                        "name": "Sebastiaan Overeem"
                    },
                    {
                        "name": "Ruud J. G. van Sloun"
                    }
                ],
                "author_detail": {
                    "name": "Ruud J. G. van Sloun"
                },
                "author": "Ruud J. G. van Sloun",
                "arxiv_doi": "10.1109/JBHI.2025.3565034",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JBHI.2025.3565034",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.15253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20547v1",
                "updated": "2025-04-29T08:49:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    49,
                    38,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T08:49:38Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    49,
                    38,
                    1,
                    119,
                    0
                ],
                "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for\n  Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for\n  Electronic Health Records"
                },
                "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement."
                },
                "authors": [
                    {
                        "name": "Jesus Lovon"
                    },
                    {
                        "name": "Thouria Ben-Haddi"
                    },
                    {
                        "name": "Jules Di Scala"
                    },
                    {
                        "name": "Jose G. Moreno"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "arxiv_affiliation": "IRIT-IRIS",
                "author": "Lynda Tamine",
                "arxiv_journal_ref": "Proceedings of the First Workshop on Patient-Oriented Language\n  Processing (CL4Health) @ LREC-COLING 2024, May 2024, Torino, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05676v2",
                "updated": "2025-04-29T08:43:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    43,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2024-08-11T02:31:13Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    2,
                    31,
                    13,
                    6,
                    224,
                    0
                ],
                "title": "Efficiency Unleashed: Inference Acceleration for LLM-based Recommender\n  Systems with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Unleashed: Inference Acceleration for LLM-based Recommender\n  Systems with Speculative Decoding"
                },
                "summary": "The past few years have witnessed a growing interest in LLM-based recommender\nsystems (RSs), although their industrial deployment remains in a preliminary\nstage. Most existing deployments leverage LLMs offline as feature enhancers,\ngenerating augmented knowledge for downstream tasks. However, in recommendation\nscenarios with numerous users and items, even offline knowledge generation with\nLLMs demands significant time and computational resources. This inefficiency\narises from the autoregressive nature of LLMs. A promising solution is\nspeculative decoding, a Draft-Then-Verify approach that increases the number of\ntokens generated per decoding step. In this work, we first identify\nrecommendation knowledge generation as a highly fitting use case for\nretrieval-based speculative decoding. Then, we discern its two characteristics:\n(1) the vast number of items and users in RSs leads to retrieval inefficiency,\nand (2) RSs exhibit high diversity tolerance for LLM-generated text. Building\non these insights, we introduce Lossless Acceleration via Speculative Decoding\nfor LLM-based Recommender Systems (LASER), which features a Customized\nRetrieval Pool to enhance retrieval efficiency and Relaxed Verification to\nimprove the acceptance rate of draft tokens. LASER achieves a 3-5x speedup on\npublic datasets and saves about 67\\% of computational resources during the\nonline A/B test on a large-scale advertising scenario with lossless downstream\nrecommendation performance. Our code is available at\nhttps://github.com/YunjiaXi/LASER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years have witnessed a growing interest in LLM-based recommender\nsystems (RSs), although their industrial deployment remains in a preliminary\nstage. Most existing deployments leverage LLMs offline as feature enhancers,\ngenerating augmented knowledge for downstream tasks. However, in recommendation\nscenarios with numerous users and items, even offline knowledge generation with\nLLMs demands significant time and computational resources. This inefficiency\narises from the autoregressive nature of LLMs. A promising solution is\nspeculative decoding, a Draft-Then-Verify approach that increases the number of\ntokens generated per decoding step. In this work, we first identify\nrecommendation knowledge generation as a highly fitting use case for\nretrieval-based speculative decoding. Then, we discern its two characteristics:\n(1) the vast number of items and users in RSs leads to retrieval inefficiency,\nand (2) RSs exhibit high diversity tolerance for LLM-generated text. Building\non these insights, we introduce Lossless Acceleration via Speculative Decoding\nfor LLM-based Recommender Systems (LASER), which features a Customized\nRetrieval Pool to enhance retrieval efficiency and Relaxed Verification to\nimprove the acceptance rate of draft tokens. LASER achieves a 3-5x speedup on\npublic datasets and saves about 67\\% of computational resources during the\nonline A/B test on a large-scale advertising scenario with lossless downstream\nrecommendation performance. Our code is available at\nhttps://github.com/YunjiaXi/LASER"
                },
                "authors": [
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Hangyu Wang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Yong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Yu"
                },
                "author": "Yong Yu",
                "arxiv_comment": "Accepted by SIGIR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17109v2",
                "updated": "2025-04-29T08:27:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    27,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2024-10-22T15:33:43Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    15,
                    33,
                    43,
                    1,
                    296,
                    0
                ],
                "title": "The FLAMINGO project: Baryon effects on the matter power spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The FLAMINGO project: Baryon effects on the matter power spectrum"
                },
                "summary": "The effect of baryon physics associated with galaxy formation onto the\nlarge-scale matter distribution of the Universe is a key uncertainty in the\ntheoretical modelling required for the interpretation of Stage IV surveys. We\nuse the FLAMINGO simulations to study the baryon response due to galaxy\nformation of the total matter power spectrum. We find that it is only well\nconverged for simulation volumes in excess of $200^3$ Mpc$^3$. We report\nresults for simulations of varying feedback intensity, which either match the\nX-ray inferred gas fractions in clusters and the $z=0$ stellar mass function,\nor shifted versions of the data, as well as for different implementations of\nAGN feedback. We package our results in the form of a Gaussian process emulator\nwhich can rapidly reproduce all the simulations' predictions to better than one\nper cent up to the comoving wavenumber $k = 10~h$ Mpc$^{-1}$ and up to $z=3$\nfor all the feedback models present in the FLAMINGO suite. We find that the\nresponse becomes stronger, the range of scales affected increases, and the\nposition of the minimum of the response moves to smaller scales as the redshift\ndecreases. We find that lower gas fractions in groups and clusters lead to a\nstronger response and that the use of collimated jets instead of thermally\ndriven winds for AGN feedback enhances the effect. Lowering the stellar masses\nat fixed cluster gas fractions also increases the magnitude of the response. We\nfind only a small (1% at $k<10~h$ Mpc$^{-1}$) dependence of our results on the\nbackground cosmology, but a wider range of cosmology variations will be needed\nto confirm this result. The response we obtain for our strongest feedback\nmodels is compatible with some of the recent analyses combining weak lensing\nwith external data. Such a response is, however, in strong tension with the\nX-ray inferred gas fractions in clusters used to calibrate the FLAMINGO model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effect of baryon physics associated with galaxy formation onto the\nlarge-scale matter distribution of the Universe is a key uncertainty in the\ntheoretical modelling required for the interpretation of Stage IV surveys. We\nuse the FLAMINGO simulations to study the baryon response due to galaxy\nformation of the total matter power spectrum. We find that it is only well\nconverged for simulation volumes in excess of $200^3$ Mpc$^3$. We report\nresults for simulations of varying feedback intensity, which either match the\nX-ray inferred gas fractions in clusters and the $z=0$ stellar mass function,\nor shifted versions of the data, as well as for different implementations of\nAGN feedback. We package our results in the form of a Gaussian process emulator\nwhich can rapidly reproduce all the simulations' predictions to better than one\nper cent up to the comoving wavenumber $k = 10~h$ Mpc$^{-1}$ and up to $z=3$\nfor all the feedback models present in the FLAMINGO suite. We find that the\nresponse becomes stronger, the range of scales affected increases, and the\nposition of the minimum of the response moves to smaller scales as the redshift\ndecreases. We find that lower gas fractions in groups and clusters lead to a\nstronger response and that the use of collimated jets instead of thermally\ndriven winds for AGN feedback enhances the effect. Lowering the stellar masses\nat fixed cluster gas fractions also increases the magnitude of the response. We\nfind only a small (1% at $k<10~h$ Mpc$^{-1}$) dependence of our results on the\nbackground cosmology, but a wider range of cosmology variations will be needed\nto confirm this result. The response we obtain for our strongest feedback\nmodels is compatible with some of the recent analyses combining weak lensing\nwith external data. Such a response is, however, in strong tension with the\nX-ray inferred gas fractions in clusters used to calibrate the FLAMINGO model."
                },
                "authors": [
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Roi Kugel"
                    },
                    {
                        "name": "Jeger C. Broxterman"
                    },
                    {
                        "name": "Marcel P. van Daalen"
                    }
                ],
                "author_detail": {
                    "name": "Marcel P. van Daalen"
                },
                "author": "Marcel P. van Daalen",
                "arxiv_doi": "10.1093/mnras/staf569",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf569",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.17109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 13 figures, published in MNRAS, emulator released publicly\n  on the FLAMINGO project web-page",
                "arxiv_journal_ref": "2025, MNRAS, Vol. 539, Issue 2, pp. 1337-1351",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17510v2",
                "updated": "2025-04-29T08:21:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    21,
                    19,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-24T12:54:30Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    54,
                    30,
                    3,
                    114,
                    0
                ],
                "title": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based\n  Open Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based\n  Open Source Projects"
                },
                "summary": "Background: Psychological safety refers to the belief that team members can\nspeak up or make mistakes without fear of negative consequences. While it is\nrecognized as important in traditional software teams, its role in open-source\nsoftware development remains understudied. Open-source contributors often\ncollaborate without formal roles or structures, where interpersonal\nrelationships can significantly influence participation. Code review, a central\nand collaborative activity in modern software development, offers a valuable\ncontext for observing such team interactions. Aims: This study investigates\nwhether team-level psychological safety, inferred from code review activities,\nis associated with contributors' sustained participation in open-source\nprojects. Method: Using data from 60,684 pull requests across multiple\nrepositories, we developed a psychological safety index based on observable\ncues such as merge decisions, comment activity, interaction diversity, and\nmentions. We analyzed the relationship between this index and contributors'\nshort-term (within 1 year) and long-term (over 4--5 years) sustained\nparticipation using three logistic regression models. Results: Contributors are\nmore likely to remain active in repositories with higher levels of\npsychological safety. Psychological safety is positively associated with both\nshort-term and long-term sustained participation. However, prior participation\nemerges as a stronger predictor of future engagement, reducing the effect of\npsychological safety when accounted for. Conclusions: This study introduces a\nscalable, data-driven approach to measuring psychological safety through pull\nrequest data and provides new empirical evidence of its relevance in sustaining\nparticipation within open-source development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Psychological safety refers to the belief that team members can\nspeak up or make mistakes without fear of negative consequences. While it is\nrecognized as important in traditional software teams, its role in open-source\nsoftware development remains understudied. Open-source contributors often\ncollaborate without formal roles or structures, where interpersonal\nrelationships can significantly influence participation. Code review, a central\nand collaborative activity in modern software development, offers a valuable\ncontext for observing such team interactions. Aims: This study investigates\nwhether team-level psychological safety, inferred from code review activities,\nis associated with contributors' sustained participation in open-source\nprojects. Method: Using data from 60,684 pull requests across multiple\nrepositories, we developed a psychological safety index based on observable\ncues such as merge decisions, comment activity, interaction diversity, and\nmentions. We analyzed the relationship between this index and contributors'\nshort-term (within 1 year) and long-term (over 4--5 years) sustained\nparticipation using three logistic regression models. Results: Contributors are\nmore likely to remain active in repositories with higher levels of\npsychological safety. Psychological safety is positively associated with both\nshort-term and long-term sustained participation. However, prior participation\nemerges as a stronger predictor of future engagement, reducing the effect of\npsychological safety when accounted for. Conclusions: This study introduces a\nscalable, data-driven approach to measuring psychological safety through pull\nrequest data and provides new empirical evidence of its relevance in sustaining\nparticipation within open-source development."
                },
                "authors": [
                    {
                        "name": "Emeralda Sesari"
                    },
                    {
                        "name": "Federica Sarro"
                    },
                    {
                        "name": "Ayushi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Ayushi Rastogi"
                },
                "author": "Ayushi Rastogi",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20519v2",
                "updated": "2025-04-30T03:22:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    22,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T07:59:46Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    59,
                    46,
                    1,
                    119,
                    0
                ],
                "title": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions\n  But Do Not Outperform Standard Public Health Messaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions\n  But Do Not Outperform Standard Public Health Messaging"
                },
                "summary": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies."
                },
                "authors": [
                    {
                        "name": "Neil K. R. Sehgal"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Anish K. Agarwal"
                    },
                    {
                        "name": "Joseph Cappella"
                    },
                    {
                        "name": "Melanie Kornides"
                    },
                    {
                        "name": "Lyle Ungar"
                    },
                    {
                        "name": "Alison Buttenheim"
                    },
                    {
                        "name": "Sharath Chandra Guntuku"
                    }
                ],
                "author_detail": {
                    "name": "Sharath Chandra Guntuku"
                },
                "author": "Sharath Chandra Guntuku",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10937v2",
                "updated": "2025-04-29T07:58:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    58,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2024-09-17T07:15:15Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    15,
                    15,
                    1,
                    261,
                    0
                ],
                "title": "Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering\n  Part I: Galaxies in the Restframe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast, Accurate and Perturbative Forward Modeling of Galaxy Clustering\n  Part I: Galaxies in the Restframe"
                },
                "summary": "Forward models of the galaxy density field enable simulation based inference\nas well as field level inference of galaxy clustering. However, these analysis\ntechniques require forward models that are both computationally fast and robust\nto modeling uncertainties in the relation between galaxies and matter. Both\nrequirements can be addressed with the Effective Field Theory of Large Scale\nStructure. Here, we focus on the physical and numerical convergence of the\nLEFTfield model. Based on the perturbative nature of the forward model, we\nderive an analytic understanding of the leading numerical errors, and we\ncompare our estimates to high-resolution and N-body references. This allows us\nto derive a set of best-practice recommendations for the numerical accuracy\nparameters, which are completely specified by the desired order of the\nperturbative solution and the cut-off scale. We verify these recommendations by\nan extended set of parameter recovery tests from fully nonlinear mock data and\nfind very consistent results. A single evaluation of the forward model takes\nseconds, making cosmological analyses of galaxy clustering data based on\nforward models computationally feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward models of the galaxy density field enable simulation based inference\nas well as field level inference of galaxy clustering. However, these analysis\ntechniques require forward models that are both computationally fast and robust\nto modeling uncertainties in the relation between galaxies and matter. Both\nrequirements can be addressed with the Effective Field Theory of Large Scale\nStructure. Here, we focus on the physical and numerical convergence of the\nLEFTfield model. Based on the perturbative nature of the forward model, we\nderive an analytic understanding of the leading numerical errors, and we\ncompare our estimates to high-resolution and N-body references. This allows us\nto derive a set of best-practice recommendations for the numerical accuracy\nparameters, which are completely specified by the desired order of the\nperturbative solution and the cut-off scale. We verify these recommendations by\nan extended set of parameter recovery tests from fully nonlinear mock data and\nfind very consistent results. A single evaluation of the forward model takes\nseconds, making cosmological analyses of galaxy clustering data based on\nforward models computationally feasible."
                },
                "authors": [
                    {
                        "name": "Julia Stadler"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Martin Reinecke"
                    }
                ],
                "author_detail": {
                    "name": "Martin Reinecke"
                },
                "author": "Martin Reinecke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09413v2",
                "updated": "2025-04-29T07:46:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    46,
                    18,
                    1,
                    119,
                    0
                ],
                "published": "2024-11-14T13:07:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Detecting Children with Autism Spectrum Disorder based on Script-Centric\n  Behavior Understanding with Emotional Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Children with Autism Spectrum Disorder based on Script-Centric\n  Behavior Understanding with Emotional Enhancement"
                },
                "summary": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis."
                },
                "authors": [
                    {
                        "name": "Wenxing Liu"
                    },
                    {
                        "name": "Yueran Pan"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Hongzhu Deng"
                    },
                    {
                        "name": "Xiaobing Zou"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "15 pages, 12 figures, sumbitted to IEEE transactions on affective\n  computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20505v1",
                "updated": "2025-04-29T07:46:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    46,
                    14,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:46:14Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    46,
                    14,
                    1,
                    119,
                    0
                ],
                "title": "MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural\n  Language for Activities of Daily Living",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural\n  Language for Activities of Daily Living"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have shown promising\npotential for human activity recognition (HAR) using ambient sensors,\nespecially through natural language reasoning and zero-shot learning. However,\nexisting datasets such as CASAS, ARAS, and MARBLE were not originally designed\nwith LLMs in mind and therefore lack the contextual richness, complexity, and\nannotation granularity required to fully exploit LLM capabilities. In this\npaper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with\nnatural Language, comprising over 21 hours of multi-user sensor data collected\nfrom 21 sessions in a smart-home environment. MuRAL is annotated with\nfine-grained natural language descriptions, resident identities, and high-level\nactivity labels, all situated in dynamic, realistic multi-resident settings. We\nbenchmark MuRAL using state-of-the-art LLMs for three core tasks: subject\nassignment, action description, and activity classification. Our results\ndemonstrate that while LLMs can provide rich semantic interpretations of\nambient data, current models still face challenges in handling multi-user\nambiguity and under-specified sensor contexts. We release MuRAL to support\nfuture research on LLM-powered, explainable, and socially aware activity\nunderstanding in smart environments. For access to the dataset, please reach\nout to us via the provided contact information. A direct link for dataset\nretrieval will be made available at this location in due course.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have shown promising\npotential for human activity recognition (HAR) using ambient sensors,\nespecially through natural language reasoning and zero-shot learning. However,\nexisting datasets such as CASAS, ARAS, and MARBLE were not originally designed\nwith LLMs in mind and therefore lack the contextual richness, complexity, and\nannotation granularity required to fully exploit LLM capabilities. In this\npaper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with\nnatural Language, comprising over 21 hours of multi-user sensor data collected\nfrom 21 sessions in a smart-home environment. MuRAL is annotated with\nfine-grained natural language descriptions, resident identities, and high-level\nactivity labels, all situated in dynamic, realistic multi-resident settings. We\nbenchmark MuRAL using state-of-the-art LLMs for three core tasks: subject\nassignment, action description, and activity classification. Our results\ndemonstrate that while LLMs can provide rich semantic interpretations of\nambient data, current models still face challenges in handling multi-user\nambiguity and under-specified sensor contexts. We release MuRAL to support\nfuture research on LLM-powered, explainable, and socially aware activity\nunderstanding in smart environments. For access to the dataset, please reach\nout to us via the provided contact information. A direct link for dataset\nretrieval will be made available at this location in due course."
                },
                "authors": [
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Julien Cumin"
                    },
                    {
                        "name": "Fano Ramparany"
                    },
                    {
                        "name": "Dominique Vaufreydaz"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Vaufreydaz"
                },
                "arxiv_affiliation": "M-PSI",
                "author": "Dominique Vaufreydaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20500v1",
                "updated": "2025-04-29T07:40:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    40,
                    0,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:40:00Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    40,
                    0,
                    1,
                    119,
                    0
                ],
                "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniDetox: Universal Detoxification of Large Language Models via Dataset\n  Distillation"
                },
                "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs."
                },
                "authors": [
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Masaru Isonuma"
                    },
                    {
                        "name": "Junichiro Mori"
                    },
                    {
                        "name": "Ichiro Sakata"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Sakata"
                },
                "author": "Ichiro Sakata",
                "arxiv_comment": "Accepted at ICLR 2025 (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.20997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20997v1",
                "updated": "2025-04-29T17:59:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    48,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:59:48Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    48,
                    1,
                    119,
                    0
                ],
                "title": "Toward Efficient Exploration by Large Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient Exploration by Large Language Model Agents"
                },
                "summary": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration."
                },
                "authors": [
                    {
                        "name": "Dilip Arumugam"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20996v1",
                "updated": "2025-04-29T17:59:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    45,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:59:45Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    59,
                    45,
                    1,
                    119,
                    0
                ],
                "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Fusion: Introducing New Modality to Frozen Large Language Models"
                },
                "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels."
                },
                "authors": [
                    {
                        "name": "Sicheng Mo"
                    },
                    {
                        "name": "Thao Nguyen"
                    },
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Siddharth Srinivasan Iyer"
                    },
                    {
                        "name": "Yijun Li"
                    },
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Abhishek Tandon"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Bolei Zhou"
                    },
                    {
                        "name": "Yuheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Li"
                },
                "author": "Yuheng Li",
                "arxiv_comment": "Project Page: https://sichengmo.github.io/XFusion/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20984v1",
                "updated": "2025-04-29T17:55:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    55,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    55,
                    52,
                    1,
                    119,
                    0
                ],
                "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE: A Security Architecture for LLM-Integrated App Systems"
                },
                "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."
                },
                "authors": [
                    {
                        "name": "Evan Li"
                    },
                    {
                        "name": "Tushin Mallick"
                    },
                    {
                        "name": "Evan Rose"
                    },
                    {
                        "name": "William Robertson"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Nita-Rotaru"
                },
                "author": "Cristina Nita-Rotaru",
                "arxiv_comment": "21 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20980v1",
                "updated": "2025-04-29T17:50:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    50,
                    29,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:50:29Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    50,
                    29,
                    1,
                    119,
                    0
                ],
                "title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jekyll-and-Hyde Tipping Point in an AI's Behavior"
                },
                "summary": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''"
                },
                "authors": [
                    {
                        "name": "Neil F. Johnson"
                    },
                    {
                        "name": "Frank Yingjie Huo"
                    }
                ],
                "author_detail": {
                    "name": "Frank Yingjie Huo"
                },
                "author": "Frank Yingjie Huo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20976v1",
                "updated": "2025-04-29T17:45:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    45,
                    4,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:45:04Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    45,
                    4,
                    1,
                    119,
                    0
                ],
                "title": "Real-Time Wayfinding Assistant for Blind and Low-Vision Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Wayfinding Assistant for Blind and Low-Vision Users"
                },
                "summary": "Navigating unfamiliar places continues to be one of the most persistent and\nessential everyday obstacles for those who are blind or have limited vision\n(BLV). Existing assistive technologies, such as GPS-based navigation systems,\nAI-powered smart glasses, and sonar-equipped canes, often face limitations in\nreal-time obstacle avoidance, precise localization, and adaptability to dynamic\nsurroundings. To investigate potential solutions, we introduced PathFinder, a\nnovel map-less navigation system that explores different models for\nunderstanding 2D images, including Vision Language Models (VLMs), Large\nLanguage Models (LLMs), and employs monocular depth estimation for free-path\ndetection. Our approach integrates a Depth-First Search (DFS) algorithm on\ndepth images to determine the longest obstacle-free path, ensuring optimal\nroute selection while maintaining computational efficiency. We conducted\ncomparative evaluations against existing AI-powered navigation methods and\nperformed a usability study with BLV participants. The results demonstrate that\nPathFinder achieves a favorable balance between accuracy, computational\nefficiency, and real-time responsiveness. Notably, it reduces mean absolute\nerror (MAE) and improves decision-making speed in outdoor navigation compared\nto AI-based alternatives. Participant feedback emphasizes the system's\nusability and effectiveness in outside situations, but also identifies issues\nin complicated indoor locations and low-light conditions. Usability testing\nrevealed that 73% of participants understood how to use the app in about a\nminute, and 80% praised its balance of accuracy, quick response, and overall\nconvenience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating unfamiliar places continues to be one of the most persistent and\nessential everyday obstacles for those who are blind or have limited vision\n(BLV). Existing assistive technologies, such as GPS-based navigation systems,\nAI-powered smart glasses, and sonar-equipped canes, often face limitations in\nreal-time obstacle avoidance, precise localization, and adaptability to dynamic\nsurroundings. To investigate potential solutions, we introduced PathFinder, a\nnovel map-less navigation system that explores different models for\nunderstanding 2D images, including Vision Language Models (VLMs), Large\nLanguage Models (LLMs), and employs monocular depth estimation for free-path\ndetection. Our approach integrates a Depth-First Search (DFS) algorithm on\ndepth images to determine the longest obstacle-free path, ensuring optimal\nroute selection while maintaining computational efficiency. We conducted\ncomparative evaluations against existing AI-powered navigation methods and\nperformed a usability study with BLV participants. The results demonstrate that\nPathFinder achieves a favorable balance between accuracy, computational\nefficiency, and real-time responsiveness. Notably, it reduces mean absolute\nerror (MAE) and improves decision-making speed in outdoor navigation compared\nto AI-based alternatives. Participant feedback emphasizes the system's\nusability and effectiveness in outside situations, but also identifies issues\nin complicated indoor locations and low-light conditions. Usability testing\nrevealed that 73% of participants understood how to use the app in about a\nminute, and 80% praised its balance of accuracy, quick response, and overall\nconvenience."
                },
                "authors": [
                    {
                        "name": "Dabbrata Das"
                    },
                    {
                        "name": "Argho Deb Das"
                    },
                    {
                        "name": "Farhan Sadaf"
                    }
                ],
                "author_detail": {
                    "name": "Farhan Sadaf"
                },
                "author": "Farhan Sadaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04992v2",
                "updated": "2025-04-29T17:42:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    42,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-06T21:42:35Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    42,
                    35,
                    3,
                    65,
                    0
                ],
                "title": "Wanda++: Pruning Large Language Models via Regional Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wanda++: Pruning Large Language Models via Regional Gradients"
                },
                "summary": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Kai Zhen"
                    },
                    {
                        "name": "Bhavana Ganesh"
                    },
                    {
                        "name": "Aram Galstyan"
                    },
                    {
                        "name": "Goeric Huybrechts"
                    },
                    {
                        "name": "Markus Müller"
                    },
                    {
                        "name": "Jonas M. Kübler"
                    },
                    {
                        "name": "Rupak Vignesh Swaminathan"
                    },
                    {
                        "name": "Athanasios Mouchtaris"
                    },
                    {
                        "name": "Sravan Babu Bodapati"
                    },
                    {
                        "name": "Nathan Susanj"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Jack FitzGerald"
                    },
                    {
                        "name": "Abhishek Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Kumar"
                },
                "author": "Abhishek Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20972v1",
                "updated": "2025-04-29T17:40:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    40,
                    29,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:40:29Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    40,
                    29,
                    1,
                    119,
                    0
                ],
                "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SetKE: Knowledge Editing for Knowledge Elements Overlap"
                },
                "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Yifan Wei"
                    },
                    {
                        "name": "Xiaoyan Yu"
                    },
                    {
                        "name": "Ran Song"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Angsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Angsheng Li"
                },
                "author": "Angsheng Li",
                "arxiv_comment": "The CR version will be updated subsequently",
                "arxiv_journal_ref": "IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20965v1",
                "updated": "2025-04-29T17:36:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    36,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:36:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    36,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM\n  Security"
                },
                "summary": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm"
                },
                "authors": [
                    {
                        "name": "Zikui Cai"
                    },
                    {
                        "name": "Shayan Shabihi"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Zora Che"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "ICLR 2025 Workshop BuildingTrust",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20964v1",
                "updated": "2025-04-29T17:34:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    34,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:34:49Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    34,
                    49,
                    1,
                    119,
                    0
                ],
                "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for\n  Operating System Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for\n  Operating System Verification"
                },
                "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench."
                },
                "authors": [
                    {
                        "name": "Shangyu Li"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Jiasi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jiasi Shen"
                },
                "author": "Jiasi Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19521v2",
                "updated": "2025-04-29T17:22:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    22,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T06:40:01Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    6,
                    40,
                    1,
                    0,
                    118,
                    0
                ],
                "title": "Security Steerability is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Steerability is All You Need"
                },
                "summary": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it. Thus, in this work we adopt an application-centric\napproach to GenAI security, and show that while LLMs cannot protect against\nad-hoc application specific threats, they can provide the framework for\napplications to protect themselves against such threats. Our first contribution\nis defining Security Steerability - a novel security measure for LLMs,\nassessing the model's capability to adhere to strict guardrails that are\ndefined in the system prompt ('Refrain from discussing about politics'). These\nguardrails, in case effective, can stop threats in the presence of malicious\nusers who attempt to circumvent the application and cause harm to its\nproviders. Our second contribution is a methodology to measure the security\nsteerability of LLMs, utilizing two newly-developed datasets: VeganRibs\nassesses the LLM behavior in forcing specific guardrails that are not security\nper se in the presence of malicious user that uses attack boosters (jailbreaks\nand perturbations), and ReverseText takes this approach further and measures\nthe LLM ability to force specific treatment of the user input as plain text\nwhile do user try to give it additional meanings...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Generative AI (GenAI) in various applications inevitably\ncomes with expanding the attack surface, combining new security threats along\nwith the traditional ones. Consequently, numerous research and industrial\ninitiatives aim to mitigate these security threats in GenAI by developing\nmetrics and designing defenses. However, while most of the GenAI security work\nfocuses on universal threats (e.g. manipulating the LLM to generate forbidden\ncontent), there is significantly less discussion on application-level security\nand how to mitigate it. Thus, in this work we adopt an application-centric\napproach to GenAI security, and show that while LLMs cannot protect against\nad-hoc application specific threats, they can provide the framework for\napplications to protect themselves against such threats. Our first contribution\nis defining Security Steerability - a novel security measure for LLMs,\nassessing the model's capability to adhere to strict guardrails that are\ndefined in the system prompt ('Refrain from discussing about politics'). These\nguardrails, in case effective, can stop threats in the presence of malicious\nusers who attempt to circumvent the application and cause harm to its\nproviders. Our second contribution is a methodology to measure the security\nsteerability of LLMs, utilizing two newly-developed datasets: VeganRibs\nassesses the LLM behavior in forcing specific guardrails that are not security\nper se in the presence of malicious user that uses attack boosters (jailbreaks\nand perturbations), and ReverseText takes this approach further and measures\nthe LLM ability to force specific treatment of the user input as plain text\nwhile do user try to give it additional meanings..."
                },
                "authors": [
                    {
                        "name": "Itay Hazan"
                    },
                    {
                        "name": "Idan Habler"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Itsik Mantin"
                    }
                ],
                "author_detail": {
                    "name": "Itsik Mantin"
                },
                "author": "Itsik Mantin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20951v1",
                "updated": "2025-04-29T17:21:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    21,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:21:20Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    21,
                    20,
                    1,
                    119,
                    0
                ],
                "title": "Information Gravity: A Field-Theoretic Model for Token Selection in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Gravity: A Field-Theoretic Model for Token Selection in\n  Large Language Models"
                },
                "summary": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity."
                },
                "authors": [
                    {
                        "name": "Maryna Vyshnyvetska"
                    }
                ],
                "author_detail": {
                    "name": "Maryna Vyshnyvetska"
                },
                "author": "Maryna Vyshnyvetska",
                "arxiv_doi": "10.5281/zenodo.15289890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.15289890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20946v1",
                "updated": "2025-04-29T17:14:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    14,
                    54,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T17:14:54Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    14,
                    54,
                    1,
                    119,
                    0
                ],
                "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning\n  Distillation From Large to Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning\n  Distillation From Large to Small Language Models"
                },
                "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications."
                },
                "authors": [
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12836v2",
                "updated": "2025-04-29T17:14:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    14,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-02-18T13:09:59Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    13,
                    9,
                    59,
                    1,
                    49,
                    0
                ],
                "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on\n  PPG-based Heart Rate Estimation"
                },
                "summary": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Mohammad Feli"
                    },
                    {
                        "name": "Iman Azimi"
                    },
                    {
                        "name": "Pasi Liljeberg"
                    },
                    {
                        "name": "Amir M. Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir M. Rahmani"
                },
                "author": "Amir M. Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20930v1",
                "updated": "2025-04-29T16:48:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    48,
                    23,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:48:23Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    48,
                    23,
                    1,
                    119,
                    0
                ],
                "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\n  through Step-by-Step Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\n  through Step-by-Step Verification"
                },
                "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs."
                },
                "authors": [
                    {
                        "name": "Ziqing Fan"
                    },
                    {
                        "name": "Cheng Liang"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20924v1",
                "updated": "2025-04-29T16:38:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    38,
                    35,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:38:35Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    38,
                    35,
                    1,
                    119,
                    0
                ],
                "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Domain-Agnostic Scalable AI Safety Ensuring Framework"
                },
                "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority\nfor real-world deployment, particularly in physical AI applications. Current\napproaches to AI safety typically address predefined domain-specific safety\nconditions, limiting their ability to generalize across contexts.\n  We propose a novel AI safety framework that ensures AI systems comply with\n\\textbf{any user-defined constraint}, with \\textbf{any desired probability},\nand across \\textbf{various domains}.\n  In this framework, we combine an AI component (e.g., neural network) with an\noptimization problem to produce responses that minimize objectives while\nsatisfying user-defined constraints with probabilities exceeding user-defined\nthresholds. For credibility assessment of the AI component, we propose\n\\textit{internal test data}, a supplementary set of safety-labeled data, and a\n\\textit{conservative testing} methodology that provides statistical validity of\nusing internal test data. We also present an approximation method of a loss\nfunction and how to compute its gradient for training.\n  We mathematically prove that probabilistic constraint satisfaction is\nguaranteed under specific, mild conditions and prove a scaling law between\nsafety and the number of internal test data. We demonstrate our framework's\neffectiveness through experiments in diverse domains: demand prediction for\nproduction decision, safe reinforcement learning within the SafetyGym\nsimulator, and guarding AI chatbot outputs. Through these experiments, we\ndemonstrate that our method guarantees safety for user-specified constraints,\noutperforms {for \\textbf{up to several order of magnitudes}} existing methods\nin low safety threshold regions, and scales effectively with respect to the\nsize of internal test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of AI systems has recently emerged as a critical priority\nfor real-world deployment, particularly in physical AI applications. Current\napproaches to AI safety typically address predefined domain-specific safety\nconditions, limiting their ability to generalize across contexts.\n  We propose a novel AI safety framework that ensures AI systems comply with\n\\textbf{any user-defined constraint}, with \\textbf{any desired probability},\nand across \\textbf{various domains}.\n  In this framework, we combine an AI component (e.g., neural network) with an\noptimization problem to produce responses that minimize objectives while\nsatisfying user-defined constraints with probabilities exceeding user-defined\nthresholds. For credibility assessment of the AI component, we propose\n\\textit{internal test data}, a supplementary set of safety-labeled data, and a\n\\textit{conservative testing} methodology that provides statistical validity of\nusing internal test data. We also present an approximation method of a loss\nfunction and how to compute its gradient for training.\n  We mathematically prove that probabilistic constraint satisfaction is\nguaranteed under specific, mild conditions and prove a scaling law between\nsafety and the number of internal test data. We demonstrate our framework's\neffectiveness through experiments in diverse domains: demand prediction for\nproduction decision, safe reinforcement learning within the SafetyGym\nsimulator, and guarding AI chatbot outputs. Through these experiments, we\ndemonstrate that our method guarantees safety for user-specified constraints,\noutperforms {for \\textbf{up to several order of magnitudes}} existing methods\nin low safety threshold regions, and scales effectively with respect to the\nsize of internal test data."
                },
                "authors": [
                    {
                        "name": "Beomjun Kim"
                    },
                    {
                        "name": "Kangyeon Kim"
                    },
                    {
                        "name": "Sunwoo Kim"
                    },
                    {
                        "name": "Heejin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Heejin Ahn"
                },
                "author": "Heejin Ahn",
                "arxiv_comment": "Experimental supplementary material will be available before May 22\n  23:59PM AOE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20922v1",
                "updated": "2025-04-29T16:38:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    38,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:38:15Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    38,
                    15,
                    1,
                    119,
                    0
                ],
                "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based\n  architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DYNAMAX: Dynamic computing for Transformers and Mamba based\n  architectures"
                },
                "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs."
                },
                "authors": [
                    {
                        "name": "Miguel Nogales"
                    },
                    {
                        "name": "Matteo Gambella"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_comment": "Accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary), 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18384v2",
                "updated": "2025-04-29T16:33:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    33,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2024-11-27T14:29:53Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    29,
                    53,
                    2,
                    332,
                    0
                ],
                "title": "Optimal In-Network Distribution of Learning Functions for a\n  Secure-by-Design Programmable Data Plane of Next-Generation Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal In-Network Distribution of Learning Functions for a\n  Secure-by-Design Programmable Data Plane of Next-Generation Networks"
                },
                "summary": "The rise of programmable data plane (PDP) and in-network computing (INC)\nparadigms paves the way for the development of network devices (switches,\nnetwork interface cards, etc.) capable of performing advanced processing tasks.\nThis allows running various types of algorithms, including machine learning,\nwithin the network itself to support user and network services. In particular,\nthis paper delves into the deployment of in-network learning models with the\naim of implementing fully distributed intrusion detection systems (IDS) or\nintrusion prevention systems (IPS). Specifically, a model is proposed for the\noptimal distribution of the IDS/IPS workload among data plane devices with the\naim of ensuring complete network security without excessively burdening the\nnormal operations of the devices. Furthermore, a meta-heuristic approach is\nproposed to reduce the long computation time required by the exact solution\nprovided by the mathematical model and its performance is evaluated. The\nanalysis conducted and the results obtained demonstrate the enormous potential\nof the proposed new approach for the creation of intelligent data planes that\nact effectively and autonomously as the first line of defense against cyber\nattacks, with minimal additional workload on the network devices involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of programmable data plane (PDP) and in-network computing (INC)\nparadigms paves the way for the development of network devices (switches,\nnetwork interface cards, etc.) capable of performing advanced processing tasks.\nThis allows running various types of algorithms, including machine learning,\nwithin the network itself to support user and network services. In particular,\nthis paper delves into the deployment of in-network learning models with the\naim of implementing fully distributed intrusion detection systems (IDS) or\nintrusion prevention systems (IPS). Specifically, a model is proposed for the\noptimal distribution of the IDS/IPS workload among data plane devices with the\naim of ensuring complete network security without excessively burdening the\nnormal operations of the devices. Furthermore, a meta-heuristic approach is\nproposed to reduce the long computation time required by the exact solution\nprovided by the mathematical model and its performance is evaluated. The\nanalysis conducted and the results obtained demonstrate the enormous potential\nof the proposed new approach for the creation of intelligent data planes that\nact effectively and autonomously as the first line of defense against cyber\nattacks, with minimal additional workload on the network devices involved."
                },
                "authors": [
                    {
                        "name": "Mattia Giovanni Spina"
                    },
                    {
                        "name": "Edoardo Scalzo"
                    },
                    {
                        "name": "Floriano De Rango"
                    },
                    {
                        "name": "Francesca Guerriero"
                    },
                    {
                        "name": "Antonio Iera"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Iera"
                },
                "author": "Antonio Iera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14823v2",
                "updated": "2025-04-29T16:31:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    31,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2024-11-22T09:44:13Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    44,
                    13,
                    4,
                    327,
                    0
                ],
                "title": "Omni-IML: Towards Unified Image Manipulation Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-IML: Towards Unified Image Manipulation Localization"
                },
                "summary": "Existing Image Manipulation Localization (IML) methods mostly rely heavily on\ntask-specific designs, making them perform well only on the target IML task,\nwhile joint training on multiple IML tasks causes significant performance\ndegradation, hindering real applications.\n  To this end, we propose Omni-IML, the first generalist model designed to\nunify IML across diverse tasks.\n  Specifically, Omni-IML achieves generalization through three key components:\n(1) a Modal Gate Encoder, which adaptively selects the optimal encoding\nmodality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts\ndecoder filters to the task at hand, and (3) an Anomaly Enhancement module that\nleverages box supervision to highlight the tampered regions and facilitate the\nlearning of task-agnostic features.\n  Beyond localization, to support interpretation of the tampered images, we\nconstruct Omni-273k, a large high-quality dataset that includes natural\nlanguage descriptions of tampered artifact. It is annotated through our\nautomatic, chain-of-thoughts annotation technique.\n  We also design a simple-yet-effective interpretation module to better utilize\nthese descriptive annotations.\n  Our extensive experiments show that our single Omni-IML model achieves\nstate-of-the-art performance across all four major IML tasks, providing a\nvaluable solution for practical deployment and a promising direction of\ngeneralist models in image forensics. Our code and dataset will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Image Manipulation Localization (IML) methods mostly rely heavily on\ntask-specific designs, making them perform well only on the target IML task,\nwhile joint training on multiple IML tasks causes significant performance\ndegradation, hindering real applications.\n  To this end, we propose Omni-IML, the first generalist model designed to\nunify IML across diverse tasks.\n  Specifically, Omni-IML achieves generalization through three key components:\n(1) a Modal Gate Encoder, which adaptively selects the optimal encoding\nmodality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts\ndecoder filters to the task at hand, and (3) an Anomaly Enhancement module that\nleverages box supervision to highlight the tampered regions and facilitate the\nlearning of task-agnostic features.\n  Beyond localization, to support interpretation of the tampered images, we\nconstruct Omni-273k, a large high-quality dataset that includes natural\nlanguage descriptions of tampered artifact. It is annotated through our\nautomatic, chain-of-thoughts annotation technique.\n  We also design a simple-yet-effective interpretation module to better utilize\nthese descriptive annotations.\n  Our extensive experiments show that our single Omni-IML model achieves\nstate-of-the-art performance across all four major IML tasks, providing a\nvaluable solution for practical deployment and a promising direction of\ngeneralist models in image forensics. Our code and dataset will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Chenfan Qu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Fengjun Guo"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20911v1",
                "updated": "2025-04-29T16:29:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    29,
                    12,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:29:12Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    29,
                    12,
                    1,
                    119,
                    0
                ],
                "title": "An Empirical Study on the Capability of LLMs in Decomposing Bug Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Capability of LLMs in Decomposing Bug Reports"
                },
                "summary": "Background: Bug reports are essential to the software development life cycle.\nThey help developers track and resolve issues, but are often difficult to\nprocess due to their complexity, which can delay resolution and affect software\nquality. Aims: This study investigates whether large language models (LLMs) can\nassist developers in automatically decomposing complex bug reports into\nsmaller, self-contained units, making them easier to understand and address.\nMethod: We conducted an empirical study on 127 resolved privacy-related bug\nreports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using\ndifferent prompting strategies. We first tested both LLMs with zero-shot\nprompts, then applied improved prompts with demonstrations (using few-shot\nprompting) to measure their abilities in bug decomposition. Results: Our\nfindings show that LLMs are capable of decomposing bug reports, but their\noverall performance still requires further improvement and strongly depends on\nthe quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT\nand DeepSeek) performed poorly. After prompt tuning, ChatGPT's true\ndecomposition rate increased by 140\\% and DeepSeek's by 163.64\\%. Conclusions:\nLLMs show potential in helping developers analyze and decompose complex bug\nreports, but they still need improvement in terms of accuracy and bug\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Bug reports are essential to the software development life cycle.\nThey help developers track and resolve issues, but are often difficult to\nprocess due to their complexity, which can delay resolution and affect software\nquality. Aims: This study investigates whether large language models (LLMs) can\nassist developers in automatically decomposing complex bug reports into\nsmaller, self-contained units, making them easier to understand and address.\nMethod: We conducted an empirical study on 127 resolved privacy-related bug\nreports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using\ndifferent prompting strategies. We first tested both LLMs with zero-shot\nprompts, then applied improved prompts with demonstrations (using few-shot\nprompting) to measure their abilities in bug decomposition. Results: Our\nfindings show that LLMs are capable of decomposing bug reports, but their\noverall performance still requires further improvement and strongly depends on\nthe quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT\nand DeepSeek) performed poorly. After prompt tuning, ChatGPT's true\ndecomposition rate increased by 140\\% and DeepSeek's by 163.64\\%. Conclusions:\nLLMs show potential in helping developers analyze and decompose complex bug\nreports, but they still need improvement in terms of accuracy and bug\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Vanessa Nava-Camal"
                    },
                    {
                        "name": "Ahmad Suleiman"
                    },
                    {
                        "name": "Yiming Tang"
                    },
                    {
                        "name": "Daqing Hou"
                    },
                    {
                        "name": "Weiyi Shang"
                    }
                ],
                "author_detail": {
                    "name": "Weiyi Shang"
                },
                "author": "Weiyi Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20896v1",
                "updated": "2025-04-29T16:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    13,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T16:13:49Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    16,
                    13,
                    49,
                    1,
                    119,
                    0
                ],
                "title": "LELANTE: LEveraging LLM for Automated ANdroid TEsting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LELANTE: LEveraging LLM for Automated ANdroid TEsting"
                },
                "summary": "Given natural language test case description for an Android application,\nexisting testing approaches require developers to manually write scripts using\ntools such as Appium and Espresso to execute the corresponding test case. This\nprocess is labor-intensive and demands significant effort to maintain as UI\ninterfaces evolve throughout development. In this work, we introduce LELANTE, a\nnovel framework that utilizes large language models (LLMs) to automate test\ncase execution without requiring pre-written scripts. LELANTE interprets\nnatural language test case descriptions, iteratively generate action plans, and\nperform the actions directly on the Android screen using its GUI. LELANTE\nemploys a screen refinement process to enhance LLM interpretability, constructs\na structured prompt for LLMs, and implements an action generation mechanism\nbased on chain-of-thought reasoning of LLMs. To further reduce computational\ncost and enhance scalability, LELANTE utilizes model distillation using a\nfoundational LLM. In experiments across 390 test cases spanning 10 popular\nAndroid applications, LELANTE achieved a 73% test execution success rate. Our\nresults demonstrate that LLMs can effectively bridge the gap between natural\nlanguage test case description and automated execution, making mobile testing\nmore scalable and adaptable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given natural language test case description for an Android application,\nexisting testing approaches require developers to manually write scripts using\ntools such as Appium and Espresso to execute the corresponding test case. This\nprocess is labor-intensive and demands significant effort to maintain as UI\ninterfaces evolve throughout development. In this work, we introduce LELANTE, a\nnovel framework that utilizes large language models (LLMs) to automate test\ncase execution without requiring pre-written scripts. LELANTE interprets\nnatural language test case descriptions, iteratively generate action plans, and\nperform the actions directly on the Android screen using its GUI. LELANTE\nemploys a screen refinement process to enhance LLM interpretability, constructs\na structured prompt for LLMs, and implements an action generation mechanism\nbased on chain-of-thought reasoning of LLMs. To further reduce computational\ncost and enhance scalability, LELANTE utilizes model distillation using a\nfoundational LLM. In experiments across 390 test cases spanning 10 popular\nAndroid applications, LELANTE achieved a 73% test execution success rate. Our\nresults demonstrate that LLMs can effectively bridge the gap between natural\nlanguage test case description and automated execution, making mobile testing\nmore scalable and adaptable."
                },
                "authors": [
                    {
                        "name": "Shamit Fatin"
                    },
                    {
                        "name": "Mehbubul Hasan Al-Quvi"
                    },
                    {
                        "name": "Haz Sameen Shahgir"
                    },
                    {
                        "name": "Sukarna Barua"
                    },
                    {
                        "name": "Anindya Iqbal"
                    },
                    {
                        "name": "Sadia Sharmin"
                    },
                    {
                        "name": "Md. Mostofa Akbar"
                    },
                    {
                        "name": "Kallol Kumar Pal"
                    },
                    {
                        "name": "A. Asif Al Rashid"
                    }
                ],
                "author_detail": {
                    "name": "A. Asif Al Rashid"
                },
                "author": "A. Asif Al Rashid",
                "arxiv_comment": "6 pages, 4 figures, 29th International Conference on Evaluation and\n  Assessment in Software Engineering (EASE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04907v2",
                "updated": "2025-04-29T15:56:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    56,
                    46,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-07T10:32:42Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    32,
                    42,
                    0,
                    97,
                    0
                ],
                "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-Bench: Human-Aligned Video Generation Benchmark"
                },
                "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment."
                },
                "authors": [
                    {
                        "name": "Hui Han"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yiwen Yuan"
                    },
                    {
                        "name": "Yuling Wu"
                    },
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Junchen Fu"
                    },
                    {
                        "name": "Youhua Li"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Li-jia Li"
                    },
                    {
                        "name": "Yongxin Ni"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Ni"
                },
                "author": "Yongxin Ni",
                "arxiv_comment": "Accepted by CVPR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20879v1",
                "updated": "2025-04-29T15:48:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    48,
                    49,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T15:48:49Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    48,
                    49,
                    1,
                    119,
                    0
                ],
                "title": "The Leaderboard Illusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Leaderboard Illusion"
                },
                "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"
                },
                "authors": [
                    {
                        "name": "Shivalika Singh"
                    },
                    {
                        "name": "Yiyang Nan"
                    },
                    {
                        "name": "Alex Wang"
                    },
                    {
                        "name": "Daniel D'Souza"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Noah Smith"
                    },
                    {
                        "name": "Beyza Ermis"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "68 pages, 18 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24175v2",
                "updated": "2025-04-29T15:38:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    38,
                    34,
                    1,
                    119,
                    0
                ],
                "published": "2024-10-31T17:42:26Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    42,
                    26,
                    3,
                    305,
                    0
                ],
                "title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07062v3",
                "updated": "2025-04-29T15:33:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    33,
                    0,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-10T00:10:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    0,
                    10,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning"
                },
                "summary": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 5.40\\% (up to 14.29\\%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 5.40\\% (up to 14.29\\%)."
                },
                "authors": [
                    {
                        "name": "Weihang Chen"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zhiqiang Li"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14562v2",
                "updated": "2025-04-29T15:24:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    24,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2024-03-21T17:06:17Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    17,
                    6,
                    17,
                    3,
                    81,
                    0
                ],
                "title": "Agentic AI: The Era of Semantic Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI: The Era of Semantic Decoding"
                },
                "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation."
                },
                "authors": [
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20849v1",
                "updated": "2025-04-29T15:19:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    19,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T15:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    19,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated\n  Marketing Text in the Music Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated\n  Marketing Text in the Music Industry"
                },
                "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent."
                },
                "authors": [
                    {
                        "name": "Anum Afzal"
                    },
                    {
                        "name": "Alexandre Mercier"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16137v2",
                "updated": "2025-04-29T15:14:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    15,
                    14,
                    35,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-21T21:04:01Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    4,
                    1,
                    0,
                    111,
                    0
                ],
                "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark"
                },
                "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences."
                },
                "authors": [
                    {
                        "name": "Jasper Götting"
                    },
                    {
                        "name": "Pedro Medeiros"
                    },
                    {
                        "name": "Jon G Sanders"
                    },
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Karam Elabd"
                    },
                    {
                        "name": "Lennart Justen"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Seth Donoughe"
                    }
                ],
                "author_detail": {
                    "name": "Seth Donoughe"
                },
                "author": "Seth Donoughe",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20835v1",
                "updated": "2025-04-29T14:59:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    59,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:59:42Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    59,
                    42,
                    1,
                    119,
                    0
                ],
                "title": "Enhancing Non-Core Language Instruction-Following in Speech LLMs via\n  Semi-Implicit Cross-Lingual CoT Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Non-Core Language Instruction-Following in Speech LLMs via\n  Semi-Implicit Cross-Lingual CoT Reasoning"
                },
                "summary": "Large language models have been extended to the speech domain, leading to the\ndevelopment of speech large language models (SLLMs). While existing SLLMs\ndemonstrate strong performance in speech instruction-following for core\nlanguages (e.g., English), they often struggle with non-core languages due to\nthe scarcity of paired speech-text data and limited multilingual semantic\nreasoning capabilities. To address this, we propose the semi-implicit\nCross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates\nspeech-to-text translation into the reasoning process of SLLMs. The XS-CoT\ngenerates four types of tokens: instruction and response tokens in both core\nand non-core languages, enabling cross-lingual transfer of reasoning\ncapabilities. To mitigate inference latency in generating target non-core\nresponse tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which\nprogressively compresses the first three types of intermediate reasoning tokens\nwhile retaining global reasoning logic during training. By leveraging the\nrobust reasoning capabilities of the core language, XS-CoT improves responses\nfor non-core languages by up to 45\\% in GPT-4 score when compared to direct\nsupervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.\nMoreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a\nslight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount\nof high-quality training data for non-core languages by leveraging the\nreasoning capabilities of core languages. To support training, we also develop\na data pipeline and open-source speech instruction-following datasets in\nJapanese, German, and French.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been extended to the speech domain, leading to the\ndevelopment of speech large language models (SLLMs). While existing SLLMs\ndemonstrate strong performance in speech instruction-following for core\nlanguages (e.g., English), they often struggle with non-core languages due to\nthe scarcity of paired speech-text data and limited multilingual semantic\nreasoning capabilities. To address this, we propose the semi-implicit\nCross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates\nspeech-to-text translation into the reasoning process of SLLMs. The XS-CoT\ngenerates four types of tokens: instruction and response tokens in both core\nand non-core languages, enabling cross-lingual transfer of reasoning\ncapabilities. To mitigate inference latency in generating target non-core\nresponse tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which\nprogressively compresses the first three types of intermediate reasoning tokens\nwhile retaining global reasoning logic during training. By leveraging the\nrobust reasoning capabilities of the core language, XS-CoT improves responses\nfor non-core languages by up to 45\\% in GPT-4 score when compared to direct\nsupervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.\nMoreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a\nslight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount\nof high-quality training data for non-core languages by leveraging the\nreasoning capabilities of core languages. To support training, we also develop\na data pipeline and open-source speech instruction-following datasets in\nJapanese, German, and French."
                },
                "authors": [
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Yufeng Tang"
                    },
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Xuelong Geng"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "10 pages, 6 figures, Submitted to ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20834v1",
                "updated": "2025-04-29T14:58:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    58,
                    43,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:58:43Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    58,
                    43,
                    1,
                    119,
                    0
                ],
                "title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for LLM Reasoning Under Memory Constraints"
                },
                "summary": "We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated."
                },
                "authors": [
                    {
                        "name": "Alan Lee"
                    },
                    {
                        "name": "Harry Tong"
                    }
                ],
                "author_detail": {
                    "name": "Harry Tong"
                },
                "author": "Harry Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20828v1",
                "updated": "2025-04-29T14:51:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    51,
                    26,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:51:26Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    51,
                    26,
                    1,
                    119,
                    0
                ],
                "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs."
                },
                "authors": [
                    {
                        "name": "Azam Ikram"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Sameh Elnikety"
                    },
                    {
                        "name": "Saurabh Bagchi"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Bagchi"
                },
                "author": "Saurabh Bagchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09089v2",
                "updated": "2025-04-29T14:37:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    37,
                    43,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-12T05:55:01Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    55,
                    1,
                    2,
                    71,
                    0
                ],
                "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocAgent: Graph-Guided LLM Agents for Code Localization"
                },
                "summary": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent."
                },
                "authors": [
                    {
                        "name": "Zhaoling Chen"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Viktor Prasanna"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xingyao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingyao Wang"
                },
                "author": "Xingyao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20799v1",
                "updated": "2025-04-29T14:13:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    13,
                    57,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    13,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation,\n  and Challenges"
                },
                "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs."
                },
                "authors": [
                    {
                        "name": "Yunseo Lee"
                    },
                    {
                        "name": "John Youngeun Song"
                    },
                    {
                        "name": "Dongsun Kim"
                    },
                    {
                        "name": "Jindae Kim"
                    },
                    {
                        "name": "Mijung Kim"
                    },
                    {
                        "name": "Jaechang Nam"
                    }
                ],
                "author_detail": {
                    "name": "Jaechang Nam"
                },
                "author": "Jaechang Nam",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20797v1",
                "updated": "2025-04-29T14:11:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    11,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:11:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    11,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning"
                },
                "summary": "Current mainstream deep learning techniques exhibit an over-reliance on\nextensive training data and a lack of adaptability to the dynamic world,\nmarking a considerable disparity from human intelligence. To bridge this gap,\nFew-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous\nlearning of new categories with limited samples without forgetting old\nknowledge. Existing FSCIL studies typically use a single model to learn\nknowledge across all sessions, inevitably leading to the stability-plasticity\ndilemma. Unlike machines, humans store varied knowledge in different cerebral\ncortices. Inspired by this characteristic, our paper aims to develop a method\nthat learns independent models for each session. It can inherently prevent\ncatastrophic forgetting. During the testing stage, our method integrates\nUncertainty Quantification (UQ) for model deployment. Our method provides a\nfresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on\nCIFAR-100 and mini-ImageNet datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current mainstream deep learning techniques exhibit an over-reliance on\nextensive training data and a lack of adaptability to the dynamic world,\nmarking a considerable disparity from human intelligence. To bridge this gap,\nFew-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous\nlearning of new categories with limited samples without forgetting old\nknowledge. Existing FSCIL studies typically use a single model to learn\nknowledge across all sessions, inevitably leading to the stability-plasticity\ndilemma. Unlike machines, humans store varied knowledge in different cerebral\ncortices. Inspired by this characteristic, our paper aims to develop a method\nthat learns independent models for each session. It can inherently prevent\ncatastrophic forgetting. During the testing stage, our method integrates\nUncertainty Quantification (UQ) for model deployment. Our method provides a\nfresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on\nCIFAR-100 and mini-ImageNet datasets."
                },
                "authors": [
                    {
                        "name": "Renye Zhang"
                    },
                    {
                        "name": "Yimin Yin"
                    },
                    {
                        "name": "Jinghua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinghua Zhang"
                },
                "author": "Jinghua Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20794v1",
                "updated": "2025-04-29T14:10:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    10,
                    10,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:10:10Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    10,
                    10,
                    1,
                    119,
                    0
                ],
                "title": "Q-Fusion: Diffusing Quantum Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Fusion: Diffusing Quantum Circuits"
                },
                "summary": "Quantum computing holds great potential for solving socially relevant and\ncomputationally complex problems. Furthermore, quantum machine learning (QML)\npromises to rapidly improve our current machine learning capabilities. However,\ncurrent noisy intermediate-scale quantum (NISQ) devices are constrained by\nlimitations in the number of qubits and gate counts, which hinder their full\ncapabilities. Furthermore, the design of quantum algorithms remains a laborious\ntask, requiring significant domain expertise and time. Quantum Architecture\nSearch (QAS) aims to streamline this process by automatically generating novel\nquantum circuits, reducing the need for manual intervention. In this paper, we\npropose a diffusion-based algorithm leveraging the LayerDAG framework to\ngenerate new quantum circuits. This method contrasts with other approaches that\nutilize large language models (LLMs), reinforcement learning (RL), variational\nautoencoders (VAE), and similar techniques. Our results demonstrate that the\nproposed model consistently generates 100% valid quantum circuit outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing holds great potential for solving socially relevant and\ncomputationally complex problems. Furthermore, quantum machine learning (QML)\npromises to rapidly improve our current machine learning capabilities. However,\ncurrent noisy intermediate-scale quantum (NISQ) devices are constrained by\nlimitations in the number of qubits and gate counts, which hinder their full\ncapabilities. Furthermore, the design of quantum algorithms remains a laborious\ntask, requiring significant domain expertise and time. Quantum Architecture\nSearch (QAS) aims to streamline this process by automatically generating novel\nquantum circuits, reducing the need for manual intervention. In this paper, we\npropose a diffusion-based algorithm leveraging the LayerDAG framework to\ngenerate new quantum circuits. This method contrasts with other approaches that\nutilize large language models (LLMs), reinforcement learning (RL), variational\nautoencoders (VAE), and similar techniques. Our results demonstrate that the\nproposed model consistently generates 100% valid quantum circuit outputs."
                },
                "authors": [
                    {
                        "name": "Collin Beaudoin"
                    },
                    {
                        "name": "Swaroop Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Ghosh"
                },
                "author": "Swaroop Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20781v1",
                "updated": "2025-04-29T14:00:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T14:00:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    0,
                    18,
                    1,
                    119,
                    0
                ],
                "title": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in Generating Design Rationale for Software Architecture\n  Decisions"
                },
                "summary": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading.\nBased on the results, we further discussed the pros and cons of the three\nprompting strategies and the strengths and limitations of the DR generated by\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading.\nBased on the results, we further discussed the pros and cons of the three\nprompting strategies and the strengths and limitations of the DR generated by\nLLMs."
                },
                "authors": [
                    {
                        "name": "Xiyu Zhou"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Beiqi Zhang"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang",
                "arxiv_comment": "28 pages, 5 images, 7 tables, Manuscript submitted to a journal\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08243v2",
                "updated": "2025-04-29T13:58:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    58,
                    44,
                    1,
                    119,
                    0
                ],
                "published": "2024-11-12T23:43:20Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    23,
                    43,
                    20,
                    1,
                    317,
                    0
                ],
                "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset"
                },
                "summary": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs."
                },
                "authors": [
                    {
                        "name": "Khaoula Chehbouni"
                    },
                    {
                        "name": "Jonathan Colaço Carr"
                    },
                    {
                        "name": "Yash More"
                    },
                    {
                        "name": "Jackie CK Cheung"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "Prepared for conference submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20771v1",
                "updated": "2025-04-29T13:52:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    52,
                    47,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:52:47Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    52,
                    47,
                    1,
                    119,
                    0
                ],
                "title": "Turing Machine Evaluation for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turing Machine Evaluation for Large Language Model"
                },
                "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench."
                },
                "authors": [
                    {
                        "name": "Haitao Wu"
                    },
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Huaxi Huang"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20763v1",
                "updated": "2025-04-29T13:44:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    44,
                    1,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:44:01Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    44,
                    1,
                    1,
                    119,
                    0
                ],
                "title": "Understanding Large Language Model Supply Chain: Structure, Domain, and\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Model Supply Chain: Structure, Domain, and\n  Vulnerabilities"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence\n(AI), driving breakthroughs in natural language understanding, text generation,\nand autonomous systems. However, the rapid growth of LLMs presents significant\nchallenges in the security and reliability of the Large Language Model Supply\nChain (LLMSC), a complex network of open-source components, libraries, and\ntools essential for LLM development and deployment. Despite its critical\nimportance, the LLMSC remains underexplored, particularly regarding its\nstructural characteristics, domain composition, and security vulnerabilities.\nTo address this gap, we conduct the first empirical study of the LLMSC,\nanalyzing a curated dataset of open-source packages from PyPI and NPM across 14\nfunctional domains. We construct a directed dependency graph comprising 15,725\nnodes, 10,402 edges, and 180 unique vulnerabilities to investigate the\nstructural characteristics of the LLMSC and analyze how security risks\npropagate through its dependency network. Our findings reveal that the LLMSC\nexhibits a ``locally dense, globally sparse'' topology, with 79.7% of\ndependency trees containing fewer than 5 nodes, while a few large trees\ndominate the ecosystem, accounting for 77.66% of all nodes. The graph is\ncharacterized by high-degree hubs, with the top 5 most connected nodes\naveraging 1,282 dependents each. Security analysis shows that critical\nvulnerabilities propagate to an average of 142.1 nodes at the second layer of\ndependency trees and peak at 237.8 affected nodes at the third layer. Notably,\ncascading risks are concentrated in critical hub nodes such as transformers,\nwhich directly or indirectly affect over 1,300 downstream packages. These\nfindings provide quantitative insights into the structural and security\ndynamics of the LLMSC and emphasize the need for targeted mitigation strategies\nto enhance ecosystem resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence\n(AI), driving breakthroughs in natural language understanding, text generation,\nand autonomous systems. However, the rapid growth of LLMs presents significant\nchallenges in the security and reliability of the Large Language Model Supply\nChain (LLMSC), a complex network of open-source components, libraries, and\ntools essential for LLM development and deployment. Despite its critical\nimportance, the LLMSC remains underexplored, particularly regarding its\nstructural characteristics, domain composition, and security vulnerabilities.\nTo address this gap, we conduct the first empirical study of the LLMSC,\nanalyzing a curated dataset of open-source packages from PyPI and NPM across 14\nfunctional domains. We construct a directed dependency graph comprising 15,725\nnodes, 10,402 edges, and 180 unique vulnerabilities to investigate the\nstructural characteristics of the LLMSC and analyze how security risks\npropagate through its dependency network. Our findings reveal that the LLMSC\nexhibits a ``locally dense, globally sparse'' topology, with 79.7% of\ndependency trees containing fewer than 5 nodes, while a few large trees\ndominate the ecosystem, accounting for 77.66% of all nodes. The graph is\ncharacterized by high-degree hubs, with the top 5 most connected nodes\naveraging 1,282 dependents each. Security analysis shows that critical\nvulnerabilities propagate to an average of 142.1 nodes at the second layer of\ndependency trees and peak at 237.8 affected nodes at the third layer. Notably,\ncascading risks are concentrated in critical hub nodes such as transformers,\nwhich directly or indirectly affect over 1,300 downstream packages. These\nfindings provide quantitative insights into the structural and security\ndynamics of the LLMSC and emphasize the need for targeted mitigation strategies\nto enhance ecosystem resilience."
                },
                "authors": [
                    {
                        "name": "Yanzhe Hu"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Tianyuan Nie"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20762v1",
                "updated": "2025-04-29T13:43:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    43,
                    26,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:43:26Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    43,
                    26,
                    1,
                    119,
                    0
                ],
                "title": "An Online Cross-layered Defense Strategy with Bandwidth Allocation for\n  Multi-channel Systems under DoS Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Online Cross-layered Defense Strategy with Bandwidth Allocation for\n  Multi-channel Systems under DoS Attacks"
                },
                "summary": "This paper proposes an online cross-layered defense strategy for\nmulti-channel systems with switched dynamics under DoS attacks. The enabling\ncondition of a channel under attacks is formulated with respect to attack flow\nand channel bandwidth, then a new networked control system model bridging the\ngap between system dynamics and network deployment is built. Based on this, the\ncross-layered defense strategy is proposed. It jointly optimizes the controller\ngain and bandwidth allocation of channels according to the real-time attack\nflow and system dynamics, by solving a mixed-integer semidefinite programming\nonline. A smart enumeration algorithm for non-convex bi-level optimization is\nproposed to analyze the stability under the strategy. Numerical examples are\ngiven to illustrate the high resilience from the cross-layered feature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an online cross-layered defense strategy for\nmulti-channel systems with switched dynamics under DoS attacks. The enabling\ncondition of a channel under attacks is formulated with respect to attack flow\nand channel bandwidth, then a new networked control system model bridging the\ngap between system dynamics and network deployment is built. Based on this, the\ncross-layered defense strategy is proposed. It jointly optimizes the controller\ngain and bandwidth allocation of channels according to the real-time attack\nflow and system dynamics, by solving a mixed-integer semidefinite programming\nonline. A smart enumeration algorithm for non-convex bi-level optimization is\nproposed to analyze the stability under the strategy. Numerical examples are\ngiven to illustrate the high resilience from the cross-layered feature."
                },
                "authors": [
                    {
                        "name": "Liheng Wan"
                    },
                    {
                        "name": "Panshuo Li"
                    },
                    {
                        "name": "James Lam"
                    }
                ],
                "author_detail": {
                    "name": "James Lam"
                },
                "author": "James Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20013v2",
                "updated": "2025-04-29T13:41:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    41,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T17:32:38Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    32,
                    38,
                    0,
                    118,
                    0
                ],
                "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation"
                },
                "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."
                },
                "authors": [
                    {
                        "name": "Beizhe Hu"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Juan Cao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Danding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Danding Wang"
                },
                "author": "Danding Wang",
                "arxiv_doi": "10.1145/3726302.3730027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM SIGIR 2025 Full Paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20756v1",
                "updated": "2025-04-29T13:34:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    34,
                    52,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T13:34:52Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    34,
                    52,
                    1,
                    119,
                    0
                ],
                "title": "Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive\n  Segmentation and Structural Feature Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive\n  Segmentation and Structural Feature Integration"
                },
                "summary": "This paper proposes a novel graph-based framework for robust and\ninterpretable multiclass fault diagnosis in rotating machinery. The method\nintegrates entropy-optimized signal segmentation, time-frequency feature\nextraction, and graph-theoretic modeling to transform vibration signals into\nstructured representations suitable for classification. Graph metrics, such as\naverage shortest path length, modularity, and spectral gap, are computed and\ncombined with local features to capture global and segment-level fault\ncharacteristics. The proposed method achieves high diagnostic accuracy when\nevaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP\nloads) and the SU gearbox and bearing datasets (under different speed-load\nconfigurations). Classification scores reach up to 99.8% accuracy on Case\nWestern Reserve University (CWRU) and 100% accuracy on the Southeast University\ndatasets using a logistic regression classifier. Furthermore, the model\nexhibits strong noise resilience, maintaining over 95.4% accuracy at high noise\nlevels (standard deviation = 0.5), and demonstrates excellent cross-domain\ntransferability with up to 99.7% F1-score in load-transfer scenarios. Compared\nto traditional techniques, this approach requires no deep learning\narchitecture, enabling lower complexity while ensuring interpretability. The\nresults confirm the method's scalability, reliability, and potential for\nreal-time deployment in industrial diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel graph-based framework for robust and\ninterpretable multiclass fault diagnosis in rotating machinery. The method\nintegrates entropy-optimized signal segmentation, time-frequency feature\nextraction, and graph-theoretic modeling to transform vibration signals into\nstructured representations suitable for classification. Graph metrics, such as\naverage shortest path length, modularity, and spectral gap, are computed and\ncombined with local features to capture global and segment-level fault\ncharacteristics. The proposed method achieves high diagnostic accuracy when\nevaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP\nloads) and the SU gearbox and bearing datasets (under different speed-load\nconfigurations). Classification scores reach up to 99.8% accuracy on Case\nWestern Reserve University (CWRU) and 100% accuracy on the Southeast University\ndatasets using a logistic regression classifier. Furthermore, the model\nexhibits strong noise resilience, maintaining over 95.4% accuracy at high noise\nlevels (standard deviation = 0.5), and demonstrates excellent cross-domain\ntransferability with up to 99.7% F1-score in load-transfer scenarios. Compared\nto traditional techniques, this approach requires no deep learning\narchitecture, enabling lower complexity while ensuring interpretability. The\nresults confirm the method's scalability, reliability, and potential for\nreal-time deployment in industrial diagnostics."
                },
                "authors": [
                    {
                        "name": "Moirangthem Tiken Singh"
                    }
                ],
                "author_detail": {
                    "name": "Moirangthem Tiken Singh"
                },
                "author": "Moirangthem Tiken Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16563v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16563v3",
                "updated": "2025-04-29T13:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    30,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-23T09:43:40Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    9,
                    43,
                    40,
                    2,
                    113,
                    0
                ],
                "title": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Agents via Global Planning and Hierarchical\n  Execution"
                },
                "summary": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agent systems based on Large Language Models (LLMs) have shown\ngreat potential in real-world applications. However, existing agent frameworks\nstill face critical limitations in task planning and execution, restricting\ntheir effectiveness and generalizability. Specifically, current planning\nmethods often lack clear global goals, leading agents to get stuck in local\nbranches, or produce non-executable plans. Meanwhile, existing execution\nmechanisms struggle to balance complexity and stability, and their limited\naction space restricts their ability to handle diverse real-world tasks. To\naddress these limitations, we propose GoalAct, a novel agent framework that\nintroduces a continuously updated global planning mechanism and integrates a\nhierarchical execution strategy. GoalAct decomposes task execution into\nhigh-level skills, including searching, coding, writing and more, thereby\nreducing planning complexity while enhancing the agents' adaptability across\ndiverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark\nwith multiple types of legal tasks that require the use of multiple types of\ntools. Experimental results demonstrate that GoalAct achieves state-of-the-art\n(SOTA) performance, with an average improvement of 12.22% in success rate.\nThese findings highlight GoalAct's potential to drive the development of more\nadvanced intelligent agent systems, making them more effective across complex\nreal-world applications. Our code can be found at\nhttps://github.com/cjj826/GoalAct."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Jingli Yang"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16563v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16563v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17857v2",
                "updated": "2025-04-29T13:13:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    13,
                    48,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-24T18:01:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    1,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation\n  Parameters with Distributional Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation\n  Parameters with Distributional Measures"
                },
                "summary": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API."
                },
                "authors": [
                    {
                        "name": "AJ Miller"
                    },
                    {
                        "name": "Fangzhou Yu"
                    },
                    {
                        "name": "Michael Brauckmann"
                    },
                    {
                        "name": "Farbod Farshidian"
                    }
                ],
                "author_detail": {
                    "name": "Farbod Farshidian"
                },
                "author": "Farbod Farshidian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19218v2",
                "updated": "2025-04-29T12:59:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    59,
                    17,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-27T12:51:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    12,
                    51,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null\n  Space of Language Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null\n  Space of Language Embeddings"
                },
                "summary": "Recent advancements in sequential recommendation have underscored the\npotential of Large Language Models (LLMs) for enhancing item embeddings.\nHowever, existing approaches face three key limitations: 1) the degradation of\nthe semantic space when high-dimensional language embeddings are mapped to\nlower-dimensional ID embeddings, 2) the underutilization of language\nembeddings, and 3) the reliance on additional trainable parameters, such as an\nadapter, to bridge the gap between the semantic and behavior spaces. In this\npaper, we introduce AlphaFuse, a simple but effective language-guided learning\nstrategy that addresses these challenges by learning ID embeddings within the\nnull space of language embeddings. Specifically, we decompose the semantic\nspace of language embeddings via Singular Value Decomposition (SVD),\ndistinguishing it into a semantic-rich row space and a semantic-sparse null\nspace. Collaborative signals are then injected into the null space, while\npreserving the rich semantics of the row space. AlphaFuse prevents degradation\nof the semantic space, integrates the retained language embeddings into the\nfinal item embeddings, and eliminates the need for auxiliary trainable modules,\nenabling seamless adaptation to any sequential recommendation framework. We\nvalidate the effectiveness and flexibility of AlphaFuse through extensive\nexperiments on three benchmark datasets, including cold-start user and\nlong-tail settings, showcasing significant improvements in both discriminative\nand diffusion-based generative sequential recommenders. Our codes and datasets\nare available at https://github.com/Hugo-Chinn/AlphaFuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in sequential recommendation have underscored the\npotential of Large Language Models (LLMs) for enhancing item embeddings.\nHowever, existing approaches face three key limitations: 1) the degradation of\nthe semantic space when high-dimensional language embeddings are mapped to\nlower-dimensional ID embeddings, 2) the underutilization of language\nembeddings, and 3) the reliance on additional trainable parameters, such as an\nadapter, to bridge the gap between the semantic and behavior spaces. In this\npaper, we introduce AlphaFuse, a simple but effective language-guided learning\nstrategy that addresses these challenges by learning ID embeddings within the\nnull space of language embeddings. Specifically, we decompose the semantic\nspace of language embeddings via Singular Value Decomposition (SVD),\ndistinguishing it into a semantic-rich row space and a semantic-sparse null\nspace. Collaborative signals are then injected into the null space, while\npreserving the rich semantics of the row space. AlphaFuse prevents degradation\nof the semantic space, integrates the retained language embeddings into the\nfinal item embeddings, and eliminates the need for auxiliary trainable modules,\nenabling seamless adaptation to any sequential recommendation framework. We\nvalidate the effectiveness and flexibility of AlphaFuse through extensive\nexperiments on three benchmark datasets, including cold-start user and\nlong-tail settings, showcasing significant improvements in both discriminative\nand diffusion-based generative sequential recommenders. Our codes and datasets\nare available at https://github.com/Hugo-Chinn/AlphaFuse."
                },
                "authors": [
                    {
                        "name": "Guoqing Hu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Zhibo Cai"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_doi": "10.1145/3726302.3729894",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3729894",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by SIGIR'25",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15358v2",
                "updated": "2025-04-29T12:42:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    42,
                    33,
                    1,
                    119,
                    0
                ],
                "published": "2025-03-19T15:58:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    58,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation"
                },
                "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."
                },
                "authors": [
                    {
                        "name": "Thomas Pickard"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Maggie Mi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Dylan Phelps"
                    },
                    {
                        "name": "Marco Idiart"
                    }
                ],
                "author_detail": {
                    "name": "Marco Idiart"
                },
                "author": "Marco Idiart",
                "arxiv_comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20708v1",
                "updated": "2025-04-29T12:39:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    39,
                    7,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:39:07Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    39,
                    7,
                    1,
                    119,
                    0
                ],
                "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You\n  Think"
                },
                "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner."
                },
                "authors": [
                    {
                        "name": "Hasan Abed Al Kader Hammoud"
                    },
                    {
                        "name": "Hani Itani"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20699v1",
                "updated": "2025-04-29T12:30:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    30,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:30:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    30,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine\n  Translation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine\n  Translation?"
                },
                "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task."
                },
                "authors": [
                    {
                        "name": "Evangelia Gogoulou"
                    },
                    {
                        "name": "Shorouq Zahra"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Luise Dürlich"
                    },
                    {
                        "name": "Joakim Nivre"
                    }
                ],
                "author_detail": {
                    "name": "Joakim Nivre"
                },
                "author": "Joakim Nivre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20684v1",
                "updated": "2025-04-29T12:07:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    7,
                    39,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T12:07:39Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    12,
                    7,
                    39,
                    1,
                    119,
                    0
                ],
                "title": "Identifying Uncertainty in Self-Adaptive Robotics with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Uncertainty in Self-Adaptive Robotics with Large Language\n  Models"
                },
                "summary": "Future self-adaptive robots are expected to operate in highly dynamic\nenvironments while effectively managing uncertainties. However, identifying the\nsources and impacts of uncertainties in such robotic systems and defining\nappropriate mitigation strategies is challenging due to the inherent complexity\nof self-adaptive robots and the lack of comprehensive knowledge about the\nvarious factors influencing uncertainty. Hence, practitioners often rely on\nintuition and past experiences from similar systems to address uncertainties.\nIn this article, we evaluate the potential of large language models (LLMs) in\nenabling a systematic and automated approach to identify uncertainties in\nself-adaptive robotics throughout the software engineering lifecycle. For this\nevaluation, we analyzed 10 advanced LLMs with varying capabilities across four\nindustrial-sized robotics case studies, gathering the practitioners'\nperspectives on the LLM-generated responses related to uncertainties. Results\nshowed that practitioners agreed with 63-88% of the LLM responses and expressed\nstrong interest in the practicality of LLMs for this purpose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future self-adaptive robots are expected to operate in highly dynamic\nenvironments while effectively managing uncertainties. However, identifying the\nsources and impacts of uncertainties in such robotic systems and defining\nappropriate mitigation strategies is challenging due to the inherent complexity\nof self-adaptive robots and the lack of comprehensive knowledge about the\nvarious factors influencing uncertainty. Hence, practitioners often rely on\nintuition and past experiences from similar systems to address uncertainties.\nIn this article, we evaluate the potential of large language models (LLMs) in\nenabling a systematic and automated approach to identify uncertainties in\nself-adaptive robotics throughout the software engineering lifecycle. For this\nevaluation, we analyzed 10 advanced LLMs with varying capabilities across four\nindustrial-sized robotics case studies, gathering the practitioners'\nperspectives on the LLM-generated responses related to uncertainties. Results\nshowed that practitioners agreed with 63-88% of the LLM responses and expressed\nstrong interest in the practicality of LLMs for this purpose."
                },
                "authors": [
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Jalil Boudjadar"
                    },
                    {
                        "name": "Mirgita Frasheri"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Peter Gorm Larsen"
                    }
                ],
                "author_detail": {
                    "name": "Peter Gorm Larsen"
                },
                "author": "Peter Gorm Larsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20673v1",
                "updated": "2025-04-29T11:57:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    57,
                    23,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:57:23Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    57,
                    23,
                    1,
                    119,
                    0
                ],
                "title": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language\n  Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language\n  Model Evaluation"
                },
                "summary": "Large language models (LLMs) play a crucial role in software engineering,\nexcelling in tasks like code generation and maintenance. However, existing\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\ncomprehensive evaluation framework that reflects real-world applications. To\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\ncode generation, code modification, and code review. These dimensions capture\nessential developer needs, ensuring a more systematic and representative\nevaluation. CoCo-Bench includes multiple programming languages and varying task\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\nuncovering significant variations in model performance, effectively\nhighlighting strengths and weaknesses. By offering a holistic and objective\nevaluation, CoCo-Bench provides valuable insights to guide future research and\ntechnological advancements in code-oriented LLMs, establishing a reliable\nbenchmark for the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) play a crucial role in software engineering,\nexcelling in tasks like code generation and maintenance. However, existing\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\ncomprehensive evaluation framework that reflects real-world applications. To\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\ncode generation, code modification, and code review. These dimensions capture\nessential developer needs, ensuring a more systematic and representative\nevaluation. CoCo-Bench includes multiple programming languages and varying task\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\nuncovering significant variations in model performance, effectively\nhighlighting strengths and weaknesses. By offering a holistic and objective\nevaluation, CoCo-Bench provides valuable insights to guide future research and\ntechnological advancements in code-oriented LLMs, establishing a reliable\nbenchmark for the field."
                },
                "authors": [
                    {
                        "name": "Wenjing Yin"
                    },
                    {
                        "name": "Tianze Sun"
                    },
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Jiawei Fang"
                    },
                    {
                        "name": "Guangyao Su"
                    },
                    {
                        "name": "Jiancheng Wang"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ziyun Dai"
                    },
                    {
                        "name": "Shuai Yuan"
                    },
                    {
                        "name": "Menghang Dong"
                    },
                    {
                        "name": "Peng Luo"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Da Lei"
                    },
                    {
                        "name": "Yajun Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xiang Ma"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Yuanjian Xu"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "Submitted to ACL 2025. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20668v1",
                "updated": "2025-04-29T11:49:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    49,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:49:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    49,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and\n  Retrieving Claims from Social Media Platforms in Multiple Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and\n  Retrieving Claims from Social Media Platforms in Multiple Languages"
                },
                "summary": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Martin Hyben"
                    },
                    {
                        "name": "Robert Moro"
                    },
                    {
                        "name": "Michal Gregor"
                    },
                    {
                        "name": "Jakub Simko"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Simko"
                },
                "author": "Jakub Simko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02467v2",
                "updated": "2025-04-29T11:33:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    33,
                    25,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-03T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    14,
                    10,
                    9,
                    1,
                    338,
                    0
                ],
                "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-2Stage: Adapting Language Models as Differentially Private Tabular\n  Data Generators"
                },
                "summary": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage."
                },
                "authors": [
                    {
                        "name": "Tejumade Afonja"
                    },
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Raouf Kerkouche"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_journal_ref": "Transactions on Machine Learning Research (03/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.6; G.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20653v1",
                "updated": "2025-04-29T11:22:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    22,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:22:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    22,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of\n  Complex Verilog Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of\n  Complex Verilog Code"
                },
                "summary": "Recent advances have demonstrated the promising capabilities of large\nlanguage models (LLMs) in generating register-transfer level (RTL) code, such\nas Verilog. However, existing LLM-based frameworks still face significant\nchallenges in accurately handling the complexity of real-world RTL designs,\nparticularly those that are large-scale and involve multi-level module\ninstantiations. To address this issue, we present ComplexVCoder, an open-source\nLLM-driven framework that enhances both the generation quality and efficiency\nof complex Verilog code. Specifically, we introduce a two-stage generation\nmechanism, which leverages an intermediate representation to enable a more\naccurate and structured transition from natural language descriptions to\nintricate Verilog designs. In addition, we introduce a rule-based alignment\nmethod and a domain-specific retrieval-augmented generation (RAG) to further\nimprove the correctness of the synthesized code by incorporating relevant\ndesign knowledge during generation. To evaluate our approach, we construct a\ncomprehensive dataset comprising 55 complex Verilog designs derived from\nreal-world implementations. We also release an open-source benchmark suite for\nsystematically assessing the quality of auto-generated RTL code together with\nthe ComplexVCoder framework. Experimental results show that ComplexVCoder\noutperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%,\nrespectively, in terms of function correctness on complex Verilog benchmarks.\nFurthermore, ComplexVcoder achieves comparable generation performances in terms\nof functionality correctness using a lightweight 32B model (Qwen2.5), rivaling\nlarger-scale models such as GPT-3.5 and DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have demonstrated the promising capabilities of large\nlanguage models (LLMs) in generating register-transfer level (RTL) code, such\nas Verilog. However, existing LLM-based frameworks still face significant\nchallenges in accurately handling the complexity of real-world RTL designs,\nparticularly those that are large-scale and involve multi-level module\ninstantiations. To address this issue, we present ComplexVCoder, an open-source\nLLM-driven framework that enhances both the generation quality and efficiency\nof complex Verilog code. Specifically, we introduce a two-stage generation\nmechanism, which leverages an intermediate representation to enable a more\naccurate and structured transition from natural language descriptions to\nintricate Verilog designs. In addition, we introduce a rule-based alignment\nmethod and a domain-specific retrieval-augmented generation (RAG) to further\nimprove the correctness of the synthesized code by incorporating relevant\ndesign knowledge during generation. To evaluate our approach, we construct a\ncomprehensive dataset comprising 55 complex Verilog designs derived from\nreal-world implementations. We also release an open-source benchmark suite for\nsystematically assessing the quality of auto-generated RTL code together with\nthe ComplexVCoder framework. Experimental results show that ComplexVCoder\noutperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%,\nrespectively, in terms of function correctness on complex Verilog benchmarks.\nFurthermore, ComplexVcoder achieves comparable generation performances in terms\nof functionality correctness using a lightweight 32B model (Qwen2.5), rivaling\nlarger-scale models such as GPT-3.5 and DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "Jian Zuo"
                    },
                    {
                        "name": "Junzhe Liu"
                    },
                    {
                        "name": "Xianyong Wang"
                    },
                    {
                        "name": "Yicheng Liu"
                    },
                    {
                        "name": "Navya Goli"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Umamaheswara Rao Tida"
                    },
                    {
                        "name": "Zhenge Jia"
                    },
                    {
                        "name": "Mengying Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mengying Zhao"
                },
                "author": "Mengying Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20644v1",
                "updated": "2025-04-29T11:13:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    18,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:13:18Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    18,
                    1,
                    119,
                    0
                ],
                "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified\n  File Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified\n  File Selection"
                },
                "summary": "Selecting high-quality pre-training data for large language models (LLMs) is\ncrucial for enhancing their overall performance under limited computation\nbudget, improving both training and sample efficiency. Recent advancements in\nfile selection primarily rely on using an existing or trained proxy model to\nassess the similarity of samples to a target domain, such as high quality\nsources BookCorpus and Wikipedia. However, upon revisiting these methods, the\ndomain-similarity selection criteria demonstrates a diversity dilemma,\ni.e.dimensional collapse in the feature space, improving performance on the\ndomain-related tasks but causing severe degradation on generic performance. To\nprevent collapse and enhance diversity, we propose a DiverSified File selection\nalgorithm (DiSF), which selects the most decorrelated text files in the feature\nspace. We approach this with a classical greedy algorithm to achieve more\nuniform eigenvalues in the feature covariance matrix of the selected texts,\nanalyzing its approximation to the optimal solution under a formulation of\n$\\gamma$-weakly submodular optimization problem. Empirically, we establish a\nbenchmark and conduct extensive experiments on the TinyLlama architecture with\nmodels from 120M to 1.1B parameters. Evaluating across nine tasks from the\nHarness framework, DiSF demonstrates a significant improvement on overall\nperformance. Specifically, DiSF saves 98.5% of 590M training files in\nSlimPajama, outperforming the full-data pre-training within a 50B training\nbudget, and achieving about 1.5x training efficiency and 5x data efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting high-quality pre-training data for large language models (LLMs) is\ncrucial for enhancing their overall performance under limited computation\nbudget, improving both training and sample efficiency. Recent advancements in\nfile selection primarily rely on using an existing or trained proxy model to\nassess the similarity of samples to a target domain, such as high quality\nsources BookCorpus and Wikipedia. However, upon revisiting these methods, the\ndomain-similarity selection criteria demonstrates a diversity dilemma,\ni.e.dimensional collapse in the feature space, improving performance on the\ndomain-related tasks but causing severe degradation on generic performance. To\nprevent collapse and enhance diversity, we propose a DiverSified File selection\nalgorithm (DiSF), which selects the most decorrelated text files in the feature\nspace. We approach this with a classical greedy algorithm to achieve more\nuniform eigenvalues in the feature covariance matrix of the selected texts,\nanalyzing its approximation to the optimal solution under a formulation of\n$\\gamma$-weakly submodular optimization problem. Empirically, we establish a\nbenchmark and conduct extensive experiments on the TinyLlama architecture with\nmodels from 120M to 1.1B parameters. Evaluating across nine tasks from the\nHarness framework, DiSF demonstrates a significant improvement on overall\nperformance. Specifically, DiSF saves 98.5% of 590M training files in\nSlimPajama, outperforming the full-data pre-training within a 50B training\nbudget, and achieving about 1.5x training efficiency and 5x data efficiency."
                },
                "authors": [
                    {
                        "name": "Ziqing Fan"
                    },
                    {
                        "name": "Siyuan Du"
                    },
                    {
                        "name": "Shengchao Hu"
                    },
                    {
                        "name": "Pingjie Wang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Yanfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Wang"
                },
                "author": "Yanfeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20643v1",
                "updated": "2025-04-29T11:13:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    6,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:13:06Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    13,
                    6,
                    1,
                    119,
                    0
                ],
                "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM\n  Creativity through Structured Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM\n  Creativity through Structured Representations"
                },
                "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI."
                },
                "authors": [
                    {
                        "name": "Moran Mizrahi"
                    },
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Dafna Shahaf"
                    }
                ],
                "author_detail": {
                    "name": "Dafna Shahaf"
                },
                "author": "Dafna Shahaf",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20635v1",
                "updated": "2025-04-29T11:04:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    4,
                    28,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T11:04:28Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    11,
                    4,
                    28,
                    1,
                    119,
                    0
                ],
                "title": "Bridging the Generalisation Gap: Synthetic Data Generation for\n  Multi-Site Clinical Model Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Generalisation Gap: Synthetic Data Generation for\n  Multi-Site Clinical Model Validation"
                },
                "summary": "Ensuring the generalisability of clinical machine learning (ML) models across\ndiverse healthcare settings remains a significant challenge due to variability\nin patient demographics, disease prevalence, and institutional practices.\nExisting model evaluation approaches often rely on real-world datasets, which\nare limited in availability, embed confounding biases, and lack the flexibility\nneeded for systematic experimentation. Furthermore, while generative models aim\nfor statistical realism, they often lack transparency and explicit control over\nfactors driving distributional shifts. In this work, we propose a novel\nstructured synthetic data framework designed for the controlled benchmarking of\nmodel robustness, fairness, and generalisability. Unlike approaches focused\nsolely on mimicking observed data, our framework provides explicit control over\nthe data generating process, including site-specific prevalence variations,\nhierarchical subgroup effects, and structured feature interactions. This\nenables targeted investigation into how models respond to specific\ndistributional shifts and potential biases. Through controlled experiments, we\ndemonstrate the framework's ability to isolate the impact of site variations,\nsupport fairness-aware audits, and reveal generalisation failures, particularly\nhighlighting how model complexity interacts with site-specific effects. This\nwork contributes a reproducible, interpretable, and configurable tool designed\nto advance the reliable deployment of ML in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the generalisability of clinical machine learning (ML) models across\ndiverse healthcare settings remains a significant challenge due to variability\nin patient demographics, disease prevalence, and institutional practices.\nExisting model evaluation approaches often rely on real-world datasets, which\nare limited in availability, embed confounding biases, and lack the flexibility\nneeded for systematic experimentation. Furthermore, while generative models aim\nfor statistical realism, they often lack transparency and explicit control over\nfactors driving distributional shifts. In this work, we propose a novel\nstructured synthetic data framework designed for the controlled benchmarking of\nmodel robustness, fairness, and generalisability. Unlike approaches focused\nsolely on mimicking observed data, our framework provides explicit control over\nthe data generating process, including site-specific prevalence variations,\nhierarchical subgroup effects, and structured feature interactions. This\nenables targeted investigation into how models respond to specific\ndistributional shifts and potential biases. Through controlled experiments, we\ndemonstrate the framework's ability to isolate the impact of site variations,\nsupport fairness-aware audits, and reveal generalisation failures, particularly\nhighlighting how model complexity interacts with site-specific effects. This\nwork contributes a reproducible, interpretable, and configurable tool designed\nto advance the reliable deployment of ML in clinical settings."
                },
                "authors": [
                    {
                        "name": "Bradley Segal"
                    },
                    {
                        "name": "Joshua Fieggen"
                    },
                    {
                        "name": "David Clifton"
                    },
                    {
                        "name": "Lei Clifton"
                    }
                ],
                "author_detail": {
                    "name": "Lei Clifton"
                },
                "author": "Lei Clifton",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20624v1",
                "updated": "2025-04-29T10:51:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    51,
                    58,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:51:58Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    51,
                    58,
                    1,
                    119,
                    0
                ],
                "title": "PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time\n  Retrieval"
                },
                "summary": "Social chatbots have become essential intelligent companions in daily\nscenarios ranging from emotional support to personal interaction. However,\nconventional chatbots with passive response mechanisms usually rely on users to\ninitiate or sustain dialogues by bringing up new topics, resulting in\ndiminished engagement and shortened dialogue duration. In this paper, we\npresent PaRT, a novel framework enabling context-aware proactive dialogues for\nsocial chatbots through personalized real-time retrieval and generation.\nSpecifically, PaRT first integrates user profiles and dialogue context into a\nlarge language model (LLM), which is initially prompted to refine user queries\nand recognize their underlying intents for the upcoming conversation. Guided by\nrefined intents, the LLM generates personalized dialogue topics, which then\nserve as targeted queries to retrieve relevant passages from RedNote. Finally,\nwe prompt LLMs with summarized passages to generate knowledge-grounded and\nengagement-optimized responses. Our approach has been running stably in a\nreal-world production environment for more than 30 days, achieving a 21.77\\%\nimprovement in the average duration of dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social chatbots have become essential intelligent companions in daily\nscenarios ranging from emotional support to personal interaction. However,\nconventional chatbots with passive response mechanisms usually rely on users to\ninitiate or sustain dialogues by bringing up new topics, resulting in\ndiminished engagement and shortened dialogue duration. In this paper, we\npresent PaRT, a novel framework enabling context-aware proactive dialogues for\nsocial chatbots through personalized real-time retrieval and generation.\nSpecifically, PaRT first integrates user profiles and dialogue context into a\nlarge language model (LLM), which is initially prompted to refine user queries\nand recognize their underlying intents for the upcoming conversation. Guided by\nrefined intents, the LLM generates personalized dialogue topics, which then\nserve as targeted queries to retrieve relevant passages from RedNote. Finally,\nwe prompt LLMs with summarized passages to generate knowledge-grounded and\nengagement-optimized responses. Our approach has been running stably in a\nreal-world production environment for more than 30 days, achieving a 21.77\\%\nimprovement in the average duration of dialogues."
                },
                "authors": [
                    {
                        "name": "Zihan Niu"
                    },
                    {
                        "name": "Zheyong Xie"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Chonggang Lu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20612v1",
                "updated": "2025-04-29T10:23:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    23,
                    11,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:23:11Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    23,
                    11,
                    1,
                    119,
                    0
                ],
                "title": "The Hidden Risks of LLM-Generated Web Application Code: A\n  Security-Centric Evaluation of Code Generation Capabilities in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Risks of LLM-Generated Web Application Code: A\n  Security-Centric Evaluation of Code Generation Capabilities in Large Language\n  Models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications."
                },
                "authors": [
                    {
                        "name": "Swaroop Dora"
                    },
                    {
                        "name": "Deven Lunkad"
                    },
                    {
                        "name": "Naziya Aslam"
                    },
                    {
                        "name": "S. Venkatesan"
                    },
                    {
                        "name": "Sandeep Kumar Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Kumar Shukla"
                },
                "author": "Sandeep Kumar Shukla",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20610v1",
                "updated": "2025-04-29T10:21:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    21,
                    40,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:21:40Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    21,
                    40,
                    1,
                    119,
                    0
                ],
                "title": "Information Retrieval in the Age of Generative AI: The RGB Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Retrieval in the Age of Generative AI: The RGB Model"
                },
                "summary": "The advent of Large Language Models (LLMs) and generative AI is fundamentally\ntransforming information retrieval and processing on the Internet, bringing\nboth great potential and significant concerns regarding content authenticity\nand reliability. This paper presents a novel quantitative approach to shed\nlight on the complex information dynamics arising from the growing use of\ngenerative AI tools. Despite their significant impact on the digital ecosystem,\nthese dynamics remain largely uncharted and poorly understood. We propose a\nstochastic model to characterize the generation, indexing, and dissemination of\ninformation in response to new topics. This scenario particularly challenges\ncurrent LLMs, which often rely on real-time Retrieval-Augmented Generation\n(RAG) techniques to overcome their static knowledge limitations. Our findings\nsuggest that the rapid pace of generative AI adoption, combined with increasing\nuser reliance, can outpace human verification, escalating the risk of\ninaccurate information proliferation across digital resources. An in-depth\nanalysis of Stack Exchange data confirms that high-quality answers inevitably\nrequire substantial time and human effort to emerge. This underscores the\nconsiderable risks associated with generating persuasive text in response to\nnew questions and highlights the critical need for responsible development and\ndeployment of future generative AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) and generative AI is fundamentally\ntransforming information retrieval and processing on the Internet, bringing\nboth great potential and significant concerns regarding content authenticity\nand reliability. This paper presents a novel quantitative approach to shed\nlight on the complex information dynamics arising from the growing use of\ngenerative AI tools. Despite their significant impact on the digital ecosystem,\nthese dynamics remain largely uncharted and poorly understood. We propose a\nstochastic model to characterize the generation, indexing, and dissemination of\ninformation in response to new topics. This scenario particularly challenges\ncurrent LLMs, which often rely on real-time Retrieval-Augmented Generation\n(RAG) techniques to overcome their static knowledge limitations. Our findings\nsuggest that the rapid pace of generative AI adoption, combined with increasing\nuser reliance, can outpace human verification, escalating the risk of\ninaccurate information proliferation across digital resources. An in-depth\nanalysis of Stack Exchange data confirms that high-quality answers inevitably\nrequire substantial time and human effort to emerge. This underscores the\nconsiderable risks associated with generating persuasive text in response to\nnew questions and highlights the critical need for responsible development and\ndeployment of future generative AI tools."
                },
                "authors": [
                    {
                        "name": "Michele Garetto"
                    },
                    {
                        "name": "Alessandro Cornacchia"
                    },
                    {
                        "name": "Franco Galante"
                    },
                    {
                        "name": "Emilio Leonardi"
                    },
                    {
                        "name": "Alessandro Nordio"
                    },
                    {
                        "name": "Alberto Tarable"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Tarable"
                },
                "author": "Alberto Tarable",
                "arxiv_comment": "To be presented at ACM SIGIR 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20609v1",
                "updated": "2025-04-29T10:19:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    19,
                    5,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T10:19:05Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    19,
                    5,
                    1,
                    119,
                    0
                ],
                "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WenyanGPT: A Large Language Model for Classical Chinese Tasks"
                },
                "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing."
                },
                "authors": [
                    {
                        "name": "Xinyu Yao"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaobing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobing Zhao"
                },
                "author": "Xiaobing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01070v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01070v3",
                "updated": "2025-04-29T10:17:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    10,
                    17,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2025-02-03T05:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    26,
                    22,
                    0,
                    34,
                    0
                ],
                "title": "An Inquiry into Datacenter TCO for LLM Inference with FP8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Inquiry into Datacenter TCO for LLM Inference with FP8"
                },
                "summary": "As large language models (LLMs) continue to scale, their inference demands\npresent significant challenges, particularly due to the high power consumption\nof AI accelerators in datacenters. These facilities require specialized cooling\nand power management systems, substantially increasing the total cost of\nownership (TCO) for cloud service providers (CSPs). In this work, we analyze\nthe computational characteristics and constraints of LLM inference from a TCO\nperspective, focusing on two representative accelerators: the Gaudi 2 and\nNVIDIA H100. We present a generalizable framework that enables CSPs to compare\nand select AI accelerators according to diverse operational requirements. Using\nthis model, we analyze the impact of FP8 precision and LLM inference workload\ncharacteristics as key factors influencing TCO. We investigate FP8\nquantization, which is gaining adoption in LLM training, as a technique to\nimprove inference throughput while maintaining cost efficiency. Furthermore,\nour analysis of LLM inference workloads reveals that performance on thin GEMMs,\nwhich dominate the decode phase, can have a greater impact than theoretical\nhardware peak performance. By studying the interaction between power\nconsumption, quantization strategies, and hardware architecture, we offer\ninsights that support informed deployment decisions and guide future\naccelerator designs to improve the TCO of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, their inference demands\npresent significant challenges, particularly due to the high power consumption\nof AI accelerators in datacenters. These facilities require specialized cooling\nand power management systems, substantially increasing the total cost of\nownership (TCO) for cloud service providers (CSPs). In this work, we analyze\nthe computational characteristics and constraints of LLM inference from a TCO\nperspective, focusing on two representative accelerators: the Gaudi 2 and\nNVIDIA H100. We present a generalizable framework that enables CSPs to compare\nand select AI accelerators according to diverse operational requirements. Using\nthis model, we analyze the impact of FP8 precision and LLM inference workload\ncharacteristics as key factors influencing TCO. We investigate FP8\nquantization, which is gaining adoption in LLM training, as a technique to\nimprove inference throughput while maintaining cost efficiency. Furthermore,\nour analysis of LLM inference workloads reveals that performance on thin GEMMs,\nwhich dominate the decode phase, can have a greater impact than theoretical\nhardware peak performance. By studying the interaction between power\nconsumption, quantization strategies, and hardware architecture, we offer\ninsights that support informed deployment decisions and guide future\naccelerator designs to improve the TCO of LLM inference."
                },
                "authors": [
                    {
                        "name": "Jiwoo Kim"
                    },
                    {
                        "name": "Joonhyung Lee"
                    },
                    {
                        "name": "Gunho Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    },
                    {
                        "name": "Youngjoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Lee"
                },
                "author": "Youngjoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01070v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01070v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20595v1",
                "updated": "2025-04-29T09:49:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    49,
                    28,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:49:28Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    49,
                    28,
                    1,
                    119,
                    0
                ],
                "title": "ReasonIR: Training Retrievers for Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonIR: Training Retrievers for Reasoning Tasks"
                },
                "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model."
                },
                "authors": [
                    {
                        "name": "Rulin Shao"
                    },
                    {
                        "name": "Rui Qiao"
                    },
                    {
                        "name": "Varsha Kishore"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Sewon Min"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    }
                ],
                "author_detail": {
                    "name": "Luke Zettlemoyer"
                },
                "author": "Luke Zettlemoyer",
                "arxiv_comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20584v1",
                "updated": "2025-04-29T09:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    39,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    39,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Hydra: Marker-Free RGB-D Hand-Eye Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Marker-Free RGB-D Hand-Eye Calibration"
                },
                "summary": "This work presents an RGB-D imaging-based approach to marker-free hand-eye\ncalibration using a novel implementation of the iterative closest point (ICP)\nalgorithm with a robust point-to-plane (PTP) objective formulated on a Lie\nalgebra. Its applicability is demonstrated through comprehensive experiments\nusing three well known serial manipulators and two RGB-D cameras. With only\nthree randomly chosen robot configurations, our approach achieves approximately\n90% successful calibrations, demonstrating 2-3x higher convergence rates to the\nglobal optimum compared to both marker-based and marker-free baselines. We also\nreport 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9\nrobot configurations over other marker-free methods. Our method exhibits\nsignificantly improved accuracy (5 mm in task space) over classical approaches\n(7 mm in task space) whilst being marker-free. The benchmarking dataset and\ncode are open sourced under Apache 2.0 License, and a ROS 2 integration with\nrobot abstraction is provided to facilitate deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an RGB-D imaging-based approach to marker-free hand-eye\ncalibration using a novel implementation of the iterative closest point (ICP)\nalgorithm with a robust point-to-plane (PTP) objective formulated on a Lie\nalgebra. Its applicability is demonstrated through comprehensive experiments\nusing three well known serial manipulators and two RGB-D cameras. With only\nthree randomly chosen robot configurations, our approach achieves approximately\n90% successful calibrations, demonstrating 2-3x higher convergence rates to the\nglobal optimum compared to both marker-based and marker-free baselines. We also\nreport 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9\nrobot configurations over other marker-free methods. Our method exhibits\nsignificantly improved accuracy (5 mm in task space) over classical approaches\n(7 mm in task space) whilst being marker-free. The benchmarking dataset and\ncode are open sourced under Apache 2.0 License, and a ROS 2 integration with\nrobot abstraction is provided to facilitate deployment."
                },
                "authors": [
                    {
                        "name": "Martin Huber"
                    },
                    {
                        "name": "Huanyu Tian"
                    },
                    {
                        "name": "Christopher E. Mower"
                    },
                    {
                        "name": "Lucas-Raphael Müller"
                    },
                    {
                        "name": "Sébastien Ourselin"
                    },
                    {
                        "name": "Christos Bergeles"
                    },
                    {
                        "name": "Tom Vercauteren"
                    }
                ],
                "author_detail": {
                    "name": "Tom Vercauteren"
                },
                "author": "Tom Vercauteren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20571v1",
                "updated": "2025-04-29T09:24:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    24,
                    30,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:24:30Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    24,
                    30,
                    1,
                    119,
                    0
                ],
                "title": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Reasoning in Large Language Models with One\n  Training Example"
                },
                "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR"
                },
                "authors": [
                    {
                        "name": "Yiping Wang"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Lucas Liu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Xuehai He"
                    },
                    {
                        "name": "Kuan Wang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Simon Shaolei Du"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20570v1",
                "updated": "2025-04-29T09:23:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    23,
                    19,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T09:23:19Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    23,
                    19,
                    1,
                    119,
                    0
                ],
                "title": "ReCIT: Reconstructing Full Private Data from Gradient in\n  Parameter-Efficient Fine-Tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCIT: Reconstructing Full Private Data from Gradient in\n  Parameter-Efficient Fine-Tuning of Large Language Models"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution\nfor adapting large language models (LLMs) to custom datasets with significantly\nreduced computational cost. When carrying out PEFT under collaborative learning\nscenarios (e.g., federated learning), it is often required to exchange model\nupdates (or gradients) across parties. These gradients, even with limited\ndimensions, can cause severe breach of data privacy. Recent works have shown\nthat both contextual prefixes and personally identifiable information (PII) can\nbe exposed through gradients. However, \\emph{simultaneously} and\n\\emph{accurately} recovering both components from the same training instance\nremains infeasible due to the following challenges: 1) limited number of PEFT\nparameters; 2) high-dimensional token spaces; and 3) large batch sizes. We\npropose ReCIT, a novel privacy attack that addresses all challenges, and\nachieves recovery of \\emph{full} private data from PEFT gradients with high\nfidelity. Specifically, ReCIT proposes to enhance the memorization capability\nof the pre-trained model through malicious fine-tuning with Personal Notes;\nReCIT also proposes a novel filter-based token extraction technique and a token\npairing mechanism, to accurately reconstruct tokens from the training sequences\nwith large batch sizes. Extensive evaluations show that ReCIT consistently\noutperforms state-of-the-art gradient inversion and memorization-based attacks\nacross different PEFT paradigms. It achieves up to 10$\\times$ higher PII\nrecovery rates and remains effective across varying batch sizes, especially in\nsettings where prefix reconstruction is intractable for conventional\napproaches. These findings highlight an urgent need to reassess the privacy\nguarantees of PEFT, especially in decentralized or shared training\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution\nfor adapting large language models (LLMs) to custom datasets with significantly\nreduced computational cost. When carrying out PEFT under collaborative learning\nscenarios (e.g., federated learning), it is often required to exchange model\nupdates (or gradients) across parties. These gradients, even with limited\ndimensions, can cause severe breach of data privacy. Recent works have shown\nthat both contextual prefixes and personally identifiable information (PII) can\nbe exposed through gradients. However, \\emph{simultaneously} and\n\\emph{accurately} recovering both components from the same training instance\nremains infeasible due to the following challenges: 1) limited number of PEFT\nparameters; 2) high-dimensional token spaces; and 3) large batch sizes. We\npropose ReCIT, a novel privacy attack that addresses all challenges, and\nachieves recovery of \\emph{full} private data from PEFT gradients with high\nfidelity. Specifically, ReCIT proposes to enhance the memorization capability\nof the pre-trained model through malicious fine-tuning with Personal Notes;\nReCIT also proposes a novel filter-based token extraction technique and a token\npairing mechanism, to accurately reconstruct tokens from the training sequences\nwith large batch sizes. Extensive evaluations show that ReCIT consistently\noutperforms state-of-the-art gradient inversion and memorization-based attacks\nacross different PEFT paradigms. It achieves up to 10$\\times$ higher PII\nrecovery rates and remains effective across varying batch sizes, especially in\nsettings where prefix reconstruction is intractable for conventional\napproaches. These findings highlight an urgent need to reassess the privacy\nguarantees of PEFT, especially in decentralized or shared training\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Ruishi He"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shouling Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shouling Ji"
                },
                "author": "Shouling Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17593v2",
                "updated": "2025-04-29T09:21:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    9,
                    21,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2024-12-23T14:10:09Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    10,
                    9,
                    0,
                    358,
                    0
                ],
                "title": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Memory Retrieval to Enhance LLM-based Generative\n  Recommendation"
                },
                "summary": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLMs) to harness user-item interaction\nhistories for item generation has emerged as a promising paradigm in generative\nrecommendation. However, the limited context window of LLMs often restricts\nthem to focusing on recent user interactions only, leading to the neglect of\nlong-term interests involved in the longer histories. To address this\nchallenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR),\nwhich is capable of storing long-term interests in the memory and extracting\nrelevant information from it for next-item generation within LLMs. Extensive\nexperimental results on two real-world datasets demonstrate the effectiveness\nof our proposed AutoMR framework in utilizing long-term interests for\ngenerative recommendation."
                },
                "authors": [
                    {
                        "name": "Chengbing Wang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fengbin Zhu"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Tianhao Shi"
                    },
                    {
                        "name": "Fuli Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Feng"
                },
                "author": "Fuli Feng",
                "arxiv_comment": "Accepted by WWW'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20557v1",
                "updated": "2025-04-29T08:57:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    57,
                    47,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T08:57:47Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    57,
                    47,
                    1,
                    119,
                    0
                ],
                "title": "SNR-aware Semantic Image Transmission with Deep Learning-based Channel\n  Estimation in Fading Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNR-aware Semantic Image Transmission with Deep Learning-based Channel\n  Estimation in Fading Channels"
                },
                "summary": "Semantic communications (SCs) play a central role in shaping the future of\nthe sixth generation (6G) wireless systems, which leverage rapid advances in\ndeep learning (DL). In this regard, end-to-end optimized DL-based joint\nsource-channel coding (JSCC) has been adopted to achieve SCs, particularly in\nimage transmission. Utilizing vision transformers in the encoder/decoder design\nhas enabled significant advancements in image semantic extraction, surpassing\ntraditional convolutional neural networks (CNNs). In this paper, we propose a\nnew JSCC paradigm for image transmission, namely Swin semantic image\ntransmission (SwinSIT), based on the Swin transformer. The Swin transformer is\nemployed to construct both the semantic encoder and decoder for efficient image\nsemantic extraction and reconstruction. Inspired by the\nsqueezing-and-excitation (SE) network, we introduce a signal-to-noise-ratio\n(SNR)-aware module that utilizes SNR feedback to adaptively perform a\ndouble-phase enhancement for the encoder-extracted semantic map and its noisy\nversion at the decoder. Additionally, a CNN-based channel estimator and\ncompensator (CEAC) module repurposes an image-denoising CNN to mitigate fading\nchannel effects. To optimize deployment in resource-constrained IoT devices, a\njoint pruning and quantization scheme compresses the SwinSIT model. Simulations\nevaluate the SwinSIT performance against conventional benchmarks demonstrating\nits effectiveness. Moreover, the model's compressed version substantially\nreduces its size while maintaining favorable PSNR performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communications (SCs) play a central role in shaping the future of\nthe sixth generation (6G) wireless systems, which leverage rapid advances in\ndeep learning (DL). In this regard, end-to-end optimized DL-based joint\nsource-channel coding (JSCC) has been adopted to achieve SCs, particularly in\nimage transmission. Utilizing vision transformers in the encoder/decoder design\nhas enabled significant advancements in image semantic extraction, surpassing\ntraditional convolutional neural networks (CNNs). In this paper, we propose a\nnew JSCC paradigm for image transmission, namely Swin semantic image\ntransmission (SwinSIT), based on the Swin transformer. The Swin transformer is\nemployed to construct both the semantic encoder and decoder for efficient image\nsemantic extraction and reconstruction. Inspired by the\nsqueezing-and-excitation (SE) network, we introduce a signal-to-noise-ratio\n(SNR)-aware module that utilizes SNR feedback to adaptively perform a\ndouble-phase enhancement for the encoder-extracted semantic map and its noisy\nversion at the decoder. Additionally, a CNN-based channel estimator and\ncompensator (CEAC) module repurposes an image-denoising CNN to mitigate fading\nchannel effects. To optimize deployment in resource-constrained IoT devices, a\njoint pruning and quantization scheme compresses the SwinSIT model. Simulations\nevaluate the SwinSIT performance against conventional benchmarks demonstrating\nits effectiveness. Moreover, the model's compressed version substantially\nreduces its size while maintaining favorable PSNR performance."
                },
                "authors": [
                    {
                        "name": "Mahmoud M. Salim"
                    },
                    {
                        "name": "Mohamed S. Abdalzaher"
                    },
                    {
                        "name": "Ali H. Muqaibel"
                    },
                    {
                        "name": "Hussein A. Elsayed"
                    },
                    {
                        "name": "Inkyu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Inkyu Lee"
                },
                "author": "Inkyu Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20547v1",
                "updated": "2025-04-29T08:49:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    49,
                    38,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T08:49:38Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    49,
                    38,
                    1,
                    119,
                    0
                ],
                "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for\n  Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for\n  Electronic Health Records"
                },
                "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement."
                },
                "authors": [
                    {
                        "name": "Jesus Lovon"
                    },
                    {
                        "name": "Thouria Ben-Haddi"
                    },
                    {
                        "name": "Jules Di Scala"
                    },
                    {
                        "name": "Jose G. Moreno"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "arxiv_affiliation": "IRIT-IRIS",
                "author": "Lynda Tamine",
                "arxiv_journal_ref": "Proceedings of the First Workshop on Patient-Oriented Language\n  Processing (CL4Health) @ LREC-COLING 2024, May 2024, Torino, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05676v2",
                "updated": "2025-04-29T08:43:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    43,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2024-08-11T02:31:13Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    2,
                    31,
                    13,
                    6,
                    224,
                    0
                ],
                "title": "Efficiency Unleashed: Inference Acceleration for LLM-based Recommender\n  Systems with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Unleashed: Inference Acceleration for LLM-based Recommender\n  Systems with Speculative Decoding"
                },
                "summary": "The past few years have witnessed a growing interest in LLM-based recommender\nsystems (RSs), although their industrial deployment remains in a preliminary\nstage. Most existing deployments leverage LLMs offline as feature enhancers,\ngenerating augmented knowledge for downstream tasks. However, in recommendation\nscenarios with numerous users and items, even offline knowledge generation with\nLLMs demands significant time and computational resources. This inefficiency\narises from the autoregressive nature of LLMs. A promising solution is\nspeculative decoding, a Draft-Then-Verify approach that increases the number of\ntokens generated per decoding step. In this work, we first identify\nrecommendation knowledge generation as a highly fitting use case for\nretrieval-based speculative decoding. Then, we discern its two characteristics:\n(1) the vast number of items and users in RSs leads to retrieval inefficiency,\nand (2) RSs exhibit high diversity tolerance for LLM-generated text. Building\non these insights, we introduce Lossless Acceleration via Speculative Decoding\nfor LLM-based Recommender Systems (LASER), which features a Customized\nRetrieval Pool to enhance retrieval efficiency and Relaxed Verification to\nimprove the acceptance rate of draft tokens. LASER achieves a 3-5x speedup on\npublic datasets and saves about 67\\% of computational resources during the\nonline A/B test on a large-scale advertising scenario with lossless downstream\nrecommendation performance. Our code is available at\nhttps://github.com/YunjiaXi/LASER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years have witnessed a growing interest in LLM-based recommender\nsystems (RSs), although their industrial deployment remains in a preliminary\nstage. Most existing deployments leverage LLMs offline as feature enhancers,\ngenerating augmented knowledge for downstream tasks. However, in recommendation\nscenarios with numerous users and items, even offline knowledge generation with\nLLMs demands significant time and computational resources. This inefficiency\narises from the autoregressive nature of LLMs. A promising solution is\nspeculative decoding, a Draft-Then-Verify approach that increases the number of\ntokens generated per decoding step. In this work, we first identify\nrecommendation knowledge generation as a highly fitting use case for\nretrieval-based speculative decoding. Then, we discern its two characteristics:\n(1) the vast number of items and users in RSs leads to retrieval inefficiency,\nand (2) RSs exhibit high diversity tolerance for LLM-generated text. Building\non these insights, we introduce Lossless Acceleration via Speculative Decoding\nfor LLM-based Recommender Systems (LASER), which features a Customized\nRetrieval Pool to enhance retrieval efficiency and Relaxed Verification to\nimprove the acceptance rate of draft tokens. LASER achieves a 3-5x speedup on\npublic datasets and saves about 67\\% of computational resources during the\nonline A/B test on a large-scale advertising scenario with lossless downstream\nrecommendation performance. Our code is available at\nhttps://github.com/YunjiaXi/LASER"
                },
                "authors": [
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Hangyu Wang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Yong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Yu"
                },
                "author": "Yong Yu",
                "arxiv_comment": "Accepted by SIGIR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20542v1",
                "updated": "2025-04-29T08:38:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    38,
                    30,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T08:38:30Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    38,
                    30,
                    1,
                    119,
                    0
                ],
                "title": "Digital Twin-Empowered Cooperative Autonomous Car-sharing Services:\n  Proof-of-Concept",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin-Empowered Cooperative Autonomous Car-sharing Services:\n  Proof-of-Concept"
                },
                "summary": "This paper presents a digital twin-empowered real-time optimal delivery\nsystem specifically validated through a proof-of-concept (PoC) demonstration of\na real-world autonomous car-sharing service. This study integrates real-time\ndata from roadside units (RSUs) and connected and autonomous vehicles (CAVs)\nwithin a digital twin of a campus environment to address the dynamic challenges\nof urban traffic. The proposed system leverages the Age of Information (AoI)\nmetric to optimize vehicle routing by maintaining data freshness and\ndynamically adapting to real-time traffic conditions. Experimental results from\nthe PoC demonstrate a 22% improvement in delivery efficiency compared to\nconventional shortest-path methods that do not consider information freshness.\nFurthermore, digital twin-based simulation results demonstrate that this\nproposed system improves overall delivery efficiency by 12% and effectively\nreduces the peak average AoI by 23% compared to the conventional method, where\neach vehicle selects the shortest route without considering information\nfreshness. This study confirms the practical feasibility of cooperative driving\nsystems, highlighting their potential to enhance smart mobility solutions\nthrough scalable digital twin deployments in complex urban environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a digital twin-empowered real-time optimal delivery\nsystem specifically validated through a proof-of-concept (PoC) demonstration of\na real-world autonomous car-sharing service. This study integrates real-time\ndata from roadside units (RSUs) and connected and autonomous vehicles (CAVs)\nwithin a digital twin of a campus environment to address the dynamic challenges\nof urban traffic. The proposed system leverages the Age of Information (AoI)\nmetric to optimize vehicle routing by maintaining data freshness and\ndynamically adapting to real-time traffic conditions. Experimental results from\nthe PoC demonstrate a 22% improvement in delivery efficiency compared to\nconventional shortest-path methods that do not consider information freshness.\nFurthermore, digital twin-based simulation results demonstrate that this\nproposed system improves overall delivery efficiency by 12% and effectively\nreduces the peak average AoI by 23% compared to the conventional method, where\neach vehicle selects the shortest route without considering information\nfreshness. This study confirms the practical feasibility of cooperative driving\nsystems, highlighting their potential to enhance smart mobility solutions\nthrough scalable digital twin deployments in complex urban environments."
                },
                "authors": [
                    {
                        "name": "Kazuma Nonomura"
                    },
                    {
                        "name": "Kui Wang"
                    },
                    {
                        "name": "Zongdian Li"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Kei Sakaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Kei Sakaguchi"
                },
                "author": "Kei Sakaguchi",
                "arxiv_comment": "The paper was accepted by the 36th IEEE Intelligent Vehicles\n  Symposium (IEEE IV 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20520v1",
                "updated": "2025-04-29T08:01:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    1,
                    27,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T08:01:27Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    8,
                    1,
                    27,
                    1,
                    119,
                    0
                ],
                "title": "PRISM: Projection-based Reward Integration for Scene-Aware\n  Real-to-Sim-to-Real Transfer with Few Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Projection-based Reward Integration for Scene-Aware\n  Real-to-Sim-to-Real Transfer with Few Demonstrations"
                },
                "summary": "Learning from few demonstrations to develop policies robust to variations in\nrobot initial positions and object poses is a problem of significant practical\ninterest in robotics. Compared to imitation learning, which often struggles to\ngeneralize from limited samples, reinforcement learning (RL) can autonomously\nexplore to obtain robust behaviors. Training RL agents through direct\ninteraction with the real world is often impractical and unsafe, while building\nsimulation environments requires extensive manual effort, such as designing\nscenes and crafting task-specific reward functions. To address these\nchallenges, we propose an integrated real-to-sim-to-real pipeline that\nconstructs simulation environments based on expert demonstrations by\nidentifying scene objects from images and retrieving their corresponding 3D\nmodels from existing libraries. We introduce a projection-based reward model\nfor RL policy training that is supervised by a vision-language model (VLM)\nusing human-guided object projection relationships as prompts, with the policy\nfurther fine-tuned using expert demonstrations. In general, our work focuses on\nthe construction of simulation environments and RL-based policy training,\nultimately enabling the deployment of reliable robotic control policies in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from few demonstrations to develop policies robust to variations in\nrobot initial positions and object poses is a problem of significant practical\ninterest in robotics. Compared to imitation learning, which often struggles to\ngeneralize from limited samples, reinforcement learning (RL) can autonomously\nexplore to obtain robust behaviors. Training RL agents through direct\ninteraction with the real world is often impractical and unsafe, while building\nsimulation environments requires extensive manual effort, such as designing\nscenes and crafting task-specific reward functions. To address these\nchallenges, we propose an integrated real-to-sim-to-real pipeline that\nconstructs simulation environments based on expert demonstrations by\nidentifying scene objects from images and retrieving their corresponding 3D\nmodels from existing libraries. We introduce a projection-based reward model\nfor RL policy training that is supervised by a vision-language model (VLM)\nusing human-guided object projection relationships as prompts, with the policy\nfurther fine-tuned using expert demonstrations. In general, our work focuses on\nthe construction of simulation environments and RL-based policy training,\nultimately enabling the deployment of reliable robotic control policies in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Haowen Sun"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Chengzhong Ma"
                    },
                    {
                        "name": "Shaolong Zhang"
                    },
                    {
                        "name": "Jiawei Ye"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Xuguang Lan"
                    }
                ],
                "author_detail": {
                    "name": "Xuguang Lan"
                },
                "author": "Xuguang Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20519v2",
                "updated": "2025-04-30T03:22:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    3,
                    22,
                    51,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T07:59:46Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    59,
                    46,
                    1,
                    119,
                    0
                ],
                "title": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions\n  But Do Not Outperform Standard Public Health Messaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations with AI Chatbots Increase Short-Term Vaccine Intentions\n  But Do Not Outperform Standard Public Health Messaging"
                },
                "summary": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based chatbots show promise in persuasive\ncommunication, but existing studies often rely on weak controls or focus on\nbelief change rather than behavioral intentions or outcomes. This\npre-registered multi-country (US, Canada, UK) randomized controlled trial\ninvolving 930 vaccine-hesitant parents evaluated brief (three-minute)\nmulti-turn conversations with LLM-based chatbots against standard public health\nmessaging approaches for increasing human papillomavirus (HPV) vaccine\nintentions for their children. Participants were randomly assigned to: (1) a\nweak control (no message), (2) a strong control reflecting the standard of care\n(reading official public health materials), or (3 and 4) one of two chatbot\nconditions. One chatbot was prompted to deliver short, conversational\nresponses, while the other used the model's default output style (longer with\nbullet points). While chatbot interactions significantly increased\nself-reported vaccination intent (by 7.1-10.3 points on a 100-point scale)\ncompared to no message, they did not outperform standard public health\nmaterials, with the conversational chatbot performing significantly worse.\nAdditionally, while the short-term effects of chatbot interactions faded during\na 15-day follow-up, the effects of public health material persisted relative to\nno message. These findings suggest that while LLMs can effectively shift\nvaccination intentions in the short-term, their incremental value over existing\npublic health communications is questionable, offering a more tempered view of\ntheir persuasive capabilities and highlighting the importance of integrating\nAI-driven tools alongside, rather than replacing, current public health\nstrategies."
                },
                "authors": [
                    {
                        "name": "Neil K. R. Sehgal"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Manuel Tonneau"
                    },
                    {
                        "name": "Anish K. Agarwal"
                    },
                    {
                        "name": "Joseph Cappella"
                    },
                    {
                        "name": "Melanie Kornides"
                    },
                    {
                        "name": "Lyle Ungar"
                    },
                    {
                        "name": "Alison Buttenheim"
                    },
                    {
                        "name": "Sharath Chandra Guntuku"
                    }
                ],
                "author_detail": {
                    "name": "Sharath Chandra Guntuku"
                },
                "author": "Sharath Chandra Guntuku",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20514v1",
                "updated": "2025-04-29T07:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    57,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:57:08Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    57,
                    8,
                    1,
                    119,
                    0
                ],
                "title": "Distributed U6G ELAA Communication Systems: Channel Measurement and\n  Small-Scale Fading Characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed U6G ELAA Communication Systems: Channel Measurement and\n  Small-Scale Fading Characterization"
                },
                "summary": "The distributed upper 6 GHz (U6G) extra-large scale antenna array (ELAA) is a\nkey enabler for future wireless communication systems, offering higher\nthroughput and wider coverage, similar to existing ELAA systems, while\neffectively mitigating unaffordable complexity and hardware overhead. Uncertain\nchannel characteristics, however, present significant bottleneck problems that\nhinder the hardware structure and algorithm design of the distributed U6G ELAA\nsystem. In response, we construct a U6G channel sounder and carry out extensive\nmeasurement campaigns across various typical scenarios. Initially, U6G channel\ncharacteristics, particularly small-scale fading characteristics, are unveiled\nand compared across different scenarios. Subsequently, the U6G ELAA channel\ncharacteristics are analyzed using a virtual array comprising 64 elements.\nFurthermore, inspired by the potential for distributed processing, we\ninvestigate U6G ELAA channel characteristics from the perspectives of subarrays\nand sub-bands, including subarray-wise nonstationarities, consistencies,\nfar-field approximations, and sub-band characteristics. Through a combination\nof analysis and measurement validation, several insights and benefits,\nparticularly suitable for distributed processing in U6G ELAA systems, are\nrevealed, which provides practical validation for the deployment of U6G ELAA\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The distributed upper 6 GHz (U6G) extra-large scale antenna array (ELAA) is a\nkey enabler for future wireless communication systems, offering higher\nthroughput and wider coverage, similar to existing ELAA systems, while\neffectively mitigating unaffordable complexity and hardware overhead. Uncertain\nchannel characteristics, however, present significant bottleneck problems that\nhinder the hardware structure and algorithm design of the distributed U6G ELAA\nsystem. In response, we construct a U6G channel sounder and carry out extensive\nmeasurement campaigns across various typical scenarios. Initially, U6G channel\ncharacteristics, particularly small-scale fading characteristics, are unveiled\nand compared across different scenarios. Subsequently, the U6G ELAA channel\ncharacteristics are analyzed using a virtual array comprising 64 elements.\nFurthermore, inspired by the potential for distributed processing, we\ninvestigate U6G ELAA channel characteristics from the perspectives of subarrays\nand sub-bands, including subarray-wise nonstationarities, consistencies,\nfar-field approximations, and sub-band characteristics. Through a combination\nof analysis and measurement validation, several insights and benefits,\nparticularly suitable for distributed processing in U6G ELAA systems, are\nrevealed, which provides practical validation for the deployment of U6G ELAA\nsystems."
                },
                "authors": [
                    {
                        "name": "Jiachen Tian"
                    },
                    {
                        "name": "Zhengtao Jin"
                    },
                    {
                        "name": "Xiayang Chen"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Wenjin Wang"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chao-Kai Wen"
                },
                "author": "Chao-Kai Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09413v2",
                "updated": "2025-04-29T07:46:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    46,
                    18,
                    1,
                    119,
                    0
                ],
                "published": "2024-11-14T13:07:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Detecting Children with Autism Spectrum Disorder based on Script-Centric\n  Behavior Understanding with Emotional Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Children with Autism Spectrum Disorder based on Script-Centric\n  Behavior Understanding with Emotional Enhancement"
                },
                "summary": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis."
                },
                "authors": [
                    {
                        "name": "Wenxing Liu"
                    },
                    {
                        "name": "Yueran Pan"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Hongzhu Deng"
                    },
                    {
                        "name": "Xiaobing Zou"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "15 pages, 12 figures, sumbitted to IEEE transactions on affective\n  computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20505v1",
                "updated": "2025-04-29T07:46:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    46,
                    14,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:46:14Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    46,
                    14,
                    1,
                    119,
                    0
                ],
                "title": "MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural\n  Language for Activities of Daily Living",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural\n  Language for Activities of Daily Living"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have shown promising\npotential for human activity recognition (HAR) using ambient sensors,\nespecially through natural language reasoning and zero-shot learning. However,\nexisting datasets such as CASAS, ARAS, and MARBLE were not originally designed\nwith LLMs in mind and therefore lack the contextual richness, complexity, and\nannotation granularity required to fully exploit LLM capabilities. In this\npaper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with\nnatural Language, comprising over 21 hours of multi-user sensor data collected\nfrom 21 sessions in a smart-home environment. MuRAL is annotated with\nfine-grained natural language descriptions, resident identities, and high-level\nactivity labels, all situated in dynamic, realistic multi-resident settings. We\nbenchmark MuRAL using state-of-the-art LLMs for three core tasks: subject\nassignment, action description, and activity classification. Our results\ndemonstrate that while LLMs can provide rich semantic interpretations of\nambient data, current models still face challenges in handling multi-user\nambiguity and under-specified sensor contexts. We release MuRAL to support\nfuture research on LLM-powered, explainable, and socially aware activity\nunderstanding in smart environments. For access to the dataset, please reach\nout to us via the provided contact information. A direct link for dataset\nretrieval will be made available at this location in due course.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have shown promising\npotential for human activity recognition (HAR) using ambient sensors,\nespecially through natural language reasoning and zero-shot learning. However,\nexisting datasets such as CASAS, ARAS, and MARBLE were not originally designed\nwith LLMs in mind and therefore lack the contextual richness, complexity, and\nannotation granularity required to fully exploit LLM capabilities. In this\npaper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with\nnatural Language, comprising over 21 hours of multi-user sensor data collected\nfrom 21 sessions in a smart-home environment. MuRAL is annotated with\nfine-grained natural language descriptions, resident identities, and high-level\nactivity labels, all situated in dynamic, realistic multi-resident settings. We\nbenchmark MuRAL using state-of-the-art LLMs for three core tasks: subject\nassignment, action description, and activity classification. Our results\ndemonstrate that while LLMs can provide rich semantic interpretations of\nambient data, current models still face challenges in handling multi-user\nambiguity and under-specified sensor contexts. We release MuRAL to support\nfuture research on LLM-powered, explainable, and socially aware activity\nunderstanding in smart environments. For access to the dataset, please reach\nout to us via the provided contact information. A direct link for dataset\nretrieval will be made available at this location in due course."
                },
                "authors": [
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Julien Cumin"
                    },
                    {
                        "name": "Fano Ramparany"
                    },
                    {
                        "name": "Dominique Vaufreydaz"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Vaufreydaz"
                },
                "arxiv_affiliation": "M-PSI",
                "author": "Dominique Vaufreydaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20500v1",
                "updated": "2025-04-29T07:40:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    40,
                    0,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:40:00Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    40,
                    0,
                    1,
                    119,
                    0
                ],
                "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniDetox: Universal Detoxification of Large Language Models via Dataset\n  Distillation"
                },
                "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs."
                },
                "authors": [
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Masaru Isonuma"
                    },
                    {
                        "name": "Junichiro Mori"
                    },
                    {
                        "name": "Ichiro Sakata"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Sakata"
                },
                "author": "Ichiro Sakata",
                "arxiv_comment": "Accepted at ICLR 2025 (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20496v1",
                "updated": "2025-04-29T07:37:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    37,
                    51,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:37:51Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    37,
                    51,
                    1,
                    119,
                    0
                ],
                "title": "Large-scale visual SLAM for in-the-wild videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale visual SLAM for in-the-wild videos"
                },
                "summary": "Accurate and robust 3D scene reconstruction from casual, in-the-wild videos\ncan significantly simplify robot deployment to new environments. However,\nreliable camera pose estimation and scene reconstruction from such\nunconstrained videos remains an open challenge. Existing visual-only SLAM\nmethods perform well on benchmark datasets but struggle with real-world footage\nwhich often exhibits uncontrolled motion including rapid rotations and pure\nforward movements, textureless regions, and dynamic objects. We analyze the\nlimitations of current methods and introduce a robust pipeline designed to\nimprove 3D reconstruction from casual videos. We build upon recent deep visual\nodometry methods but increase robustness in several ways. Camera intrinsics are\nautomatically recovered from the first few frames using structure-from-motion.\nDynamic objects and less-constrained areas are masked with a predictive model.\nAdditionally, we leverage monocular depth estimates to regularize bundle\nadjustment, mitigating errors in low-parallax situations. Finally, we integrate\nplace recognition and loop closure to reduce long-term drift and refine both\nintrinsics and pose estimates through global bundle adjustment. We demonstrate\nlarge-scale contiguous 3D models from several online videos in various\nenvironments. In contrast, baseline methods typically produce locally\ninconsistent results at several points, producing separate segments or\ndistorted maps. In lieu of ground-truth pose data, we evaluate map consistency,\nexecution time and visual accuracy of re-rendered NeRF models. Our proposed\nsystem establishes a new baseline for visual reconstruction from casual\nuncontrolled videos found online, demonstrating more consistent reconstructions\nover longer sequences of in-the-wild videos than previously achieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and robust 3D scene reconstruction from casual, in-the-wild videos\ncan significantly simplify robot deployment to new environments. However,\nreliable camera pose estimation and scene reconstruction from such\nunconstrained videos remains an open challenge. Existing visual-only SLAM\nmethods perform well on benchmark datasets but struggle with real-world footage\nwhich often exhibits uncontrolled motion including rapid rotations and pure\nforward movements, textureless regions, and dynamic objects. We analyze the\nlimitations of current methods and introduce a robust pipeline designed to\nimprove 3D reconstruction from casual videos. We build upon recent deep visual\nodometry methods but increase robustness in several ways. Camera intrinsics are\nautomatically recovered from the first few frames using structure-from-motion.\nDynamic objects and less-constrained areas are masked with a predictive model.\nAdditionally, we leverage monocular depth estimates to regularize bundle\nadjustment, mitigating errors in low-parallax situations. Finally, we integrate\nplace recognition and loop closure to reduce long-term drift and refine both\nintrinsics and pose estimates through global bundle adjustment. We demonstrate\nlarge-scale contiguous 3D models from several online videos in various\nenvironments. In contrast, baseline methods typically produce locally\ninconsistent results at several points, producing separate segments or\ndistorted maps. In lieu of ground-truth pose data, we evaluate map consistency,\nexecution time and visual accuracy of re-rendered NeRF models. Our proposed\nsystem establishes a new baseline for visual reconstruction from casual\nuncontrolled videos found online, demonstrating more consistent reconstructions\nover longer sequences of in-the-wild videos than previously achieved."
                },
                "authors": [
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Torsten Sattler"
                    },
                    {
                        "name": "Malcolm Mielle"
                    },
                    {
                        "name": "Achim J. Lilienthal"
                    },
                    {
                        "name": "Martin Magnusson"
                    }
                ],
                "author_detail": {
                    "name": "Martin Magnusson"
                },
                "author": "Martin Magnusson",
                "arxiv_comment": "fix the overview figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20493v1",
                "updated": "2025-04-29T07:34:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    34,
                    22,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:34:22Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    34,
                    22,
                    1,
                    119,
                    0
                ],
                "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM\n  Reasoning via Adaptive Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM\n  Reasoning via Adaptive Token Compression"
                },
                "summary": "While reasoning large language models (LLMs) demonstrate remarkable\nperformance across various tasks, they also contain notable security\nvulnerabilities. Recent research has uncovered a \"thinking-stopped\"\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\nforcibly interrupt the inference process, resulting in empty responses that\ncompromise LLM-integrated applications. However, existing methods triggering\nthis vulnerability require complex mathematical word problems with long\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\ndefine this vulnerability, we propose a novel prompt injection attack named\n\"Reasoning Interruption Attack\", based on adaptive token compression. We\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\nstructures than mathematical word problems. We develop a systematic approach to\nefficiently collect attack prompts and an adaptive token compression framework\nthat utilizes LLMs to automatically compress these prompts. Experiments show\nour compression framework significantly reduces prompt length while maintaining\neffective attack capabilities. We further investigate the attack's performance\nvia output prefix and analyze the underlying causes of the vulnerability,\nproviding valuable insights for improving security in reasoning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning large language models (LLMs) demonstrate remarkable\nperformance across various tasks, they also contain notable security\nvulnerabilities. Recent research has uncovered a \"thinking-stopped\"\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\nforcibly interrupt the inference process, resulting in empty responses that\ncompromise LLM-integrated applications. However, existing methods triggering\nthis vulnerability require complex mathematical word problems with long\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\ndefine this vulnerability, we propose a novel prompt injection attack named\n\"Reasoning Interruption Attack\", based on adaptive token compression. We\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\nstructures than mathematical word problems. We develop a systematic approach to\nefficiently collect attack prompts and an adaptive token compression framework\nthat utilizes LLMs to automatically compress these prompts. Experiments show\nour compression framework significantly reduces prompt length while maintaining\neffective attack capabilities. We further investigate the attack's performance\nvia output prefix and analyze the underlying causes of the vulnerability,\nproviding valuable insights for improving security in reasoning LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10498v2",
                "updated": "2025-04-29T07:33:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    33,
                    38,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-07T13:43:53Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    43,
                    53,
                    0,
                    97,
                    0
                ],
                "title": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation\n  for Large Language Models"
                },
                "summary": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval."
                },
                "authors": [
                    {
                        "name": "Jianling Lu"
                    },
                    {
                        "name": "Mingqi Lv"
                    },
                    {
                        "name": "Tieming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tieming Chen"
                },
                "author": "Tieming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19423v2",
                "updated": "2025-04-29T07:28:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    28,
                    55,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-28T02:14:08Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    14,
                    8,
                    0,
                    118,
                    0
                ],
                "title": "MER 2025: When Affective Computing Meets Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MER 2025: When Affective Computing Meets Large Language Models"
                },
                "summary": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MER2025 is the third year of our MER series of challenges, aiming to bring\ntogether researchers in the affective computing community to explore emerging\ntrends and future directions in the field. Previously, MER2023 focused on\nmulti-label learning, noise robustness, and semi-supervised learning, while\nMER2024 introduced a new track dedicated to open-vocabulary emotion\nrecognition. This year, MER2025 centers on the theme \"When Affective Computing\nMeets Large Language Models (LLMs)\".We aim to shift the paradigm from\ntraditional categorical frameworks reliant on predefined emotion taxonomies to\nLLM-driven generative methods, offering innovative solutions for more accurate\nand reliable emotion understanding. The challenge features four tracks:\nMER-SEMI focuses on fixed categorical emotion recognition enhanced by\nsemi-supervised learning; MER-FG explores fine-grained emotions, expanding\nrecognition from basic to nuanced emotional states; MER-DES incorporates\nmultimodal cues (beyond emotion words) into predictions to enhance model\ninterpretability; MER-PR investigates whether emotion prediction results can\nimprove personality recognition performance. For the first three tracks,\nbaseline code is available at MERTools, and datasets can be accessed via\nHugging Face. For the last track, the dataset and baseline code are available\non GitHub."
                },
                "authors": [
                    {
                        "name": "Zheng Lian"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Kele Xu"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Xuefei Liu"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Zebang Cheng"
                    },
                    {
                        "name": "Haolin Zuo"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Xiaojiang Peng"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Ya Li"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Guoying Zhao"
                    },
                    {
                        "name": "Björn W. Schuller"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17163v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17163v3",
                "updated": "2025-04-29T07:28:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    28,
                    4,
                    1,
                    119,
                    0
                ],
                "published": "2025-02-24T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    58,
                    42,
                    0,
                    55,
                    0
                ],
                "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation"
                },
                "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. Our\ndataset is available at https://github.com/amazon-science/MEMERAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. Our\ndataset is available at https://github.com/amazon-science/MEMERAG"
                },
                "authors": [
                    {
                        "name": "María Andrea Cruz Blandón"
                    },
                    {
                        "name": "Jayasimha Talur"
                    },
                    {
                        "name": "Bruno Charron"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Marcello Federico"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Federico"
                },
                "author": "Marcello Federico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17163v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17163v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20485v1",
                "updated": "2025-04-29T07:24:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    24,
                    34,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:24:34Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    24,
                    34,
                    1,
                    119,
                    0
                ],
                "title": "Sleeping Giants -- Activating Dormant Java Deserialization Gadget Chains\n  through Stealthy Code Changes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sleeping Giants -- Activating Dormant Java Deserialization Gadget Chains\n  through Stealthy Code Changes"
                },
                "summary": "Java deserialization gadget chains are a well-researched critical software\nweakness. The vast majority of known gadget chains rely on gadgets from\nsoftware dependencies. Furthermore, it has been shown that small code changes\nin dependencies have enabled these gadget chains. This makes gadget chain\ndetection a purely reactive endeavor. Even if one dependency's deployment\npipeline employs gadget chain detection, a gadget chain can still result from\ngadgets in other dependencies. In this work, we assess how likely small code\nchanges are to enable a gadget chain. These changes could either be accidental\nor intentional as part of a supply chain attack. Specifically, we show that\nclass serializability is a strongly fluctuating property over a dependency's\nevolution. Then, we investigate three change patterns by which an attacker\ncould stealthily introduce gadgets into a dependency. We apply these patterns\nto 533 dependencies and run three state-of-the-art gadget chain detectors both\non the original and the modified dependencies. The tools detect that applying\nthe modification patterns can activate/inject gadget chains in 26.08% of the\ndependencies we selected. Finally, we verify the newly detected chains. As\nsuch, we identify dormant gadget chains in 53 dependencies that could be added\nthrough minor code modifications. This both shows that Java deserialization\ngadget chains are a broad liability to software and proves dormant gadget\nchains as a lucrative supply chain attack vector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Java deserialization gadget chains are a well-researched critical software\nweakness. The vast majority of known gadget chains rely on gadgets from\nsoftware dependencies. Furthermore, it has been shown that small code changes\nin dependencies have enabled these gadget chains. This makes gadget chain\ndetection a purely reactive endeavor. Even if one dependency's deployment\npipeline employs gadget chain detection, a gadget chain can still result from\ngadgets in other dependencies. In this work, we assess how likely small code\nchanges are to enable a gadget chain. These changes could either be accidental\nor intentional as part of a supply chain attack. Specifically, we show that\nclass serializability is a strongly fluctuating property over a dependency's\nevolution. Then, we investigate three change patterns by which an attacker\ncould stealthily introduce gadgets into a dependency. We apply these patterns\nto 533 dependencies and run three state-of-the-art gadget chain detectors both\non the original and the modified dependencies. The tools detect that applying\nthe modification patterns can activate/inject gadget chains in 26.08% of the\ndependencies we selected. Finally, we verify the newly detected chains. As\nsuch, we identify dormant gadget chains in 53 dependencies that could be added\nthrough minor code modifications. This both shows that Java deserialization\ngadget chains are a broad liability to software and proves dormant gadget\nchains as a lucrative supply chain attack vector."
                },
                "authors": [
                    {
                        "name": "Bruno Kreyssig"
                    },
                    {
                        "name": "Sabine Houy"
                    },
                    {
                        "name": "Timothée Riom"
                    },
                    {
                        "name": "Alexandre Bartel"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Bartel"
                },
                "author": "Alexandre Bartel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20484v1",
                "updated": "2025-04-29T07:24:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    24,
                    25,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:24:25Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    24,
                    25,
                    1,
                    119,
                    0
                ],
                "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context\n  Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Language Adaption through Cross-lingual In-Context\n  Pre-training"
                },
                "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation."
                },
                "authors": [
                    {
                        "name": "Linjuan Wu"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Weiming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Lu"
                },
                "author": "Weiming Lu",
                "arxiv_comment": "12 pages, 6 figures, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20472v1",
                "updated": "2025-04-29T07:13:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    13,
                    53,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:13:53Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    13,
                    53,
                    1,
                    119,
                    0
                ],
                "title": "Robustness via Referencing: Defending against Prompt Injection Attacks\n  by Referencing the Executed Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness via Referencing: Defending against Prompt Injection Attacks\n  by Referencing the Executed Instruction"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance and\nhave come to dominate the field of natural language processing (NLP) across\nvarious tasks. However, due to their strong instruction-following capabilities\nand inability to distinguish between instructions and data content, LLMs are\nvulnerable to prompt injection attacks. These attacks manipulate LLMs into\ndeviating from the original input instructions and executing maliciously\ninjected instructions within data content, such as web documents retrieved from\nsearch engines. Existing defense methods, including prompt-engineering and\nfine-tuning approaches, typically instruct models to follow the original input\ninstructions while suppressing their tendencies to execute injected\ninstructions. However, our experiments reveal that suppressing\ninstruction-following tendencies is challenging. Through analyzing failure\ncases, we observe that although LLMs tend to respond to any recognized\ninstructions, they are aware of which specific instructions they are executing\nand can correctly reference them within the original prompt. Motivated by these\nfindings, we propose a novel defense method that leverages, rather than\nsuppresses, the instruction-following abilities of LLMs. Our approach prompts\nLLMs to generate responses that include both answers and their corresponding\ninstruction references. Based on these references, we filter out answers not\nassociated with the original input instructions. Comprehensive experiments\ndemonstrate that our method outperforms prompt-engineering baselines and\nachieves performance comparable to fine-tuning methods, reducing the attack\nsuccess rate (ASR) to 0 percent in some scenarios. Moreover, our approach has\nminimal impact on overall utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance and\nhave come to dominate the field of natural language processing (NLP) across\nvarious tasks. However, due to their strong instruction-following capabilities\nand inability to distinguish between instructions and data content, LLMs are\nvulnerable to prompt injection attacks. These attacks manipulate LLMs into\ndeviating from the original input instructions and executing maliciously\ninjected instructions within data content, such as web documents retrieved from\nsearch engines. Existing defense methods, including prompt-engineering and\nfine-tuning approaches, typically instruct models to follow the original input\ninstructions while suppressing their tendencies to execute injected\ninstructions. However, our experiments reveal that suppressing\ninstruction-following tendencies is challenging. Through analyzing failure\ncases, we observe that although LLMs tend to respond to any recognized\ninstructions, they are aware of which specific instructions they are executing\nand can correctly reference them within the original prompt. Motivated by these\nfindings, we propose a novel defense method that leverages, rather than\nsuppresses, the instruction-following abilities of LLMs. Our approach prompts\nLLMs to generate responses that include both answers and their corresponding\ninstruction references. Based on these references, we filter out answers not\nassociated with the original input instructions. Comprehensive experiments\ndemonstrate that our method outperforms prompt-engineering baselines and\nachieves performance comparable to fine-tuning methods, reducing the attack\nsuccess rate (ASR) to 0 percent in some scenarios. Moreover, our approach has\nminimal impact on overall utility."
                },
                "authors": [
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04199v2",
                "updated": "2025-04-29T07:13:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    13,
                    9,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-05T15:09:39Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    15,
                    9,
                    39,
                    5,
                    95,
                    0
                ],
                "title": "Investigating and Mitigating Stereotype-aware Unfairness in LLM-based\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating and Mitigating Stereotype-aware Unfairness in LLM-based\n  Recommendations"
                },
                "summary": "Large Language Models (LLMs) have demonstrated unprecedented language\nunderstanding and reasoning capabilities to capture diverse user preferences\nand advance personalized recommendations. Despite the growing interest in\nLLM-based recommendations, unique challenges are brought to the trustworthiness\nof LLM-based recommender systems (LLM-RS). Compared to unique user/item\nrepresentations in conventional recommender systems, users and items share the\ntextual representation (e.g., word embeddings) in LLM-based recommendations.\nRecent studies have revealed that LLMs are likely to inherit stereotypes that\nare embedded ubiquitously in word embeddings, due to their training on\nlarge-scale uncurated datasets. This leads to LLM-RS exhibiting stereotypical\nlinguistic associations between users and items, causing a form of two-sided\n(i.e., user-to-item) recommendation fairness. However, there remains a lack of\nstudies investigating the unfairness of LLM-RS due to intrinsic stereotypes,\nwhich can simultaneously involve user and item groups. To bridge this gap, this\nstudy reveals a new variant of fairness between stereotype groups containing\nboth users and items, to quantify discrimination against stereotypes in LLM-RS.\nMoreover, in this paper, to mitigate stereotype-aware unfairness in textual\nuser and item representations, we propose a novel framework named\nMixture-of-Stereotypes (MoS). In particular, an insightful stereotype-wise\nrouting strategy over multiple stereotype-relevant experts is designed, aiming\nto learn unbiased representations against different stereotypes in LLM-RS.\nExtensive experiments are conducted to analyze the influence of\nstereotype-aware fairness in LLM-RS and the effectiveness of our proposed\nmethods, which consistently outperform competitive benchmarks under various\nfairness settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated unprecedented language\nunderstanding and reasoning capabilities to capture diverse user preferences\nand advance personalized recommendations. Despite the growing interest in\nLLM-based recommendations, unique challenges are brought to the trustworthiness\nof LLM-based recommender systems (LLM-RS). Compared to unique user/item\nrepresentations in conventional recommender systems, users and items share the\ntextual representation (e.g., word embeddings) in LLM-based recommendations.\nRecent studies have revealed that LLMs are likely to inherit stereotypes that\nare embedded ubiquitously in word embeddings, due to their training on\nlarge-scale uncurated datasets. This leads to LLM-RS exhibiting stereotypical\nlinguistic associations between users and items, causing a form of two-sided\n(i.e., user-to-item) recommendation fairness. However, there remains a lack of\nstudies investigating the unfairness of LLM-RS due to intrinsic stereotypes,\nwhich can simultaneously involve user and item groups. To bridge this gap, this\nstudy reveals a new variant of fairness between stereotype groups containing\nboth users and items, to quantify discrimination against stereotypes in LLM-RS.\nMoreover, in this paper, to mitigate stereotype-aware unfairness in textual\nuser and item representations, we propose a novel framework named\nMixture-of-Stereotypes (MoS). In particular, an insightful stereotype-wise\nrouting strategy over multiple stereotype-relevant experts is designed, aiming\nto learn unbiased representations against different stereotypes in LLM-RS.\nExtensive experiments are conducted to analyze the influence of\nstereotype-aware fairness in LLM-RS and the effectiveness of our proposed\nmethods, which consistently outperform competitive benchmarks under various\nfairness settings."
                },
                "authors": [
                    {
                        "name": "Zihuai Zhao"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yao Wu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20469v1",
                "updated": "2025-04-29T07:10:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    10,
                    53,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T07:10:53Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    7,
                    10,
                    53,
                    1,
                    119,
                    0
                ],
                "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large\n  Language Models"
                },
                "summary": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing."
                },
                "authors": [
                    {
                        "name": "Enfa Fane"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Steven R. Corman"
                    }
                ],
                "author_detail": {
                    "name": "Steven R. Corman"
                },
                "author": "Steven R. Corman",
                "arxiv_comment": "Accepted to The 19th International Workshop on Semantic Evaluation\n  (Semeval 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20464v1",
                "updated": "2025-04-29T06:55:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    55,
                    15,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T06:55:15Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    55,
                    15,
                    1,
                    119,
                    0
                ],
                "title": "A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement\n  Learning"
                },
                "summary": "Graphical User Interface (GUI) agents, driven by Multi-modal Large Language\nModels (MLLMs), have emerged as a promising paradigm for enabling intelligent\ninteraction with digital systems. This paper provides a structured summary of\nrecent advances in GUI agents, focusing on architectures enhanced by\nReinforcement Learning (RL). We first formalize GUI agent tasks as Markov\nDecision Processes and discuss typical execution environments and evaluation\nmetrics. We then review the modular architecture of (M)LLM-based GUI agents,\ncovering Perception, Planning, and Acting modules, and trace their evolution\nthrough representative works. Furthermore, we categorize GUI agent training\nmethodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and\nRL-based approaches, highlighting the progression from simple prompt\nengineering to dynamic policy learning via RL. Our summary illustrates how\nrecent innovations in multimodal perception, decision reasoning, and adaptive\naction generation have significantly improved the generalization and robustness\nof GUI agents in complex real-world environments. We conclude by identifying\nkey challenges and future directions for building more capable and reliable GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) agents, driven by Multi-modal Large Language\nModels (MLLMs), have emerged as a promising paradigm for enabling intelligent\ninteraction with digital systems. This paper provides a structured summary of\nrecent advances in GUI agents, focusing on architectures enhanced by\nReinforcement Learning (RL). We first formalize GUI agent tasks as Markov\nDecision Processes and discuss typical execution environments and evaluation\nmetrics. We then review the modular architecture of (M)LLM-based GUI agents,\ncovering Perception, Planning, and Acting modules, and trace their evolution\nthrough representative works. Furthermore, we categorize GUI agent training\nmethodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and\nRL-based approaches, highlighting the progression from simple prompt\nengineering to dynamic policy learning via RL. Our summary illustrates how\nrecent innovations in multimodal perception, decision reasoning, and adaptive\naction generation have significantly improved the generalization and robustness\nof GUI agents in complex real-world environments. We conclude by identifying\nkey challenges and future directions for building more capable and reliable GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Kaer Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaer Huang"
                },
                "author": "Kaer Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20462v2",
                "updated": "2025-04-30T10:20:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    10,
                    20,
                    10,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-29T06:50:48Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    50,
                    48,
                    1,
                    119,
                    0
                ],
                "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with\n  Multi-Modality Observation Data"
                },
                "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Mingyi Li"
                    },
                    {
                        "name": "Yuan Yuan"
                    },
                    {
                        "name": "Mengbai Xiao"
                    },
                    {
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20459v1",
                "updated": "2025-04-29T06:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    39,
                    20,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T06:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    39,
                    20,
                    1,
                    119,
                    0
                ],
                "title": "SAS-Prompt: Large Language Models as Numerical Optimizers for Robot\n  Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAS-Prompt: Large Language Models as Numerical Optimizers for Robot\n  Self-Improvement"
                },
                "summary": "We demonstrate the ability of large language models (LLMs) to perform\niterative self-improvement of robot policies. An important insight of this\npaper is that LLMs have a built-in ability to perform (stochastic) numerical\noptimization and that this property can be leveraged for explainable robot\npolicy search. Based on this insight, we introduce the SAS Prompt (Summarize,\nAnalyze, Synthesize) -- a single prompt that enables iterative learning and\nadaptation of robot behavior by combining the LLM's ability to retrieve, reason\nand optimize over previous robot traces in order to synthesize new, unseen\nbehavior. Our approach can be regarded as an early example of a new family of\nexplainable policy search methods that are entirely implemented within an LLM.\nWe evaluate our approach both in simulation and on a real-robot table tennis\ntask. Project website: sites.google.com/asu.edu/sas-llm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate the ability of large language models (LLMs) to perform\niterative self-improvement of robot policies. An important insight of this\npaper is that LLMs have a built-in ability to perform (stochastic) numerical\noptimization and that this property can be leveraged for explainable robot\npolicy search. Based on this insight, we introduce the SAS Prompt (Summarize,\nAnalyze, Synthesize) -- a single prompt that enables iterative learning and\nadaptation of robot behavior by combining the LLM's ability to retrieve, reason\nand optimize over previous robot traces in order to synthesize new, unseen\nbehavior. Our approach can be regarded as an early example of a new family of\nexplainable policy search methods that are entirely implemented within an LLM.\nWe evaluate our approach both in simulation and on a real-robot table tennis\ntask. Project website: sites.google.com/asu.edu/sas-llm/"
                },
                "authors": [
                    {
                        "name": "Heni Ben Amor"
                    },
                    {
                        "name": "Laura Graesser"
                    },
                    {
                        "name": "Atil Iscen"
                    },
                    {
                        "name": "David D'Ambrosio"
                    },
                    {
                        "name": "Saminda Abeyruwan"
                    },
                    {
                        "name": "Alex Bewley"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Kamalesh Kalirathinam"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Pannag Sanketi"
                    }
                ],
                "author_detail": {
                    "name": "Pannag Sanketi"
                },
                "author": "Pannag Sanketi",
                "arxiv_comment": "ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20452v1",
                "updated": "2025-04-29T06:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    2,
                    16,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T06:02:16Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    6,
                    2,
                    16,
                    1,
                    119,
                    0
                ],
                "title": "Enhancing News Recommendation with Hierarchical LLM Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing News Recommendation with Hierarchical LLM Prompting"
                },
                "summary": "Personalized news recommendation systems often struggle to effectively\ncapture the complexity of user preferences, as they rely heavily on shallow\nrepresentations, such as article titles and abstracts. To address this problem,\nwe introduce a novel method, namely PNR-LLM, for Large Language Models for\nPersonalized News Recommendation. Specifically, PNR-LLM harnesses the\ngeneration capabilities of LLMs to enrich news titles and abstracts, and\nconsequently improves recommendation quality. PNR-LLM contains a novel module,\nNews Enrichment via LLMs, which generates deeper semantic information and\nrelevant entities from articles, transforming shallow contents into richer\nrepresentations. We further propose an attention mechanism to aggregate\nenriched semantic- and entity-level data, forming unified user and news\nembeddings that reveal a more accurate user-news match. Extensive experiments\non MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.\nMoreover, the proposed data enrichment module is model-agnostic, and we\nempirically show that applying our proposed module to multiple existing models\ncan further improve their performance, verifying the advantage of our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized news recommendation systems often struggle to effectively\ncapture the complexity of user preferences, as they rely heavily on shallow\nrepresentations, such as article titles and abstracts. To address this problem,\nwe introduce a novel method, namely PNR-LLM, for Large Language Models for\nPersonalized News Recommendation. Specifically, PNR-LLM harnesses the\ngeneration capabilities of LLMs to enrich news titles and abstracts, and\nconsequently improves recommendation quality. PNR-LLM contains a novel module,\nNews Enrichment via LLMs, which generates deeper semantic information and\nrelevant entities from articles, transforming shallow contents into richer\nrepresentations. We further propose an attention mechanism to aggregate\nenriched semantic- and entity-level data, forming unified user and news\nembeddings that reveal a more accurate user-news match. Extensive experiments\non MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.\nMoreover, the proposed data enrichment module is model-agnostic, and we\nempirically show that applying our proposed module to multiple existing models\ncan further improve their performance, verifying the advantage of our design."
                },
                "authors": [
                    {
                        "name": "Hai-Dang Kieu"
                    },
                    {
                        "name": "Delvin Ce Zhang"
                    },
                    {
                        "name": "Minh Duc Nguyen"
                    },
                    {
                        "name": "Min Xu"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Dung D. Le"
                    }
                ],
                "author_detail": {
                    "name": "Dung D. Le"
                },
                "author": "Dung D. Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20451v1",
                "updated": "2025-04-29T05:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    58,
                    19,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T05:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    58,
                    19,
                    1,
                    119,
                    0
                ],
                "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine\n  Translation for English-Korean Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine\n  Translation for English-Korean Pairs"
                },
                "summary": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation."
                },
                "authors": [
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Harsh Sharma"
                    },
                    {
                        "name": "Jieun Han"
                    },
                    {
                        "name": "Sunny Jeong"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Vered Shwartz"
                    }
                ],
                "author_detail": {
                    "name": "Vered Shwartz"
                },
                "author": "Vered Shwartz",
                "arxiv_comment": "Accepted at SemEval-2025 Workshop (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20444v1",
                "updated": "2025-04-29T05:35:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    35,
                    23,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T05:35:23Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    35,
                    23,
                    1,
                    119,
                    0
                ],
                "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other\n  LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other\n  LLMs?"
                },
                "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst."
                },
                "authors": [
                    {
                        "name": "Mika Hämäläinen"
                    }
                ],
                "author_detail": {
                    "name": "Mika Hämäläinen"
                },
                "author": "Mika Hämäläinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20437v1",
                "updated": "2025-04-29T05:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    27,
                    2,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T05:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    27,
                    2,
                    1,
                    119,
                    0
                ],
                "title": "GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language\nunderstanding and generation but face significant memory bottlenecks during\ntraining. GaLore, Gradient Low-Rank Projection, addresses this issue by\nleveraging the inherent low-rank structure of weight gradients, enabling\nsubstantial memory savings without sacrificing performance. Recent works\nfurther extend GaLore from various aspects, including low-bit quantization and\nhigher-order tensor structures. However, there are several remaining challenges\nfor GaLore, such as the computational overhead of SVD for subspace updates and\nthe integration with state-of-the-art training parallelization strategies\n(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable\nGaLore framework that addresses these challenges and incorporates recent\nadvancements. In addition, we demonstrate the scalability of GaLore 2 by\npre-training Llama 7B from scratch using up to 500 billion training tokens,\nhighlighting its potential impact on real LLM pre-training scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language\nunderstanding and generation but face significant memory bottlenecks during\ntraining. GaLore, Gradient Low-Rank Projection, addresses this issue by\nleveraging the inherent low-rank structure of weight gradients, enabling\nsubstantial memory savings without sacrificing performance. Recent works\nfurther extend GaLore from various aspects, including low-bit quantization and\nhigher-order tensor structures. However, there are several remaining challenges\nfor GaLore, such as the computational overhead of SVD for subspace updates and\nthe integration with state-of-the-art training parallelization strategies\n(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable\nGaLore framework that addresses these challenges and incorporates recent\nadvancements. In addition, we demonstrate the scalability of GaLore 2 by\npre-training Llama 7B from scratch using up to 500 billion training tokens,\nhighlighting its potential impact on real LLM pre-training scenarios."
                },
                "authors": [
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Andrew Gu"
                    },
                    {
                        "name": "Jane Xu"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Jiawei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhao"
                },
                "author": "Jiawei Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02102v2",
                "updated": "2025-04-29T05:23:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    23,
                    13,
                    1,
                    119,
                    0
                ],
                "published": "2024-10-02T23:46:48Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    23,
                    46,
                    48,
                    2,
                    276,
                    0
                ],
                "title": "Racing Thoughts: Explaining Contextualization Errors in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Racing Thoughts: Explaining Contextualization Errors in Large Language\n  Models"
                },
                "summary": "The profound success of transformer-based language models can largely be\nattributed to their ability to integrate relevant contextual information from\nan input sequence in order to generate a response or complete a task. However,\nwe know very little about the algorithms that a model employs to implement this\ncapability, nor do we understand their failure modes. For example, given the\nprompt \"John is going fishing, so he walks over to the bank. Can he make an ATM\ntransaction?\", a model may incorrectly respond \"Yes\" if it has not properly\ncontextualized \"bank\" as a geographical feature, rather than a financial\ninstitution. We propose the LLM Race Conditions Hypothesis as an explanation of\ncontextualization errors of this form. This hypothesis identifies dependencies\nbetween tokens (e.g., \"bank\" must be properly contextualized before the final\ntoken, \"?\", integrates information from \"bank\"), and claims that\ncontextualization errors are a result of violating these dependencies. Using a\nvariety of techniques from mechanistic intepretability, we provide\ncorrelational and causal evidence in support of the hypothesis, and suggest\ninference-time interventions to address it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The profound success of transformer-based language models can largely be\nattributed to their ability to integrate relevant contextual information from\nan input sequence in order to generate a response or complete a task. However,\nwe know very little about the algorithms that a model employs to implement this\ncapability, nor do we understand their failure modes. For example, given the\nprompt \"John is going fishing, so he walks over to the bank. Can he make an ATM\ntransaction?\", a model may incorrectly respond \"Yes\" if it has not properly\ncontextualized \"bank\" as a geographical feature, rather than a financial\ninstitution. We propose the LLM Race Conditions Hypothesis as an explanation of\ncontextualization errors of this form. This hypothesis identifies dependencies\nbetween tokens (e.g., \"bank\" must be properly contextualized before the final\ntoken, \"?\", integrates information from \"bank\"), and claims that\ncontextualization errors are a result of violating these dependencies. Using a\nvariety of techniques from mechanistic intepretability, we provide\ncorrelational and causal evidence in support of the hypothesis, and suggest\ninference-time interventions to address it."
                },
                "authors": [
                    {
                        "name": "Michael A. Lepori"
                    },
                    {
                        "name": "Michael C. Mozer"
                    },
                    {
                        "name": "Asma Ghandeharioun"
                    }
                ],
                "author_detail": {
                    "name": "Asma Ghandeharioun"
                },
                "author": "Asma Ghandeharioun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12059v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12059v4",
                "updated": "2025-04-29T05:18:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    18,
                    29,
                    1,
                    119,
                    0
                ],
                "published": "2023-10-18T15:48:07Z",
                "published_parsed": [
                    2023,
                    10,
                    18,
                    15,
                    48,
                    7,
                    2,
                    291,
                    0
                ],
                "title": "Evaluating the Symbol Binding Ability of Large Language Models for\n  Multiple-Choice Questions in Vietnamese General Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Symbol Binding Ability of Large Language Models for\n  Multiple-Choice Questions in Vietnamese General Education"
                },
                "summary": "In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only."
                },
                "authors": [
                    {
                        "name": "Duc-Vu Nguyen"
                    },
                    {
                        "name": "Quoc-Nam Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc-Nam Nguyen"
                },
                "author": "Quoc-Nam Nguyen",
                "arxiv_comment": "Accepted at SoICT 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12059v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12059v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20433v1",
                "updated": "2025-04-29T05:12:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    12,
                    53,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T05:12:53Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    5,
                    12,
                    53,
                    1,
                    119,
                    0
                ],
                "title": "Fiber to the Room: Key Technologies, Challenges, and Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fiber to the Room: Key Technologies, Challenges, and Prospects"
                },
                "summary": "Fiber to the Room (FTTR) is a next-generation access network designed to\ndeliver high bandwidth, low latency, and room-level optical coverage. This\npaper presents a comprehensive analysis of the FTTR system architecture and\nprotocol stack, focusing on three key technical aspects: centralized scheduling\nand control, integrated management and maintenance, and green energy-saving\nmechanisms. A simplified FTTR architecture based on the convergence of the\nmedium access control (MAC) and physical (PHY) layers is introduced to enhance\ncoordination and scheduling efficiency. An extended remote management scheme,\nbased on the optical network unit management and control interface (OMCI), is\ndescribed to enable unified control across main fiber units (MFUs) and\nsub-fiber units (SFUs). Furthermore, a service-aware energy-saving framework is\ndiscussed for dynamic power optimization. The paper also explores the\nintegration of artificial intelligence (AI) and passive sensing into FTTR\nsystems to support intelligent scheduling, energy management, and\nenvironment-aware optimization. These insights provide technical guidance for\nthe scalable deployment and future evolution of FTTR networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fiber to the Room (FTTR) is a next-generation access network designed to\ndeliver high bandwidth, low latency, and room-level optical coverage. This\npaper presents a comprehensive analysis of the FTTR system architecture and\nprotocol stack, focusing on three key technical aspects: centralized scheduling\nand control, integrated management and maintenance, and green energy-saving\nmechanisms. A simplified FTTR architecture based on the convergence of the\nmedium access control (MAC) and physical (PHY) layers is introduced to enhance\ncoordination and scheduling efficiency. An extended remote management scheme,\nbased on the optical network unit management and control interface (OMCI), is\ndescribed to enable unified control across main fiber units (MFUs) and\nsub-fiber units (SFUs). Furthermore, a service-aware energy-saving framework is\ndiscussed for dynamic power optimization. The paper also explores the\nintegration of artificial intelligence (AI) and passive sensing into FTTR\nsystems to support intelligent scheduling, energy management, and\nenvironment-aware optimization. These insights provide technical guidance for\nthe scalable deployment and future evolution of FTTR networks."
                },
                "authors": [
                    {
                        "name": "Jinhan Cai"
                    },
                    {
                        "name": "Xiaolong Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Tianhai Chang"
                    },
                    {
                        "name": "Gangxiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Gangxiang Shen"
                },
                "author": "Gangxiang Shen",
                "arxiv_comment": "12 pages, 7 figures,Submitted to arXiv, journal submission pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14634v4",
                "updated": "2025-04-29T04:47:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    47,
                    58,
                    1,
                    119,
                    0
                ],
                "published": "2024-09-23T00:09:34Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    0,
                    9,
                    34,
                    0,
                    267,
                    0
                ],
                "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in\n  Research-Paper Facet Recombination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scideator: Human-LLM Scientific Idea Generation Grounded in\n  Research-Paper Facet Recombination"
                },
                "summary": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas, and facet-based ideation is an established\nframework for idea generation. To see how large language models (LLMs) might\nassist in this process, we contribute a novel mixed-initiative ideation tool\ncalled Scideator. Starting from a user-provided set of scientific papers,\nScideator extracts key facets -- purposes, mechanisms, and evaluations -- from\nthese and related papers, allowing users to explore the idea space by\ninteractively recombining facets to synthesize inventive ideas. Scideator also\nhelps users gauge idea originality by searching the literature for overlaps,\nassessing idea novelty and providing explanations. To support these tasks,\nScideator introduces three LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty\nChecker. In a within-subjects user study (N=22) with computer-science\nresearchers comparing Scideator to a strong baseline, our tool provided\nsignificantly more creativity support, particularly with respect to\nexploration, which participants considered the most important factor for idea\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas, and facet-based ideation is an established\nframework for idea generation. To see how large language models (LLMs) might\nassist in this process, we contribute a novel mixed-initiative ideation tool\ncalled Scideator. Starting from a user-provided set of scientific papers,\nScideator extracts key facets -- purposes, mechanisms, and evaluations -- from\nthese and related papers, allowing users to explore the idea space by\ninteractively recombining facets to synthesize inventive ideas. Scideator also\nhelps users gauge idea originality by searching the literature for overlaps,\nassessing idea novelty and providing explanations. To support these tasks,\nScideator introduces three LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty\nChecker. In a within-subjects user study (N=22) with computer-science\nresearchers comparing Scideator to a strong baseline, our tool provided\nsignificantly more creativity support, particularly with respect to\nexploration, which participants considered the most important factor for idea\ngeneration."
                },
                "authors": [
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Simra Shahid"
                    },
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Daniel S. Weld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel S. Weld"
                },
                "author": "Daniel S. Weld",
                "arxiv_comment": "Updated with new and improved user study",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2, I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20426v1",
                "updated": "2025-04-29T04:42:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    42,
                    2,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T04:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    42,
                    2,
                    1,
                    119,
                    0
                ],
                "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis\n  based on Structured Function Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis\n  based on Structured Function Library"
                },
                "summary": "The advancement of reasoning capabilities in Large Language Models (LLMs)\nrequires substantial amounts of high-quality reasoning data, particularly in\nmathematics. Existing data synthesis methods, such as data augmentation from\nannotated training sets or direct question generation based on relevant\nknowledge points and documents, have expanded datasets but face challenges in\nmastering the inner logic of the problem during generation and ensuring the\nverifiability of the solutions. To address these issues, we propose RV-Syn, a\nnovel Rational and Verifiable mathematical Synthesis approach. RV-Syn\nconstructs a structured mathematical operation function library based on\ninitial seed problems and generates computational graphs as solutions by\ncombining Python-formatted functions from this library. These graphs are then\nback-translated into complex problems. Based on the constructed computation\ngraph, we achieve solution-guided logic-aware problem generation. Furthermore,\nthe executability of the computational graph ensures the verifiability of the\nsolving process. Experimental results show that RV-Syn surpasses existing\nsynthesis methods, including those involving human-generated problems,\nachieving greater efficient data scaling. This approach provides a scalable\nframework for generating high-quality reasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of reasoning capabilities in Large Language Models (LLMs)\nrequires substantial amounts of high-quality reasoning data, particularly in\nmathematics. Existing data synthesis methods, such as data augmentation from\nannotated training sets or direct question generation based on relevant\nknowledge points and documents, have expanded datasets but face challenges in\nmastering the inner logic of the problem during generation and ensuring the\nverifiability of the solutions. To address these issues, we propose RV-Syn, a\nnovel Rational and Verifiable mathematical Synthesis approach. RV-Syn\nconstructs a structured mathematical operation function library based on\ninitial seed problems and generates computational graphs as solutions by\ncombining Python-formatted functions from this library. These graphs are then\nback-translated into complex problems. Based on the constructed computation\ngraph, we achieve solution-guided logic-aware problem generation. Furthermore,\nthe executability of the computational graph ensures the verifiability of the\nsolving process. Experimental results show that RV-Syn surpasses existing\nsynthesis methods, including those involving human-generated problems,\nachieving greater efficient data scaling. This approach provides a scalable\nframework for generating high-quality reasoning datasets."
                },
                "authors": [
                    {
                        "name": "Jiapeng Wang"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20419v1",
                "updated": "2025-04-29T04:31:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    31,
                    58,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T04:31:58Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    31,
                    58,
                    1,
                    119,
                    0
                ],
                "title": "Plant Disease Detection through Multimodal Large Language Models and\n  Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plant Disease Detection through Multimodal Large Language Models and\n  Convolutional Neural Networks"
                },
                "summary": "Automation in agriculture plays a vital role in addressing challenges related\nto crop monitoring and disease management, particularly through early detection\nsystems. This study investigates the effectiveness of combining multimodal\nLarge Language Models (LLMs), specifically GPT-4o, with Convolutional Neural\nNetworks (CNNs) for automated plant disease classification using leaf imagery.\nLeveraging the PlantVillage dataset, we systematically evaluate model\nperformance across zero-shot, few-shot, and progressive fine-tuning scenarios.\nA comparative analysis between GPT-4o and the widely used ResNet-50 model was\nconducted across three resolutions (100, 150, and 256 pixels) and two plant\nspecies (apple and corn). Results indicate that fine-tuned GPT-4o models\nachieved slightly better performance compared to the performance of ResNet-50,\nachieving up to 98.12% classification accuracy on apple leaf images, compared\nto 96.88% achieved by ResNet-50, with improved generalization and near-zero\ntraining loss. However, zero-shot performance of GPT-4o was significantly\nlower, underscoring the need for minimal training. Additional evaluations on\ncross-resolution and cross-plant generalization revealed the models'\nadaptability and limitations when applied to new domains. The findings\nhighlight the promise of integrating multimodal LLMs into automated disease\ndetection pipelines, enhancing the scalability and intelligence of precision\nagriculture systems while reducing the dependence on large, labeled datasets\nand high-resolution sensor infrastructure. Large Language Models, Vision\nLanguage Models, LLMs and CNNs, Disease Detection with Vision Language Models,\nVLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation in agriculture plays a vital role in addressing challenges related\nto crop monitoring and disease management, particularly through early detection\nsystems. This study investigates the effectiveness of combining multimodal\nLarge Language Models (LLMs), specifically GPT-4o, with Convolutional Neural\nNetworks (CNNs) for automated plant disease classification using leaf imagery.\nLeveraging the PlantVillage dataset, we systematically evaluate model\nperformance across zero-shot, few-shot, and progressive fine-tuning scenarios.\nA comparative analysis between GPT-4o and the widely used ResNet-50 model was\nconducted across three resolutions (100, 150, and 256 pixels) and two plant\nspecies (apple and corn). Results indicate that fine-tuned GPT-4o models\nachieved slightly better performance compared to the performance of ResNet-50,\nachieving up to 98.12% classification accuracy on apple leaf images, compared\nto 96.88% achieved by ResNet-50, with improved generalization and near-zero\ntraining loss. However, zero-shot performance of GPT-4o was significantly\nlower, underscoring the need for minimal training. Additional evaluations on\ncross-resolution and cross-plant generalization revealed the models'\nadaptability and limitations when applied to new domains. The findings\nhighlight the promise of integrating multimodal LLMs into automated disease\ndetection pipelines, enhancing the scalability and intelligence of precision\nagriculture systems while reducing the dependence on large, labeled datasets\nand high-resolution sensor infrastructure. Large Language Models, Vision\nLanguage Models, LLMs and CNNs, Disease Detection with Vision Language Models,\nVLMs"
                },
                "authors": [
                    {
                        "name": "Konstantinos I. Roumeliotis"
                    },
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    },
                    {
                        "name": "Nikolaos D. Tselikas"
                    },
                    {
                        "name": "Dimitrios K. Nasiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios K. Nasiopoulos"
                },
                "author": "Dimitrios K. Nasiopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20414v1",
                "updated": "2025-04-29T04:23:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    23,
                    10,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T04:23:10Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    4,
                    23,
                    10,
                    1,
                    119,
                    0
                ],
                "title": "Enhancing Leakage Attacks on Searchable Symmetric Encryption Using\n  LLM-Based Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Leakage Attacks on Searchable Symmetric Encryption Using\n  LLM-Based Synthetic Data Generation"
                },
                "summary": "Searchable Symmetric Encryption (SSE) enables efficient search capabilities\nover encrypted data, allowing users to maintain privacy while utilizing cloud\nstorage. However, SSE schemes are vulnerable to leakage attacks that exploit\naccess patterns, search frequency, and volume information. Existing studies\nfrequently assume that adversaries possess a substantial fraction of the\nencrypted dataset to mount effective inference attacks, implying there is a\ndatabase leakage of such documents, thus, an assumption that may not hold in\nreal-world scenarios. In this work, we investigate the feasibility of enhancing\nleakage attacks under a more realistic threat model in which adversaries have\naccess to minimal leaked data. We propose a novel approach that leverages large\nlanguage models (LLMs), specifically GPT-4 variants, to generate synthetic\ndocuments that statistically and semantically resemble the real-world dataset\nof Enron emails. Using the email corpus as a case study, we evaluate the\neffectiveness of synthetic data generated via random sampling and hierarchical\nclustering methods on the performance of the SAP (Search Access Pattern)\nkeyword inference attack restricted to token volumes only. Our results\ndemonstrate that, while the choice of LLM has limited effect, increasing\ndataset size and employing clustering-based generation significantly improve\nattack accuracy, achieving comparable performance to attacks using larger\namounts of real data. We highlight the growing relevance of LLMs in adversarial\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searchable Symmetric Encryption (SSE) enables efficient search capabilities\nover encrypted data, allowing users to maintain privacy while utilizing cloud\nstorage. However, SSE schemes are vulnerable to leakage attacks that exploit\naccess patterns, search frequency, and volume information. Existing studies\nfrequently assume that adversaries possess a substantial fraction of the\nencrypted dataset to mount effective inference attacks, implying there is a\ndatabase leakage of such documents, thus, an assumption that may not hold in\nreal-world scenarios. In this work, we investigate the feasibility of enhancing\nleakage attacks under a more realistic threat model in which adversaries have\naccess to minimal leaked data. We propose a novel approach that leverages large\nlanguage models (LLMs), specifically GPT-4 variants, to generate synthetic\ndocuments that statistically and semantically resemble the real-world dataset\nof Enron emails. Using the email corpus as a case study, we evaluate the\neffectiveness of synthetic data generated via random sampling and hierarchical\nclustering methods on the performance of the SAP (Search Access Pattern)\nkeyword inference attack restricted to token volumes only. Our results\ndemonstrate that, while the choice of LLM has limited effect, increasing\ndataset size and employing clustering-based generation significantly improve\nattack accuracy, achieving comparable performance to attacks using larger\namounts of real data. We highlight the growing relevance of LLMs in adversarial\ncontexts."
                },
                "authors": [
                    {
                        "name": "Joshua Chiu"
                    },
                    {
                        "name": "Partha Protim Paul"
                    },
                    {
                        "name": "Zahin Wahab"
                    }
                ],
                "author_detail": {
                    "name": "Zahin Wahab"
                },
                "author": "Zahin Wahab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]