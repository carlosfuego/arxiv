[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v1",
                "updated": "2025-04-23T11:18:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV3Sb5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV3Sb5"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, Fc, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of Tc = 78 K. Notably, the pump-induced band shifts at\nFc were comparable to those caused by thermal effects at Tc. These findings\nsuggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges\nabove 150 K, with out-of-plane electronic correlations leading to the $2\\times2\n\\times 2$ CDW near Tc, offering key insights into the interplay between the\nelectronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, Fc, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of Tc = 78 K. Notably, the pump-induced band shifts at\nFc were comparable to those caused by thermal effects at Tc. These findings\nsuggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges\nabove 150 K, with out-of-plane electronic correlations leading to the $2\\times2\n\\times 2$ CDW near Tc, offering key insights into the interplay between the\nelectronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v2",
                "updated": "2025-04-17T21:19:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    21,
                    19,
                    19,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camgöz"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.17789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17789v1",
                "updated": "2025-04-24T17:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    59,
                    56,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    59,
                    56,
                    3,
                    114,
                    0
                ],
                "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models"
                },
                "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs."
                },
                "authors": [
                    {
                        "name": "Xu Ma"
                    },
                    {
                        "name": "Peize Sun"
                    },
                    {
                        "name": "Haoyu Ma"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Chih-Yao Ma"
                    },
                    {
                        "name": "Jialiang Wang"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Xiaoliang Dai"
                    },
                    {
                        "name": "Yujun Shi"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Yushi Hu"
                    },
                    {
                        "name": "Artsiom Sanakoyeu"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Ji Hou"
                    },
                    {
                        "name": "Junjiao Tian"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Tingbo Hou"
                    },
                    {
                        "name": "Yen-Cheng Liu"
                    },
                    {
                        "name": "Zecheng He"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Matt Feiszli"
                    },
                    {
                        "name": "Peizhao Zhang"
                    },
                    {
                        "name": "Peter Vajda"
                    },
                    {
                        "name": "Sam Tsai"
                    },
                    {
                        "name": "Yun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Fu"
                },
                "author": "Yun Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17780v1",
                "updated": "2025-04-24T17:56:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    56,
                    22,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:56:22Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    56,
                    22,
                    3,
                    114,
                    0
                ],
                "title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replay to Remember: Retaining Domain Knowledge in Streaming Language\n  Models"
                },
                "summary": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Sneh Pillai"
                    }
                ],
                "author_detail": {
                    "name": "Sneh Pillai"
                },
                "arxiv_affiliation": "University of Massachusetts Dartmouth",
                "author": "Sneh Pillai",
                "arxiv_comment": "8 pages 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08585v2",
                "updated": "2025-04-24T17:42:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    42,
                    16,
                    3,
                    114,
                    0
                ],
                "published": "2025-03-11T16:21:23Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    21,
                    23,
                    1,
                    70,
                    0
                ],
                "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding"
                },
                "summary": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis."
                },
                "authors": [
                    {
                        "name": "Shehreen Azad"
                    },
                    {
                        "name": "Vibhav Vineet"
                    },
                    {
                        "name": "Yogesh Singh Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Singh Rawat"
                },
                "author": "Yogesh Singh Rawat",
                "arxiv_comment": "Accepted in CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17768v1",
                "updated": "2025-04-24T17:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    39,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    39,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"
                },
                "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Robert Li"
                    },
                    {
                        "name": "Renjie Huang"
                    },
                    {
                        "name": "Sebastian Ruder"
                    },
                    {
                        "name": "Kelly Marchisio"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v1",
                "updated": "2025-04-24T17:25:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17752v1",
                "updated": "2025-04-24T17:10:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    10,
                    18,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:10:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    10,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "Disaggregated Deep Learning via In-Physics Computing at Radio Frequency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Deep Learning via In-Physics Computing at Radio Frequency"
                },
                "summary": "Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,\nrely on deep learning to enable a wide range of intelligent applications,\nincluding object recognition, environment perception, and autonomous\nnavigation. However, deploying deep learning models directly on the often\nresource-constrained edge devices demands significant memory footprints and\ncomputational power for real-time inference using traditional digital computing\narchitectures. In this paper, we present WISE, a novel computing architecture\nfor wireless edge networks designed to overcome energy constraints in deep\nlearning inference. WISE achieves this goal through two key innovations:\ndisaggregated model access via wireless broadcasting and in-physics computation\nof general complex-valued matrix-vector multiplications directly at radio\nfrequency. Using a software-defined radio platform with wirelessly broadcast\nmodel weights over the air, we demonstrate that WISE achieves 95.7% image\nclassification accuracy with ultra-low operation power of 6.0 fJ/MAC per\nclient, corresponding to a computation efficiency of 165.8 TOPS/W. This\napproach enables energy-efficient deep learning inference on wirelessly\nconnected edge devices, achieving more than two orders of magnitude improvement\nin efficiency compared to traditional digital computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,\nrely on deep learning to enable a wide range of intelligent applications,\nincluding object recognition, environment perception, and autonomous\nnavigation. However, deploying deep learning models directly on the often\nresource-constrained edge devices demands significant memory footprints and\ncomputational power for real-time inference using traditional digital computing\narchitectures. In this paper, we present WISE, a novel computing architecture\nfor wireless edge networks designed to overcome energy constraints in deep\nlearning inference. WISE achieves this goal through two key innovations:\ndisaggregated model access via wireless broadcasting and in-physics computation\nof general complex-valued matrix-vector multiplications directly at radio\nfrequency. Using a software-defined radio platform with wirelessly broadcast\nmodel weights over the air, we demonstrate that WISE achieves 95.7% image\nclassification accuracy with ultra-low operation power of 6.0 fJ/MAC per\nclient, corresponding to a computation efficiency of 165.8 TOPS/W. This\napproach enables energy-efficient deep learning inference on wirelessly\nconnected edge devices, achieving more than two orders of magnitude improvement\nin efficiency compared to traditional digital computing."
                },
                "authors": [
                    {
                        "name": "Zhihui Gao"
                    },
                    {
                        "name": "Sri Krishna Vadlamani"
                    },
                    {
                        "name": "Kfir Sulimany"
                    },
                    {
                        "name": "Dirk Englund"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingjun Chen"
                },
                "author": "Tingjun Chen",
                "arxiv_comment": "11 pages, 4 figures. Supplementary Information: 54 pages, 20 figures,\n  1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07991v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07991v3",
                "updated": "2025-04-24T16:46:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    46,
                    59,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-11T22:18:16Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    22,
                    18,
                    16,
                    1,
                    42,
                    0
                ],
                "title": "Exact Simulation of Longitudinal Data from Marginal Structural Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact Simulation of Longitudinal Data from Marginal Structural Models"
                },
                "summary": "Simulating longitudinal data from specified marginal structural models is a\ncrucial but challenging task for evaluating causal inference methods and\ninforming study design. While data generation typically proceeds in a fully\nconditional manner using structural equations according to a temporal ordering,\nit is difficult to ensure alignment between conditional distributions and the\ntarget marginal causal effects, which presents a fundamental challenge. To\naddress this, we propose a flexible and efficient algorithm for simulating\nlongitudinal data that adheres exactly to a specified marginal structural\nmodel. Our approach accommodates time-to-event outcomes and extends naturally\nto survival settings, which are prevalent in applied research. Compared to\nexisting approaches, it offers several advantages: it enables exact simulation\nfrom a known causal model rather than relying on approximations; avoids\nrestrictive assumptions about the data-generating process; and remains\ncomputationally efficient by requiring only the evaluation of analytical\nexpressions, rather than Monte Carlo methods or numerical integration. Through\nsimulation studies replicating realistic scenarios, we validate the method's\naccuracy and utility. Our method will facilitate researchers in effectively\nsimulating data with target causal structures for their specific scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating longitudinal data from specified marginal structural models is a\ncrucial but challenging task for evaluating causal inference methods and\ninforming study design. While data generation typically proceeds in a fully\nconditional manner using structural equations according to a temporal ordering,\nit is difficult to ensure alignment between conditional distributions and the\ntarget marginal causal effects, which presents a fundamental challenge. To\naddress this, we propose a flexible and efficient algorithm for simulating\nlongitudinal data that adheres exactly to a specified marginal structural\nmodel. Our approach accommodates time-to-event outcomes and extends naturally\nto survival settings, which are prevalent in applied research. Compared to\nexisting approaches, it offers several advantages: it enables exact simulation\nfrom a known causal model rather than relying on approximations; avoids\nrestrictive assumptions about the data-generating process; and remains\ncomputationally efficient by requiring only the evaluation of analytical\nexpressions, rather than Monte Carlo methods or numerical integration. Through\nsimulation studies replicating realistic scenarios, we validate the method's\naccuracy and utility. Our method will facilitate researchers in effectively\nsimulating data with target causal structures for their specific scenarios."
                },
                "authors": [
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Daniel de Vassimon Manela"
                    },
                    {
                        "name": "Chase Mathis"
                    },
                    {
                        "name": "Jens Magelund Tarp"
                    },
                    {
                        "name": "Robin J. Evans"
                    }
                ],
                "author_detail": {
                    "name": "Robin J. Evans"
                },
                "author": "Robin J. Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07991v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07991v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17723v1",
                "updated": "2025-04-24T16:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    36,
                    19,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:36:19Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    36,
                    19,
                    3,
                    114,
                    0
                ],
                "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework"
                },
                "summary": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Natan Levy"
                    },
                    {
                        "name": "Adiel Ashrov"
                    },
                    {
                        "name": "Guy Katz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Katz"
                },
                "author": "Guy Katz",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17720v1",
                "updated": "2025-04-24T16:32:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    32,
                    31,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:32:31Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    32,
                    31,
                    3,
                    114,
                    0
                ],
                "title": "Multilingual Performance Biases of Large Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Performance Biases of Large Language Models in Education"
                },
                "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."
                },
                "authors": [
                    {
                        "name": "Vansh Gupta"
                    },
                    {
                        "name": "Sankalan Pal Chowdhury"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03416v3",
                "updated": "2025-04-24T16:23:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    23,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2024-11-05T18:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    57,
                    45,
                    1,
                    310,
                    0
                ],
                "title": "Efficient Iterative Proximal Variational Inference Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Iterative Proximal Variational Inference Motion Planning"
                },
                "summary": "Motion planning under uncertainty can be cast as a stochastic optimal control\nproblem where the optimal posterior distribution has an explicit form. To\napproximate this posterior, this work frames an optimization problem in the\nspace of Gaussian distributions by solving a Variational Inference (VI) in the\npath distribution space. For linear-Gaussian stochastic dynamics, we propose a\nproximal algorithm to solve for an optimal Gaussian proposal iteratively. The\ncomputational bottleneck is evaluating the gradients with respect to the\nproposal over a dense trajectory. We exploit the sparse motion planning factor\ngraph and Gaussian Belief Propagation (GBP), allowing for parallel computing of\nthese gradients on Graphics Processing Units (GPUs). We term the novel paradigm\nas the Parallel Gaussian Variational Inference Motion Planning (P-GVIMP).\nBuilding on the efficient algorithm for linear Gaussian systems, we then\npropose an iterative paradigm based on Statistical Linear Regression (SLR)\ntechniques to solve motion planning for nonlinear stochastic systems, where the\nP-GVIMP serves as a sub-routine for the linearized time-varying system. We\nvalidate the proposed framework on various robotic systems, demonstrating\nsignificant speed acceleration achieved by leveraging parallel computation and\nsuccessful planning solutions for nonlinear systems under uncertainty. An\nopen-sourced implementation is presented at https://github.com/hzyu17/VIMP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion planning under uncertainty can be cast as a stochastic optimal control\nproblem where the optimal posterior distribution has an explicit form. To\napproximate this posterior, this work frames an optimization problem in the\nspace of Gaussian distributions by solving a Variational Inference (VI) in the\npath distribution space. For linear-Gaussian stochastic dynamics, we propose a\nproximal algorithm to solve for an optimal Gaussian proposal iteratively. The\ncomputational bottleneck is evaluating the gradients with respect to the\nproposal over a dense trajectory. We exploit the sparse motion planning factor\ngraph and Gaussian Belief Propagation (GBP), allowing for parallel computing of\nthese gradients on Graphics Processing Units (GPUs). We term the novel paradigm\nas the Parallel Gaussian Variational Inference Motion Planning (P-GVIMP).\nBuilding on the efficient algorithm for linear Gaussian systems, we then\npropose an iterative paradigm based on Statistical Linear Regression (SLR)\ntechniques to solve motion planning for nonlinear stochastic systems, where the\nP-GVIMP serves as a sub-routine for the linearized time-varying system. We\nvalidate the proposed framework on various robotic systems, demonstrating\nsignificant speed acceleration achieved by leveraging parallel computation and\nsuccessful planning solutions for nonlinear systems under uncertainty. An\nopen-sourced implementation is presented at https://github.com/hzyu17/VIMP."
                },
                "authors": [
                    {
                        "name": "Zinuo Chang"
                    },
                    {
                        "name": "Hongzhe Yu"
                    },
                    {
                        "name": "Patricio Vela"
                    },
                    {
                        "name": "Yongxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Chen"
                },
                "author": "Yongxin Chen",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14020v2",
                "updated": "2025-04-24T16:19:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    19,
                    43,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-18T18:18:45Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    18,
                    18,
                    45,
                    4,
                    108,
                    0
                ],
                "title": "HyDra: SOT-CAM Based Vector Symbolic Macro for Hyperdimensional\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyDra: SOT-CAM Based Vector Symbolic Macro for Hyperdimensional\n  Computing"
                },
                "summary": "Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its\nnoise robustness, parallelism, energy efficiency, and low computational\noverhead. Hardware accelerators are being explored to further enhance its\nperformance, but current solutions are often limited by application specificity\nand the latency of encoding and similarity search. This paper presents a\ngeneralized, reconfigurable on-chip training and inference architecture for\nHDC, utilizing spin-orbit-torque magnetic (SOT-MRAM) content-addressable memory\n(CAM). The proposed SOT-CAM array integrates storage and computation, enabling\nin-memory execution of key HDC operations: binding (bitwise multiplication),\npermutation (bit rotation), and efficient similarity search. To mitigate\ninterconnect parasitic effect in similarity search, a four-stage voltage\nscaling scheme has been proposed to ensure accurate Hamming distance\nrepresentation. Additionally, a novel bit drop method replaces bit rotation\nduring read operations, and an HDC-specific adder reduces energy and area by\n1.51x and 1.43x, respectively. Benchmarked at 7nm, the architecture achieves\nenergy reductions of 21.5x, 552.74x, 1.45x, and 282.57x for addition,\npermutation, multiplication, and search operations, respectively, compared to\nCMOS-based HDC. Against state-of-the-art HD accelerators, it achieves a 2.27x\nlower energy consumption and outperforms CPU and eGPU implementations by 2702x\nand 23161x, respectively, with less than 3% drop in accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its\nnoise robustness, parallelism, energy efficiency, and low computational\noverhead. Hardware accelerators are being explored to further enhance its\nperformance, but current solutions are often limited by application specificity\nand the latency of encoding and similarity search. This paper presents a\ngeneralized, reconfigurable on-chip training and inference architecture for\nHDC, utilizing spin-orbit-torque magnetic (SOT-MRAM) content-addressable memory\n(CAM). The proposed SOT-CAM array integrates storage and computation, enabling\nin-memory execution of key HDC operations: binding (bitwise multiplication),\npermutation (bit rotation), and efficient similarity search. To mitigate\ninterconnect parasitic effect in similarity search, a four-stage voltage\nscaling scheme has been proposed to ensure accurate Hamming distance\nrepresentation. Additionally, a novel bit drop method replaces bit rotation\nduring read operations, and an HDC-specific adder reduces energy and area by\n1.51x and 1.43x, respectively. Benchmarked at 7nm, the architecture achieves\nenergy reductions of 21.5x, 552.74x, 1.45x, and 282.57x for addition,\npermutation, multiplication, and search operations, respectively, compared to\nCMOS-based HDC. Against state-of-the-art HD accelerators, it achieves a 2.27x\nlower energy consumption and outperforms CPU and eGPU implementations by 2702x\nand 23161x, respectively, with less than 3% drop in accuracy"
                },
                "authors": [
                    {
                        "name": "Md Mizanur Rahaman Nayan"
                    },
                    {
                        "name": "Che-Kai Liu"
                    },
                    {
                        "name": "Zishen Wan"
                    },
                    {
                        "name": "Arijit Raychowdhury"
                    },
                    {
                        "name": "Azad J Naeemi"
                    }
                ],
                "author_detail": {
                    "name": "Azad J Naeemi"
                },
                "author": "Azad J Naeemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17717v1",
                "updated": "2025-04-24T16:19:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    19,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:19:13Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    19,
                    13,
                    3,
                    114,
                    0
                ],
                "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series\n  Analysis and Interpretable Patient-Similarity Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Detection of Multidrug Resistance Using Multivariate Time Series\n  Analysis and Interpretable Patient-Similarity Representations"
                },
                "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care."
                },
                "authors": [
                    {
                        "name": "Óscar Escudero-Arnanz"
                    },
                    {
                        "name": "Antonio G. Marques"
                    },
                    {
                        "name": "Inmaculada Mora-Jiménez"
                    },
                    {
                        "name": "Joaquín Álvarez-Rodríguez"
                    },
                    {
                        "name": "Cristina Soguero-Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Soguero-Ruiz"
                },
                "author": "Cristina Soguero-Ruiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17699v1",
                "updated": "2025-04-24T16:08:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    8,
                    52,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:08:52Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    8,
                    52,
                    3,
                    114,
                    0
                ],
                "title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction"
                },
                "summary": "Multimodal click-through rate (CTR) prediction is a key technique in\nindustrial recommender systems. It leverages heterogeneous modalities such as\ntext, images, and behavioral logs to capture high-order feature interactions\nbetween users and items, thereby enhancing the system's understanding of user\ninterests and its ability to predict click behavior. The primary challenge in\nthis field lies in effectively utilizing the rich semantic information from\nmultiple modalities while satisfying the low-latency requirements of online\ninference in real-world applications. To foster progress in this area, the\nMultimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop\nformulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:\nthis task aims to explore multimodal information extraction and item\nrepresentation learning methods that enhance recommendation tasks; and (2) Task\n2 of Multimodal CTR Prediction: this task aims to explore what multimodal\nrecommendation model can effectively leverage multimodal embedding features and\nachieve better performance. In this paper, we propose a novel model for Task 2,\nnamed Quadratic Interest Network (QIN) for Multimodal CTR Prediction.\nSpecifically, QIN employs adaptive sparse target attention to extract\nmultimodal user behavior features, and leverages Quadratic Neural Networks to\ncapture high-order feature interactions. As a result, QIN achieved an AUC of\n0.9798 on the leaderboard and ranked second in the competition. The model code,\ntraining logs, hyperparameter configurations, and checkpoints are available at\nhttps://github.com/salmon1802/QIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal click-through rate (CTR) prediction is a key technique in\nindustrial recommender systems. It leverages heterogeneous modalities such as\ntext, images, and behavioral logs to capture high-order feature interactions\nbetween users and items, thereby enhancing the system's understanding of user\ninterests and its ability to predict click behavior. The primary challenge in\nthis field lies in effectively utilizing the rich semantic information from\nmultiple modalities while satisfying the low-latency requirements of online\ninference in real-world applications. To foster progress in this area, the\nMultimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop\nformulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:\nthis task aims to explore multimodal information extraction and item\nrepresentation learning methods that enhance recommendation tasks; and (2) Task\n2 of Multimodal CTR Prediction: this task aims to explore what multimodal\nrecommendation model can effectively leverage multimodal embedding features and\nachieve better performance. In this paper, we propose a novel model for Task 2,\nnamed Quadratic Interest Network (QIN) for Multimodal CTR Prediction.\nSpecifically, QIN employs adaptive sparse target attention to extract\nmultimodal user behavior features, and leverages Quadratic Neural Networks to\ncapture high-order feature interactions. As a result, QIN achieved an AUC of\n0.9798 on the leaderboard and ranked second in the competition. The model code,\ntraining logs, hyperparameter configurations, and checkpoints are available at\nhttps://github.com/salmon1802/QIN."
                },
                "authors": [
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Hanwei Li"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ziniu Yu"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17698v1",
                "updated": "2025-04-24T16:07:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    7,
                    45,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:07:45Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    7,
                    45,
                    3,
                    114,
                    0
                ],
                "title": "Self-Supervised Noise Adaptive MRI Denoising via Repetition to\n  Repetition (Rep2Rep) Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Supervised Noise Adaptive MRI Denoising via Repetition to\n  Repetition (Rep2Rep) Learning"
                },
                "summary": "Purpose: This work proposes a novel self-supervised noise-adaptive image\ndenoising framework, called Repetition to Repetition (Rep2Rep) learning, for\nlow-field (<1T) MRI applications. Methods: Rep2Rep learning extends the\nNoise2Noise framework by training a neural network on two repeated MRI\nacquisitions, using one repetition as input and another as target, without\nrequiring ground-truth data. It incorporates noise-adaptive training, enabling\ndenoising generalization across varying noise levels and flexible inference\nwith any number of repetitions. Performance was evaluated on both synthetic\nnoisy brain MRI and 0.55T prostate MRI data, and compared against supervised\nlearning and Monte Carlo Stein's Unbiased Risk Estimator (MC-SURE). Results:\nRep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets.\nOn synthetic brain data, it achieved denoising quality comparable to supervised\nlearning and surpassed MC-SURE, particularly in preserving structural details\nand reducing residual noise. On the 0.55T prostate MRI dataset, a reader study\nshowed radiologists preferred Rep2Rep-denoised 2-average images over 8-average\nnoisy images. Rep2Rep demonstrated robustness to noise-level discrepancies\nbetween training and inference, supporting its practical implementation.\nConclusion: Rep2Rep learning offers an effective self-supervised denoising for\nlow-field MRI by leveraging routinely acquired multi-repetition data. Its\nnoise-adaptivity enables generalization to different SNR regimes without clean\nreference images. This makes Rep2Rep learning a promising tool for improving\nimage quality and scan efficiency in low-field MRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: This work proposes a novel self-supervised noise-adaptive image\ndenoising framework, called Repetition to Repetition (Rep2Rep) learning, for\nlow-field (<1T) MRI applications. Methods: Rep2Rep learning extends the\nNoise2Noise framework by training a neural network on two repeated MRI\nacquisitions, using one repetition as input and another as target, without\nrequiring ground-truth data. It incorporates noise-adaptive training, enabling\ndenoising generalization across varying noise levels and flexible inference\nwith any number of repetitions. Performance was evaluated on both synthetic\nnoisy brain MRI and 0.55T prostate MRI data, and compared against supervised\nlearning and Monte Carlo Stein's Unbiased Risk Estimator (MC-SURE). Results:\nRep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets.\nOn synthetic brain data, it achieved denoising quality comparable to supervised\nlearning and surpassed MC-SURE, particularly in preserving structural details\nand reducing residual noise. On the 0.55T prostate MRI dataset, a reader study\nshowed radiologists preferred Rep2Rep-denoised 2-average images over 8-average\nnoisy images. Rep2Rep demonstrated robustness to noise-level discrepancies\nbetween training and inference, supporting its practical implementation.\nConclusion: Rep2Rep learning offers an effective self-supervised denoising for\nlow-field MRI by leveraging routinely acquired multi-repetition data. Its\nnoise-adaptivity enables generalization to different SNR regimes without clean\nreference images. This makes Rep2Rep learning a promising tool for improving\nimage quality and scan efficiency in low-field MRI."
                },
                "authors": [
                    {
                        "name": "Nikola Janjušević"
                    },
                    {
                        "name": "Jingjia Chen"
                    },
                    {
                        "name": "Luke Ginocchio"
                    },
                    {
                        "name": "Mary Bruno"
                    },
                    {
                        "name": "Yuhui Huang"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Hersh Chandarana"
                    },
                    {
                        "name": "Li Feng"
                    }
                ],
                "author_detail": {
                    "name": "Li Feng"
                },
                "author": "Li Feng",
                "arxiv_comment": "13 pages, 9 figures, 1 table, supplementary information at end of\n  document",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17695v1",
                "updated": "2025-04-24T16:03:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    3,
                    11,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:03:11Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    3,
                    11,
                    3,
                    114,
                    0
                ],
                "title": "PICO: Reconstructing 3D People In Contact with Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICO: Reconstructing 3D People In Contact with Objects"
                },
                "summary": "Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de."
                },
                "authors": [
                    {
                        "name": "Alpár Cseke"
                    },
                    {
                        "name": "Shashank Tripathi"
                    },
                    {
                        "name": "Sai Kumar Dwivedi"
                    },
                    {
                        "name": "Arjun Lakshmipathy"
                    },
                    {
                        "name": "Agniv Chatterjee"
                    },
                    {
                        "name": "Michael J. Black"
                    },
                    {
                        "name": "Dimitrios Tzionas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Tzionas"
                },
                "author": "Dimitrios Tzionas",
                "arxiv_comment": "Accepted in CVPR'25. Project Page: https://pico.is.tue.mpg.de",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17685v1",
                "updated": "2025-04-24T15:55:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    55,
                    10,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:55:10Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    55,
                    10,
                    3,
                    114,
                    0
                ],
                "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks"
                },
                "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."
                },
                "authors": [
                    {
                        "name": "Haru-Tada Sato"
                    },
                    {
                        "name": "Fuka Matsuzaki"
                    },
                    {
                        "name": "Jun-ichiro Takahashi"
                    }
                ],
                "author_detail": {
                    "name": "Jun-ichiro Takahashi"
                },
                "author": "Jun-ichiro Takahashi",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04318v2",
                "updated": "2025-04-24T15:50:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    50,
                    10,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-06T01:28:50Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    1,
                    28,
                    50,
                    6,
                    96,
                    0
                ],
                "title": "Variational Self-Supervised Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Self-Supervised Learning"
                },
                "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques."
                },
                "authors": [
                    {
                        "name": "Mehmet Can Yavuz"
                    },
                    {
                        "name": "Berrin Yanikoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berrin Yanikoglu"
                },
                "author": "Berrin Yanikoglu",
                "arxiv_comment": "NeurIPS 2025 - SSL Workshop Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17677v1",
                "updated": "2025-04-24T15:47:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:47:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models"
                },
                "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience."
                },
                "authors": [
                    {
                        "name": "Jarne Thys"
                    },
                    {
                        "name": "Sebe Vanbrabant"
                    },
                    {
                        "name": "Davy Vanacken"
                    },
                    {
                        "name": "Gustavo Rovelo Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Rovelo Ruiz"
                },
                "author": "Gustavo Rovelo Ruiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17674v1",
                "updated": "2025-04-24T15:45:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    45,
                    5,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:45:05Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    45,
                    5,
                    3,
                    114,
                    0
                ],
                "title": "Energy Considerations of Large Language Model Inference and Efficiency\n  Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Considerations of Large Language Model Inference and Efficiency\n  Optimizations"
                },
                "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Jared Fernandez"
                    },
                    {
                        "name": "Clara Na"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Sasha Luccioni"
                    },
                    {
                        "name": "Emma Strubell"
                    }
                ],
                "author_detail": {
                    "name": "Emma Strubell"
                },
                "author": "Emma Strubell",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05255v2",
                "updated": "2025-04-24T15:43:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    43,
                    32,
                    3,
                    114,
                    0
                ],
                "published": "2025-01-09T14:12:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    12,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "CallNavi, A Challenge and Empirical Study on LLM Function Calling and\n  Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CallNavi, A Challenge and Empirical Study on LLM Function Calling and\n  Routing"
                },
                "summary": "API-driven chatbot systems are increasingly integral to software engineering\napplications, yet their effectiveness hinges on accurately generating and\nexecuting API calls. This is particularly challenging in scenarios requiring\nmulti-step interactions with complex parameterization and nested API\ndependencies. Addressing these challenges, this work contributes to the\nevaluation and assessment of AI-based software development through three key\nadvancements: (1) the introduction of a novel dataset specifically designed for\nbenchmarking API function selection, parameter generation, and nested API\nexecution; (2) an empirical evaluation of state-of-the-art language models,\nanalyzing their performance across varying task complexities in API function\ngeneration and parameter accuracy; and (3) a hybrid approach to API routing,\ncombining general-purpose large language models for API selection with\nfine-tuned models and prompt engineering for parameter generation. These\ninnovations significantly improve API execution in chatbot systems, offering\npractical methodologies for enhancing software design, testing, and operational\nworkflows in real-world software engineering contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API-driven chatbot systems are increasingly integral to software engineering\napplications, yet their effectiveness hinges on accurately generating and\nexecuting API calls. This is particularly challenging in scenarios requiring\nmulti-step interactions with complex parameterization and nested API\ndependencies. Addressing these challenges, this work contributes to the\nevaluation and assessment of AI-based software development through three key\nadvancements: (1) the introduction of a novel dataset specifically designed for\nbenchmarking API function selection, parameter generation, and nested API\nexecution; (2) an empirical evaluation of state-of-the-art language models,\nanalyzing their performance across varying task complexities in API function\ngeneration and parameter accuracy; and (3) a hybrid approach to API routing,\ncombining general-purpose large language models for API selection with\nfine-tuned models and prompt engineering for parameter generation. These\ninnovations significantly improve API execution in chatbot systems, offering\npractical methodologies for enhancing software design, testing, and operational\nworkflows in real-world software engineering contexts."
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    },
                    {
                        "name": "Andrey Boytsov"
                    },
                    {
                        "name": "Ulrick Ble"
                    },
                    {
                        "name": "Anne Goujon"
                    }
                ],
                "author_detail": {
                    "name": "Anne Goujon"
                },
                "author": "Anne Goujon",
                "arxiv_journal_ref": "The 29th International Conference on Evaluation and Assessment in\n  Software Engineering (EASE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17672v1",
                "updated": "2025-04-24T15:40:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    40,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:40:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    40,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Cross-region Model Training with Communication-Computation Overlapping\n  and Delay Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-region Model Training with Communication-Computation Overlapping\n  and Delay Compensation"
                },
                "summary": "Training large language models (LLMs) requires massive computational\nresources, often necessitating the aggregation of geographically distributed\ndata centers (\\ie, cross-region training). However, the high communication\nlatency in wide-area networks severely degrades the efficiency of traditional\ndistributed training. While methods like DiLoCo reduce communication frequency,\nthey suffer from blocking synchronization. Streaming DiLoCo alleviates this\nissue via communication-computation overlapping but introduces update staleness\nand model inconsistency due to delayed global updates and partial\nsynchronization. These factors impair convergence, especially when aggressive\noverlap is needed to mask high latency. We propose CoCoDC, a novel distributed\ntraining framework with communication-computation overlapping and delay\ncompensation, to explicitly tackle these challenges. Within the CoCoDC\nframework, we specifically develop a novel Delay Compensation strategy based on\nTaylor expansion to effectively mitigate the staleness and an Adaptive\nTransmission strategy that dynamically schedules model fragment synchronization\nto optimize bandwidth usage and accelerate convergence. Extensive experiments\nhighlight the superior performance of CoCoDC over both DiLoCo and Streaming\nDiLoCo regarding final accuracy and training speed. Specifically, CoCoDC\nreduces the training steps needed to reach a comparable perplexity by up to\n21.0% compared to Streaming DiLoCo. Our work provides an effective solution for\nscalable and efficient cross-region LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) requires massive computational\nresources, often necessitating the aggregation of geographically distributed\ndata centers (\\ie, cross-region training). However, the high communication\nlatency in wide-area networks severely degrades the efficiency of traditional\ndistributed training. While methods like DiLoCo reduce communication frequency,\nthey suffer from blocking synchronization. Streaming DiLoCo alleviates this\nissue via communication-computation overlapping but introduces update staleness\nand model inconsistency due to delayed global updates and partial\nsynchronization. These factors impair convergence, especially when aggressive\noverlap is needed to mask high latency. We propose CoCoDC, a novel distributed\ntraining framework with communication-computation overlapping and delay\ncompensation, to explicitly tackle these challenges. Within the CoCoDC\nframework, we specifically develop a novel Delay Compensation strategy based on\nTaylor expansion to effectively mitigate the staleness and an Adaptive\nTransmission strategy that dynamically schedules model fragment synchronization\nto optimize bandwidth usage and accelerate convergence. Extensive experiments\nhighlight the superior performance of CoCoDC over both DiLoCo and Streaming\nDiLoCo regarding final accuracy and training speed. Specifically, CoCoDC\nreduces the training steps needed to reach a comparable perplexity by up to\n21.0% compared to Streaming DiLoCo. Our work provides an effective solution for\nscalable and efficient cross-region LLM training."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Yunming Liao"
                    },
                    {
                        "name": "Zhiwei Yao"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17669v1",
                "updated": "2025-04-24T15:38:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    38,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:38:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    38,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a HIPAA Compliant Agentic AI System in Healthcare"
                },
                "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification."
                },
                "authors": [
                    {
                        "name": "Subash Neupane"
                    },
                    {
                        "name": "Shaswata Mitra"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Rahimi"
                },
                "author": "Shahram Rahimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17665v1",
                "updated": "2025-04-24T15:34:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    34,
                    24,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:34:24Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    34,
                    24,
                    3,
                    114,
                    0
                ],
                "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for\n  Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for\n  Mathematics"
                },
                "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain."
                },
                "authors": [
                    {
                        "name": "Zena Al-Khalili"
                    },
                    {
                        "name": "Nick Howell"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17660v1",
                "updated": "2025-04-24T15:29:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    29,
                    39,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:29:39Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    29,
                    39,
                    3,
                    114,
                    0
                ],
                "title": "Effortless, Simulation-Efficient Bayesian Inference using Tabular\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effortless, Simulation-Efficient Bayesian Inference using Tabular\n  Foundation Models"
                },
                "summary": "Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PF eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PF eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems."
                },
                "authors": [
                    {
                        "name": "Julius Vetter"
                    },
                    {
                        "name": "Manuel Gloeckler"
                    },
                    {
                        "name": "Daniel Gedon"
                    },
                    {
                        "name": "Jakob H. Macke"
                    }
                ],
                "author_detail": {
                    "name": "Jakob H. Macke"
                },
                "author": "Jakob H. Macke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10117v3",
                "updated": "2025-04-24T15:29:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    29,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-01-17T11:05:42Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    5,
                    42,
                    4,
                    17,
                    0
                ],
                "title": "Prediction Sets and Conformal Inference with Interval Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction Sets and Conformal Inference with Interval Outcomes"
                },
                "summary": "Given data on a scalar random variable $Y$, a prediction set for $Y$ with\nmiscoverage level $\\alpha$ is a set of values for $Y$ that contains a randomly\ndrawn $Y$ with probability $1 - \\alpha$, where $\\alpha \\in (0,1)$. Among all\nprediction sets that satisfy this coverage property, the oracle prediction set\nis the one with the smallest volume. This paper provides estimation methods of\nsuch prediction sets given observed conditioning covariates when $Y$ is\n\\textit{censored} or \\textit{measured in intervals}. We first characterise the\noracle prediction set under interval censoring and develop a consistent\nestimator for the shortest prediction {\\it interval} that satisfies this\ncoverage property.These consistency results are extended to accommodate cases\nwhere the prediction set consists of multiple disjoint intervals. We use\nconformal inference to construct a prediction set that achieves finite-sample\nvalidity under censoring and maintains consistency as sample size increases,\nusing a conformity score function designed for interval data. The procedure\naccommodates the prediction uncertainty that is irreducible (due to the\nstochastic nature of outcomes), the modelling uncertainty due to partial\nidentification and also sampling uncertainty that gets reduced as samples get\nlarger. We conduct a set of Monte Carlo simulations and an application to data\nfrom the Current Population Survey. The results highlight the robustness and\nefficiency of the proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given data on a scalar random variable $Y$, a prediction set for $Y$ with\nmiscoverage level $\\alpha$ is a set of values for $Y$ that contains a randomly\ndrawn $Y$ with probability $1 - \\alpha$, where $\\alpha \\in (0,1)$. Among all\nprediction sets that satisfy this coverage property, the oracle prediction set\nis the one with the smallest volume. This paper provides estimation methods of\nsuch prediction sets given observed conditioning covariates when $Y$ is\n\\textit{censored} or \\textit{measured in intervals}. We first characterise the\noracle prediction set under interval censoring and develop a consistent\nestimator for the shortest prediction {\\it interval} that satisfies this\ncoverage property.These consistency results are extended to accommodate cases\nwhere the prediction set consists of multiple disjoint intervals. We use\nconformal inference to construct a prediction set that achieves finite-sample\nvalidity under censoring and maintains consistency as sample size increases,\nusing a conformity score function designed for interval data. The procedure\naccommodates the prediction uncertainty that is irreducible (due to the\nstochastic nature of outcomes), the modelling uncertainty due to partial\nidentification and also sampling uncertainty that gets reduced as samples get\nlarger. We conduct a set of Monte Carlo simulations and an application to data\nfrom the Current Population Survey. The results highlight the robustness and\nefficiency of the proposed methods."
                },
                "authors": [
                    {
                        "name": "Weiguang Liu"
                    },
                    {
                        "name": "Áureo de Paula"
                    },
                    {
                        "name": "Elie Tamer"
                    }
                ],
                "author_detail": {
                    "name": "Elie Tamer"
                },
                "author": "Elie Tamer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16975v3",
                "updated": "2025-04-24T15:22:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    22,
                    23,
                    3,
                    114,
                    0
                ],
                "published": "2023-10-25T20:20:09Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    20,
                    20,
                    9,
                    2,
                    298,
                    0
                ],
                "title": "Efficient Neural Network Approaches for Conditional Optimal Transport\n  with Applications in Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Neural Network Approaches for Conditional Optimal Transport\n  with Applications in Bayesian Inference"
                },
                "summary": "We present two neural network approaches that approximate the solutions of\nstatic and dynamic\n$\\unicode{x1D450}\\unicode{x1D45C}\\unicode{x1D45B}\\unicode{x1D451}\\unicode{x1D456}\\unicode{x1D461}\\unicode{x1D456}\\unicode{x1D45C}\\unicode{x1D45B}\\unicode{x1D44E}\\unicode{x1D459}\\unicode{x0020}\\unicode{x1D45C}\\unicode{x1D45D}\\unicode{x1D461}\\unicode{x1D456}\\unicode{x1D45A}\\unicode{x1D44E}\\unicode{x1D459}\\unicode{x0020}\\unicode{x1D461}\\unicode{x1D45F}\\unicode{x1D44E}\\unicode{x1D45B}\\unicode{x1D460}\\unicode{x1D45D}\\unicode{x1D45C}\\unicode{x1D45F}\\unicode{x1D461}$\n(COT) problems. Both approaches enable conditional sampling and conditional\ndensity estimation, which are core tasks in Bayesian\ninference$\\unicode{x2013}$particularly in the simulation-based\n($\\unicode{x201C}$likelihood-free$\\unicode{x201D}$) setting. Our methods\nrepresent the target conditional distribution as a transformation of a\ntractable reference distribution. Obtaining such a transformation, chosen here\nto be an approximation of the COT map, is computationally challenging even in\nmoderate dimensions. To improve scalability, our numerical algorithms use\nneural networks to parameterize candidate maps and further exploit the\nstructure of the COT problem. Our static approach approximates the map as the\ngradient of a partially input-convex neural network. It uses a novel numerical\nimplementation to increase computational efficiency compared to\nstate-of-the-art alternatives. Our dynamic approach approximates the\nconditional optimal transport via the flow map of a regularized neural ODE;\ncompared to the static approach, it is slower to train but offers more modeling\nchoices and can lead to faster sampling. We demonstrate both algorithms\nnumerically, comparing them with competing state-of-the-art approaches, using\nbenchmark datasets and simulation-based Bayesian inverse problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present two neural network approaches that approximate the solutions of\nstatic and dynamic\n$\\unicode{x1D450}\\unicode{x1D45C}\\unicode{x1D45B}\\unicode{x1D451}\\unicode{x1D456}\\unicode{x1D461}\\unicode{x1D456}\\unicode{x1D45C}\\unicode{x1D45B}\\unicode{x1D44E}\\unicode{x1D459}\\unicode{x0020}\\unicode{x1D45C}\\unicode{x1D45D}\\unicode{x1D461}\\unicode{x1D456}\\unicode{x1D45A}\\unicode{x1D44E}\\unicode{x1D459}\\unicode{x0020}\\unicode{x1D461}\\unicode{x1D45F}\\unicode{x1D44E}\\unicode{x1D45B}\\unicode{x1D460}\\unicode{x1D45D}\\unicode{x1D45C}\\unicode{x1D45F}\\unicode{x1D461}$\n(COT) problems. Both approaches enable conditional sampling and conditional\ndensity estimation, which are core tasks in Bayesian\ninference$\\unicode{x2013}$particularly in the simulation-based\n($\\unicode{x201C}$likelihood-free$\\unicode{x201D}$) setting. Our methods\nrepresent the target conditional distribution as a transformation of a\ntractable reference distribution. Obtaining such a transformation, chosen here\nto be an approximation of the COT map, is computationally challenging even in\nmoderate dimensions. To improve scalability, our numerical algorithms use\nneural networks to parameterize candidate maps and further exploit the\nstructure of the COT problem. Our static approach approximates the map as the\ngradient of a partially input-convex neural network. It uses a novel numerical\nimplementation to increase computational efficiency compared to\nstate-of-the-art alternatives. Our dynamic approach approximates the\nconditional optimal transport via the flow map of a regularized neural ODE;\ncompared to the static approach, it is slower to train but offers more modeling\nchoices and can lead to faster sampling. We demonstrate both algorithms\nnumerically, comparing them with competing state-of-the-art approaches, using\nbenchmark datasets and simulation-based Bayesian inverse problems."
                },
                "authors": [
                    {
                        "name": "Zheyu Oliver Wang"
                    },
                    {
                        "name": "Ricardo Baptista"
                    },
                    {
                        "name": "Youssef Marzouk"
                    },
                    {
                        "name": "Lars Ruthotto"
                    },
                    {
                        "name": "Deepanshu Verma"
                    }
                ],
                "author_detail": {
                    "name": "Deepanshu Verma"
                },
                "author": "Deepanshu Verma",
                "arxiv_comment": "26 pages, 7 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 62M45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16871v2",
                "updated": "2025-04-24T15:21:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    21,
                    54,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T16:46:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    46,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge"
                },
                "summary": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks"
                },
                "authors": [
                    {
                        "name": "Mirian Hipolito Garcia"
                    },
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Anastasios Kyrillidis"
                    },
                    {
                        "name": "Robert Sim"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01650v2",
                "updated": "2025-04-24T15:21:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    21,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-02T12:00:09Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    0,
                    9,
                    2,
                    92,
                    0
                ],
                "title": "Sparse Gaussian Neural Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Gaussian Neural Processes"
                },
                "summary": "Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant recent advances in probabilistic meta-learning, it is\ncommon for practitioners to avoid using deep learning models due to a\ncomparative lack of interpretability. Instead, many practitioners simply use\nnon-meta-models such as Gaussian processes with interpretable priors, and\nconduct the tedious procedure of training their model from scratch for each\ntask they encounter. While this is justifiable for tasks with a limited number\nof data points, the cubic computational cost of exact Gaussian process\ninference renders this prohibitive when each task has many observations. To\nremedy this, we introduce a family of models that meta-learn sparse Gaussian\nprocess inference. Not only does this enable rapid prediction on new tasks with\nsparse Gaussian processes, but since our models have clear interpretations as\nmembers of the neural process family, it also allows manual elicitation of\npriors in a neural process for the first time. In meta-learning regimes for\nwhich the number of observed tasks is small or for which expert domain\nknowledge is available, this offers a crucial advantage."
                },
                "authors": [
                    {
                        "name": "Tommy Rochussen"
                    },
                    {
                        "name": "Vincent Fortuin"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Fortuin"
                },
                "author": "Vincent Fortuin",
                "arxiv_comment": "Proceedings of the 7th Symposium on Advances in Approximate Bayesian\n  Inference, PMLR, 2025. 25 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10479v2",
                "updated": "2025-04-24T15:15:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    15,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-15T03:06:14Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    3,
                    6,
                    14,
                    5,
                    167,
                    0
                ],
                "title": "Unlocking Large Language Model's Planning Capabilities with Maximum\n  Diversity Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Large Language Model's Planning Capabilities with Maximum\n  Diversity Fine-tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive task-solving\ncapabilities through prompting techniques and system designs, including solving\nplanning tasks (e.g., math proofs, basic travel planning) when sufficient data\nis available online and used during pre-training. However, for planning tasks\nwith limited prior data (e.g., blocks world, advanced travel planning), the\nperformance of LLMs, including proprietary models like GPT and Gemini, is poor.\nThis paper investigates the impact of fine-tuning on the planning capabilities\nof LLMs, revealing that LLMs can achieve strong performance in planning through\nsubstantial (tens of thousands of specific examples) fine-tuning. Yet, this\nprocess incurs high economic, time, and computational costs for each planning\nproblem variation. To address this, we propose Clustering-Based Maximum\nDiversity Sampling (CMDS), which selects diverse and representative data to\nenhance sample efficiency and the model's generalization capability. Extensive\nevaluations demonstrate that CMDS-l, a baseline method combining CMDS with\nlanguage embeddings, outperforms random sampling. Furthermore, we introduce a\nnovel algorithm, CMDS-g, which encodes planning task instances with their graph\nrepresentations into the embedding space. Empirical results show that CMDS-g\nconsistently outperforms baseline methods across various scales and multiple\nbenchmark domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive task-solving\ncapabilities through prompting techniques and system designs, including solving\nplanning tasks (e.g., math proofs, basic travel planning) when sufficient data\nis available online and used during pre-training. However, for planning tasks\nwith limited prior data (e.g., blocks world, advanced travel planning), the\nperformance of LLMs, including proprietary models like GPT and Gemini, is poor.\nThis paper investigates the impact of fine-tuning on the planning capabilities\nof LLMs, revealing that LLMs can achieve strong performance in planning through\nsubstantial (tens of thousands of specific examples) fine-tuning. Yet, this\nprocess incurs high economic, time, and computational costs for each planning\nproblem variation. To address this, we propose Clustering-Based Maximum\nDiversity Sampling (CMDS), which selects diverse and representative data to\nenhance sample efficiency and the model's generalization capability. Extensive\nevaluations demonstrate that CMDS-l, a baseline method combining CMDS with\nlanguage embeddings, outperforms random sampling. Furthermore, we introduce a\nnovel algorithm, CMDS-g, which encodes planning task instances with their graph\nrepresentations into the embedding space. Empirical results show that CMDS-g\nconsistently outperforms baseline methods across various scales and multiple\nbenchmark domains."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Varakantham"
                },
                "author": "Pradeep Varakantham",
                "arxiv_comment": "8 pages of main paper, 2 pages of references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08659v3",
                "updated": "2025-04-24T15:12:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    12,
                    52,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-09T06:33:47Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    6,
                    33,
                    47,
                    6,
                    40,
                    0
                ],
                "title": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks"
                },
                "summary": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference."
                },
                "authors": [
                    {
                        "name": "Shuqi Shen"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Hui Zhong"
                    },
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Xinhu Zheng"
                    },
                    {
                        "name": "Hai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Yang"
                },
                "author": "Hai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11242v4",
                "updated": "2025-04-24T14:58:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    58,
                    40,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-17T14:47:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse"
                },
                "summary": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. 26 out of 27 models aligned using Trust-Align\nsubstantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.\nSpecifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),\nQAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly\nenhances models' ability to correctly refuse and provide quality citations. We\nalso demonstrate the effectiveness of Trust-Align across different open-weight\nmodels, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),\nand Phi3.5 (3.8b). We release our code at\nhttps://github.com/declare-lab/trust-align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. 26 out of 27 models aligned using Trust-Align\nsubstantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.\nSpecifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),\nQAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly\nenhances models' ability to correctly refuse and provide quality citations. We\nalso demonstrate the effectiveness of Trust-Align across different open-weight\nmodels, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),\nand Phi3.5 (3.8b). We release our code at\nhttps://github.com/declare-lab/trust-align."
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Shang Hong Sim"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Hai Leong Chieu"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "Published at ICLR 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17628v1",
                "updated": "2025-04-24T14:50:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    50,
                    10,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:50:10Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    50,
                    10,
                    3,
                    114,
                    0
                ],
                "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with\n  Self-attention Diffusion Models and the Potential for Text-Guided\n  Customization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with\n  Self-attention Diffusion Models and the Potential for Text-Guided\n  Customization"
                },
                "summary": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging."
                },
                "authors": [
                    {
                        "name": "Abderrachid Hamrani"
                    },
                    {
                        "name": "Daniela Leizaola"
                    },
                    {
                        "name": "Renato Sousa"
                    },
                    {
                        "name": "Jose P. Ponce"
                    },
                    {
                        "name": "Stanley Mathis"
                    },
                    {
                        "name": "David G. Armstrong"
                    },
                    {
                        "name": "Anuradha Godavarty"
                    }
                ],
                "author_detail": {
                    "name": "Anuradha Godavarty"
                },
                "author": "Anuradha Godavarty",
                "arxiv_comment": "12 pages, 8 figures, journal article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17626v1",
                "updated": "2025-04-24T14:48:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    48,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:48:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    48,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Improving Open-World Object Localization by Discovering Background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Open-World Object Localization by Discovering Background"
                },
                "summary": "Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task."
                },
                "authors": [
                    {
                        "name": "Ashish Singh"
                    },
                    {
                        "name": "Michael J. Jones"
                    },
                    {
                        "name": "Kuan-Chuan Peng"
                    },
                    {
                        "name": "Anoop Cherian"
                    },
                    {
                        "name": "Moitreya Chatterjee"
                    },
                    {
                        "name": "Erik Learned-Miller"
                    }
                ],
                "author_detail": {
                    "name": "Erik Learned-Miller"
                },
                "author": "Erik Learned-Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10838v2",
                "updated": "2025-04-24T14:46:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    46,
                    47,
                    3,
                    114,
                    0
                ],
                "published": "2024-11-16T16:41:54Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    16,
                    41,
                    54,
                    5,
                    321,
                    0
                ],
                "title": "Anisotropy in Pantheon+ supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anisotropy in Pantheon+ supernovae"
                },
                "summary": "We employ Maximum Likelihood Estimators to examine the Pantheon+ catalogue of\nType Ia supernovae for large scale anisotropies in the expansion rate of the\nUniverse. The analyses are carried out in the heliocentric frame, the CMB\nframe, as well as the Local Group frame. In all frames, the Hubble expansion\nrate in the redshift range 0.023 < z < 0.15 is found to have a statistically\nsignificant dipolar variation exceeding 1.5 km/s/Mpc, i.e. bigger than the\nclaimed 1% uncertainty in the SH0ES measurement of the Hubble parameter H_0.\nThe deceleration parameter too has a redshift-dependent dipolar modulation at >\n5 sigma significance, consistent with previous findings using the SDSSII/SNLS3\nJoint Lightcurve Analysis catalogue. The inferred cosmic acceleration cannot\ntherefore be due to a Cosmological Constant, but is likely a general\nrelativistic effect due to the anomalous bulk flow in our local Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We employ Maximum Likelihood Estimators to examine the Pantheon+ catalogue of\nType Ia supernovae for large scale anisotropies in the expansion rate of the\nUniverse. The analyses are carried out in the heliocentric frame, the CMB\nframe, as well as the Local Group frame. In all frames, the Hubble expansion\nrate in the redshift range 0.023 < z < 0.15 is found to have a statistically\nsignificant dipolar variation exceeding 1.5 km/s/Mpc, i.e. bigger than the\nclaimed 1% uncertainty in the SH0ES measurement of the Hubble parameter H_0.\nThe deceleration parameter too has a redshift-dependent dipolar modulation at >\n5 sigma significance, consistent with previous findings using the SDSSII/SNLS3\nJoint Lightcurve Analysis catalogue. The inferred cosmic acceleration cannot\ntherefore be due to a Cosmological Constant, but is likely a general\nrelativistic effect due to the anomalous bulk flow in our local Universe."
                },
                "authors": [
                    {
                        "name": "Animesh Sah"
                    },
                    {
                        "name": "Mohamed Rameez"
                    },
                    {
                        "name": "Subir Sarkar"
                    },
                    {
                        "name": "Christos Tsagas"
                    }
                ],
                "author_detail": {
                    "name": "Christos Tsagas"
                },
                "author": "Christos Tsagas",
                "arxiv_comment": "Comment: 34 pages, 11 figures, 9 tables; Accepted for publication in\n  EPJ C; Code available on:\n  https://github.com/Shin107/Anisotropy-in-Pantheon-Plus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17622v1",
                "updated": "2025-04-24T14:44:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    44,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:44:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    44,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Likelihood-Free Variational Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-Free Variational Autoencoders"
                },
                "summary": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17611v1",
                "updated": "2025-04-24T14:35:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    35,
                    34,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:35:34Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    35,
                    34,
                    3,
                    114,
                    0
                ],
                "title": "Some Results on Generalized Familywise Error Rate Controlling Procedures\n  under Dependence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some Results on Generalized Familywise Error Rate Controlling Procedures\n  under Dependence"
                },
                "summary": "The topic of multiple hypotheses testing now has a potpourri of novel\ntheories and ubiquitous applications in diverse scientific fields. However, the\nuniversal utility of this field often hinders the possibility of having a\ngeneralized theory that accommodates every scenario. This tradeoff is better\nreflected through the lens of dependence, a central piece behind the\ntheoretical and applied developments of multiple testing. Although omnipresent\nin many scientific avenues, the nature and extent of dependence vary\nsubstantially with the context and complexity of the particular scenario.\nPositive dependence is the norm in testing many treatments versus a single\ncontrol or in spatial statistics. On the contrary, negative dependence arises\nnaturally in tests based on split samples and in cyclical, ordered comparisons.\nIn GWAS, the SNP markers are generally considered to be weakly dependent.\nGeneralized familywise error rate (k-FWER) control has been one of the\nprominent frequentist approaches in simultaneous inference. However, the\nperformances of k-FWER controlling procedures are yet unexplored under\ndifferent dependencies. This paper revisits the classical testing problem of\nnormal means in different correlated frameworks. We establish upper bounds on\nthe generalized familywise error rates under each dependence, consequently\ngiving rise to improved testing procedures. Towards this, we present improved\nprobability inequalities, which are of independent theoretical interest",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The topic of multiple hypotheses testing now has a potpourri of novel\ntheories and ubiquitous applications in diverse scientific fields. However, the\nuniversal utility of this field often hinders the possibility of having a\ngeneralized theory that accommodates every scenario. This tradeoff is better\nreflected through the lens of dependence, a central piece behind the\ntheoretical and applied developments of multiple testing. Although omnipresent\nin many scientific avenues, the nature and extent of dependence vary\nsubstantially with the context and complexity of the particular scenario.\nPositive dependence is the norm in testing many treatments versus a single\ncontrol or in spatial statistics. On the contrary, negative dependence arises\nnaturally in tests based on split samples and in cyclical, ordered comparisons.\nIn GWAS, the SNP markers are generally considered to be weakly dependent.\nGeneralized familywise error rate (k-FWER) control has been one of the\nprominent frequentist approaches in simultaneous inference. However, the\nperformances of k-FWER controlling procedures are yet unexplored under\ndifferent dependencies. This paper revisits the classical testing problem of\nnormal means in different correlated frameworks. We establish upper bounds on\nthe generalized familywise error rates under each dependence, consequently\ngiving rise to improved testing procedures. Towards this, we present improved\nprobability inequalities, which are of independent theoretical interest"
                },
                "authors": [
                    {
                        "name": "Monitirtha Dey"
                    },
                    {
                        "name": "Subir Kumar Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Subir Kumar Bhandari"
                },
                "author": "Subir Kumar Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07263v3",
                "updated": "2025-04-24T14:34:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    34,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2023-10-11T07:39:42Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    39,
                    42,
                    2,
                    284,
                    0
                ],
                "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPAL: Corrective Planning of Robot Actions with Large Language Models"
                },
                "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation."
                },
                "authors": [
                    {
                        "name": "Frank Joublin"
                    },
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Pavel Smirnov"
                    },
                    {
                        "name": "Felix Ocker"
                    },
                    {
                        "name": "Joerg Deigmoeller"
                    },
                    {
                        "name": "Anna Belardinelli"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Stephan Hasler"
                    },
                    {
                        "name": "Daniel Tanneberg"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "arxiv_doi": "10.1109/ICRA57147.2024.10610434",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICRA57147.2024.10610434",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.07263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE International Conference on Robotics and Automation (ICRA) 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12533v3",
                "updated": "2025-04-24T14:28:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    28,
                    14,
                    3,
                    114,
                    0
                ],
                "published": "2024-03-19T08:09:44Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    8,
                    9,
                    44,
                    1,
                    79,
                    0
                ],
                "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot\n  Group Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot\n  Group Interactions"
                },
                "summary": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed."
                },
                "authors": [
                    {
                        "name": "Daniel Tanneberg"
                    },
                    {
                        "name": "Felix Ocker"
                    },
                    {
                        "name": "Stephan Hasler"
                    },
                    {
                        "name": "Joerg Deigmoeller"
                    },
                    {
                        "name": "Anna Belardinelli"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Heiko Wersing"
                    },
                    {
                        "name": "Bernhard Sendhoff"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "arxiv_doi": "10.1109/IROS58592.2024.10801517",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10801517",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17600v1",
                "updated": "2025-04-24T14:26:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    26,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:26:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    26,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Model Choice Matters for Age Inference on the Red Giant Branch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Choice Matters for Age Inference on the Red Giant Branch"
                },
                "summary": "Galactic archaeology relies on accurate stellar parameters to reconstruct the\ngalaxy's history, including information on stellar ages. While the precision of\ndata has improved significantly in recent years, stellar models used for age\ninference have not improved at a similar rate. In fact, different models yield\nnotably different age predictions for the same observational data. In this\npaper, we assess the difference in age predictions of various widely used model\ngrids for stars along the red giant branch. Using open source software, we\nconduct a comparison of four different evolution grids and we find that age\nestimations become less reliable if stellar mass is not known, with differences\noccasionally exceeding $80\\%$. Additionally, we note significant disagreements\nin the models' age estimations at non-solar metallicity. Finally, we present a\nmethod for including theoretical uncertainties from stellar evolutionary tracks\nin age inferences of red giants, aimed at improving the accuracy of age\nestimation techniques used in the galactic archaeology community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galactic archaeology relies on accurate stellar parameters to reconstruct the\ngalaxy's history, including information on stellar ages. While the precision of\ndata has improved significantly in recent years, stellar models used for age\ninference have not improved at a similar rate. In fact, different models yield\nnotably different age predictions for the same observational data. In this\npaper, we assess the difference in age predictions of various widely used model\ngrids for stars along the red giant branch. Using open source software, we\nconduct a comparison of four different evolution grids and we find that age\nestimations become less reliable if stellar mass is not known, with differences\noccasionally exceeding $80\\%$. Additionally, we note significant disagreements\nin the models' age estimations at non-solar metallicity. Finally, we present a\nmethod for including theoretical uncertainties from stellar evolutionary tracks\nin age inferences of red giants, aimed at improving the accuracy of age\nestimation techniques used in the galactic archaeology community."
                },
                "authors": [
                    {
                        "name": "Leslie M. Morales"
                    },
                    {
                        "name": "Jamie Tayar"
                    },
                    {
                        "name": "Zachary R. Claytor"
                    }
                ],
                "author_detail": {
                    "name": "Zachary R. Claytor"
                },
                "author": "Zachary R. Claytor",
                "arxiv_comment": "25 pages, 18 figures, Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17582v1",
                "updated": "2025-04-24T14:12:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    12,
                    57,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:12:57Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    12,
                    57,
                    3,
                    114,
                    0
                ],
                "title": "Occlusion-Aware Self-Supervised Monocular Depth Estimation for\n  Weak-Texture Endoscopic Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occlusion-Aware Self-Supervised Monocular Depth Estimation for\n  Weak-Texture Endoscopic Images"
                },
                "summary": "We propose a self-supervised monocular depth estimation network tailored for\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\nmonocular images. Existing methods, though accurate, typically assume\nconsistent illumination, which is often violated due to dynamic lighting and\nocclusions caused by GI motility. These variations lead to incorrect geometric\ninterpretations and unreliable self-supervised signals, degrading depth\nreconstruction quality. To address this, we introduce an occlusion-aware\nself-supervised framework. First, we incorporate an occlusion mask for data\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\nocclusion scenarios. This enhances the model's ability to learn robust depth\nfeatures under partial visibility. Second, we leverage semantic segmentation\nguided by non-negative matrix factorization, clustering convolutional\nactivations to generate pseudo-labels in texture-deprived regions, thereby\nimproving segmentation accuracy and mitigating information loss from lighting\nchanges. Experimental results on the SCARED dataset show that our method\nachieves state-of-the-art performance in self-supervised depth estimation.\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\nstrong generalization across diverse endoscopic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a self-supervised monocular depth estimation network tailored for\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\nmonocular images. Existing methods, though accurate, typically assume\nconsistent illumination, which is often violated due to dynamic lighting and\nocclusions caused by GI motility. These variations lead to incorrect geometric\ninterpretations and unreliable self-supervised signals, degrading depth\nreconstruction quality. To address this, we introduce an occlusion-aware\nself-supervised framework. First, we incorporate an occlusion mask for data\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\nocclusion scenarios. This enhances the model's ability to learn robust depth\nfeatures under partial visibility. Second, we leverage semantic segmentation\nguided by non-negative matrix factorization, clustering convolutional\nactivations to generate pseudo-labels in texture-deprived regions, thereby\nimproving segmentation accuracy and mitigating information loss from lighting\nchanges. Experimental results on the SCARED dataset show that our method\nachieves state-of-the-art performance in self-supervised depth estimation.\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\nstrong generalization across diverse endoscopic environments."
                },
                "authors": [
                    {
                        "name": "Zebo Huang"
                    },
                    {
                        "name": "Yinghui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Wang"
                },
                "author": "Yinghui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07347v2",
                "updated": "2025-04-24T14:10:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    10,
                    22,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-10T00:12:12Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    0,
                    12,
                    12,
                    3,
                    100,
                    0
                ],
                "title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents"
                },
                "summary": "As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development."
                },
                "authors": [
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Jim Dai"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17577v1",
                "updated": "2025-04-24T14:08:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    8,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:08:49Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    8,
                    49,
                    3,
                    114,
                    0
                ],
                "title": "TileLang: A Composable Tiled Programming Model for AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TileLang: A Composable Tiled Programming Model for AI Systems"
                },
                "summary": "Modern AI workloads rely heavily on optimized computing kernels for both\ntraining and inference. These AI kernels follow well-defined data-flow\npatterns, such as moving tiles between DRAM and SRAM and performing a sequence\nof computations on those tiles. However, writing high-performance kernels\nremains complex despite the clarity of these patterns. Achieving peak\nperformance requires careful, hardware-centric optimizations to fully leverage\nmodern accelerators. While domain-specific compilers attempt to reduce the\nburden of writing high-performance kernels, they often struggle with usability\nand expressiveness gaps. In this paper, we present TileLang, a generalized\ntiled programming model for more efficient AI Kernel programming. TileLang\ndecouples scheduling space (thread binding, layout, tensorize and pipeline)\nfrom dataflow, and encapsulated them as a set of customization annotations and\nprimitives. This approach allows users to focus on the kernel's data-flow\nitself, while leaving most other optimizations to compilers. We conduct\ncomprehensive experiments on commonly-used devices, across numerous\nexperiments, our evaluation shows that TileLang can achieve state-of-the-art\nperformance in key kernels, demonstrating that its unified block-and-thread\nparadigm and transparent scheduling capabilities deliver both the power and\nflexibility demanded by modern AI system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI workloads rely heavily on optimized computing kernels for both\ntraining and inference. These AI kernels follow well-defined data-flow\npatterns, such as moving tiles between DRAM and SRAM and performing a sequence\nof computations on those tiles. However, writing high-performance kernels\nremains complex despite the clarity of these patterns. Achieving peak\nperformance requires careful, hardware-centric optimizations to fully leverage\nmodern accelerators. While domain-specific compilers attempt to reduce the\nburden of writing high-performance kernels, they often struggle with usability\nand expressiveness gaps. In this paper, we present TileLang, a generalized\ntiled programming model for more efficient AI Kernel programming. TileLang\ndecouples scheduling space (thread binding, layout, tensorize and pipeline)\nfrom dataflow, and encapsulated them as a set of customization annotations and\nprimitives. This approach allows users to focus on the kernel's data-flow\nitself, while leaving most other optimizations to compilers. We conduct\ncomprehensive experiments on commonly-used devices, across numerous\nexperiments, our evaluation shows that TileLang can achieve state-of-the-art\nperformance in key kernels, demonstrating that its unified block-and-thread\nparadigm and transparent scheduling capabilities deliver both the power and\nflexibility demanded by modern AI system development."
                },
                "authors": [
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Yining Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Wenhao Xie"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17565v1",
                "updated": "2025-04-24T13:57:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:57:53Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training"
                },
                "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17562v1",
                "updated": "2025-04-24T13:56:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    56,
                    43,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:56:43Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    56,
                    43,
                    3,
                    114,
                    0
                ],
                "title": "When Does Metadata Conditioning (NOT) Work for Language Model\n  Pre-Training? A Study with Context-Free Grammars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Does Metadata Conditioning (NOT) Work for Language Model\n  Pre-Training? A Study with Context-Free Grammars"
                },
                "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."
                },
                "authors": [
                    {
                        "name": "Rei Higuchi"
                    },
                    {
                        "name": "Ryotaro Kawata"
                    },
                    {
                        "name": "Naoki Nishikawa"
                    },
                    {
                        "name": "Kazusato Oko"
                    },
                    {
                        "name": "Shoichiro Yamaguchi"
                    },
                    {
                        "name": "Sosuke Kobayashi"
                    },
                    {
                        "name": "Seiya Tokui"
                    },
                    {
                        "name": "Kohei Hayashi"
                    },
                    {
                        "name": "Daisuke Okanohara"
                    },
                    {
                        "name": "Taiji Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Taiji Suzuki"
                },
                "author": "Taiji Suzuki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07687v2",
                "updated": "2025-04-24T13:53:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    53,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-10T12:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection"
                },
                "summary": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets, often\npolitically or virally motivated-pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets, often\npolitically or virally motivated-pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17550v1",
                "updated": "2025-04-24T13:40:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    40,
                    27,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:40:27Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    40,
                    27,
                    3,
                    114,
                    0
                ],
                "title": "HalluLens: LLM Hallucination Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluLens: LLM Hallucination Benchmark"
                },
                "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."
                },
                "authors": [
                    {
                        "name": "Yejin Bang"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Alan Schelten"
                    },
                    {
                        "name": "Anthony Hartshorn"
                    },
                    {
                        "name": "Tara Fowler"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17547v1",
                "updated": "2025-04-24T13:37:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    37,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:37:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    37,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering\n  Systems: The Lifecycle of Knowledge in Visual Reasoning Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Knowledge-Based Vision Question Answering\n  Systems: The Lifecycle of Knowledge in Visual Reasoning Task"
                },
                "summary": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications."
                },
                "authors": [
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Zonghan Wu"
                    },
                    {
                        "name": "Huan Huo"
                    },
                    {
                        "name": "Guandong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Xu"
                },
                "author": "Guandong Xu",
                "arxiv_comment": "20 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17544v1",
                "updated": "2025-04-24T13:32:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    30,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:32:30Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    30,
                    3,
                    114,
                    0
                ],
                "title": "Auditing the Ethical Logic of Generative AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing the Ethical Logic of Generative AI Models"
                },
                "summary": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts."
                },
                "authors": [
                    {
                        "name": "W. Russell Neuman"
                    },
                    {
                        "name": "Chad Coleman"
                    },
                    {
                        "name": "Ali Dasdan"
                    },
                    {
                        "name": "Safinah Ali"
                    },
                    {
                        "name": "Manan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Manan Shah"
                },
                "author": "Manan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17542v1",
                "updated": "2025-04-24T13:32:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:32:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Large Language Model-Driven Concolic Execution for Highly Structured\n  Test Input Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Concolic Execution for Highly Structured\n  Test Input Generation"
                },
                "summary": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). The experimental result is promising: it shows that\nCottontail outperforms state-of-the-art approaches (SymCC and Marco) by 14.15%\nand 14.31% in terms of line coverage. Besides, Cottontail found 6 previously\nunknown vulnerabilities (six new CVEs have been assigned). We have reported\nthese issues to developers, and 4 out of them have been fixed so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). The experimental result is promising: it shows that\nCottontail outperforms state-of-the-art approaches (SymCC and Marco) by 14.15%\nand 14.31% in terms of line coverage. Besides, Cottontail found 6 previously\nunknown vulnerabilities (six new CVEs have been assigned). We have reported\nthese issues to developers, and 4 out of them have been fixed so far."
                },
                "authors": [
                    {
                        "name": "Haoxin Tu"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Yuxian Li"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    },
                    {
                        "name": "Marcel Böhme"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Böhme"
                },
                "author": "Marcel Böhme",
                "arxiv_comment": "18 pages (including Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14088v2",
                "updated": "2025-04-24T13:24:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    24,
                    42,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-20T08:04:07Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    8,
                    4,
                    7,
                    3,
                    172,
                    0
                ],
                "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter\n  Reallocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaL: Efficient RLHF Training of Large Language Models with Parameter\n  Reallocation"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF ."
                },
                "authors": [
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Kaiwei Li"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Huanchen Zhang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "arxiv_comment": "11 pages (20 pages with references and the appendix), 17 figures.\n  Accepted by MLSys 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17531v1",
                "updated": "2025-04-24T13:19:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    19,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:19:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    19,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Machine-Generated Code for the Resolution of User Intentions"
                },
                "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions."
                },
                "authors": [
                    {
                        "name": "Justus Flerlage"
                    },
                    {
                        "name": "Ilja Behnke"
                    },
                    {
                        "name": "Odej Kao"
                    }
                ],
                "author_detail": {
                    "name": "Odej Kao"
                },
                "author": "Odej Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v4",
                "updated": "2025-04-24T13:16:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    16,
                    26,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02868v2",
                "updated": "2025-04-24T13:07:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    7,
                    21,
                    3,
                    114,
                    0
                ],
                "published": "2024-12-03T22:06:55Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    6,
                    55,
                    1,
                    338,
                    0
                ],
                "title": "Enhancing LLMs with Smart Preprocessing for EHR Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs with Smart Preprocessing for EHR Analysis"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nnatural language processing; however, their application in sensitive domains\nsuch as healthcare, especially in processing Electronic Health Records (EHRs),\nis constrained by limited computational resources and privacy concerns. This\npaper introduces a compact LLM framework optimized for local deployment in\nenvironments with stringent privacy requirements and restricted access to\nhigh-performance GPUs. Our approach leverages simple yet powerful preprocessing\ntechniques, including regular expressions (regex) and Retrieval-Augmented\nGeneration (RAG), to extract and highlight critical information from clinical\nnotes. By pre-filtering long, unstructured text, we enhance the performance of\nsmaller LLMs on EHR-related tasks. Our framework is evaluated using zero-shot\nand few-shot learning paradigms on both private and publicly available datasets\n(MIMIC-IV), with additional comparisons against fine-tuned LLMs on MIMIC-IV.\nExperimental results demonstrate that our preprocessing strategy significantly\nsupercharges the performance of smaller LLMs, making them well-suited for\nprivacy-sensitive and resource-constrained applications. This study offers\nvaluable insights into optimizing LLM performance for local, secure, and\nefficient healthcare applications. It provides practical guidance for\nreal-world deployment for LLMs while tackling challenges related to privacy,\ncomputational feasibility, and clinical applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nnatural language processing; however, their application in sensitive domains\nsuch as healthcare, especially in processing Electronic Health Records (EHRs),\nis constrained by limited computational resources and privacy concerns. This\npaper introduces a compact LLM framework optimized for local deployment in\nenvironments with stringent privacy requirements and restricted access to\nhigh-performance GPUs. Our approach leverages simple yet powerful preprocessing\ntechniques, including regular expressions (regex) and Retrieval-Augmented\nGeneration (RAG), to extract and highlight critical information from clinical\nnotes. By pre-filtering long, unstructured text, we enhance the performance of\nsmaller LLMs on EHR-related tasks. Our framework is evaluated using zero-shot\nand few-shot learning paradigms on both private and publicly available datasets\n(MIMIC-IV), with additional comparisons against fine-tuned LLMs on MIMIC-IV.\nExperimental results demonstrate that our preprocessing strategy significantly\nsupercharges the performance of smaller LLMs, making them well-suited for\nprivacy-sensitive and resource-constrained applications. This study offers\nvaluable insights into optimizing LLM performance for local, secure, and\nefficient healthcare applications. It provides practical guidance for\nreal-world deployment for LLMs while tackling challenges related to privacy,\ncomputational feasibility, and clinical applicability."
                },
                "authors": [
                    {
                        "name": "Yixiang Qu"
                    },
                    {
                        "name": "Yifan Dai"
                    },
                    {
                        "name": "Shilin Yu"
                    },
                    {
                        "name": "Pradham Tanikella"
                    },
                    {
                        "name": "Travis Schrank"
                    },
                    {
                        "name": "Trevor Hackman"
                    },
                    {
                        "name": "Didong Li"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17522v1",
                "updated": "2025-04-24T13:03:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    3,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:03:13Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    3,
                    13,
                    3,
                    114,
                    0
                ],
                "title": "Towards One-Stage End-to-End Table Structure Recognition with Parallel\n  Regression for Diverse Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards One-Stage End-to-End Table Structure Recognition with Parallel\n  Regression for Diverse Scenarios"
                },
                "summary": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet."
                },
                "authors": [
                    {
                        "name": "Anyi Xiao"
                    },
                    {
                        "name": "Cihui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cihui Yang"
                },
                "author": "Cihui Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17516v1",
                "updated": "2025-04-24T12:58:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    58,
                    41,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T12:58:41Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    58,
                    41,
                    3,
                    114,
                    0
                ],
                "title": "EP250108a/SN 2025kg: A Broad-Line Type Ic Supernova Associated with a\n  Fast X-ray Transient Showing Evidence of Extended CSM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EP250108a/SN 2025kg: A Broad-Line Type Ic Supernova Associated with a\n  Fast X-ray Transient Showing Evidence of Extended CSM Interaction"
                },
                "summary": "We present optical, radio, and X-ray observations of EP250108a/SN 2025kg, a\nbroad-line Type Ic supernova (SN Ic-BL) accompanying an Einstein Probe (EP)\nfast X-ray transient (FXT) at $z=0.176$. EP250108a/SN 2025kg possesses a\ndouble-peaked optical light curve and its spectrum transitions from a blue\nunderlying continuum to a typical SN Ic-BL spectrum over time. We fit a\nradioactive decay model to the second peak of the optical light curve and find\nSN parameters that are consistent with the SNe Ic-BL population, while its\nX-ray and radio properties are consistent with those of low-luminosity GRB\n(LLGRB) 060218/SN 2006aj. We explore three scenarios to understand the system's\nmulti-wavelength emission -- (a) SN ejecta interacting with an extended\ncircumstellar medium (CSM), (b) the shocked cocoon of a collapsar-driven jet\nchoked in its stellar envelope, and (c) the shocked cocoon of a\ncollapsar-driven jet choked in an extended CSM. All three models can explain\nthe optical light curve and are also consistent with the radio and X-ray\nobservations. We favor models (a) and (c) because they self-consistently\nexplain both the X-ray prompt emission and first optical peak, but we do not\nrule out model (b). From the properties of the first peak in models (a) and\n(c), we find evidence that EP250108a/SN 2025kg interacts with an extended CSM,\nand infer an envelope mass $M_{\\rm e} \\sim 0.1\\,\\rm M_\\odot$ and radius $R_{\\rm\ne} \\sim 4 \\times 10^{13}$ cm. EP250108a/SN 2025kg's multi-wavelength properties\nmake it a close analog to LLGRB 060218/SN 2006aj, and highlight the power of\nearly follow-up observations in mapping the environments of massive stars prior\nto core collapse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present optical, radio, and X-ray observations of EP250108a/SN 2025kg, a\nbroad-line Type Ic supernova (SN Ic-BL) accompanying an Einstein Probe (EP)\nfast X-ray transient (FXT) at $z=0.176$. EP250108a/SN 2025kg possesses a\ndouble-peaked optical light curve and its spectrum transitions from a blue\nunderlying continuum to a typical SN Ic-BL spectrum over time. We fit a\nradioactive decay model to the second peak of the optical light curve and find\nSN parameters that are consistent with the SNe Ic-BL population, while its\nX-ray and radio properties are consistent with those of low-luminosity GRB\n(LLGRB) 060218/SN 2006aj. We explore three scenarios to understand the system's\nmulti-wavelength emission -- (a) SN ejecta interacting with an extended\ncircumstellar medium (CSM), (b) the shocked cocoon of a collapsar-driven jet\nchoked in its stellar envelope, and (c) the shocked cocoon of a\ncollapsar-driven jet choked in an extended CSM. All three models can explain\nthe optical light curve and are also consistent with the radio and X-ray\nobservations. We favor models (a) and (c) because they self-consistently\nexplain both the X-ray prompt emission and first optical peak, but we do not\nrule out model (b). From the properties of the first peak in models (a) and\n(c), we find evidence that EP250108a/SN 2025kg interacts with an extended CSM,\nand infer an envelope mass $M_{\\rm e} \\sim 0.1\\,\\rm M_\\odot$ and radius $R_{\\rm\ne} \\sim 4 \\times 10^{13}$ cm. EP250108a/SN 2025kg's multi-wavelength properties\nmake it a close analog to LLGRB 060218/SN 2006aj, and highlight the power of\nearly follow-up observations in mapping the environments of massive stars prior\nto core collapse."
                },
                "authors": [
                    {
                        "name": "Gokul P. Srinivasaragavan"
                    },
                    {
                        "name": "Hamid Hamidani"
                    },
                    {
                        "name": "Genevieve Schroeder"
                    },
                    {
                        "name": "Nikhil Sarin"
                    },
                    {
                        "name": "Anna Y. Q. Ho"
                    },
                    {
                        "name": "Anthony L. Piro"
                    },
                    {
                        "name": "S. Bradley Cenko"
                    },
                    {
                        "name": "Shreya Anand"
                    },
                    {
                        "name": "Jesper Sollerman"
                    },
                    {
                        "name": "Daniel A. Perley"
                    },
                    {
                        "name": "Keiichi Maeda"
                    },
                    {
                        "name": "Brendan O'Connor"
                    },
                    {
                        "name": "Hanindyo Kuncarayakti"
                    },
                    {
                        "name": "M. Coleman Miller"
                    },
                    {
                        "name": "Tomás Ahumada"
                    },
                    {
                        "name": "Jada Vail"
                    },
                    {
                        "name": "Paul Duffell"
                    },
                    {
                        "name": "Ranadeep Ghosh Dastidar"
                    },
                    {
                        "name": "Igor Andreoni"
                    },
                    {
                        "name": "Aleksandra Bochenek"
                    },
                    {
                        "name": "Seán J. Brennan"
                    },
                    {
                        "name": "Jonathan Carney"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "James Freeburn"
                    },
                    {
                        "name": "Avishay Gal-Yam"
                    },
                    {
                        "name": "Wynn Jacobson-Galán"
                    },
                    {
                        "name": "Mansi M. Kasliwal"
                    },
                    {
                        "name": "Jiaxuan Li"
                    },
                    {
                        "name": "Maggie L. Li"
                    },
                    {
                        "name": "Niharika Sravan"
                    },
                    {
                        "name": "Daniel E. Warshofsky"
                    }
                ],
                "author_detail": {
                    "name": "Daniel E. Warshofsky"
                },
                "author": "Daniel E. Warshofsky",
                "arxiv_comment": "21 pages, 11 Figures, Submitted to ApJ Letters, Comments Welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17510v1",
                "updated": "2025-04-24T12:54:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    54,
                    30,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T12:54:30Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    54,
                    30,
                    3,
                    114,
                    0
                ],
                "title": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based\n  Open Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe to Stay: Psychological Safety Sustains Participation in Pull-based\n  Open Source Projects"
                },
                "summary": "Psychological safety is the belief that team members can speak up or make\nmistakes without fear of negative consequences. While it is recognized as\nimportant in traditional software teams, its role in open-source development\nremains understudied. Yet, open-source contributors often collaborate without\nformal roles or structures, where interpersonal relationship can make or break\nparticipation. In this study, we examine whether team-level psychological\nsafety, inferred from code review activities, is associated with contributors'\ncontinued participation in open-source projects. Code review is a central and\ncollaborative activity in modern software development, which offers a rich\ncontext for observing team interactions. Based on 60,684 pull requests, we\nconstruct a psychological safety index using cues such as merge decisions,\ncomment activity, interaction diversity, and mentions. We analyze its\nrelationship with contributors' short-term (after 1 year) and long-term (after\n4-5 years) sustained participation using three logistic regression models. Our\nfindings show that contributors are more likely to remain active in\nrepositories with higher levels of psychological safety. Psychological safety\nis positively associated with both short-term and future sustained\nparticipation. However, when prior participation is included, it becomes the\nstronger predictor of future sustained participation, while the effect of\npsychological safety becomes smaller. This study introduces a scalable approach\nto study psychological safety through pull request data and provides new\nevidence that it matters in open-source development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psychological safety is the belief that team members can speak up or make\nmistakes without fear of negative consequences. While it is recognized as\nimportant in traditional software teams, its role in open-source development\nremains understudied. Yet, open-source contributors often collaborate without\nformal roles or structures, where interpersonal relationship can make or break\nparticipation. In this study, we examine whether team-level psychological\nsafety, inferred from code review activities, is associated with contributors'\ncontinued participation in open-source projects. Code review is a central and\ncollaborative activity in modern software development, which offers a rich\ncontext for observing team interactions. Based on 60,684 pull requests, we\nconstruct a psychological safety index using cues such as merge decisions,\ncomment activity, interaction diversity, and mentions. We analyze its\nrelationship with contributors' short-term (after 1 year) and long-term (after\n4-5 years) sustained participation using three logistic regression models. Our\nfindings show that contributors are more likely to remain active in\nrepositories with higher levels of psychological safety. Psychological safety\nis positively associated with both short-term and future sustained\nparticipation. However, when prior participation is included, it becomes the\nstronger predictor of future sustained participation, while the effect of\npsychological safety becomes smaller. This study introduces a scalable approach\nto study psychological safety through pull request data and provides new\nevidence that it matters in open-source development."
                },
                "authors": [
                    {
                        "name": "Emeralda Sesari"
                    },
                    {
                        "name": "Federica Sarro"
                    },
                    {
                        "name": "Ayushi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Ayushi Rastogi"
                },
                "author": "Ayushi Rastogi",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05346v2",
                "updated": "2025-04-24T12:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    48,
                    52,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-07T21:32:32Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    21,
                    32,
                    32,
                    4,
                    38,
                    0
                ],
                "title": "Probabilistic Subspace Manifolds for Contextual Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Subspace Manifolds for Contextual Inference in Large\n  Language Models"
                },
                "summary": "Representing token embeddings as probability distributions over learned\nmanifolds allows for more flexible contextual inference, reducing\nrepresentational rigidity while enhancing semantic granularity. Comparative\nevaluations demonstrate that probabilistic embeddings improve neighborhood\nconsistency and decrease redundancy, ensuring that token relationships remain\nmore structurally coherent across fine-tuning iterations. The integration of\nprobabilistic subspaces within attention mechanisms facilitates more adaptive\ncontextual weighting, enabling models to capture latent dependencies that would\notherwise be obscured in conventional embeddings. Experimental results\nhighlight increased robustness against adversarial modifications, with\nprobabilistic embeddings preserving contextual integrity even under\nperturbation-based evaluation scenarios. Performance assessments indicate that\nprobabilistic representations achieve greater adaptability in domain-specific\napplications, mitigating the need for extensive retraining when shifting across\nlinguistic domains. Computational trade-offs remain within operationally\nfeasible limits, with marginal increases in inference latency balanced against\nthe benefits of enhanced representation stability and contextual\nexpressiveness. The capacity to encode structured uncertainty provides\nadvantages in generative modeling tasks, particularly where maintaining\ncoherence across extended sequences requires a representation framework capable\nof handling ambiguous or context-dependent linguistic constructs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representing token embeddings as probability distributions over learned\nmanifolds allows for more flexible contextual inference, reducing\nrepresentational rigidity while enhancing semantic granularity. Comparative\nevaluations demonstrate that probabilistic embeddings improve neighborhood\nconsistency and decrease redundancy, ensuring that token relationships remain\nmore structurally coherent across fine-tuning iterations. The integration of\nprobabilistic subspaces within attention mechanisms facilitates more adaptive\ncontextual weighting, enabling models to capture latent dependencies that would\notherwise be obscured in conventional embeddings. Experimental results\nhighlight increased robustness against adversarial modifications, with\nprobabilistic embeddings preserving contextual integrity even under\nperturbation-based evaluation scenarios. Performance assessments indicate that\nprobabilistic representations achieve greater adaptability in domain-specific\napplications, mitigating the need for extensive retraining when shifting across\nlinguistic domains. Computational trade-offs remain within operationally\nfeasible limits, with marginal increases in inference latency balanced against\nthe benefits of enhanced representation stability and contextual\nexpressiveness. The capacity to encode structured uncertainty provides\nadvantages in generative modeling tasks, particularly where maintaining\ncoherence across extended sequences requires a representation framework capable\nof handling ambiguous or context-dependent linguistic constructs."
                },
                "authors": [
                    {
                        "name": "Christopher Nightingale"
                    },
                    {
                        "name": "Dominic Lavington"
                    },
                    {
                        "name": "Jonathan Thistlethwaite"
                    },
                    {
                        "name": "Sebastian Penhaligon"
                    },
                    {
                        "name": "Thomas Belinski"
                    },
                    {
                        "name": "David Boldo"
                    }
                ],
                "author_detail": {
                    "name": "David Boldo"
                },
                "author": "David Boldo",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09818v2",
                "updated": "2025-04-24T12:46:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    46,
                    5,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-14T02:39:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    2,
                    39,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "Transferable text data distillation by trajectory matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable text data distillation by trajectory matching"
                },
                "summary": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama)."
                },
                "authors": [
                    {
                        "name": "Rong Yao"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yifei Fu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Fanyi Du"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15909v2",
                "updated": "2025-04-24T12:39:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    39,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-22T13:55:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    55,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "Synergizing RAG and Reasoning: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing RAG and Reasoning: A Systematic Review"
                },
                "summary": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yunfan Gao"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Yijie Zhong"
                    },
                    {
                        "name": "Yuxi Bi"
                    },
                    {
                        "name": "Ming Xue"
                    },
                    {
                        "name": "Haofen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haofen Wang"
                },
                "author": "Haofen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17497v1",
                "updated": "2025-04-24T12:38:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    38,
                    3,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T12:38:03Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    38,
                    3,
                    3,
                    114,
                    0
                ],
                "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or\n  Enhanced Virtual Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining GCN Structural Learning with LLM Chemical Knowledge for or\n  Enhanced Virtual Screening"
                },
                "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines."
                },
                "authors": [
                    {
                        "name": "Radia Berreziga"
                    },
                    {
                        "name": "Mohammed Brahimi"
                    },
                    {
                        "name": "Khairedine Kraim"
                    },
                    {
                        "name": "Hamid Azzoune"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Azzoune"
                },
                "author": "Hamid Azzoune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17480v1",
                "updated": "2025-04-24T12:15:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    15,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T12:15:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    15,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation"
                },
                "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."
                },
                "authors": [
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Shunfan Zhengc"
                    },
                    {
                        "name": "Linlin Wanga"
                    },
                    {
                        "name": "Xiaoling Wang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11496v2",
                "updated": "2025-04-24T11:47:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    47,
                    44,
                    3,
                    114,
                    0
                ],
                "published": "2024-12-16T07:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    16,
                    41,
                    0,
                    351,
                    0
                ],
                "title": "Capacity of Hierarchical Secure Coded Gradient Aggregation with\n  Straggling Communication Links",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capacity of Hierarchical Secure Coded Gradient Aggregation with\n  Straggling Communication Links"
                },
                "summary": "The growing privacy concerns in distributed learning have led to the\nwidespread adoption of secure aggregation techniques in distributed machine\nlearning systems, such as federated learning. Motivated by a coded gradient\naggregation problem in a user-helper-master hierarchical network setting with\nstraggling communication links, we formulate a new secure hierarchical coded\ngradient aggregation problem. In our setting, \\( K \\) users communicate with\nthe master through an intermediate layer of \\( N \\) helpers, who can\ncommunicate with each other. With a resiliency threshold of \\( N_r \\) for\nstraggling communication links, and at most \\( T \\) colluding helpers and any\nnumber of colluding users, the master aims to recover the sum of all users'\ngradients while remaining unaware of any individual gradient that exceeds the\nexpected sum. In addition, helpers cannot infer more about users' gradients\nthan what is already known by the colluding users. We propose an achievable\nscheme where users' upload messages are based on a globally known Vandermonde\nmatrix, and helper communication is facilitated using an extended Vandermonde\nmatrix with special structural properties. A matching converse bound is also\nderived, establishing the optimal result for this hierarchical coded gradient\naggregation problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing privacy concerns in distributed learning have led to the\nwidespread adoption of secure aggregation techniques in distributed machine\nlearning systems, such as federated learning. Motivated by a coded gradient\naggregation problem in a user-helper-master hierarchical network setting with\nstraggling communication links, we formulate a new secure hierarchical coded\ngradient aggregation problem. In our setting, \\( K \\) users communicate with\nthe master through an intermediate layer of \\( N \\) helpers, who can\ncommunicate with each other. With a resiliency threshold of \\( N_r \\) for\nstraggling communication links, and at most \\( T \\) colluding helpers and any\nnumber of colluding users, the master aims to recover the sum of all users'\ngradients while remaining unaware of any individual gradient that exceeds the\nexpected sum. In addition, helpers cannot infer more about users' gradients\nthan what is already known by the colluding users. We propose an achievable\nscheme where users' upload messages are based on a globally known Vandermonde\nmatrix, and helper communication is facilitated using an extended Vandermonde\nmatrix with special structural properties. A matching converse bound is also\nderived, establishing the optimal result for this hierarchical coded gradient\naggregation problem."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Nan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Liu"
                },
                "author": "Nan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17454v1",
                "updated": "2025-04-24T11:35:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    35,
                    43,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    35,
                    43,
                    3,
                    114,
                    0
                ],
                "title": "Adaptive Orchestration of Modular Generative Information Access Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Orchestration of Modular Generative Information Access Systems"
                },
                "summary": "Advancements in large language models (LLMs) have driven the emergence of\ncomplex new systems to provide access to information, that we will collectively\nrefer to as modular generative information access (GenIA) systems. They\nintegrate a broad and evolving range of specialized components, including LLMs,\nretrieval models, and a heterogeneous set of sources and tools. While\nmodularity offers flexibility, it also raises critical challenges: How can we\nsystematically characterize the space of possible modules and their\ninteractions? How can we automate and optimize interactions among these\nheterogeneous components? And, how do we enable this modular system to\ndynamically adapt to varying user query requirements and evolving module\ncapabilities? In this perspective paper, we argue that the architecture of\nfuture modular generative information access systems will not just assemble\npowerful components, but enable a self-organizing system through real-time\nadaptive orchestration -- where components' interactions are dynamically\nconfigured for each user input, maximizing information relevance while\nminimizing computational overhead. We give provisional answers to the questions\nraised above with a roadmap that depicts the key principles and methods for\ndesigning such an adaptive modular system. We identify pressing challenges, and\npropose avenues for addressing them in the years ahead. This perspective urges\nthe IR community to rethink modular system designs for developing adaptive,\nself-optimizing, and future-ready architectures that evolve alongside their\nrapidly advancing underlying technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have driven the emergence of\ncomplex new systems to provide access to information, that we will collectively\nrefer to as modular generative information access (GenIA) systems. They\nintegrate a broad and evolving range of specialized components, including LLMs,\nretrieval models, and a heterogeneous set of sources and tools. While\nmodularity offers flexibility, it also raises critical challenges: How can we\nsystematically characterize the space of possible modules and their\ninteractions? How can we automate and optimize interactions among these\nheterogeneous components? And, how do we enable this modular system to\ndynamically adapt to varying user query requirements and evolving module\ncapabilities? In this perspective paper, we argue that the architecture of\nfuture modular generative information access systems will not just assemble\npowerful components, but enable a self-organizing system through real-time\nadaptive orchestration -- where components' interactions are dynamically\nconfigured for each user input, maximizing information relevance while\nminimizing computational overhead. We give provisional answers to the questions\nraised above with a roadmap that depicts the key principles and methods for\ndesigning such an adaptive modular system. We identify pressing challenges, and\npropose avenues for addressing them in the years ahead. This perspective urges\nthe IR community to rethink modular system designs for developing adaptive,\nself-optimizing, and future-ready architectures that evolve alongside their\nrapidly advancing underlying technologies."
                },
                "authors": [
                    {
                        "name": "Mohanna Hoveyda"
                    },
                    {
                        "name": "Harrie Oosterhuis"
                    },
                    {
                        "name": "Arjen P. de Vries"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    }
                ],
                "author_detail": {
                    "name": "Faegheh Hasibi"
                },
                "author": "Faegheh Hasibi",
                "arxiv_doi": "10.1145/3726302.3730351",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730351",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.17454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGIR 2025 Perspective Paper Track",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17449v1",
                "updated": "2025-04-24T11:28:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    28,
                    40,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:28:40Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    28,
                    40,
                    3,
                    114,
                    0
                ],
                "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant\n  Inference in Pretrained Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant\n  Inference in Pretrained Language Models"
                },
                "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Lidan Shou"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Guiming Xie"
                    },
                    {
                        "name": "Xuejian Gong"
                    }
                ],
                "author_detail": {
                    "name": "Xuejian Gong"
                },
                "author": "Xuejian Gong",
                "arxiv_comment": "Accepted by VLDBJ 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17448v1",
                "updated": "2025-04-24T11:28:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    28,
                    0,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:28:00Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    28,
                    0,
                    3,
                    114,
                    0
                ],
                "title": "CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated\n  Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated\n  Active Learning"
                },
                "summary": "Active learning (AL) reduces human annotation costs for machine learning\nsystems by strategically selecting the most informative unlabeled data for\nannotation, but performing it individually may still be insufficient due to\nrestricted data diversity and annotation budget. Federated Active Learning\n(FAL) addresses this by facilitating collaborative data selection and model\ntraining, while preserving the confidentiality of raw data samples. Yet,\nexisting FAL methods fail to account for the heterogeneity of data distribution\nacross clients and the associated fluctuations in global and local model\nparameters, adversely affecting model accuracy. To overcome these challenges,\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\nhigh epistemic variations (EVs), which notably oscillate around the decision\nboundaries during training. To achieve both effectiveness and efficiency,\n\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\ninconsistencies across training epochs, 2) calibrating decision boundaries of\ninaccurate models with a new alignment loss, and 3) enhancing data selection\nefficiency via a data freeze and awaken mechanism with subset sampling.\nExperiments show that CHASe surpasses various established baselines in terms of\neffectiveness and efficiency, validated across diverse datasets, model\ncomplexities, and heterogeneous federation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active learning (AL) reduces human annotation costs for machine learning\nsystems by strategically selecting the most informative unlabeled data for\nannotation, but performing it individually may still be insufficient due to\nrestricted data diversity and annotation budget. Federated Active Learning\n(FAL) addresses this by facilitating collaborative data selection and model\ntraining, while preserving the confidentiality of raw data samples. Yet,\nexisting FAL methods fail to account for the heterogeneity of data distribution\nacross clients and the associated fluctuations in global and local model\nparameters, adversely affecting model accuracy. To overcome these challenges,\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\nhigh epistemic variations (EVs), which notably oscillate around the decision\nboundaries during training. To achieve both effectiveness and efficiency,\n\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\ninconsistencies across training epochs, 2) calibrating decision boundaries of\ninaccurate models with a new alignment loss, and 3) enhancing data selection\nefficiency via a data freeze and awaken mechanism with subset sampling.\nExperiments show that CHASe surpasses various established baselines in terms of\neffectiveness and efficiency, validated across diverse datasets, model\ncomplexities, and heterogeneous federation settings."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "arxiv_comment": "Accepted by TKDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17447v1",
                "updated": "2025-04-24T11:19:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    19,
                    18,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:19:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    19,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAG: Frame Selection Augmented Generation for Long Video and Long\n  Document Understanding"
                },
                "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG"
                },
                "authors": [
                    {
                        "name": "De-An Huang"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Jan Kautz"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kautz"
                },
                "author": "Jan Kautz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17445v1",
                "updated": "2025-04-24T11:14:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    14,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:14:13Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    14,
                    13,
                    3,
                    114,
                    0
                ],
                "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text\n  Augmentation"
                },
                "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance."
                },
                "authors": [
                    {
                        "name": "Anna Lieb"
                    },
                    {
                        "name": "Maneesh Arora"
                    },
                    {
                        "name": "Eni Mustafaraj"
                    }
                ],
                "author_detail": {
                    "name": "Eni Mustafaraj"
                },
                "author": "Eni Mustafaraj",
                "arxiv_comment": "Presented at IC2S2 2024 in Philadelphia, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03624v2",
                "updated": "2025-04-24T11:05:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    5,
                    58,
                    3,
                    114,
                    0
                ],
                "published": "2024-08-07T08:34:48Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    8,
                    34,
                    48,
                    2,
                    220,
                    0
                ],
                "title": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging"
                },
                "summary": "Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios."
                },
                "authors": [
                    {
                        "name": "Senkang Hu"
                    },
                    {
                        "name": "Zhengru Fang"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Sam Kwong"
                    }
                ],
                "author_detail": {
                    "name": "Sam Kwong"
                },
                "author": "Sam Kwong",
                "arxiv_comment": "Accepted by IEEE Transactions on Mobile Computing (TMC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11539v2",
                "updated": "2025-04-24T10:58:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    58,
                    6,
                    3,
                    114,
                    0
                ],
                "published": "2024-10-15T12:14:01Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    1,
                    1,
                    289,
                    0
                ],
                "title": "Transfer Learning with Foundational Models for Time Series Forecasting\n  using Low-Rank Adaptations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning with Foundational Models for Time Series Forecasting\n  using Low-Rank Adaptations"
                },
                "summary": "Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI."
                },
                "authors": [
                    {
                        "name": "M. Germán-Morales"
                    },
                    {
                        "name": "A. J. Rivera-Rivas"
                    },
                    {
                        "name": "M. J. del Jesus Díaz"
                    },
                    {
                        "name": "C. J. Carmona"
                    }
                ],
                "author_detail": {
                    "name": "C. J. Carmona"
                },
                "author": "C. J. Carmona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17432v1",
                "updated": "2025-04-24T10:51:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    51,
                    52,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:51:52Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    51,
                    52,
                    3,
                    114,
                    0
                ],
                "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs"
                },
                "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities."
                },
                "authors": [
                    {
                        "name": "Tiancheng Gu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Weidong Cai"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "13 pages, 8 figures, Project page: https://garygutc.github.io/UniME",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15975v2",
                "updated": "2025-04-24T10:39:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    39,
                    43,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-22T15:23:37Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    23,
                    37,
                    1,
                    112,
                    0
                ],
                "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition"
                },
                "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."
                },
                "authors": [
                    {
                        "name": "Peter Fletcher"
                    }
                ],
                "author_detail": {
                    "name": "Peter Fletcher"
                },
                "author": "Peter Fletcher",
                "arxiv_comment": "64 pages, 23 figures. Version 2: mathematical supplement added, 98\n  pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.2; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17427v1",
                "updated": "2025-04-24T10:33:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    33,
                    26,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:33:26Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    33,
                    26,
                    3,
                    114,
                    0
                ],
                "title": "Beyond Whole Dialogue Modeling: Contextual Disentanglement for\n  Conversational Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Whole Dialogue Modeling: Contextual Disentanglement for\n  Conversational Recommendation"
                },
                "summary": "Conversational recommender systems aim to provide personalized\nrecommendations by analyzing and utilizing contextual information related to\ndialogue. However, existing methods typically model the dialogue context as a\nwhole, neglecting the inherent complexity and entanglement within the dialogue.\nSpecifically, a dialogue comprises both focus information and background\ninformation, which mutually influence each other. Current methods tend to model\nthese two types of information mixedly, leading to misinterpretation of users'\nactual needs, thereby lowering the accuracy of recommendations. To address this\nissue, this paper proposes a novel model to introduce contextual\ndisentanglement for improving conversational recommender systems, named\nDisenCRS. The proposed model DisenCRS employs a dual disentanglement framework,\nincluding self-supervised contrastive disentanglement and counterfactual\ninference disentanglement, to effectively distinguish focus information and\nbackground information from the dialogue context under unsupervised conditions.\nMoreover, we design an adaptive prompt learning module to automatically select\nthe most suitable prompt based on the specific dialogue context, fully\nleveraging the power of large language models. Experimental results on two\nwidely used public datasets demonstrate that DisenCRS significantly outperforms\nexisting conversational recommendation models, achieving superior performance\non both item recommendation and response generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational recommender systems aim to provide personalized\nrecommendations by analyzing and utilizing contextual information related to\ndialogue. However, existing methods typically model the dialogue context as a\nwhole, neglecting the inherent complexity and entanglement within the dialogue.\nSpecifically, a dialogue comprises both focus information and background\ninformation, which mutually influence each other. Current methods tend to model\nthese two types of information mixedly, leading to misinterpretation of users'\nactual needs, thereby lowering the accuracy of recommendations. To address this\nissue, this paper proposes a novel model to introduce contextual\ndisentanglement for improving conversational recommender systems, named\nDisenCRS. The proposed model DisenCRS employs a dual disentanglement framework,\nincluding self-supervised contrastive disentanglement and counterfactual\ninference disentanglement, to effectively distinguish focus information and\nbackground information from the dialogue context under unsupervised conditions.\nMoreover, we design an adaptive prompt learning module to automatically select\nthe most suitable prompt based on the specific dialogue context, fully\nleveraging the power of large language models. Experimental results on two\nwidely used public datasets demonstrate that DisenCRS significantly outperforms\nexisting conversational recommendation models, achieving superior performance\non both item recommendation and response generation tasks."
                },
                "authors": [
                    {
                        "name": "Guojia An"
                    },
                    {
                        "name": "Jie Zou"
                    },
                    {
                        "name": "Jiwei Wei"
                    },
                    {
                        "name": "Chaoning Zhang"
                    },
                    {
                        "name": "Fuming Sun"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17426v1",
                "updated": "2025-04-24T10:30:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    30,
                    40,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:30:40Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    30,
                    40,
                    3,
                    114,
                    0
                ],
                "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in\n  Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Leveraging Large Language Model Summaries for Topic Modeling in\n  Source Code"
                },
                "summary": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories."
                },
                "authors": [
                    {
                        "name": "Michele Carissimi"
                    },
                    {
                        "name": "Martina Saletta"
                    },
                    {
                        "name": "Claudio Ferretti"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Ferretti"
                },
                "author": "Claudio Ferretti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17425v1",
                "updated": "2025-04-24T10:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    26,
                    47,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    26,
                    47,
                    3,
                    114,
                    0
                ],
                "title": "MeerKAT discovers a jet-driven bow shock near GRS 1915+105. How an\n  invisible large-scale jet sculpts a microquasar's environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeerKAT discovers a jet-driven bow shock near GRS 1915+105. How an\n  invisible large-scale jet sculpts a microquasar's environment"
                },
                "summary": "Black holes, both supermassive and stellar-mass, impact the evolution of\ntheir surroundings on a large range of scales. While the role of supermassive\nblack holes is well studied, the effects of stellar-mass black holes on their\nsurroundings, particularly in inducing structures in the interstellar medium\n(ISM), remain under explored.\n  This study focuses on the black hole X-ray binary GRS 1915+105, renowned for\nits active jets, and the primary aim is to unveil and characterise the impact\nof GRS 1915+105 on its environment by identifying structures induced by jet-ISM\ninteraction. Methods: We observed GRS 1915+105 with MeerKAT for a total\nexposure time of 14~hr, and we obtained the deepest image of GRS 1915+105 to\ndate. Using a previously proposed self-similar model for large-scale jets, we\ninferred the properties of both the jets and the ISM, providing insights into\nthe jet-ISM interaction site.\n  Our observations revealed a bow shock structure near GRS 1915+105, likely\ninduced by a jet interacting with the ISM and blowing an overpressured cavity\nin the medium. We constrained the ISM density to 100--160 particles\\,cm$^{-3}$\nwhile assuming a temperature range of 10$^4$--10$^6$\\,K, which implies a bow\nshock expansion velocity of $20\\,{\\rm km\\,s}^{-1}<\\dot{L} <\\,360\\,{\\rm\nkm\\,s}^{-1}$. We estimate that the jet responsible for the formation of the bow\nshock has an age between 0.09 and 0.22 Myr, and the time-averaged energy rate\nConclusions: Our results confirm that in stellar-mass black holes, the energy\ndissipated through jets can be comparable to the accretion energy, and through\nthe interaction of the jet with the ISM, such energy is transferred back to the\nenvironment.\n  This feedback mechanism mirrors the powerful influence of supermassive black\nholes on their environments, underscoring the significant role a black hole's\nactivity has in shaping its surroundings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black holes, both supermassive and stellar-mass, impact the evolution of\ntheir surroundings on a large range of scales. While the role of supermassive\nblack holes is well studied, the effects of stellar-mass black holes on their\nsurroundings, particularly in inducing structures in the interstellar medium\n(ISM), remain under explored.\n  This study focuses on the black hole X-ray binary GRS 1915+105, renowned for\nits active jets, and the primary aim is to unveil and characterise the impact\nof GRS 1915+105 on its environment by identifying structures induced by jet-ISM\ninteraction. Methods: We observed GRS 1915+105 with MeerKAT for a total\nexposure time of 14~hr, and we obtained the deepest image of GRS 1915+105 to\ndate. Using a previously proposed self-similar model for large-scale jets, we\ninferred the properties of both the jets and the ISM, providing insights into\nthe jet-ISM interaction site.\n  Our observations revealed a bow shock structure near GRS 1915+105, likely\ninduced by a jet interacting with the ISM and blowing an overpressured cavity\nin the medium. We constrained the ISM density to 100--160 particles\\,cm$^{-3}$\nwhile assuming a temperature range of 10$^4$--10$^6$\\,K, which implies a bow\nshock expansion velocity of $20\\,{\\rm km\\,s}^{-1}<\\dot{L} <\\,360\\,{\\rm\nkm\\,s}^{-1}$. We estimate that the jet responsible for the formation of the bow\nshock has an age between 0.09 and 0.22 Myr, and the time-averaged energy rate\nConclusions: Our results confirm that in stellar-mass black holes, the energy\ndissipated through jets can be comparable to the accretion energy, and through\nthe interaction of the jet with the ISM, such energy is transferred back to the\nenvironment.\n  This feedback mechanism mirrors the powerful influence of supermassive black\nholes on their environments, underscoring the significant role a black hole's\nactivity has in shaping its surroundings."
                },
                "authors": [
                    {
                        "name": "S. E. Motta"
                    },
                    {
                        "name": "P. Atri"
                    },
                    {
                        "name": "James H. Matthews"
                    },
                    {
                        "name": "Jakob van den Eijnden"
                    },
                    {
                        "name": "Rob P. Fender"
                    },
                    {
                        "name": "James C. A. Miller-Jones"
                    },
                    {
                        "name": "Ian Heywood"
                    },
                    {
                        "name": "Patrick Woudt"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Woudt"
                },
                "author": "Patrick Woudt",
                "arxiv_doi": "10.1051/0004-6361/202452838",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452838",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.17425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 3 figures, 2 tables, accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17421v1",
                "updated": "2025-04-24T10:24:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    24,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:24:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    24,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Towards Harnessing the Collaborative Power of Large and Small Models for\n  Domain Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Harnessing the Collaborative Power of Large and Small Models for\n  Domain Tasks"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bingjie Yan"
                    },
                    {
                        "name": "Tianyuan Zou"
                    },
                    {
                        "name": "Jianqing Zhang"
                    },
                    {
                        "name": "Zixuan Gu"
                    },
                    {
                        "name": "Jianbing Ding"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xiaozhou Ye"
                    },
                    {
                        "name": "Ye Ouyang"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ya-Qin Zhang"
                },
                "author": "Ya-Qin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14423v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14423v3",
                "updated": "2025-04-24T10:16:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    16,
                    44,
                    3,
                    114,
                    0
                ],
                "published": "2024-11-21T18:55:23Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    55,
                    23,
                    3,
                    326,
                    0
                ],
                "title": "PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and\n  Video Diffusion for 4D Dynamic Physical Scene Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and\n  Video Diffusion for 4D Dynamic Physical Scene Simulation"
                },
                "summary": "Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce PhysFlow, a\nnovel approach that leverages multi-modal foundation models and video diffusion\nto achieve enhanced 4D dynamic scene simulation. Our method utilizes\nmulti-modal models to identify material types and initialize material\nparameters through image queries, while simultaneously inferring 3D Gaussian\nsplats for detailed scene representation. We further refine these material\nparameters using video diffusion with a differentiable Material Point Method\n(MPM) and optical flow guidance rather than render loss or Score Distillation\nSampling (SDS) loss. This integrated framework enables accurate prediction and\nrealistic simulation of dynamic interactions in real-world scenarios, advancing\nboth accuracy and flexibility in physics-based simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce PhysFlow, a\nnovel approach that leverages multi-modal foundation models and video diffusion\nto achieve enhanced 4D dynamic scene simulation. Our method utilizes\nmulti-modal models to identify material types and initialize material\nparameters through image queries, while simultaneously inferring 3D Gaussian\nsplats for detailed scene representation. We further refine these material\nparameters using video diffusion with a differentiable Material Point Method\n(MPM) and optical flow guidance rather than render loss or Score Distillation\nSampling (SDS) loss. This integrated framework enables accurate prediction and\nrealistic simulation of dynamic interactions in real-world scenarios, advancing\nboth accuracy and flexibility in physics-based simulations."
                },
                "authors": [
                    {
                        "name": "Zhuoman Liu"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Yan Luximon"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhang"
                },
                "author": "Di Zhang",
                "arxiv_comment": "CVPR 2025. Homepage: https://zhuomanliu.github.io/PhysFlow/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14423v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14423v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11658v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11658v3",
                "updated": "2025-04-24T10:05:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    5,
                    23,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-17T10:49:23Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    49,
                    23,
                    0,
                    48,
                    0
                ],
                "title": "\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks\n  by digital natives about location data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks\n  by digital natives about location data"
                },
                "summary": "Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach."
                },
                "authors": [
                    {
                        "name": "Antoine Boutet"
                    },
                    {
                        "name": "Victor Morel"
                    }
                ],
                "author_detail": {
                    "name": "Victor Morel"
                },
                "author": "Victor Morel",
                "arxiv_comment": "Accepted for publication at ICWSM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11658v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17404v1",
                "updated": "2025-04-24T09:53:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    53,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:53:49Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    53,
                    49,
                    3,
                    114,
                    0
                ],
                "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI\n  Co-Alignment to Sustainable Symbiotic Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI\n  Co-Alignment to Sustainable Symbiotic Society"
                },
                "summary": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology."
                },
                "authors": [
                    {
                        "name": "Feifei Zhao"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Enmeng Lu"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Haibo Tong"
                    },
                    {
                        "name": "Yao Liang"
                    },
                    {
                        "name": "Dongqi Liang"
                    },
                    {
                        "name": "Kang Sun"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17403v1",
                "updated": "2025-04-24T09:49:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    49,
                    18,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:49:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    49,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "Coding for Computation: Efficient Compression of Neural Networks for\n  Reconfigurable Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding for Computation: Efficient Compression of Neural Networks for\n  Reconfigurable Hardware"
                },
                "summary": "As state of the art neural networks (NNs) continue to grow in size, their\nresource-efficient implementation becomes ever more important. In this paper,\nwe introduce a compression scheme that reduces the number of computations\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\nachieved by combining pruning via regularized training, weight sharing and\nlinear computation coding (LCC). Contrary to common NN compression techniques,\nwhere the objective is to reduce the memory used for storing the weights of the\nNNs, our approach is optimized to reduce the number of additions required for\ninference in a hardware-friendly manner. The proposed scheme achieves\ncompetitive performance for simple multilayer perceptrons, as well as for large\nscale deep NNs such as ResNet-34.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As state of the art neural networks (NNs) continue to grow in size, their\nresource-efficient implementation becomes ever more important. In this paper,\nwe introduce a compression scheme that reduces the number of computations\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\nachieved by combining pruning via regularized training, weight sharing and\nlinear computation coding (LCC). Contrary to common NN compression techniques,\nwhere the objective is to reduce the memory used for storing the weights of the\nNNs, our approach is optimized to reduce the number of additions required for\ninference in a hardware-friendly manner. The proposed scheme achieves\ncompetitive performance for simple multilayer perceptrons, as well as for large\nscale deep NNs such as ResNet-34."
                },
                "authors": [
                    {
                        "name": "Hans Rosenberger"
                    },
                    {
                        "name": "Rodrigo Fischer"
                    },
                    {
                        "name": "Johanna S. Fröhlich"
                    },
                    {
                        "name": "Ali Bereyhi"
                    },
                    {
                        "name": "Ralf R. Müller"
                    }
                ],
                "author_detail": {
                    "name": "Ralf R. Müller"
                },
                "author": "Ralf R. Müller",
                "arxiv_comment": "Accepted at the 2025 IEEE Statistical Signal Processing (SSP)\n  Workshop, Edinburgh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17402v1",
                "updated": "2025-04-24T09:47:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    47,
                    14,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:47:14Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    47,
                    14,
                    3,
                    114,
                    0
                ],
                "title": "Assessing the Capability of Large Language Models for Domain-Specific\n  Ontology Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Capability of Large Language Models for Domain-Specific\n  Ontology Generation"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques."
                },
                "authors": [
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Mohammad Javad Saeedizade"
                    },
                    {
                        "name": "Robin Keskisarkka"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Eva Blomqvist"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17401v1",
                "updated": "2025-04-24T09:46:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    46,
                    15,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:46:15Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    46,
                    15,
                    3,
                    114,
                    0
                ],
                "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity\n  Estimation via Long-range Spatial Dependencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity\n  Estimation via Long-range Spatial Dependencies"
                },
                "summary": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Jialang Xu"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Baoru Huang"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Evangelos B. Mazomenos"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos B. Mazomenos"
                },
                "author": "Evangelos B. Mazomenos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19151v2",
                "updated": "2025-04-24T09:40:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    40,
                    21,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-27T21:27:32Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    21,
                    27,
                    32,
                    4,
                    271,
                    0
                ],
                "title": "Can LLMs Really Learn to Translate a Low-Resource Language from One\n  Grammar Book?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Really Learn to Translate a Low-Resource Language from One\n  Grammar Book?"
                },
                "summary": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests that prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an XLR language unseen by LLMs - a noteworthy\ncase of linguistics helping an NLP task. We investigate the source of this\ntranslation ability, finding almost all improvements stem from the book's\nparallel examples rather than its grammatical explanations. We find similar\nresults for Nepali and Guarani, seen low-resource languages, and we achieve\nperformance comparable to an LLM with a grammar book by simply fine-tuning an\nencoder-decoder translation model. We then investigate where grammar books help\nby testing two linguistic tasks, grammaticality judgment and gloss prediction,\nand we explore what kind of grammatical knowledge helps by introducing a\ntypological feature prompt that achieves leading results on these more relevant\ntasks. We thus emphasise the importance of task-appropriate data for XLR\nlanguages: parallel examples for translation, and grammatical data for\nlinguistic tasks. As we find no evidence that long-context LLMs can make\neffective use of grammatical explanations for XLR translation, we conclude data\ncollection for multilingual XLR tasks such as translation is best focused on\nparallel data over linguistic description.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests that prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an XLR language unseen by LLMs - a noteworthy\ncase of linguistics helping an NLP task. We investigate the source of this\ntranslation ability, finding almost all improvements stem from the book's\nparallel examples rather than its grammatical explanations. We find similar\nresults for Nepali and Guarani, seen low-resource languages, and we achieve\nperformance comparable to an LLM with a grammar book by simply fine-tuning an\nencoder-decoder translation model. We then investigate where grammar books help\nby testing two linguistic tasks, grammaticality judgment and gloss prediction,\nand we explore what kind of grammatical knowledge helps by introducing a\ntypological feature prompt that achieves leading results on these more relevant\ntasks. We thus emphasise the importance of task-appropriate data for XLR\nlanguages: parallel examples for translation, and grammatical data for\nlinguistic tasks. As we find no evidence that long-context LLMs can make\neffective use of grammatical explanations for XLR translation, we conclude data\ncollection for multilingual XLR tasks such as translation is best focused on\nparallel data over linguistic description."
                },
                "authors": [
                    {
                        "name": "Seth Aycock"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Khalil Sima'an"
                    }
                ],
                "author_detail": {
                    "name": "Khalil Sima'an"
                },
                "author": "Khalil Sima'an",
                "arxiv_comment": "Accepted at ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17395v1",
                "updated": "2025-04-24T09:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    31,
                    8,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    31,
                    8,
                    3,
                    114,
                    0
                ],
                "title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object\n  Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object\n  Counting"
                },
                "summary": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+."
                },
                "authors": [
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Guorong Li"
                    },
                    {
                        "name": "Laiyun Qing"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Michael Sheng"
                    },
                    {
                        "name": "Yuankai Qi"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17276v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17276v4",
                "updated": "2025-04-24T08:52:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    52,
                    19,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-25T04:45:53Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    4,
                    45,
                    53,
                    1,
                    177,
                    0
                ],
                "title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure"
                },
                "summary": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree."
                },
                "authors": [
                    {
                        "name": "Jikai Wang"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Zi Ye"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Published in TACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17276v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17276v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17376v1",
                "updated": "2025-04-24T08:50:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    50,
                    1,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:50:01Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    50,
                    1,
                    3,
                    114,
                    0
                ],
                "title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and\n  Hardware Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and\n  Hardware Acceleration"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second."
                },
                "authors": [
                    {
                        "name": "Maoyang Xiang"
                    },
                    {
                        "name": "Ramesh Fernando"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11743v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11743v3",
                "updated": "2025-04-24T08:43:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    43,
                    29,
                    3,
                    114,
                    0
                ],
                "published": "2024-03-18T12:55:40Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    12,
                    55,
                    40,
                    0,
                    78,
                    0
                ],
                "title": "PARMESAN: Parameter-Free Memory Search and Transduction for Dense\n  Prediction Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARMESAN: Parameter-Free Memory Search and Transduction for Dense\n  Prediction Tasks"
                },
                "summary": "This work addresses flexibility in deep learning by means of transductive\nreasoning. For adaptation to new data and tasks, e.g., in continual learning,\nexisting methods typically involve tuning learnable parameters or complete\nre-training from scratch, rendering such approaches unflexible in practice. We\nargue that the notion of separating computation from memory by the means of\ntransduction can act as a stepping stone for solving these issues. We therefore\npropose PARMESAN (parameter-free memory search and transduction), a scalable\nmethod which leverages a memory module for solving dense prediction tasks. At\ninference, hidden representations in memory are being searched to find\ncorresponding patterns. In contrast to other methods that rely on continuous\ntraining of learnable parameters, PARMESAN learns via memory consolidation\nsimply by modifying stored contents. Our method is compatible with commonly\nused architectures and canonically transfers to 1D, 2D, and 3D grid-based data.\nThe capabilities of our approach are demonstrated at the complex task of\ncontinual learning. PARMESAN learns by 3-4 orders of magnitude faster than\nestablished baselines while being on par in terms of predictive performance,\nhardware-efficiency, and knowledge retention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses flexibility in deep learning by means of transductive\nreasoning. For adaptation to new data and tasks, e.g., in continual learning,\nexisting methods typically involve tuning learnable parameters or complete\nre-training from scratch, rendering such approaches unflexible in practice. We\nargue that the notion of separating computation from memory by the means of\ntransduction can act as a stepping stone for solving these issues. We therefore\npropose PARMESAN (parameter-free memory search and transduction), a scalable\nmethod which leverages a memory module for solving dense prediction tasks. At\ninference, hidden representations in memory are being searched to find\ncorresponding patterns. In contrast to other methods that rely on continuous\ntraining of learnable parameters, PARMESAN learns via memory consolidation\nsimply by modifying stored contents. Our method is compatible with commonly\nused architectures and canonically transfers to 1D, 2D, and 3D grid-based data.\nThe capabilities of our approach are demonstrated at the complex task of\ncontinual learning. PARMESAN learns by 3-4 orders of magnitude faster than\nestablished baselines while being on par in terms of predictive performance,\nhardware-efficiency, and knowledge retention."
                },
                "authors": [
                    {
                        "name": "Philip Matthias Winter"
                    },
                    {
                        "name": "Maria Wimmer"
                    },
                    {
                        "name": "David Major"
                    },
                    {
                        "name": "Dimitrios Lenis"
                    },
                    {
                        "name": "Astrid Berg"
                    },
                    {
                        "name": "Theresa Neubauer"
                    },
                    {
                        "name": "Gaia Romana De Paolis"
                    },
                    {
                        "name": "Johannes Novotny"
                    },
                    {
                        "name": "Sophia Ulonska"
                    },
                    {
                        "name": "Katja Bühler"
                    }
                ],
                "author_detail": {
                    "name": "Katja Bühler"
                },
                "author": "Katja Bühler",
                "arxiv_doi": "10.1007/978-3-031-85181-0_1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85181-0_1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11743v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the author's accepted manuscript of a paper published in\n  Lecture Notes in Computer Science (LNCS), volume 15297, Proceedings of DAGM\n  GCPR 2024. 25 pages, 7 figures",
                "arxiv_journal_ref": "LNCS, volume 15297, 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17060v2",
                "updated": "2025-04-24T08:36:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    36,
                    47,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-24T11:21:08Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    11,
                    21,
                    8,
                    0,
                    55,
                    0
                ],
                "title": "Data Analysis Prediction over Multiple Unseen Datasets: A Vector\n  Embedding Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Analysis Prediction over Multiple Unseen Datasets: A Vector\n  Embedding Approach"
                },
                "summary": "The massive increase in the data volume and dataset availability for analysts\ncompels researchers to focus on data content and select high-quality datasets\nto enhance the performance of analytics operators. While selecting the highest\nquality data for analysis highly increases task accuracy and efficiency, it is\nstill a hard task, especially when the number of available inputs is very\nlarge. To address this issue, we propose a novel methodology that infers the\noutcome of analytics operators by creating a model from datasets similar to the\nqueried one. Dataset similarity is performed via projecting each dataset to a\nvector embedding representation. The vectorization process is performed using\nour proposed deep learning model NumTabData2Vec, which takes a whole dataset\nand projects it into a lower vector embedding representation space. Through\nexperimental evaluation, we compare the prediction performance and the\nexecution time of our framework to another state-of-the-art modelling operator\nframework, illustrating that our approach predicts analytics outcomes\naccurately. Furthermore, our vectorization model can project different\nreal-world scenarios to a lower vector embedding representation and distinguish\nbetween them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The massive increase in the data volume and dataset availability for analysts\ncompels researchers to focus on data content and select high-quality datasets\nto enhance the performance of analytics operators. While selecting the highest\nquality data for analysis highly increases task accuracy and efficiency, it is\nstill a hard task, especially when the number of available inputs is very\nlarge. To address this issue, we propose a novel methodology that infers the\noutcome of analytics operators by creating a model from datasets similar to the\nqueried one. Dataset similarity is performed via projecting each dataset to a\nvector embedding representation. The vectorization process is performed using\nour proposed deep learning model NumTabData2Vec, which takes a whole dataset\nand projects it into a lower vector embedding representation space. Through\nexperimental evaluation, we compare the prediction performance and the\nexecution time of our framework to another state-of-the-art modelling operator\nframework, illustrating that our approach predicts analytics outcomes\naccurately. Furthermore, our vectorization model can project different\nreal-world scenarios to a lower vector embedding representation and distinguish\nbetween them."
                },
                "authors": [
                    {
                        "name": "Andreas Loizou"
                    },
                    {
                        "name": "Dimitrios Tsoumakos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Tsoumakos"
                },
                "author": "Dimitrios Tsoumakos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05297v2",
                "updated": "2025-04-24T08:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    34,
                    58,
                    3,
                    114,
                    0
                ],
                "published": "2025-03-07T10:23:12Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    10,
                    23,
                    12,
                    4,
                    66,
                    0
                ],
                "title": "regMMD: An R package for parametric estimation and regression with\n  maximum mean discrepancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "regMMD: An R package for parametric estimation and regression with\n  maximum mean discrepancy"
                },
                "summary": "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided."
                },
                "authors": [
                    {
                        "name": "Pierre Alquier"
                    },
                    {
                        "name": "Mathieu Gerber"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Gerber"
                },
                "author": "Mathieu Gerber",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17366v1",
                "updated": "2025-04-24T08:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    27,
                    48,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:27:48Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    27,
                    48,
                    3,
                    114,
                    0
                ],
                "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from\n  Live Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from\n  Live Streams"
                },
                "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."
                },
                "authors": [
                    {
                        "name": "Yongxuan Wu"
                    },
                    {
                        "name": "Runyu Chen"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Hongjin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Hongjin Qian"
                },
                "author": "Hongjin Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17360v1",
                "updated": "2025-04-24T08:21:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    21,
                    4,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:21:04Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    21,
                    4,
                    3,
                    114,
                    0
                ],
                "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in\n  Healthcare"
                },
                "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4."
                },
                "authors": [
                    {
                        "name": "Jose G. Moreno"
                    },
                    {
                        "name": "Jesus Lovon"
                    },
                    {
                        "name": "M'Rick Robin-Charlet"
                    },
                    {
                        "name": "Christine Damase-Michel"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "arxiv_affiliation": "IRIT-IRIS",
                "author": "Lynda Tamine",
                "arxiv_journal_ref": "Workshop CL4Health @ NAACL 2025, May 2025, Albuquerque, New\n  Mexico, United States",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17356v1",
                "updated": "2025-04-24T08:16:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    16,
                    36,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:16:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    16,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning"
                },
                "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhang"
                    },
                    {
                        "name": "Xiaohan Huang"
                    },
                    {
                        "name": "Yi Du"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Zhen Meng"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15576v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15576v4",
                "updated": "2025-04-24T08:00:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    0,
                    36,
                    3,
                    114,
                    0
                ],
                "published": "2024-12-20T05:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    17,
                    6,
                    4,
                    355,
                    0
                ],
                "title": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped\n  Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped\n  Robot Learning"
                },
                "summary": "This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65%. Our\nproject page is https://quart-online.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65%. Our\nproject page is https://quart-online.github.io."
                },
                "authors": [
                    {
                        "name": "Xinyang Tong"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Yiguo Fan"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Hongyin Zhang"
                    },
                    {
                        "name": "Yonghao Dang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Shangke Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Shangke Lyu"
                },
                "author": "Shangke Lyu",
                "arxiv_comment": "Accepted to ICRA 2025; Github page: https://quart-online.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15576v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15576v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17334v1",
                "updated": "2025-04-24T07:52:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    52,
                    9,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:52:09Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    52,
                    9,
                    3,
                    114,
                    0
                ],
                "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent"
                },
                "summary": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories."
                },
                "authors": [
                    {
                        "name": "Chuer Chen"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Danqing Shi"
                    },
                    {
                        "name": "Shixiong Cao"
                    },
                    {
                        "name": "Nan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Nan Cao"
                },
                "author": "Nan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15788v2",
                "updated": "2025-04-24T07:49:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    49,
                    24,
                    3,
                    114,
                    0
                ],
                "published": "2024-02-24T11:08:54Z",
                "published_parsed": [
                    2024,
                    2,
                    24,
                    11,
                    8,
                    54,
                    5,
                    55,
                    0
                ],
                "title": "Outward compactness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outward compactness"
                },
                "summary": "We introduce and study a new type of compactness principle for strong logics\nthat, roughly speaking, infers the consistency of a theory from the consistency\nof its small fragments in certain outer models of the set-theoretic universe.\nWe refer to this type of compactness property as outward compactness, and we\nshow that instances of this type of principle for second-order logic can be\nused to characterize various large cardinal notions between measurability and\nextendibility, directly generalizing a classical result of Magidor that\ncharacterizes extendible cardinals as the strong compactness cardinals of\nsecond-order logic. In addition, we generalize a result of Makowsky that shows\nthat Vop\\v{e}nka's Principle is equivalent to the existence of compactness\ncardinals for all abstract logics by characterizing the principle \"Ord is\nWoodin\" through outward compactness properties of abstract logics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a new type of compactness principle for strong logics\nthat, roughly speaking, infers the consistency of a theory from the consistency\nof its small fragments in certain outer models of the set-theoretic universe.\nWe refer to this type of compactness property as outward compactness, and we\nshow that instances of this type of principle for second-order logic can be\nused to characterize various large cardinal notions between measurability and\nextendibility, directly generalizing a classical result of Magidor that\ncharacterizes extendible cardinals as the strong compactness cardinals of\nsecond-order logic. In addition, we generalize a result of Makowsky that shows\nthat Vop\\v{e}nka's Principle is equivalent to the existence of compactness\ncardinals for all abstract logics by characterizing the principle \"Ord is\nWoodin\" through outward compactness properties of abstract logics."
                },
                "authors": [
                    {
                        "name": "Peter Holy"
                    },
                    {
                        "name": "Philipp Lücke"
                    },
                    {
                        "name": "Sandra Müller"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Müller"
                },
                "author": "Sandra Müller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17332v1",
                "updated": "2025-04-24T07:48:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    26,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    26,
                    3,
                    114,
                    0
                ],
                "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation\n  Detection"
                },
                "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Lu Yuan"
                    },
                    {
                        "name": "Zhengxuan Zhang"
                    },
                    {
                        "name": "Qing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qing Zhao"
                },
                "author": "Qing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17331v1",
                "updated": "2025-04-24T07:48:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    9,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:48:09Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    9,
                    3,
                    114,
                    0
                ],
                "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual\n  Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual\n  Reality"
                },
                "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility."
                },
                "authors": [
                    {
                        "name": "Süleyman Özdel"
                    },
                    {
                        "name": "Kadir Burak Buldu"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Efe Bozkir"
                    }
                ],
                "author_detail": {
                    "name": "Efe Bozkir"
                },
                "author": "Efe Bozkir",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16427v2",
                "updated": "2025-04-24T07:35:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    35,
                    3,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T05:25:13Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    25,
                    13,
                    2,
                    113,
                    0
                ],
                "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark"
                },
                "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA."
                },
                "authors": [
                    {
                        "name": "Hanlei Zhang"
                    },
                    {
                        "name": "Zhuohang Li"
                    },
                    {
                        "name": "Yeshuang Zhu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Peiwu Wang"
                    },
                    {
                        "name": "Haige Zhu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jinchao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Zhang"
                },
                "author": "Jinchao Zhang",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13471v2",
                "updated": "2025-04-24T07:30:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    30,
                    24,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-18T05:25:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    25,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs"
                },
                "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combines techniques like rejection fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress models to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combines techniques like rejection fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress models to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas."
                },
                "authors": [
                    {
                        "name": "Jiliang Ni"
                    },
                    {
                        "name": "Jiachen Pu"
                    },
                    {
                        "name": "Zhongyi Yang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Xiaoliang Xiao"
                    },
                    {
                        "name": "Dakui Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jingfeng Luo"
                    },
                    {
                        "name": "Conggang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Conggang Hu"
                },
                "author": "Conggang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17323v1",
                "updated": "2025-04-24T07:26:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    26,
                    18,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:26:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    26,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "CKMDiff: A Generative Diffusion Model for CKM Construction via Inverse\n  Problems with Learned Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CKMDiff: A Generative Diffusion Model for CKM Construction via Inverse\n  Problems with Learned Priors"
                },
                "summary": "Channel knowledge map (CKM) is a promising technology to enable\nenvironment-aware wireless communications and sensing with greatly enhanced\nperformance, by offering location-specific channel prior information for future\nwireless networks. One fundamental problem for CKM-enabled wireless systems\nlies in how to construct high-quality and complete CKM for all locations of\ninterest, based on only limited and noisy on-site channel knowledge data. This\nproblem resembles the long-standing ill-posed inverse problem, which tries to\ninfer from a set of limited and noisy observations the cause factors that\nproduced them. By utilizing the recent advances of solving inverse problems\nwith learned priors using generative artificial intelligence (AI), we propose\nCKMDiff, a conditional diffusion model that can be applied to perform various\ntasks for CKM constructions such as denoising, inpainting, and\nsuper-resolution, without having to know the physical environment maps or\ntransceiver locations. Furthermore, we propose an environment-aware data\naugmentation mechanism to enhance the model's ability to learn implicit\nrelations between electromagnetic propagation patterns and spatial-geometric\nfeatures. Extensive numerical results are provided based on the CKMImageNet and\nRadioMapSeer datasets, which demonstrate that the proposed CKMDiff achieves\nstate-of-the-art performance, outperforming various benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel knowledge map (CKM) is a promising technology to enable\nenvironment-aware wireless communications and sensing with greatly enhanced\nperformance, by offering location-specific channel prior information for future\nwireless networks. One fundamental problem for CKM-enabled wireless systems\nlies in how to construct high-quality and complete CKM for all locations of\ninterest, based on only limited and noisy on-site channel knowledge data. This\nproblem resembles the long-standing ill-posed inverse problem, which tries to\ninfer from a set of limited and noisy observations the cause factors that\nproduced them. By utilizing the recent advances of solving inverse problems\nwith learned priors using generative artificial intelligence (AI), we propose\nCKMDiff, a conditional diffusion model that can be applied to perform various\ntasks for CKM constructions such as denoising, inpainting, and\nsuper-resolution, without having to know the physical environment maps or\ntransceiver locations. Furthermore, we propose an environment-aware data\naugmentation mechanism to enhance the model's ability to learn implicit\nrelations between electromagnetic propagation patterns and spatial-geometric\nfeatures. Extensive numerical results are provided based on the CKMImageNet and\nRadioMapSeer datasets, which demonstrate that the proposed CKMDiff achieves\nstate-of-the-art performance, outperforming various benchmark methods."
                },
                "authors": [
                    {
                        "name": "Shen Fu"
                    },
                    {
                        "name": "Yong Zeng"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Cheng-Xiang Wang"
                    },
                    {
                        "name": "Xiqi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiqi Gao"
                },
                "author": "Xiqi Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.17780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17780v1",
                "updated": "2025-04-24T17:56:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    56,
                    22,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:56:22Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    56,
                    22,
                    3,
                    114,
                    0
                ],
                "title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Replay to Remember: Retaining Domain Knowledge in Streaming Language\n  Models"
                },
                "summary": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Sneh Pillai"
                    }
                ],
                "author_detail": {
                    "name": "Sneh Pillai"
                },
                "arxiv_affiliation": "University of Massachusetts Dartmouth",
                "author": "Sneh Pillai",
                "arxiv_comment": "8 pages 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08585v2",
                "updated": "2025-04-24T17:42:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    42,
                    16,
                    3,
                    114,
                    0
                ],
                "published": "2025-03-11T16:21:23Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    21,
                    23,
                    1,
                    70,
                    0
                ],
                "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding"
                },
                "summary": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis."
                },
                "authors": [
                    {
                        "name": "Shehreen Azad"
                    },
                    {
                        "name": "Vibhav Vineet"
                    },
                    {
                        "name": "Yogesh Singh Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Singh Rawat"
                },
                "author": "Yogesh Singh Rawat",
                "arxiv_comment": "Accepted in CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17768v1",
                "updated": "2025-04-24T17:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    39,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    39,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"
                },
                "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Robert Li"
                    },
                    {
                        "name": "Renjie Huang"
                    },
                    {
                        "name": "Sebastian Ruder"
                    },
                    {
                        "name": "Kelly Marchisio"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17325v2",
                "updated": "2025-04-24T17:39:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    39,
                    3,
                    3,
                    114,
                    0
                ],
                "published": "2024-04-26T11:14:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    11,
                    14,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Towards Scalable Multi-Chip Wireless Networks with Near-Field Time\n  Reversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Multi-Chip Wireless Networks with Near-Field Time\n  Reversal"
                },
                "summary": "The concept of Wireless Network-on-Chip (WNoC) has emerged as a potential\nsolution to address the escalating communication demands of modern computing\nsystems due to its low-latency, versatility, and reconfigurability. However,\nfor WNoC to fulfill its potential, it is essential to establish multiple\nhigh-speed wireless links across chips. Unfortunately, the compact and enclosed\nnature of computing packages introduces significant challenges in the form of\nCo-Channel Interference and Inter-Symbol Interference, which not only hinder\nthe deployment of multiple spatial channels but also severely restrict the\nsymbol rate of each individual channel. In this paper, we posit that Time\nReversal (TR) could be effective in addressing both impairments in this static\nscenario thanks to its spatiotemporal focusing capabilities even in the near\nfield. Through comprehensive full-wave simulations and bit error rate analysis\nin multiple scenarios and at multiple frequency bands, we provide evidence that\nTR can increase the symbol rate by an order of magnitude, enabling the\ndeployment of multiple concurrent links and achieving aggregate speeds\nexceeding 100 Gb/s. Finally, we evaluate the impact of reducing the sampling\nrate of the TR filter on the achievable speeds, paving the way to practical\nTR-based wireless communications at the chip scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of Wireless Network-on-Chip (WNoC) has emerged as a potential\nsolution to address the escalating communication demands of modern computing\nsystems due to its low-latency, versatility, and reconfigurability. However,\nfor WNoC to fulfill its potential, it is essential to establish multiple\nhigh-speed wireless links across chips. Unfortunately, the compact and enclosed\nnature of computing packages introduces significant challenges in the form of\nCo-Channel Interference and Inter-Symbol Interference, which not only hinder\nthe deployment of multiple spatial channels but also severely restrict the\nsymbol rate of each individual channel. In this paper, we posit that Time\nReversal (TR) could be effective in addressing both impairments in this static\nscenario thanks to its spatiotemporal focusing capabilities even in the near\nfield. Through comprehensive full-wave simulations and bit error rate analysis\nin multiple scenarios and at multiple frequency bands, we provide evidence that\nTR can increase the symbol rate by an order of magnitude, enabling the\ndeployment of multiple concurrent links and achieving aggregate speeds\nexceeding 100 Gb/s. Finally, we evaluate the impact of reducing the sampling\nrate of the TR filter on the achievable speeds, paving the way to practical\nTR-based wireless communications at the chip scale."
                },
                "authors": [
                    {
                        "name": "Ama Bandara"
                    },
                    {
                        "name": "Fátima Rodríguez-Galán"
                    },
                    {
                        "name": "Pau Talarn"
                    },
                    {
                        "name": "Elana Pereira de Santana"
                    },
                    {
                        "name": "Evgenii Vinogradov"
                    },
                    {
                        "name": "Peter Haring Bolívar"
                    },
                    {
                        "name": "Eduard Alarcón"
                    },
                    {
                        "name": "Sergi Abadal"
                    }
                ],
                "author_detail": {
                    "name": "Sergi Abadal"
                },
                "author": "Sergi Abadal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v1",
                "updated": "2025-04-24T17:25:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17732v1",
                "updated": "2025-04-24T16:46:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    46,
                    32,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:46:32Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    46,
                    32,
                    3,
                    114,
                    0
                ],
                "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt\n  State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt\n  State Space Model"
                },
                "summary": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration."
                },
                "authors": [
                    {
                        "name": "Zhanwen Liu"
                    },
                    {
                        "name": "Sai Zhou"
                    },
                    {
                        "name": "Yuchao Dai"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yisheng An"
                    },
                    {
                        "name": "Xiangmo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmo Zhao"
                },
                "author": "Xiangmo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17725v1",
                "updated": "2025-04-24T16:38:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    38,
                    37,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:38:37Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    38,
                    37,
                    3,
                    114,
                    0
                ],
                "title": "STGen: A Novel Lightweight IoT Testbed for Generating Sensor Traffic for\n  the Experimentation of IoT Protocol and its Application in Hybrid Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STGen: A Novel Lightweight IoT Testbed for Generating Sensor Traffic for\n  the Experimentation of IoT Protocol and its Application in Hybrid Network"
                },
                "summary": "A Wireless Sensor Network (WSN) is a network that does not rely on a fixed\ninfrastructure and consists of numerous sensors, such as temperature, humidity,\nGPS, and cameras, equipped with onboard processors that manage and monitor the\nenvironment in a specific area. As a result, building a real sensor network\ntestbed for verifying, validating, or experimenting with a newly designed\nprotocol presents considerable challenges in adapting a laboratory scenario due\nto the significant financial and logistical barriers, such as the need for\nspecialized hardware and large-scale deployments. Additionally, WSN suffers\nfrom severe constraints such as restricted power supply, short communication\nrange, limited bandwidth availability, and restricted memory storage.\nAddressing these challenges, this work presents a flexible testbed solution\nnamed STGen that enables researchers to experiment with IoT protocols in a\nhybrid environment that emulates WSN implementations with the physical Internet\nthrough a dedicated physical server named STGen core, which receives sensor\ntraffic and processes it for further actions. The STGen testbed is lightweight\nin memory usage and easy to deploy. Most importantly, STGen supports\nlarge-scale distributed systems, facilitates experimentation with IoT\nprotocols, and enables integration with back-end services for big data\nanalytics and statistical insights. The key feature of STGen is the integration\nof real-world IoT protocols and their applications with WSN. Its modular and\nlightweight design makes STGen efficient and enables it to outperform other\npopular testbeds, such as Gotham and GothX, reducing memory usage by 89\\%.\nWhile GothX takes approximately 26 minutes to establish a large topology with\nfour VM nodes and 498 Docker nodes, STGen requires only 1.645 seconds to\ninitialize the platform with 500 sensor nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Wireless Sensor Network (WSN) is a network that does not rely on a fixed\ninfrastructure and consists of numerous sensors, such as temperature, humidity,\nGPS, and cameras, equipped with onboard processors that manage and monitor the\nenvironment in a specific area. As a result, building a real sensor network\ntestbed for verifying, validating, or experimenting with a newly designed\nprotocol presents considerable challenges in adapting a laboratory scenario due\nto the significant financial and logistical barriers, such as the need for\nspecialized hardware and large-scale deployments. Additionally, WSN suffers\nfrom severe constraints such as restricted power supply, short communication\nrange, limited bandwidth availability, and restricted memory storage.\nAddressing these challenges, this work presents a flexible testbed solution\nnamed STGen that enables researchers to experiment with IoT protocols in a\nhybrid environment that emulates WSN implementations with the physical Internet\nthrough a dedicated physical server named STGen core, which receives sensor\ntraffic and processes it for further actions. The STGen testbed is lightweight\nin memory usage and easy to deploy. Most importantly, STGen supports\nlarge-scale distributed systems, facilitates experimentation with IoT\nprotocols, and enables integration with back-end services for big data\nanalytics and statistical insights. The key feature of STGen is the integration\nof real-world IoT protocols and their applications with WSN. Its modular and\nlightweight design makes STGen efficient and enables it to outperform other\npopular testbeds, such as Gotham and GothX, reducing memory usage by 89\\%.\nWhile GothX takes approximately 26 minutes to establish a large topology with\nfour VM nodes and 498 Docker nodes, STGen requires only 1.645 seconds to\ninitialize the platform with 500 sensor nodes."
                },
                "authors": [
                    {
                        "name": "Hasan MA Islam"
                    },
                    {
                        "name": "S. Nath"
                    },
                    {
                        "name": "M. Rahman"
                    },
                    {
                        "name": "N. Shahriar"
                    },
                    {
                        "name": "M. K. M. Khan"
                    },
                    {
                        "name": "R. Islam"
                    }
                ],
                "author_detail": {
                    "name": "R. Islam"
                },
                "author": "R. Islam",
                "arxiv_comment": "23 Pages, 12 Figures, Submitted to ACM Transactions on Sensor\n  Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17723v1",
                "updated": "2025-04-24T16:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    36,
                    19,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:36:19Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    36,
                    19,
                    3,
                    114,
                    0
                ],
                "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework"
                },
                "summary": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Natan Levy"
                    },
                    {
                        "name": "Adiel Ashrov"
                    },
                    {
                        "name": "Guy Katz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Katz"
                },
                "author": "Guy Katz",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17720v1",
                "updated": "2025-04-24T16:32:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    32,
                    31,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:32:31Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    32,
                    31,
                    3,
                    114,
                    0
                ],
                "title": "Multilingual Performance Biases of Large Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Performance Biases of Large Language Models in Education"
                },
                "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."
                },
                "authors": [
                    {
                        "name": "Vansh Gupta"
                    },
                    {
                        "name": "Sankalan Pal Chowdhury"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17704v1",
                "updated": "2025-04-24T16:11:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    11,
                    1,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:11:01Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    11,
                    1,
                    3,
                    114,
                    0
                ],
                "title": "Safety in Large Reasoning Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety in Large Reasoning Models: A Survey"
                },
                "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Baolong Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Zhongzhi Li"
                    },
                    {
                        "name": "Junfeng Fang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Fang"
                },
                "author": "Junfeng Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00473v3",
                "updated": "2025-04-24T16:09:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    9,
                    26,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-01T16:00:08Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    8,
                    5,
                    32,
                    0
                ],
                "title": "Weak-to-Strong Diffusion with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Diffusion with Reflection"
                },
                "summary": "The goal of diffusion generative models is to align the learned distribution\nwith the real data distribution through gradient score matching. However,\ninherent limitations in training data quality, modeling strategies, and\narchitectural design lead to inevitable gap between generated outputs and real\ndata. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel\nframework that utilizes the estimated difference between existing weak and\nstrong models (i.e., weak-to-strong difference) to bridge the gap between an\nideal model and a strong model. By employing a reflective operation that\nalternates between denoising and inversion with weak-to-strong difference, we\ntheoretically understand that W2SD steers latent variables along sampling\ntrajectories toward regions of the real data distribution. W2SD is highly\nflexible and broadly applicable, enabling diverse improvements through the\nstrategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,\ngood experts vs. bad experts in MoE). Extensive experiments demonstrate that\nW2SD significantly improves human preference, aesthetic quality, and prompt\nadherence, achieving SOTA performance across various modalities (e.g., image,\nvideo), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For\nexample, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to\n90% over the original results. Moreover, the performance gains achieved by W2SD\nmarkedly outweigh its additional computational overhead, while the cumulative\nimprovements from different weak-to-strong difference further solidify its\npractical utility and deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of diffusion generative models is to align the learned distribution\nwith the real data distribution through gradient score matching. However,\ninherent limitations in training data quality, modeling strategies, and\narchitectural design lead to inevitable gap between generated outputs and real\ndata. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel\nframework that utilizes the estimated difference between existing weak and\nstrong models (i.e., weak-to-strong difference) to bridge the gap between an\nideal model and a strong model. By employing a reflective operation that\nalternates between denoising and inversion with weak-to-strong difference, we\ntheoretically understand that W2SD steers latent variables along sampling\ntrajectories toward regions of the real data distribution. W2SD is highly\nflexible and broadly applicable, enabling diverse improvements through the\nstrategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,\ngood experts vs. bad experts in MoE). Extensive experiments demonstrate that\nW2SD significantly improves human preference, aesthetic quality, and prompt\nadherence, achieving SOTA performance across various modalities (e.g., image,\nvideo), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For\nexample, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to\n90% over the original results. Moreover, the performance gains achieved by W2SD\nmarkedly outweigh its additional computational overhead, while the cumulative\nimprovements from different weak-to-strong difference further solidify its\npractical utility and deployability."
                },
                "authors": [
                    {
                        "name": "Lichen Bai"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Zeke Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Xie"
                },
                "author": "Zeke Xie",
                "arxiv_comment": "23 pages, 23 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17692v1",
                "updated": "2025-04-24T16:01:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    1,
                    48,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T16:01:48Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    1,
                    48,
                    3,
                    114,
                    0
                ],
                "title": "User Profiles: The Achilles' Heel of Web Browsers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Profiles: The Achilles' Heel of Web Browsers"
                },
                "summary": "Web browsers provide the security foundation for our online experiences.\nSignificant research has been done into the security of browsers themselves,\nbut relatively little investigation has been done into how they interact with\nthe operating system or the file system. In this work, we provide the first\nsystematic security study of browser profiles, the on-disk persistence layer of\nbrowsers, used for storing everything from users' authentication cookies and\nbrowser extensions to certificate trust decisions and device permissions. We\nshow that, except for the Tor Browser, all modern browsers store sensitive data\nin home directories with little to no integrity or confidentiality controls. We\nshow that security measures like password and cookie encryption can be easily\nbypassed. In addition, HTTPS can be sidestepped entirely by deploying malicious\nroot certificates within users' browser profiles. The Public Key Infrastructure\n(PKI), the backbone of the secure Web. HTTPS can be fully bypassed with the\ndeployment of custom potentially malicious root certificates. More worryingly,\nwe show how these powerful attacks can be fully mounted directly from web\nbrowsers themselves, through the File System Access API, a recent feature added\nby Chromium browsers that enables a website to directly manipulate a user's\nfile system via JavaScript. In a series of case studies, we demonstrate how an\nattacker can install malicious browser extensions, inject additional root\ncertificates, hijack HTTPS traffic, and enable websites to access hardware\ndevices like the camera and GPS. Based on our findings, we argue that\nresearchers and browser vendors need to develop and deploy more secure\nmechanisms for protecting users' browser data against file system attackers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web browsers provide the security foundation for our online experiences.\nSignificant research has been done into the security of browsers themselves,\nbut relatively little investigation has been done into how they interact with\nthe operating system or the file system. In this work, we provide the first\nsystematic security study of browser profiles, the on-disk persistence layer of\nbrowsers, used for storing everything from users' authentication cookies and\nbrowser extensions to certificate trust decisions and device permissions. We\nshow that, except for the Tor Browser, all modern browsers store sensitive data\nin home directories with little to no integrity or confidentiality controls. We\nshow that security measures like password and cookie encryption can be easily\nbypassed. In addition, HTTPS can be sidestepped entirely by deploying malicious\nroot certificates within users' browser profiles. The Public Key Infrastructure\n(PKI), the backbone of the secure Web. HTTPS can be fully bypassed with the\ndeployment of custom potentially malicious root certificates. More worryingly,\nwe show how these powerful attacks can be fully mounted directly from web\nbrowsers themselves, through the File System Access API, a recent feature added\nby Chromium browsers that enables a website to directly manipulate a user's\nfile system via JavaScript. In a series of case studies, we demonstrate how an\nattacker can install malicious browser extensions, inject additional root\ncertificates, hijack HTTPS traffic, and enable websites to access hardware\ndevices like the camera and GPS. Based on our findings, we argue that\nresearchers and browser vendors need to develop and deploy more secure\nmechanisms for protecting users' browser data against file system attackers."
                },
                "authors": [
                    {
                        "name": "Dolière Francis Somé"
                    },
                    {
                        "name": "Moaz Airan"
                    },
                    {
                        "name": "Zakir Durumeric"
                    },
                    {
                        "name": "Cristian-Alexandru Staicu"
                    }
                ],
                "author_detail": {
                    "name": "Cristian-Alexandru Staicu"
                },
                "author": "Cristian-Alexandru Staicu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17685v1",
                "updated": "2025-04-24T15:55:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    55,
                    10,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:55:10Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    55,
                    10,
                    3,
                    114,
                    0
                ],
                "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks"
                },
                "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."
                },
                "authors": [
                    {
                        "name": "Haru-Tada Sato"
                    },
                    {
                        "name": "Fuka Matsuzaki"
                    },
                    {
                        "name": "Jun-ichiro Takahashi"
                    }
                ],
                "author_detail": {
                    "name": "Jun-ichiro Takahashi"
                },
                "author": "Jun-ichiro Takahashi",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17677v1",
                "updated": "2025-04-24T15:47:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:47:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models"
                },
                "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience."
                },
                "authors": [
                    {
                        "name": "Jarne Thys"
                    },
                    {
                        "name": "Sebe Vanbrabant"
                    },
                    {
                        "name": "Davy Vanacken"
                    },
                    {
                        "name": "Gustavo Rovelo Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Rovelo Ruiz"
                },
                "author": "Gustavo Rovelo Ruiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17674v1",
                "updated": "2025-04-24T15:45:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    45,
                    5,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:45:05Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    45,
                    5,
                    3,
                    114,
                    0
                ],
                "title": "Energy Considerations of Large Language Model Inference and Efficiency\n  Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Considerations of Large Language Model Inference and Efficiency\n  Optimizations"
                },
                "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Jared Fernandez"
                    },
                    {
                        "name": "Clara Na"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Sasha Luccioni"
                    },
                    {
                        "name": "Emma Strubell"
                    }
                ],
                "author_detail": {
                    "name": "Emma Strubell"
                },
                "author": "Emma Strubell",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05255v2",
                "updated": "2025-04-24T15:43:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    43,
                    32,
                    3,
                    114,
                    0
                ],
                "published": "2025-01-09T14:12:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    12,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "CallNavi, A Challenge and Empirical Study on LLM Function Calling and\n  Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CallNavi, A Challenge and Empirical Study on LLM Function Calling and\n  Routing"
                },
                "summary": "API-driven chatbot systems are increasingly integral to software engineering\napplications, yet their effectiveness hinges on accurately generating and\nexecuting API calls. This is particularly challenging in scenarios requiring\nmulti-step interactions with complex parameterization and nested API\ndependencies. Addressing these challenges, this work contributes to the\nevaluation and assessment of AI-based software development through three key\nadvancements: (1) the introduction of a novel dataset specifically designed for\nbenchmarking API function selection, parameter generation, and nested API\nexecution; (2) an empirical evaluation of state-of-the-art language models,\nanalyzing their performance across varying task complexities in API function\ngeneration and parameter accuracy; and (3) a hybrid approach to API routing,\ncombining general-purpose large language models for API selection with\nfine-tuned models and prompt engineering for parameter generation. These\ninnovations significantly improve API execution in chatbot systems, offering\npractical methodologies for enhancing software design, testing, and operational\nworkflows in real-world software engineering contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API-driven chatbot systems are increasingly integral to software engineering\napplications, yet their effectiveness hinges on accurately generating and\nexecuting API calls. This is particularly challenging in scenarios requiring\nmulti-step interactions with complex parameterization and nested API\ndependencies. Addressing these challenges, this work contributes to the\nevaluation and assessment of AI-based software development through three key\nadvancements: (1) the introduction of a novel dataset specifically designed for\nbenchmarking API function selection, parameter generation, and nested API\nexecution; (2) an empirical evaluation of state-of-the-art language models,\nanalyzing their performance across varying task complexities in API function\ngeneration and parameter accuracy; and (3) a hybrid approach to API routing,\ncombining general-purpose large language models for API selection with\nfine-tuned models and prompt engineering for parameter generation. These\ninnovations significantly improve API execution in chatbot systems, offering\npractical methodologies for enhancing software design, testing, and operational\nworkflows in real-world software engineering contexts."
                },
                "authors": [
                    {
                        "name": "Yewei Song"
                    },
                    {
                        "name": "Xunzhu Tang"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Saad Ezzini"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé F. Bissyandé"
                    },
                    {
                        "name": "Andrey Boytsov"
                    },
                    {
                        "name": "Ulrick Ble"
                    },
                    {
                        "name": "Anne Goujon"
                    }
                ],
                "author_detail": {
                    "name": "Anne Goujon"
                },
                "author": "Anne Goujon",
                "arxiv_journal_ref": "The 29th International Conference on Evaluation and Assessment in\n  Software Engineering (EASE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17672v1",
                "updated": "2025-04-24T15:40:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    40,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:40:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    40,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Cross-region Model Training with Communication-Computation Overlapping\n  and Delay Compensation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-region Model Training with Communication-Computation Overlapping\n  and Delay Compensation"
                },
                "summary": "Training large language models (LLMs) requires massive computational\nresources, often necessitating the aggregation of geographically distributed\ndata centers (\\ie, cross-region training). However, the high communication\nlatency in wide-area networks severely degrades the efficiency of traditional\ndistributed training. While methods like DiLoCo reduce communication frequency,\nthey suffer from blocking synchronization. Streaming DiLoCo alleviates this\nissue via communication-computation overlapping but introduces update staleness\nand model inconsistency due to delayed global updates and partial\nsynchronization. These factors impair convergence, especially when aggressive\noverlap is needed to mask high latency. We propose CoCoDC, a novel distributed\ntraining framework with communication-computation overlapping and delay\ncompensation, to explicitly tackle these challenges. Within the CoCoDC\nframework, we specifically develop a novel Delay Compensation strategy based on\nTaylor expansion to effectively mitigate the staleness and an Adaptive\nTransmission strategy that dynamically schedules model fragment synchronization\nto optimize bandwidth usage and accelerate convergence. Extensive experiments\nhighlight the superior performance of CoCoDC over both DiLoCo and Streaming\nDiLoCo regarding final accuracy and training speed. Specifically, CoCoDC\nreduces the training steps needed to reach a comparable perplexity by up to\n21.0% compared to Streaming DiLoCo. Our work provides an effective solution for\nscalable and efficient cross-region LLM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) requires massive computational\nresources, often necessitating the aggregation of geographically distributed\ndata centers (\\ie, cross-region training). However, the high communication\nlatency in wide-area networks severely degrades the efficiency of traditional\ndistributed training. While methods like DiLoCo reduce communication frequency,\nthey suffer from blocking synchronization. Streaming DiLoCo alleviates this\nissue via communication-computation overlapping but introduces update staleness\nand model inconsistency due to delayed global updates and partial\nsynchronization. These factors impair convergence, especially when aggressive\noverlap is needed to mask high latency. We propose CoCoDC, a novel distributed\ntraining framework with communication-computation overlapping and delay\ncompensation, to explicitly tackle these challenges. Within the CoCoDC\nframework, we specifically develop a novel Delay Compensation strategy based on\nTaylor expansion to effectively mitigate the staleness and an Adaptive\nTransmission strategy that dynamically schedules model fragment synchronization\nto optimize bandwidth usage and accelerate convergence. Extensive experiments\nhighlight the superior performance of CoCoDC over both DiLoCo and Streaming\nDiLoCo regarding final accuracy and training speed. Specifically, CoCoDC\nreduces the training steps needed to reach a comparable perplexity by up to\n21.0% compared to Streaming DiLoCo. Our work provides an effective solution for\nscalable and efficient cross-region LLM training."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Yunming Liao"
                    },
                    {
                        "name": "Zhiwei Yao"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17671v1",
                "updated": "2025-04-24T15:39:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    39,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:39:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    39,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction"
                },
                "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Yuanchang Ye"
                    },
                    {
                        "name": "Weiyan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Weiyan Wen"
                },
                "author": "Weiyan Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17669v1",
                "updated": "2025-04-24T15:38:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    38,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:38:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    38,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a HIPAA Compliant Agentic AI System in Healthcare"
                },
                "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification."
                },
                "authors": [
                    {
                        "name": "Subash Neupane"
                    },
                    {
                        "name": "Shaswata Mitra"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Rahimi"
                },
                "author": "Shahram Rahimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17665v1",
                "updated": "2025-04-24T15:34:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    34,
                    24,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:34:24Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    34,
                    24,
                    3,
                    114,
                    0
                ],
                "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for\n  Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for\n  Mathematics"
                },
                "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain."
                },
                "authors": [
                    {
                        "name": "Zena Al-Khalili"
                    },
                    {
                        "name": "Nick Howell"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17655v1",
                "updated": "2025-04-24T15:25:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    25,
                    37,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T15:25:37Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    25,
                    37,
                    3,
                    114,
                    0
                ],
                "title": "Aerial Image Classification in Scarce and Unconstrained Environments via\n  Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial Image Classification in Scarce and Unconstrained Environments via\n  Conformal Prediction"
                },
                "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies."
                },
                "authors": [
                    {
                        "name": "Farhad Pourkamali-Anaraki"
                    }
                ],
                "author_detail": {
                    "name": "Farhad Pourkamali-Anaraki"
                },
                "author": "Farhad Pourkamali-Anaraki",
                "arxiv_comment": "17 pages, 5 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16871v2",
                "updated": "2025-04-24T15:21:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    21,
                    54,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T16:46:06Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    46,
                    6,
                    2,
                    113,
                    0
                ],
                "title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge"
                },
                "summary": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks"
                },
                "authors": [
                    {
                        "name": "Mirian Hipolito Garcia"
                    },
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Anastasios Kyrillidis"
                    },
                    {
                        "name": "Robert Sim"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10479v2",
                "updated": "2025-04-24T15:15:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    15,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-15T03:06:14Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    3,
                    6,
                    14,
                    5,
                    167,
                    0
                ],
                "title": "Unlocking Large Language Model's Planning Capabilities with Maximum\n  Diversity Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Large Language Model's Planning Capabilities with Maximum\n  Diversity Fine-tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive task-solving\ncapabilities through prompting techniques and system designs, including solving\nplanning tasks (e.g., math proofs, basic travel planning) when sufficient data\nis available online and used during pre-training. However, for planning tasks\nwith limited prior data (e.g., blocks world, advanced travel planning), the\nperformance of LLMs, including proprietary models like GPT and Gemini, is poor.\nThis paper investigates the impact of fine-tuning on the planning capabilities\nof LLMs, revealing that LLMs can achieve strong performance in planning through\nsubstantial (tens of thousands of specific examples) fine-tuning. Yet, this\nprocess incurs high economic, time, and computational costs for each planning\nproblem variation. To address this, we propose Clustering-Based Maximum\nDiversity Sampling (CMDS), which selects diverse and representative data to\nenhance sample efficiency and the model's generalization capability. Extensive\nevaluations demonstrate that CMDS-l, a baseline method combining CMDS with\nlanguage embeddings, outperforms random sampling. Furthermore, we introduce a\nnovel algorithm, CMDS-g, which encodes planning task instances with their graph\nrepresentations into the embedding space. Empirical results show that CMDS-g\nconsistently outperforms baseline methods across various scales and multiple\nbenchmark domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive task-solving\ncapabilities through prompting techniques and system designs, including solving\nplanning tasks (e.g., math proofs, basic travel planning) when sufficient data\nis available online and used during pre-training. However, for planning tasks\nwith limited prior data (e.g., blocks world, advanced travel planning), the\nperformance of LLMs, including proprietary models like GPT and Gemini, is poor.\nThis paper investigates the impact of fine-tuning on the planning capabilities\nof LLMs, revealing that LLMs can achieve strong performance in planning through\nsubstantial (tens of thousands of specific examples) fine-tuning. Yet, this\nprocess incurs high economic, time, and computational costs for each planning\nproblem variation. To address this, we propose Clustering-Based Maximum\nDiversity Sampling (CMDS), which selects diverse and representative data to\nenhance sample efficiency and the model's generalization capability. Extensive\nevaluations demonstrate that CMDS-l, a baseline method combining CMDS with\nlanguage embeddings, outperforms random sampling. Furthermore, we introduce a\nnovel algorithm, CMDS-g, which encodes planning task instances with their graph\nrepresentations into the embedding space. Empirical results show that CMDS-g\nconsistently outperforms baseline methods across various scales and multiple\nbenchmark domains."
                },
                "authors": [
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Varakantham"
                },
                "author": "Pradeep Varakantham",
                "arxiv_comment": "8 pages of main paper, 2 pages of references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08659v3",
                "updated": "2025-04-24T15:12:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    12,
                    52,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-09T06:33:47Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    6,
                    33,
                    47,
                    6,
                    40,
                    0
                ],
                "title": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks"
                },
                "summary": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference."
                },
                "authors": [
                    {
                        "name": "Shuqi Shen"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Hui Zhong"
                    },
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Xinhu Zheng"
                    },
                    {
                        "name": "Hai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Yang"
                },
                "author": "Hai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11242v4",
                "updated": "2025-04-24T14:58:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    58,
                    40,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-17T14:47:33Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    14,
                    47,
                    33,
                    1,
                    261,
                    0
                ],
                "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded\n  Attributions and Learning to Refuse"
                },
                "summary": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. 26 out of 27 models aligned using Trust-Align\nsubstantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.\nSpecifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),\nQAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly\nenhances models' ability to correctly refuse and provide quality citations. We\nalso demonstrate the effectiveness of Trust-Align across different open-weight\nmodels, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),\nand Phi3.5 (3.8b). We release our code at\nhttps://github.com/declare-lab/trust-align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are an integral component of retrieval-augmented generation (RAG)\nsystems. While many studies focus on evaluating the overall quality of\nend-to-end RAG systems, there is a gap in understanding the appropriateness of\nLLMs for the RAG task. To address this, we introduce Trust-Score, a holistic\nmetric that evaluates the trustworthiness of LLMs within the RAG framework. Our\nresults show that various prompting methods, such as in-context learning, fail\nto effectively adapt LLMs to the RAG task as measured by Trust-Score.\nConsequently, we propose Trust-Align, a method to align LLMs for improved\nTrust-Score performance. 26 out of 27 models aligned using Trust-Align\nsubstantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.\nSpecifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),\nQAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly\nenhances models' ability to correctly refuse and provide quality citations. We\nalso demonstrate the effectiveness of Trust-Align across different open-weight\nmodels, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),\nand Phi3.5 (3.8b). We release our code at\nhttps://github.com/declare-lab/trust-align."
                },
                "authors": [
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Shang Hong Sim"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Hai Leong Chieu"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "Published at ICLR 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10742v2",
                "updated": "2025-04-24T14:53:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    53,
                    57,
                    3,
                    114,
                    0
                ],
                "published": "2025-03-13T17:47:52Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    47,
                    52,
                    3,
                    72,
                    0
                ],
                "title": "Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large\n  Vision Language Models on Long-Form Video Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large\n  Vision Language Models on Long-Form Video Processing"
                },
                "summary": "Vision language models (VLMs) demonstrate strong capabilities in jointly\nprocessing visual and textual data. However, they often incur substantial\ncomputational overhead due to redundant visual information, particularly in\nlong-form video scenarios. Existing approaches predominantly focus on either\nvision token pruning, which may overlook spatio-temporal dependencies, or\nkeyframe selection, which identifies informative frames but discards others,\nthus disrupting contextual continuity. In this work, we propose KVTP\n(Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the\ndrawbacks of token pruning and keyframe selection. By adaptively assigning\npruning rates based on frame relevance to the query, KVTP effectively retains\nessential contextual information while significantly reducing redundant\ncomputation. To thoroughly evaluate the long-form video understanding\ncapacities of VLMs, we curated and reorganized subsets from VideoMME,\nEgoSchema, and NextQA into a unified benchmark named SparseKV-QA that\nhighlights real-world scenarios with sparse but crucial events. Our experiments\nwith VLMs of various scales show that KVTP can reduce token usage by 80%\nwithout compromising spatiotemporal and contextual consistency, significantly\ncutting computation while maintaining the performance. These results\ndemonstrate our approach's effectiveness in efficient long-video processing,\nfacilitating more scalable VLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) demonstrate strong capabilities in jointly\nprocessing visual and textual data. However, they often incur substantial\ncomputational overhead due to redundant visual information, particularly in\nlong-form video scenarios. Existing approaches predominantly focus on either\nvision token pruning, which may overlook spatio-temporal dependencies, or\nkeyframe selection, which identifies informative frames but discards others,\nthus disrupting contextual continuity. In this work, we propose KVTP\n(Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the\ndrawbacks of token pruning and keyframe selection. By adaptively assigning\npruning rates based on frame relevance to the query, KVTP effectively retains\nessential contextual information while significantly reducing redundant\ncomputation. To thoroughly evaluate the long-form video understanding\ncapacities of VLMs, we curated and reorganized subsets from VideoMME,\nEgoSchema, and NextQA into a unified benchmark named SparseKV-QA that\nhighlights real-world scenarios with sparse but crucial events. Our experiments\nwith VLMs of various scales show that KVTP can reduce token usage by 80%\nwithout compromising spatiotemporal and contextual consistency, significantly\ncutting computation while maintaining the performance. These results\ndemonstrate our approach's effectiveness in efficient long-video processing,\nfacilitating more scalable VLM deployment."
                },
                "authors": [
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07263v3",
                "updated": "2025-04-24T14:34:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    34,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2023-10-11T07:39:42Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    39,
                    42,
                    2,
                    284,
                    0
                ],
                "title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoPAL: Corrective Planning of Robot Actions with Large Language Models"
                },
                "summary": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation."
                },
                "authors": [
                    {
                        "name": "Frank Joublin"
                    },
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Pavel Smirnov"
                    },
                    {
                        "name": "Felix Ocker"
                    },
                    {
                        "name": "Joerg Deigmoeller"
                    },
                    {
                        "name": "Anna Belardinelli"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Stephan Hasler"
                    },
                    {
                        "name": "Daniel Tanneberg"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "arxiv_doi": "10.1109/ICRA57147.2024.10610434",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICRA57147.2024.10610434",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.07263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE International Conference on Robotics and Automation (ICRA) 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12533v3",
                "updated": "2025-04-24T14:28:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    28,
                    14,
                    3,
                    114,
                    0
                ],
                "published": "2024-03-19T08:09:44Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    8,
                    9,
                    44,
                    1,
                    79,
                    0
                ],
                "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot\n  Group Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot\n  Group Interactions"
                },
                "summary": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed."
                },
                "authors": [
                    {
                        "name": "Daniel Tanneberg"
                    },
                    {
                        "name": "Felix Ocker"
                    },
                    {
                        "name": "Stephan Hasler"
                    },
                    {
                        "name": "Joerg Deigmoeller"
                    },
                    {
                        "name": "Anna Belardinelli"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Heiko Wersing"
                    },
                    {
                        "name": "Bernhard Sendhoff"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "arxiv_doi": "10.1109/IROS58592.2024.10801517",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10801517",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07347v2",
                "updated": "2025-04-24T14:10:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    10,
                    22,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-10T00:12:12Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    0,
                    12,
                    12,
                    3,
                    100,
                    0
                ],
                "title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents"
                },
                "summary": "As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As demand for Large Language Models (LLMs) and AI agents rapidly grows,\noptimizing systems for efficient LLM inference becomes critical. While\nsignificant efforts have focused on system-level engineering, little is\nexplored from a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for large language\nmodel (LLM) inference, bridging the gap between the queueing theory and LLM\nsystem communities. In particular, we study the throughput aspect in LLM\ninference systems. We prove that a large class of 'work-conserving' scheduling\nalgorithms can achieve maximum throughput for individual inference LLM engine,\nhighlighting 'work-conserving' as a key design principle in practice. In a\nnetwork of LLM agents, work-conserving scheduling alone is insufficient,\nparticularly when facing specific workload structures and multi-class workflows\nthat require more sophisticated scheduling strategies. Evaluations of\nreal-world systems show that Orca and Sarathi-serve are throughput-optimal,\nreassuring practitioners, while FasterTransformer and vanilla vLLM are not\nmaximally stable and should be used with caution. Our results highlight the\nsubstantial benefits that the queueing community can offer in improving LLM\ninference systems and call for more interdisciplinary development."
                },
                "authors": [
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Jim Dai"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17565v1",
                "updated": "2025-04-24T13:57:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:57:53Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training"
                },
                "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07687v2",
                "updated": "2025-04-24T13:53:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    53,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-10T12:16:32Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    12,
                    16,
                    32,
                    3,
                    100,
                    0
                ],
                "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection"
                },
                "summary": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets, often\npolitically or virally motivated-pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, particularly video-based platforms, have become deeply embedded\nin daily life, concurrently amplifying risks of misinformation dissemination.\nConsequently, multimodal fake news detection has garnered significant research\nattention. However, existing datasets predominantly comprise user-generated\nvideos characterized by crude editing and limited public engagement, whereas\nprofessionally crafted fake news videos disseminated by media outlets, often\npolitically or virally motivated-pose substantially greater societal harm. To\naddress this gap, we construct FMNV, a novel dataset exclusively composed of\nnews videos published by media organizations. Through empirical analysis of\nexisting datasets and our curated collection, we categorize fake news videos\ninto four distinct types. Building upon this taxonomy, we employ Large Language\nModels (LLMs) to automatically generate deceptive content by manipulating\nauthentic media-published news videos. Furthermore, we propose FMNVD, a\nbaseline model featuring a dual-stream architecture integrating CLIP and Faster\nR-CNN for video feature extraction, enhanced by co-attention mechanisms for\nfeature refinement and multimodal aggregation. Comparative experiments\ndemonstrate both the generalization capability of FMNV across multiple\nbaselines and the superior detection efficacy of FMNVD. This work establishes\ncritical benchmarks for detecting high-impact fake news in media ecosystems\nwhile advancing methodologies for cross-modal inconsistency analysis."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13199v2",
                "updated": "2025-04-24T13:46:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    46,
                    16,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-14T21:10:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    21,
                    10,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency,\n  and Ethics in Vision-Language Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency,\n  and Ethics in Vision-Language Tasks"
                },
                "summary": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework."
                },
                "authors": [
                    {
                        "name": "Mohammad Saleh"
                    },
                    {
                        "name": "Azadeh Tabatabaei"
                    }
                ],
                "author_detail": {
                    "name": "Azadeh Tabatabaei"
                },
                "author": "Azadeh Tabatabaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17550v1",
                "updated": "2025-04-24T13:40:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    40,
                    27,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:40:27Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    40,
                    27,
                    3,
                    114,
                    0
                ],
                "title": "HalluLens: LLM Hallucination Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluLens: LLM Hallucination Benchmark"
                },
                "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."
                },
                "authors": [
                    {
                        "name": "Yejin Bang"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Alan Schelten"
                    },
                    {
                        "name": "Anthony Hartshorn"
                    },
                    {
                        "name": "Tara Fowler"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17547v1",
                "updated": "2025-04-24T13:37:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    37,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:37:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    37,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering\n  Systems: The Lifecycle of Knowledge in Visual Reasoning Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Knowledge-Based Vision Question Answering\n  Systems: The Lifecycle of Knowledge in Visual Reasoning Task"
                },
                "summary": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications."
                },
                "authors": [
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Zonghan Wu"
                    },
                    {
                        "name": "Huan Huo"
                    },
                    {
                        "name": "Guandong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Xu"
                },
                "author": "Guandong Xu",
                "arxiv_comment": "20 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17544v1",
                "updated": "2025-04-24T13:32:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    30,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:32:30Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    30,
                    3,
                    114,
                    0
                ],
                "title": "Auditing the Ethical Logic of Generative AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing the Ethical Logic of Generative AI Models"
                },
                "summary": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts."
                },
                "authors": [
                    {
                        "name": "W. Russell Neuman"
                    },
                    {
                        "name": "Chad Coleman"
                    },
                    {
                        "name": "Ali Dasdan"
                    },
                    {
                        "name": "Safinah Ali"
                    },
                    {
                        "name": "Manan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Manan Shah"
                },
                "author": "Manan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17542v1",
                "updated": "2025-04-24T13:32:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:32:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Large Language Model-Driven Concolic Execution for Highly Structured\n  Test Input Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Driven Concolic Execution for Highly Structured\n  Test Input Generation"
                },
                "summary": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). The experimental result is promising: it shows that\nCottontail outperforms state-of-the-art approaches (SymCC and Marco) by 14.15%\nand 14.31% in terms of line coverage. Besides, Cottontail found 6 previously\nunknown vulnerabilities (six new CVEs have been assigned). We have reported\nthese issues to developers, and 4 out of them have been fixed so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). The experimental result is promising: it shows that\nCottontail outperforms state-of-the-art approaches (SymCC and Marco) by 14.15%\nand 14.31% in terms of line coverage. Besides, Cottontail found 6 previously\nunknown vulnerabilities (six new CVEs have been assigned). We have reported\nthese issues to developers, and 4 out of them have been fixed so far."
                },
                "authors": [
                    {
                        "name": "Haoxin Tu"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Yuxian Li"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    },
                    {
                        "name": "Marcel Böhme"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Böhme"
                },
                "author": "Marcel Böhme",
                "arxiv_comment": "18 pages (including Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14088v2",
                "updated": "2025-04-24T13:24:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    24,
                    42,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-20T08:04:07Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    8,
                    4,
                    7,
                    3,
                    172,
                    0
                ],
                "title": "ReaL: Efficient RLHF Training of Large Language Models with Parameter\n  Reallocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReaL: Efficient RLHF Training of Large Language Models with Parameter\n  Reallocation"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for\nempowering large language model (LLM) applications. Compared with the\nsupervised training process of LLMs, the RLHF training process is much more\nsophisticated, requiring a diverse range of computation workloads with\nintricate dependencies between multiple LLM instances. Therefore, simply\nadopting the fixed parallelization strategies from supervised training for LLMs\ncan be insufficient for RLHF and result in low training efficiency. To overcome\nthis limitation, we propose a novel technique named parameter ReaLlocation,\nwhich dynamically adapts the parallelization strategies for different workloads\nduring training by redistributing LLM parameters across the training cluster.\nBuilding upon this idea, we introduce ReaL, a pioneering system for efficient\nRLHF training. ReaL introduces the concept of an execution plan, which defines\na fine-grained resource allocation and parallelization strategy particularly\ndesigned for RLHF training. Based on this concept, ReaL employs a tailored\nsearch algorithm with a lightweight run-time estimator to automatically\ndiscover an efficient execution plan for an instance of RLHF experiment.\nSubsequently, the runtime engine deploys the selected plan by effectively\nparallelizing computations and redistributing parameters. We evaluate ReaL on\nthe LLaMA models with up to 70 billion parameters and 128 GPUs. The\nexperimental results demonstrate that ReaL achieves speedups of up to\n$3.58\\times$ compared to baseline methods. Furthermore, the execution plans\ngenerated by ReaL exhibit an average of $81\\%$ performance improvement over\nheuristic approaches based on Megatron-LM in the long-context scenario. The\nsource code of ReaL is publicly available at\nhttps://github.com/openpsi-project/ReaLHF ."
                },
                "authors": [
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Kaiwei Li"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Huanchen Zhang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "arxiv_comment": "11 pages (20 pages with references and the appendix), 17 figures.\n  Accepted by MLSys 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17531v1",
                "updated": "2025-04-24T13:19:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    19,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:19:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    19,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "Towards Machine-Generated Code for the Resolution of User Intentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Machine-Generated Code for the Resolution of User Intentions"
                },
                "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions."
                },
                "authors": [
                    {
                        "name": "Justus Flerlage"
                    },
                    {
                        "name": "Ilja Behnke"
                    },
                    {
                        "name": "Odej Kao"
                    }
                ],
                "author_detail": {
                    "name": "Odej Kao"
                },
                "author": "Odej Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17529v1",
                "updated": "2025-04-24T13:17:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    17,
                    18,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:17:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    17,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval"
                },
                "summary": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform."
                },
                "authors": [
                    {
                        "name": "Youngjune Lee"
                    },
                    {
                        "name": "Haeyu Jeong"
                    },
                    {
                        "name": "Changgeon Lim"
                    },
                    {
                        "name": "Jeong Choi"
                    },
                    {
                        "name": "Hongjun Lim"
                    },
                    {
                        "name": "Hangon Kim"
                    },
                    {
                        "name": "Jiyoon Kwon"
                    },
                    {
                        "name": "Saehun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Saehun Kim"
                },
                "author": "Saehun Kim",
                "arxiv_comment": "Accepted to SIGIR 2025 Industry Track. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v4",
                "updated": "2025-04-24T13:16:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    16,
                    26,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17524v1",
                "updated": "2025-04-24T13:08:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    8,
                    36,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:08:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    8,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot\n  Learning for Color Image Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot\n  Learning for Color Image Inpainting"
                },
                "summary": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results."
                },
                "authors": [
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Mengxiao Geng"
                    },
                    {
                        "name": "Liu Shi"
                    },
                    {
                        "name": "Qiegen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiegen Liu"
                },
                "author": "Qiegen Liu",
                "arxiv_comment": "11 pages,10 figures,Submit to tcsvt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02868v2",
                "updated": "2025-04-24T13:07:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    7,
                    21,
                    3,
                    114,
                    0
                ],
                "published": "2024-12-03T22:06:55Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    6,
                    55,
                    1,
                    338,
                    0
                ],
                "title": "Enhancing LLMs with Smart Preprocessing for EHR Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLMs with Smart Preprocessing for EHR Analysis"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nnatural language processing; however, their application in sensitive domains\nsuch as healthcare, especially in processing Electronic Health Records (EHRs),\nis constrained by limited computational resources and privacy concerns. This\npaper introduces a compact LLM framework optimized for local deployment in\nenvironments with stringent privacy requirements and restricted access to\nhigh-performance GPUs. Our approach leverages simple yet powerful preprocessing\ntechniques, including regular expressions (regex) and Retrieval-Augmented\nGeneration (RAG), to extract and highlight critical information from clinical\nnotes. By pre-filtering long, unstructured text, we enhance the performance of\nsmaller LLMs on EHR-related tasks. Our framework is evaluated using zero-shot\nand few-shot learning paradigms on both private and publicly available datasets\n(MIMIC-IV), with additional comparisons against fine-tuned LLMs on MIMIC-IV.\nExperimental results demonstrate that our preprocessing strategy significantly\nsupercharges the performance of smaller LLMs, making them well-suited for\nprivacy-sensitive and resource-constrained applications. This study offers\nvaluable insights into optimizing LLM performance for local, secure, and\nefficient healthcare applications. It provides practical guidance for\nreal-world deployment for LLMs while tackling challenges related to privacy,\ncomputational feasibility, and clinical applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nnatural language processing; however, their application in sensitive domains\nsuch as healthcare, especially in processing Electronic Health Records (EHRs),\nis constrained by limited computational resources and privacy concerns. This\npaper introduces a compact LLM framework optimized for local deployment in\nenvironments with stringent privacy requirements and restricted access to\nhigh-performance GPUs. Our approach leverages simple yet powerful preprocessing\ntechniques, including regular expressions (regex) and Retrieval-Augmented\nGeneration (RAG), to extract and highlight critical information from clinical\nnotes. By pre-filtering long, unstructured text, we enhance the performance of\nsmaller LLMs on EHR-related tasks. Our framework is evaluated using zero-shot\nand few-shot learning paradigms on both private and publicly available datasets\n(MIMIC-IV), with additional comparisons against fine-tuned LLMs on MIMIC-IV.\nExperimental results demonstrate that our preprocessing strategy significantly\nsupercharges the performance of smaller LLMs, making them well-suited for\nprivacy-sensitive and resource-constrained applications. This study offers\nvaluable insights into optimizing LLM performance for local, secure, and\nefficient healthcare applications. It provides practical guidance for\nreal-world deployment for LLMs while tackling challenges related to privacy,\ncomputational feasibility, and clinical applicability."
                },
                "authors": [
                    {
                        "name": "Yixiang Qu"
                    },
                    {
                        "name": "Yifan Dai"
                    },
                    {
                        "name": "Shilin Yu"
                    },
                    {
                        "name": "Pradham Tanikella"
                    },
                    {
                        "name": "Travis Schrank"
                    },
                    {
                        "name": "Trevor Hackman"
                    },
                    {
                        "name": "Didong Li"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09818v2",
                "updated": "2025-04-24T12:46:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    46,
                    5,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-14T02:39:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    2,
                    39,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "Transferable text data distillation by trajectory matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable text data distillation by trajectory matching"
                },
                "summary": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large language model (LLM), as the size of large models\nincreases, it also brings higher training costs. There is a urgent need to\nminimize the data size in LLM training. Compared with data selection method,\nthe data distillation method aims to synthesize a small number of data samples\nto achieve the training effect of the full data set and has better flexibility.\nDespite its successes in computer vision, the discreteness of text data has\nhitherto stymied its exploration in natural language processing (NLP). In this\nwork, we proposed a method that involves learning pseudo prompt data based on\ntrajectory matching and finding its nearest neighbor ID to achieve\ncross-architecture transfer. During the distillation process, we introduce a\nregularization loss to improve the robustness of our distilled data. To our\nbest knowledge, this is the first data distillation work suitable for text\ngeneration tasks such as instruction tuning. Evaluations on two benchmarks,\nincluding ARC-Easy and MMLU instruction tuning datasets, established the\nsuperiority of our distillation approach over the SOTA data selection method\nLESS. Furthermore, our method demonstrates a good transferability over LLM\nstructures (i.e., OPT to Llama)."
                },
                "authors": [
                    {
                        "name": "Rong Yao"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yifei Fu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Fanyi Du"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15909v2",
                "updated": "2025-04-24T12:39:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    39,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-22T13:55:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    55,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "Synergizing RAG and Reasoning: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing RAG and Reasoning: A Systematic Review"
                },
                "summary": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs), particularly in\nreasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to\nunprecedented levels. By synergizing retrieval mechanisms with advanced\nreasoning, LLMs can now tackle increasingly complex problems. This paper\npresents a systematic review of the collaborative interplay between RAG and\nreasoning, clearly defining \"reasoning\" within the RAG context. It construct a\ncomprehensive taxonomy encompassing multi-dimensional collaborative objectives,\nrepresentative paradigms, and technical implementations, and analyze the\nbidirectional synergy methods. Additionally, we critically evaluate current\nlimitations in RAG assessment, including the absence of intermediate\nsupervision for multi-step reasoning and practical challenges related to\ncost-risk trade-offs. To bridge theory and practice, we provide practical\nguidelines tailored to diverse real-world applications. Finally, we identify\npromising research directions, such as graph-based knowledge integration,\nhybrid model collaboration, and RL-driven optimization. Overall, this work\npresents a theoretical framework and practical foundation to advance RAG\nsystems in academia and industry, fostering the next generation of RAG\nsolutions."
                },
                "authors": [
                    {
                        "name": "Yunfan Gao"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Yijie Zhong"
                    },
                    {
                        "name": "Yuxi Bi"
                    },
                    {
                        "name": "Ming Xue"
                    },
                    {
                        "name": "Haofen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haofen Wang"
                },
                "author": "Haofen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17497v1",
                "updated": "2025-04-24T12:38:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    38,
                    3,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T12:38:03Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    38,
                    3,
                    3,
                    114,
                    0
                ],
                "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or\n  Enhanced Virtual Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining GCN Structural Learning with LLM Chemical Knowledge for or\n  Enhanced Virtual Screening"
                },
                "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines."
                },
                "authors": [
                    {
                        "name": "Radia Berreziga"
                    },
                    {
                        "name": "Mohammed Brahimi"
                    },
                    {
                        "name": "Khairedine Kraim"
                    },
                    {
                        "name": "Hamid Azzoune"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Azzoune"
                },
                "author": "Hamid Azzoune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14614v2",
                "updated": "2025-04-24T12:24:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    24,
                    32,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-20T13:44:39Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    13,
                    44,
                    39,
                    6,
                    110,
                    0
                ],
                "title": "Measurement-device-independent quantum key distribution with asymmetric\n  sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement-device-independent quantum key distribution with asymmetric\n  sources"
                },
                "summary": "Measurement-device-independent quantum key distribution (MDI-QKD), which\neliminates all the attacks from the eavesdropper to the measurement party, has\nbeen one of the most promising technology for the implementation of end-to-end\nquantum networks. In practice, the asymmetry of both sources and channels is\ngenerally inevitable. Therefore, we propose a theory to analyze the performance\nwhen any two MDI users in networks communicates using asymmetric sources in\ndistinct single or multiple temporal modes. As a specific application, we model\nto obtain the key rate of MDI-QKD with weak coherent pulse source and\nspontaneous parametric down-conversion source, and compare the performance to\nthe cases with symmetric (i.e. identical) sources. The result demonstrates that\nthe actual performance does not degrade due to the asymmetry of sources. In\ncontrary, it maintains at a good level over the entire distance we study. This\nwork provides a theoretical basis for analyzing and optimizing MDI-QKD networks\nwith asymmetric sources, and thus paving the way for the practical deployment\nof completely asymmetric MDI-QKD networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement-device-independent quantum key distribution (MDI-QKD), which\neliminates all the attacks from the eavesdropper to the measurement party, has\nbeen one of the most promising technology for the implementation of end-to-end\nquantum networks. In practice, the asymmetry of both sources and channels is\ngenerally inevitable. Therefore, we propose a theory to analyze the performance\nwhen any two MDI users in networks communicates using asymmetric sources in\ndistinct single or multiple temporal modes. As a specific application, we model\nto obtain the key rate of MDI-QKD with weak coherent pulse source and\nspontaneous parametric down-conversion source, and compare the performance to\nthe cases with symmetric (i.e. identical) sources. The result demonstrates that\nthe actual performance does not degrade due to the asymmetry of sources. In\ncontrary, it maintains at a good level over the entire distance we study. This\nwork provides a theoretical basis for analyzing and optimizing MDI-QKD networks\nwith asymmetric sources, and thus paving the way for the practical deployment\nof completely asymmetric MDI-QKD networks."
                },
                "authors": [
                    {
                        "name": "Jia-Jv Deng"
                    },
                    {
                        "name": "Feng-Yu Lu"
                    },
                    {
                        "name": "Zhen-Qiu Zhong"
                    },
                    {
                        "name": "Xiao-Hai Zhan"
                    },
                    {
                        "name": "Zhen-Qiang Yin"
                    },
                    {
                        "name": "Shuang Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "De-Yong He"
                    },
                    {
                        "name": "Guang-Can Guo"
                    },
                    {
                        "name": "Zheng-Fu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zheng-Fu Han"
                },
                "arxiv_affiliation": "Hefei National Laboratory, University of Science and Technology of China, Hefei, China",
                "author": "Zheng-Fu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17480v1",
                "updated": "2025-04-24T12:15:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    15,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T12:15:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    15,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation"
                },
                "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."
                },
                "authors": [
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Shunfan Zhengc"
                    },
                    {
                        "name": "Linlin Wanga"
                    },
                    {
                        "name": "Xiaoling Wang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17461v1",
                "updated": "2025-04-24T11:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    52,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    52,
                    13,
                    3,
                    114,
                    0
                ],
                "title": "Evaluating Time Series Models for Urban Wastewater Management:\n  Predictive Performance, Model Complexity and Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Time Series Models for Urban Wastewater Management:\n  Predictive Performance, Model Complexity and Resilience"
                },
                "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository."
                },
                "authors": [
                    {
                        "name": "Vipin Singh"
                    },
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Teodor Chiaburu"
                    },
                    {
                        "name": "Felix Biessmann"
                    }
                ],
                "author_detail": {
                    "name": "Felix Biessmann"
                },
                "author": "Felix Biessmann",
                "arxiv_comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17454v1",
                "updated": "2025-04-24T11:35:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    35,
                    43,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    35,
                    43,
                    3,
                    114,
                    0
                ],
                "title": "Adaptive Orchestration of Modular Generative Information Access Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Orchestration of Modular Generative Information Access Systems"
                },
                "summary": "Advancements in large language models (LLMs) have driven the emergence of\ncomplex new systems to provide access to information, that we will collectively\nrefer to as modular generative information access (GenIA) systems. They\nintegrate a broad and evolving range of specialized components, including LLMs,\nretrieval models, and a heterogeneous set of sources and tools. While\nmodularity offers flexibility, it also raises critical challenges: How can we\nsystematically characterize the space of possible modules and their\ninteractions? How can we automate and optimize interactions among these\nheterogeneous components? And, how do we enable this modular system to\ndynamically adapt to varying user query requirements and evolving module\ncapabilities? In this perspective paper, we argue that the architecture of\nfuture modular generative information access systems will not just assemble\npowerful components, but enable a self-organizing system through real-time\nadaptive orchestration -- where components' interactions are dynamically\nconfigured for each user input, maximizing information relevance while\nminimizing computational overhead. We give provisional answers to the questions\nraised above with a roadmap that depicts the key principles and methods for\ndesigning such an adaptive modular system. We identify pressing challenges, and\npropose avenues for addressing them in the years ahead. This perspective urges\nthe IR community to rethink modular system designs for developing adaptive,\nself-optimizing, and future-ready architectures that evolve alongside their\nrapidly advancing underlying technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in large language models (LLMs) have driven the emergence of\ncomplex new systems to provide access to information, that we will collectively\nrefer to as modular generative information access (GenIA) systems. They\nintegrate a broad and evolving range of specialized components, including LLMs,\nretrieval models, and a heterogeneous set of sources and tools. While\nmodularity offers flexibility, it also raises critical challenges: How can we\nsystematically characterize the space of possible modules and their\ninteractions? How can we automate and optimize interactions among these\nheterogeneous components? And, how do we enable this modular system to\ndynamically adapt to varying user query requirements and evolving module\ncapabilities? In this perspective paper, we argue that the architecture of\nfuture modular generative information access systems will not just assemble\npowerful components, but enable a self-organizing system through real-time\nadaptive orchestration -- where components' interactions are dynamically\nconfigured for each user input, maximizing information relevance while\nminimizing computational overhead. We give provisional answers to the questions\nraised above with a roadmap that depicts the key principles and methods for\ndesigning such an adaptive modular system. We identify pressing challenges, and\npropose avenues for addressing them in the years ahead. This perspective urges\nthe IR community to rethink modular system designs for developing adaptive,\nself-optimizing, and future-ready architectures that evolve alongside their\nrapidly advancing underlying technologies."
                },
                "authors": [
                    {
                        "name": "Mohanna Hoveyda"
                    },
                    {
                        "name": "Harrie Oosterhuis"
                    },
                    {
                        "name": "Arjen P. de Vries"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    }
                ],
                "author_detail": {
                    "name": "Faegheh Hasibi"
                },
                "author": "Faegheh Hasibi",
                "arxiv_doi": "10.1145/3726302.3730351",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730351",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.17454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGIR 2025 Perspective Paper Track",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17445v1",
                "updated": "2025-04-24T11:14:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    14,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T11:14:13Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    14,
                    13,
                    3,
                    114,
                    0
                ],
                "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text\n  Augmentation"
                },
                "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance."
                },
                "authors": [
                    {
                        "name": "Anna Lieb"
                    },
                    {
                        "name": "Maneesh Arora"
                    },
                    {
                        "name": "Eni Mustafaraj"
                    }
                ],
                "author_detail": {
                    "name": "Eni Mustafaraj"
                },
                "author": "Eni Mustafaraj",
                "arxiv_comment": "Presented at IC2S2 2024 in Philadelphia, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03624v2",
                "updated": "2025-04-24T11:05:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    11,
                    5,
                    58,
                    3,
                    114,
                    0
                ],
                "published": "2024-08-07T08:34:48Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    8,
                    34,
                    48,
                    2,
                    220,
                    0
                ],
                "title": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging"
                },
                "summary": "Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios."
                },
                "authors": [
                    {
                        "name": "Senkang Hu"
                    },
                    {
                        "name": "Zhengru Fang"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Sam Kwong"
                    }
                ],
                "author_detail": {
                    "name": "Sam Kwong"
                },
                "author": "Sam Kwong",
                "arxiv_comment": "Accepted by IEEE Transactions on Mobile Computing (TMC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11539v2",
                "updated": "2025-04-24T10:58:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    58,
                    6,
                    3,
                    114,
                    0
                ],
                "published": "2024-10-15T12:14:01Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    1,
                    1,
                    289,
                    0
                ],
                "title": "Transfer Learning with Foundational Models for Time Series Forecasting\n  using Low-Rank Adaptations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning with Foundational Models for Time Series Forecasting\n  using Low-Rank Adaptations"
                },
                "summary": "Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational Models are an emerging widely used technique of GenAI. These\nmodels are distinguished by their scalability and the ease with which they can\nbe adapted through the exploitation of Transfer Learning. The availability of\nhigh computational power and large datasets have supported their development,\nachieving a high generalization capacity due to the enormous and heterogeneous\namounts of data used in their initial training. These characteristics\ncontribute to a solid base that can be adapted or adjusted to a wide range of\ntasks, increasing their applicability. This study proposes the methodology\nLLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for\nthe Time Series Forecasting task. An adequate time-series prompting schema and\nLow-Rank Adaptations are used to enhance the knowledge of the model with\ndiverse time series datasets, known as the fine-tuning phase. A study divided\nin two stages has been performed for evaluating the effectiveness of the\nproposed methodology. Initially, a comparison was made between the performance\nof LLIAM and different state-of-the-art DL algorithms, including Recurrent\nNeural Networks and Temporal Convolutional Networks, as well as a LLM-based\nmethod, TimeLLM. Following this, a zero-shot study is presented in order to\nevaluate the generalization capacity of the proposed methodology with time\nseries datasets from unknown domains not considered in the model training. The\noutcomes of this investigation demonstrate the efficacy of LLIAM, highlighting\nthat this straightforward and general approach can attain competent results\nwithout the necessity for applying complex modifications. This work also\nencourages the use of available resources (such as these pre-trained models)\nand efficient fine-tuning techniques to avoid unnecessary and costly training,\nnarrowing the gap between the goals of traditional AI and Green AI."
                },
                "authors": [
                    {
                        "name": "M. Germán-Morales"
                    },
                    {
                        "name": "A. J. Rivera-Rivas"
                    },
                    {
                        "name": "M. J. del Jesus Díaz"
                    },
                    {
                        "name": "C. J. Carmona"
                    }
                ],
                "author_detail": {
                    "name": "C. J. Carmona"
                },
                "author": "C. J. Carmona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17432v1",
                "updated": "2025-04-24T10:51:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    51,
                    52,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:51:52Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    51,
                    52,
                    3,
                    114,
                    0
                ],
                "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs"
                },
                "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities."
                },
                "authors": [
                    {
                        "name": "Tiancheng Gu"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Weidong Cai"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "13 pages, 8 figures, Project page: https://garygutc.github.io/UniME",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17426v1",
                "updated": "2025-04-24T10:30:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    30,
                    40,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:30:40Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    30,
                    40,
                    3,
                    114,
                    0
                ],
                "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in\n  Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Leveraging Large Language Model Summaries for Topic Modeling in\n  Source Code"
                },
                "summary": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories."
                },
                "authors": [
                    {
                        "name": "Michele Carissimi"
                    },
                    {
                        "name": "Martina Saletta"
                    },
                    {
                        "name": "Claudio Ferretti"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Ferretti"
                },
                "author": "Claudio Ferretti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17421v1",
                "updated": "2025-04-24T10:24:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    24,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T10:24:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    10,
                    24,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Towards Harnessing the Collaborative Power of Large and Small Models for\n  Domain Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Harnessing the Collaborative Power of Large and Small Models for\n  Domain Tasks"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bingjie Yan"
                    },
                    {
                        "name": "Tianyuan Zou"
                    },
                    {
                        "name": "Jianqing Zhang"
                    },
                    {
                        "name": "Zixuan Gu"
                    },
                    {
                        "name": "Jianbing Ding"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xiaozhou Ye"
                    },
                    {
                        "name": "Ye Ouyang"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ya-Qin Zhang"
                },
                "author": "Ya-Qin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17402v1",
                "updated": "2025-04-24T09:47:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    47,
                    14,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:47:14Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    47,
                    14,
                    3,
                    114,
                    0
                ],
                "title": "Assessing the Capability of Large Language Models for Domain-Specific\n  Ontology Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Capability of Large Language Models for Domain-Specific\n  Ontology Generation"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques."
                },
                "authors": [
                    {
                        "name": "Anna Sofia Lippolis"
                    },
                    {
                        "name": "Mohammad Javad Saeedizade"
                    },
                    {
                        "name": "Robin Keskisarkka"
                    },
                    {
                        "name": "Aldo Gangemi"
                    },
                    {
                        "name": "Eva Blomqvist"
                    },
                    {
                        "name": "Andrea Giovanni Nuzzolese"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Giovanni Nuzzolese"
                },
                "author": "Andrea Giovanni Nuzzolese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19151v2",
                "updated": "2025-04-24T09:40:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    40,
                    21,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-27T21:27:32Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    21,
                    27,
                    32,
                    4,
                    271,
                    0
                ],
                "title": "Can LLMs Really Learn to Translate a Low-Resource Language from One\n  Grammar Book?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Really Learn to Translate a Low-Resource Language from One\n  Grammar Book?"
                },
                "summary": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests that prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an XLR language unseen by LLMs - a noteworthy\ncase of linguistics helping an NLP task. We investigate the source of this\ntranslation ability, finding almost all improvements stem from the book's\nparallel examples rather than its grammatical explanations. We find similar\nresults for Nepali and Guarani, seen low-resource languages, and we achieve\nperformance comparable to an LLM with a grammar book by simply fine-tuning an\nencoder-decoder translation model. We then investigate where grammar books help\nby testing two linguistic tasks, grammaticality judgment and gloss prediction,\nand we explore what kind of grammatical knowledge helps by introducing a\ntypological feature prompt that achieves leading results on these more relevant\ntasks. We thus emphasise the importance of task-appropriate data for XLR\nlanguages: parallel examples for translation, and grammatical data for\nlinguistic tasks. As we find no evidence that long-context LLMs can make\neffective use of grammatical explanations for XLR translation, we conclude data\ncollection for multilingual XLR tasks such as translation is best focused on\nparallel data over linguistic description.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely low-resource (XLR) languages lack substantial corpora for training\nNLP models, motivating the use of all available resources such as dictionaries\nand grammar books. Machine Translation from One Book (Tanzer et al., 2024)\nsuggests that prompting long-context LLMs with one grammar book enables\nEnglish-Kalamang translation, an XLR language unseen by LLMs - a noteworthy\ncase of linguistics helping an NLP task. We investigate the source of this\ntranslation ability, finding almost all improvements stem from the book's\nparallel examples rather than its grammatical explanations. We find similar\nresults for Nepali and Guarani, seen low-resource languages, and we achieve\nperformance comparable to an LLM with a grammar book by simply fine-tuning an\nencoder-decoder translation model. We then investigate where grammar books help\nby testing two linguistic tasks, grammaticality judgment and gloss prediction,\nand we explore what kind of grammatical knowledge helps by introducing a\ntypological feature prompt that achieves leading results on these more relevant\ntasks. We thus emphasise the importance of task-appropriate data for XLR\nlanguages: parallel examples for translation, and grammatical data for\nlinguistic tasks. As we find no evidence that long-context LLMs can make\neffective use of grammatical explanations for XLR translation, we conclude data\ncollection for multilingual XLR tasks such as translation is best focused on\nparallel data over linguistic description."
                },
                "authors": [
                    {
                        "name": "Seth Aycock"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Khalil Sima'an"
                    }
                ],
                "author_detail": {
                    "name": "Khalil Sima'an"
                },
                "author": "Khalil Sima'an",
                "arxiv_comment": "Accepted at ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17384v1",
                "updated": "2025-04-24T09:08:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    8,
                    24,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T09:08:24Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    8,
                    24,
                    3,
                    114,
                    0
                ],
                "title": "On the workflow, opportunities and challenges of developing foundation\n  model in geophysics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the workflow, opportunities and challenges of developing foundation\n  model in geophysics"
                },
                "summary": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field."
                },
                "authors": [
                    {
                        "name": "Hanlin Sheng"
                    },
                    {
                        "name": "Xinming Wu"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Haibin Di"
                    },
                    {
                        "name": "Sergey Fomel"
                    },
                    {
                        "name": "Jintao Li"
                    },
                    {
                        "name": "Xu Si"
                    }
                ],
                "author_detail": {
                    "name": "Xu Si"
                },
                "author": "Xu Si",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17376v1",
                "updated": "2025-04-24T08:50:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    50,
                    1,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:50:01Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    50,
                    1,
                    3,
                    114,
                    0
                ],
                "title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and\n  Hardware Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and\n  Hardware Acceleration"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second."
                },
                "authors": [
                    {
                        "name": "Maoyang Xiang"
                    },
                    {
                        "name": "Ramesh Fernando"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16688v2",
                "updated": "2025-04-24T08:37:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    37,
                    38,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T13:19:35Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    19,
                    35,
                    2,
                    113,
                    0
                ],
                "title": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation\n  for 6G: MLR, ANOVA, and Residual Distribution Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation\n  for 6G: MLR, ANOVA, and Residual Distribution Analysis"
                },
                "summary": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models with one to five components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models with one to five components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments."
                },
                "authors": [
                    {
                        "name": "Nahshon Mokua Obiri"
                    },
                    {
                        "name": "Kristof Van Laerhoven"
                    }
                ],
                "author_detail": {
                    "name": "Kristof Van Laerhoven"
                },
                "author": "Kristof Van Laerhoven",
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media. This is the accepted version of the article: To appear in the\n  2025 Joint European Conference on Networks and Communications & 6G Summit\n  (EuCNC/6G Summit)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17366v1",
                "updated": "2025-04-24T08:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    27,
                    48,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:27:48Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    27,
                    48,
                    3,
                    114,
                    0
                ],
                "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from\n  Live Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from\n  Live Streams"
                },
                "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."
                },
                "authors": [
                    {
                        "name": "Yongxuan Wu"
                    },
                    {
                        "name": "Runyu Chen"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Hongjin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Hongjin Qian"
                },
                "author": "Hongjin Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17360v1",
                "updated": "2025-04-24T08:21:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    21,
                    4,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:21:04Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    21,
                    4,
                    3,
                    114,
                    0
                ],
                "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in\n  Healthcare"
                },
                "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4."
                },
                "authors": [
                    {
                        "name": "Jose G. Moreno"
                    },
                    {
                        "name": "Jesus Lovon"
                    },
                    {
                        "name": "M'Rick Robin-Charlet"
                    },
                    {
                        "name": "Christine Damase-Michel"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "arxiv_affiliation": "IRIT-IRIS",
                "author": "Lynda Tamine",
                "arxiv_journal_ref": "Workshop CL4Health @ NAACL 2025, May 2025, Albuquerque, New\n  Mexico, United States",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17356v1",
                "updated": "2025-04-24T08:16:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    16,
                    36,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T08:16:36Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    16,
                    36,
                    3,
                    114,
                    0
                ],
                "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning"
                },
                "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhang"
                    },
                    {
                        "name": "Xiaohan Huang"
                    },
                    {
                        "name": "Yi Du"
                    },
                    {
                        "name": "Ziyue Qiao"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Zhen Meng"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Meng Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Xiao"
                },
                "author": "Meng Xiao",
                "arxiv_comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17334v1",
                "updated": "2025-04-24T07:52:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    52,
                    9,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:52:09Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    52,
                    9,
                    3,
                    114,
                    0
                ],
                "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent"
                },
                "summary": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A data story typically integrates data facts from multiple perspectives and\nstances to construct a comprehensive and objective narrative. However,\nretrieving these facts demands time for data search and challenges the\ncreator's analytical skills. In this work, we introduce DataScout, an\ninteractive system that automatically performs reasoning and stance-based data\nfacts retrieval to augment the user's statement. Particularly, DataScout\nleverages an LLM-based agent to construct a retrieval tree, enabling\ncollaborative control of its expansion between users and the agent. The\ninterface visualizes the retrieval tree as a mind map that eases users to\nintuitively steer the retrieval direction and effectively engage in reasoning\nand analysis. We evaluate the proposed system through case studies and in-depth\nexpert interviews. Our evaluation demonstrates that DataScout can effectively\nretrieve multifaceted data facts from different stances, helping users verify\ntheir statements and enhance the credibility of their stories."
                },
                "authors": [
                    {
                        "name": "Chuer Chen"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Danqing Shi"
                    },
                    {
                        "name": "Shixiong Cao"
                    },
                    {
                        "name": "Nan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Nan Cao"
                },
                "author": "Nan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17332v1",
                "updated": "2025-04-24T07:48:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    26,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:48:26Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    26,
                    3,
                    114,
                    0
                ],
                "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation\n  Detection"
                },
                "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Lu Yuan"
                    },
                    {
                        "name": "Zhengxuan Zhang"
                    },
                    {
                        "name": "Qing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qing Zhao"
                },
                "author": "Qing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17331v1",
                "updated": "2025-04-24T07:48:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    9,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:48:09Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    48,
                    9,
                    3,
                    114,
                    0
                ],
                "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual\n  Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual\n  Reality"
                },
                "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility."
                },
                "authors": [
                    {
                        "name": "Süleyman Özdel"
                    },
                    {
                        "name": "Kadir Burak Buldu"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "name": "Efe Bozkir"
                    }
                ],
                "author_detail": {
                    "name": "Efe Bozkir"
                },
                "author": "Efe Bozkir",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16427v2",
                "updated": "2025-04-24T07:35:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    35,
                    3,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-23T05:25:13Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    25,
                    13,
                    2,
                    113,
                    0
                ],
                "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark"
                },
                "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA."
                },
                "authors": [
                    {
                        "name": "Hanlei Zhang"
                    },
                    {
                        "name": "Zhuohang Li"
                    },
                    {
                        "name": "Yeshuang Zhu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Peiwu Wang"
                    },
                    {
                        "name": "Haige Zhu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jinchao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Zhang"
                },
                "author": "Jinchao Zhang",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13471v2",
                "updated": "2025-04-24T07:30:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    30,
                    24,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-18T05:25:22Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    25,
                    22,
                    4,
                    108,
                    0
                ],
                "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient\n  LLMs"
                },
                "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combines techniques like rejection fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress models to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combines techniques like rejection fine-tuning, reinforcement\nlearning, and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress models to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas."
                },
                "authors": [
                    {
                        "name": "Jiliang Ni"
                    },
                    {
                        "name": "Jiachen Pu"
                    },
                    {
                        "name": "Zhongyi Yang"
                    },
                    {
                        "name": "Kun Zhou"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Xiaoliang Xiao"
                    },
                    {
                        "name": "Dakui Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jingfeng Luo"
                    },
                    {
                        "name": "Conggang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Conggang Hu"
                },
                "author": "Conggang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15231v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15231v4",
                "updated": "2025-04-24T07:21:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    21,
                    44,
                    3,
                    114,
                    0
                ],
                "published": "2024-06-21T15:19:21Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    15,
                    19,
                    21,
                    4,
                    173,
                    0
                ],
                "title": "Synthetic Lyrics Detection Across Languages and Genres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Lyrics Detection Across Languages and Genres"
                },
                "summary": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users."
                },
                "authors": [
                    {
                        "name": "Yanis Labrak"
                    },
                    {
                        "name": "Markus Frohmann"
                    },
                    {
                        "name": "Gabriel Meseguer-Brocal"
                    },
                    {
                        "name": "Elena V. Epure"
                    }
                ],
                "author_detail": {
                    "name": "Elena V. Epure"
                },
                "author": "Elena V. Epure",
                "arxiv_comment": "Published in the TrustNLP Workshop at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15231v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15231v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17311v1",
                "updated": "2025-04-24T07:12:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    12,
                    37,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T07:12:37Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    7,
                    12,
                    37,
                    3,
                    114,
                    0
                ],
                "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for\n  Robustness Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for\n  Robustness Evaluation"
                },
                "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."
                },
                "authors": [
                    {
                        "name": "Yulia Otmakhova"
                    },
                    {
                        "name": "Hung Thinh Truong"
                    },
                    {
                        "name": "Rahmad Mahendra"
                    },
                    {
                        "name": "Zenan Zhai"
                    },
                    {
                        "name": "Rongxin Zhu"
                    },
                    {
                        "name": "Daniel Beck"
                    },
                    {
                        "name": "Jey Han Lau"
                    }
                ],
                "author_detail": {
                    "name": "Jey Han Lau"
                },
                "author": "Jey Han Lau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17304v1",
                "updated": "2025-04-24T06:59:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    59,
                    16,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T06:59:16Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    59,
                    16,
                    3,
                    114,
                    0
                ],
                "title": "You Are What You Bought: Generating Customer Personas for E-commerce\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Are What You Bought: Generating Customer Personas for E-commerce\n  Applications"
                },
                "summary": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K."
                },
                "authors": [
                    {
                        "name": "Yimin Shi"
                    },
                    {
                        "name": "Yang Fei"
                    },
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Haixun Wang"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "arxiv_doi": "10.1145/3726302.3730118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.17304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17295v1",
                "updated": "2025-04-24T06:43:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    43,
                    29,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T06:43:29Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    43,
                    29,
                    3,
                    114,
                    0
                ],
                "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance\n  Domain Using Object-Centric Process Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Enhanced Business Process Automation: A Case Study in the Insurance\n  Domain Using Object-Centric Process Mining"
                },
                "summary": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations."
                },
                "authors": [
                    {
                        "name": "Shahrzad Khayatbashi"
                    },
                    {
                        "name": "Viktor Sjölind"
                    },
                    {
                        "name": "Anders Granåker"
                    },
                    {
                        "name": "Amin Jalali"
                    }
                ],
                "author_detail": {
                    "name": "Amin Jalali"
                },
                "author": "Amin Jalali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17086v2",
                "updated": "2025-04-24T06:43:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    43,
                    15,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-24T12:05:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    12,
                    5,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "Automatically Evaluating the Paper Reviewing Capability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Evaluating the Paper Reviewing Capability of Large\n  Language Models"
                },
                "summary": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time."
                },
                "authors": [
                    {
                        "name": "Hyungyu Shin"
                    },
                    {
                        "name": "Jingyu Tang"
                    },
                    {
                        "name": "Yoonjoo Lee"
                    },
                    {
                        "name": "Nayoung Kim"
                    },
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Ji Yong Cho"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01163v2",
                "updated": "2025-04-24T06:33:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    33,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-01-02T09:33:13Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    33,
                    13,
                    3,
                    2,
                    0
                ],
                "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer"
                },
                "summary": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiajun Deng"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Feras Dayoub"
                    },
                    {
                        "name": "Ian Reid"
                    }
                ],
                "author_detail": {
                    "name": "Ian Reid"
                },
                "author": "Ian Reid",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15840v2",
                "updated": "2025-04-24T06:30:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    30,
                    19,
                    3,
                    114,
                    0
                ],
                "published": "2025-03-20T04:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    4,
                    40,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Automatic Generation of Safety-compliant Linear Temporal Logic via Large\n  Language Model: A Self-supervised Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Generation of Safety-compliant Linear Temporal Logic via Large\n  Language Model: A Self-supervised Framework"
                },
                "summary": "Converting high-level tasks described by natural language into formal\nspecifications like Linear Temporal Logic (LTL) is a key step towards providing\nformal safety guarantees over cyber-physical systems (CPS). While the\ncompliance of the formal specifications themselves against the safety\nrestrictions imposed on CPS is crucial for ensuring safety, most existing works\nonly focus on translation consistency between natural languages and formal\nspecifications. In this paper, we introduce AutoSafeLTL, a self-supervised\nframework that utilizes large language models (LLMs) to automate the generation\nof LTL specifications complying with a set of safety restrictions while\npreserving their logical consistency and semantic accuracy. As a key insight,\nour framework integrates Language Inclusion check with an automated\ncounterexample-guided modification mechanism to ensure the safety-compliance of\nthe resulting LTL specifications. In particular, we develop 1) an\nLLM-as-an-Aligner, which performs atomic proposition matching between generated\nLTL specifications and safety restrictions to enforce semantic alignment; and\n2) an LLM-as-a-Critic, which automates LTL specification refinement by\ninterpreting counterexamples derived from Language Inclusion checks.\nExperimental results demonstrate that our architecture effectively guarantees\nsafety-compliance for the generated LTL specifications, achieving a 0%\nviolation rate against imposed safety restrictions. This shows the potential of\nour work in synergizing AI and formal verification techniques, enhancing\nsafety-aware specification generation and automatic verification for both AI\nand critical CPS applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting high-level tasks described by natural language into formal\nspecifications like Linear Temporal Logic (LTL) is a key step towards providing\nformal safety guarantees over cyber-physical systems (CPS). While the\ncompliance of the formal specifications themselves against the safety\nrestrictions imposed on CPS is crucial for ensuring safety, most existing works\nonly focus on translation consistency between natural languages and formal\nspecifications. In this paper, we introduce AutoSafeLTL, a self-supervised\nframework that utilizes large language models (LLMs) to automate the generation\nof LTL specifications complying with a set of safety restrictions while\npreserving their logical consistency and semantic accuracy. As a key insight,\nour framework integrates Language Inclusion check with an automated\ncounterexample-guided modification mechanism to ensure the safety-compliance of\nthe resulting LTL specifications. In particular, we develop 1) an\nLLM-as-an-Aligner, which performs atomic proposition matching between generated\nLTL specifications and safety restrictions to enforce semantic alignment; and\n2) an LLM-as-a-Critic, which automates LTL specification refinement by\ninterpreting counterexamples derived from Language Inclusion checks.\nExperimental results demonstrate that our architecture effectively guarantees\nsafety-compliance for the generated LTL specifications, achieving a 0%\nviolation rate against imposed safety restrictions. This shows the potential of\nour work in synergizing AI and formal verification techniques, enhancing\nsafety-aware specification generation and automatic verification for both AI\nand critical CPS applications."
                },
                "authors": [
                    {
                        "name": "Junle Li"
                    },
                    {
                        "name": "Meiqi Tian"
                    },
                    {
                        "name": "Bingzhuo Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhuo Zhong"
                },
                "author": "Bingzhuo Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17287v1",
                "updated": "2025-04-24T06:28:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    28,
                    18,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T06:28:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    28,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "Combining Static and Dynamic Approaches for Mining and Testing\n  Constraints for RESTful API Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Static and Dynamic Approaches for Mining and Testing\n  Constraints for RESTful API Testing"
                },
                "summary": "In API testing, deriving logical constraints on API response bodies is\ncrucial in generating the test cases to cover various aspects of RESTful APIs.\nHowever, existing approaches are limited to dynamic analysis in which\nconstraints are extracted from the execution of APIs as part of the system\nunder test. The key limitation of such a dynamic approach is its\nunder-estimation in which inputs in API executions are not sufficiently diverse\nto uncover actual constraints on API response bodies. In this paper, we propose\nto combine a novel static analysis approach (in which the constraints for API\nresponse bodies are mined from API specifications), with the dynamic approach\n(which relies on API execution data). We leverage large language models (LLMs)\nto comprehend the API specifications, mine constraints for response bodies, and\ngenerate test cases. To reduce LLMs' hallucination, we apply an\nObservation-Confirmation (OC) scheme which uses initial prompts to\ncontextualize constraints. %, allowing subsequent prompts to more accurately\nconfirm their presence. Our empirical results show that~LLMs with OC prompting\nachieve high precision in constraint mining with the average of 91.2%. When\ncombining static and dynamic analysis, our tool, RBCTest , achieves a precision\nof 78.5%. RBCTest detects 107 constraints that the dynamic approach misses and\n46 more precise constraints. We also use its generated test cases to detect 21\nmismatches between the API specification and actual response data for 8\nreal-world APIs. Four of the mismatches were, in fact, reported in developers'\nforums.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In API testing, deriving logical constraints on API response bodies is\ncrucial in generating the test cases to cover various aspects of RESTful APIs.\nHowever, existing approaches are limited to dynamic analysis in which\nconstraints are extracted from the execution of APIs as part of the system\nunder test. The key limitation of such a dynamic approach is its\nunder-estimation in which inputs in API executions are not sufficiently diverse\nto uncover actual constraints on API response bodies. In this paper, we propose\nto combine a novel static analysis approach (in which the constraints for API\nresponse bodies are mined from API specifications), with the dynamic approach\n(which relies on API execution data). We leverage large language models (LLMs)\nto comprehend the API specifications, mine constraints for response bodies, and\ngenerate test cases. To reduce LLMs' hallucination, we apply an\nObservation-Confirmation (OC) scheme which uses initial prompts to\ncontextualize constraints. %, allowing subsequent prompts to more accurately\nconfirm their presence. Our empirical results show that~LLMs with OC prompting\nachieve high precision in constraint mining with the average of 91.2%. When\ncombining static and dynamic analysis, our tool, RBCTest , achieves a precision\nof 78.5%. RBCTest detects 107 constraints that the dynamic approach misses and\n46 more precise constraints. We also use its generated test cases to detect 21\nmismatches between the API specification and actual response data for 8\nreal-world APIs. Four of the mismatches were, in fact, reported in developers'\nforums."
                },
                "authors": [
                    {
                        "name": "Hieu Huynh"
                    },
                    {
                        "name": "Tri Le"
                    },
                    {
                        "name": "Vu Nguyen"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Tien N. Nguyen"
                },
                "author": "Tien N. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17280v1",
                "updated": "2025-04-24T06:14:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    14,
                    1,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T06:14:01Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    6,
                    14,
                    1,
                    3,
                    114,
                    0
                ],
                "title": "EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy"
                },
                "summary": "The field of keypoint extraction, which is essential for vision applications\nlike Structure from Motion (SfM) and Simultaneous Localization and Mapping\n(SLAM), has evolved from relying on handcrafted methods to leveraging deep\nlearning techniques. While deep learning approaches have significantly improved\nperformance, they often incur substantial computational costs, limiting their\ndeployment in real-time edge applications. Efforts to create lightweight neural\nnetworks have seen some success, yet they often result in trade-offs between\nefficiency and accuracy. Additionally, the high-dimensional descriptors\ngenerated by these networks poses challenges for distributed applications\nrequiring efficient communication and coordination, highlighting the need for\ncompact yet competitively accurate descriptors. In this paper, we present\nEdgePoint2, a series of lightweight keypoint detection and description neural\nnetworks specifically tailored for edge computing applications on embedded\nsystem. The network architecture is optimized for efficiency without\nsacrificing accuracy. To train compact descriptors, we introduce a combination\nof Orthogonal Procrustes loss and similarity loss, which can serve as a general\napproach for hypersphere embedding distillation tasks. Additionally, we offer\n14 sub-models to satisfy diverse application requirements. Our experiments\ndemonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)\naccuracy and efficiency across various challenging scenarios while employing\nlower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2\noffers significant advantages in flexibility, robustness, and versatility.\nConsequently, EdgePoint2 emerges as a highly competitive option for visual\ntasks, especially in contexts demanding adaptability to diverse computational\nand communication constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of keypoint extraction, which is essential for vision applications\nlike Structure from Motion (SfM) and Simultaneous Localization and Mapping\n(SLAM), has evolved from relying on handcrafted methods to leveraging deep\nlearning techniques. While deep learning approaches have significantly improved\nperformance, they often incur substantial computational costs, limiting their\ndeployment in real-time edge applications. Efforts to create lightweight neural\nnetworks have seen some success, yet they often result in trade-offs between\nefficiency and accuracy. Additionally, the high-dimensional descriptors\ngenerated by these networks poses challenges for distributed applications\nrequiring efficient communication and coordination, highlighting the need for\ncompact yet competitively accurate descriptors. In this paper, we present\nEdgePoint2, a series of lightweight keypoint detection and description neural\nnetworks specifically tailored for edge computing applications on embedded\nsystem. The network architecture is optimized for efficiency without\nsacrificing accuracy. To train compact descriptors, we introduce a combination\nof Orthogonal Procrustes loss and similarity loss, which can serve as a general\napproach for hypersphere embedding distillation tasks. Additionally, we offer\n14 sub-models to satisfy diverse application requirements. Our experiments\ndemonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)\naccuracy and efficiency across various challenging scenarios while employing\nlower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2\noffers significant advantages in flexibility, robustness, and versatility.\nConsequently, EdgePoint2 emerges as a highly competitive option for visual\ntasks, especially in contexts demanding adaptability to diverse computational\nand communication constraints."
                },
                "authors": [
                    {
                        "name": "Haodi Yao"
                    },
                    {
                        "name": "Fenghua He"
                    },
                    {
                        "name": "Ning Hao"
                    },
                    {
                        "name": "Chen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Chen Xie"
                },
                "author": "Chen Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17264v1",
                "updated": "2025-04-24T05:48:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    5,
                    48,
                    57,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T05:48:57Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    5,
                    48,
                    57,
                    3,
                    114,
                    0
                ],
                "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer\n  and Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer\n  and Contrastive Learning"
                },
                "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."
                },
                "authors": [
                    {
                        "name": "Zhaolu Kang"
                    },
                    {
                        "name": "Hongtian Cai"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Jinzhe Li"
                    },
                    {
                        "name": "Nanfei Gu"
                    }
                ],
                "author_detail": {
                    "name": "Nanfei Gu"
                },
                "author": "Nanfei Gu",
                "arxiv_comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10100v3",
                "updated": "2025-04-24T05:33:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    5,
                    33,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-01-17T10:39:09Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    39,
                    9,
                    4,
                    17,
                    0
                ],
                "title": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic World Model: A Neural Network Simulator for Robust Policy\n  Optimization in Robotics"
                },
                "summary": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chenhao Li"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24117v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24117v4",
                "updated": "2025-04-24T05:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    5,
                    20,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2024-10-31T16:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    16,
                    46,
                    52,
                    3,
                    305,
                    0
                ],
                "title": "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level\n  Code Translation and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level\n  Code Translation and Validation"
                },
                "summary": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc. We propose\nAlphaTrans, a neuro-symbolic approach to automate repository-level code\ntranslation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans breaks down these projects into 17874\nfragments and translates the entire repository. 96.40% of the translated\nfragments are syntactically correct, and AlphaTrans validates the translations'\nruntime behavior and functional correctness for 27.03% and 25.14% of fragments.\nOn average, the integrated translation and validation take 34 hours to\ntranslate a project, showing its scalability in practice. For the incorrect\ntranslations, AlphaTrans generates a report including existing translation,\nstack trace, test errors, or assertion failures. We provided these artifacts to\ntwo developers to fix the translation bugs in four projects. They were able to\nfix the issues in 20.1 hours on average and achieve all passing tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc. We propose\nAlphaTrans, a neuro-symbolic approach to automate repository-level code\ntranslation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans breaks down these projects into 17874\nfragments and translates the entire repository. 96.40% of the translated\nfragments are syntactically correct, and AlphaTrans validates the translations'\nruntime behavior and functional correctness for 27.03% and 25.14% of fragments.\nOn average, the integrated translation and validation take 34 hours to\ntranslate a project, showing its scalability in practice. For the incorrect\ntranslations, AlphaTrans generates a report including existing translation,\nstack trace, test errors, or assertion failures. We provided these artifacts to\ntwo developers to fix the translation bugs in four projects. They were able to\nfix the issues in 20.1 hours on average and achieve all passing tests."
                },
                "authors": [
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Kaiyao Ke"
                    },
                    {
                        "name": "Mrigank Pawagi"
                    },
                    {
                        "name": "Muhammad Salman Abid"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand",
                "arxiv_doi": "10.1145/3729379",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729379",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.24117v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24117v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in FSE 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17249v1",
                "updated": "2025-04-24T04:58:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    58,
                    47,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T04:58:47Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    58,
                    47,
                    3,
                    114,
                    0
                ],
                "title": "Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and\n  Customizable 3D-printed Humanoid Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and\n  Customizable 3D-printed Humanoid Robot"
                },
                "summary": "Despite significant interest and advancements in humanoid robotics, most\nexisting commercially available hardware remains high-cost, closed-source, and\nnon-transparent within the robotics community. This lack of accessibility and\ncustomization hinders the growth of the field and the broader development of\nhumanoid technologies. To address these challenges and promote democratization\nin humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source\nhumanoid robot designed to be accessible, customizable, and beneficial for the\nentire community. The core of this design is a modular 3D-printed gearbox for\nthe actuators and robot body. All components can be sourced from widely\navailable e-commerce platforms and fabricated using standard desktop 3D\nprinters, keeping the total hardware cost under $5,000 (based on U.S. market\nprices). The design emphasizes modularity and ease of fabrication. To address\nthe inherent limitations of 3D-printed gearboxes, such as reduced strength and\ndurability compared to metal alternatives, we adopted a cycloidal gear design,\nwhich provides an optimal form factor in this context. Extensive testing was\nconducted on the 3D-printed actuators to validate their durability and\nalleviate concerns about the reliability of plastic components. To demonstrate\nthe capabilities of Berkeley Humanoid Lite, we conducted a series of\nexperiments, including the development of a locomotion controller using\nreinforcement learning. These experiments successfully showcased zero-shot\npolicy transfer from simulation to hardware, highlighting the platform's\nsuitability for research validation. By fully open-sourcing the hardware\ndesign, embedded code, and training and deployment frameworks, we aim for\nBerkeley Humanoid Lite to serve as a pivotal step toward democratizing the\ndevelopment of humanoid robotics. All resources are available at\nhttps://lite.berkeley-humanoid.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant interest and advancements in humanoid robotics, most\nexisting commercially available hardware remains high-cost, closed-source, and\nnon-transparent within the robotics community. This lack of accessibility and\ncustomization hinders the growth of the field and the broader development of\nhumanoid technologies. To address these challenges and promote democratization\nin humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source\nhumanoid robot designed to be accessible, customizable, and beneficial for the\nentire community. The core of this design is a modular 3D-printed gearbox for\nthe actuators and robot body. All components can be sourced from widely\navailable e-commerce platforms and fabricated using standard desktop 3D\nprinters, keeping the total hardware cost under $5,000 (based on U.S. market\nprices). The design emphasizes modularity and ease of fabrication. To address\nthe inherent limitations of 3D-printed gearboxes, such as reduced strength and\ndurability compared to metal alternatives, we adopted a cycloidal gear design,\nwhich provides an optimal form factor in this context. Extensive testing was\nconducted on the 3D-printed actuators to validate their durability and\nalleviate concerns about the reliability of plastic components. To demonstrate\nthe capabilities of Berkeley Humanoid Lite, we conducted a series of\nexperiments, including the development of a locomotion controller using\nreinforcement learning. These experiments successfully showcased zero-shot\npolicy transfer from simulation to hardware, highlighting the platform's\nsuitability for research validation. By fully open-sourcing the hardware\ndesign, embedded code, and training and deployment frameworks, we aim for\nBerkeley Humanoid Lite to serve as a pivotal step toward democratizing the\ndevelopment of humanoid robotics. All resources are available at\nhttps://lite.berkeley-humanoid.org."
                },
                "authors": [
                    {
                        "name": "Yufeng Chi"
                    },
                    {
                        "name": "Qiayuan Liao"
                    },
                    {
                        "name": "Junfeng Long"
                    },
                    {
                        "name": "Xiaoyu Huang"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Koushil Sreenath"
                    }
                ],
                "author_detail": {
                    "name": "Koushil Sreenath"
                },
                "author": "Koushil Sreenath",
                "arxiv_comment": "Accepted in Robotics: Science and Systems (RSS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17238v1",
                "updated": "2025-04-24T04:22:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    22,
                    0,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T04:22:00Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    22,
                    0,
                    3,
                    114,
                    0
                ],
                "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn\n  Supportive Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn\n  Supportive Dialogues"
                },
                "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations."
                },
                "authors": [
                    {
                        "name": "Jinfeng Zhou"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Jianing Yin"
                    },
                    {
                        "name": "Yongkang Huang"
                    },
                    {
                        "name": "Yihan Shi"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Rongsheng Zhang"
                    },
                    {
                        "name": "Tangjie Lv"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17232v1",
                "updated": "2025-04-24T03:57:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    57,
                    27,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T03:57:27Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    57,
                    27,
                    3,
                    114,
                    0
                ],
                "title": "Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting,\n  Accident Prediction, and Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting,\n  Accident Prediction, and Image Classification"
                },
                "summary": "This study proposes an integrated machine learning framework for advanced\ntraffic analysis, combining time-series forecasting, classification, and\ncomputer vision techniques. The system utilizes an ARIMA(2,0,1) model for\ntraffic prediction (MAE: 2.1), an XGBoost classifier for accident severity\nclassification (100% accuracy on balanced data), and a Convolutional Neural\nNetwork (CNN) for traffic image classification (92% accuracy). Tested on\ndiverse datasets, the framework outperforms baseline models and identifies key\nfactors influencing accident severity, including weather and road\ninfrastructure. Its modular design supports deployment in smart city systems\nfor real-time monitoring, accident prevention, and resource optimization,\ncontributing to the evolution of intelligent transportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes an integrated machine learning framework for advanced\ntraffic analysis, combining time-series forecasting, classification, and\ncomputer vision techniques. The system utilizes an ARIMA(2,0,1) model for\ntraffic prediction (MAE: 2.1), an XGBoost classifier for accident severity\nclassification (100% accuracy on balanced data), and a Convolutional Neural\nNetwork (CNN) for traffic image classification (92% accuracy). Tested on\ndiverse datasets, the framework outperforms baseline models and identifies key\nfactors influencing accident severity, including weather and road\ninfrastructure. Its modular design supports deployment in smart city systems\nfor real-time monitoring, accident prevention, and resource optimization,\ncontributing to the evolution of intelligent transportation systems."
                },
                "authors": [
                    {
                        "name": "Nivedita M"
                    },
                    {
                        "name": "Yasmeen Shajitha S"
                    }
                ],
                "author_detail": {
                    "name": "Yasmeen Shajitha S"
                },
                "author": "Yasmeen Shajitha S",
                "arxiv_comment": "5 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11114v2",
                "updated": "2025-04-24T03:38:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    38,
                    8,
                    3,
                    114,
                    0
                ],
                "published": "2024-11-17T16:08:34Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    16,
                    8,
                    34,
                    6,
                    322,
                    0
                ],
                "title": "JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of\n  Representation and Circuit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of\n  Representation and Circuit"
                },
                "summary": "Despite the outstanding performance of Large language Models (LLMs) in\ndiverse tasks, they are vulnerable to jailbreak attacks, wherein adversarial\nprompts are crafted to bypass their security mechanisms and elicit unexpected\nresponses. Although jailbreak attacks are prevalent, the understanding of their\nunderlying mechanisms remains limited. Recent studies have explained typical\njailbreaking behavior (e.g., the degree to which the model refuses to respond)\nof LLMs by analyzing representation shifts in their latent space caused by\njailbreak prompts or identifying key neurons that contribute to the success of\njailbreak attacks. However, these studies neither explore diverse jailbreak\npatterns nor provide a fine-grained explanation from the failure of circuit to\nthe changes of representational, leaving significant gaps in uncovering the\njailbreak mechanism. In this paper, we propose JailbreakLens, an interpretation\nframework that analyzes jailbreak mechanisms from both representation (which\nreveals how jailbreaks alter the model's harmfulness perception) and circuit\nperspectives~(which uncovers the causes of these deceptions by identifying key\ncircuits contributing to the vulnerability), tracking their evolution\nthroughout the entire response generation process. We then conduct an in-depth\nevaluation of jailbreak behavior on five mainstream LLMs under seven jailbreak\nstrategies. Our evaluation reveals that jailbreak prompts amplify components\nthat reinforce affirmative responses while suppressing those that produce\nrefusal. This manipulation shifts model representations toward safe clusters to\ndeceive the LLM, leading it to provide detailed responses instead of refusals.\nNotably, we find a strong and consistent correlation between representation\ndeception and activation shift of key circuits across diverse jailbreak methods\nand multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the outstanding performance of Large language Models (LLMs) in\ndiverse tasks, they are vulnerable to jailbreak attacks, wherein adversarial\nprompts are crafted to bypass their security mechanisms and elicit unexpected\nresponses. Although jailbreak attacks are prevalent, the understanding of their\nunderlying mechanisms remains limited. Recent studies have explained typical\njailbreaking behavior (e.g., the degree to which the model refuses to respond)\nof LLMs by analyzing representation shifts in their latent space caused by\njailbreak prompts or identifying key neurons that contribute to the success of\njailbreak attacks. However, these studies neither explore diverse jailbreak\npatterns nor provide a fine-grained explanation from the failure of circuit to\nthe changes of representational, leaving significant gaps in uncovering the\njailbreak mechanism. In this paper, we propose JailbreakLens, an interpretation\nframework that analyzes jailbreak mechanisms from both representation (which\nreveals how jailbreaks alter the model's harmfulness perception) and circuit\nperspectives~(which uncovers the causes of these deceptions by identifying key\ncircuits contributing to the vulnerability), tracking their evolution\nthroughout the entire response generation process. We then conduct an in-depth\nevaluation of jailbreak behavior on five mainstream LLMs under seven jailbreak\nstrategies. Our evaluation reveals that jailbreak prompts amplify components\nthat reinforce affirmative responses while suppressing those that produce\nrefusal. This manipulation shifts model representations toward safe clusters to\ndeceive the LLM, leading it to provide detailed responses instead of refusals.\nNotably, we find a strong and consistent correlation between representation\ndeception and activation shift of key circuits across diverse jailbreak methods\nand multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Zeqing He"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Huiyu Xu"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Qinglong Wang"
                    },
                    {
                        "name": "Rui Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zheng"
                },
                "author": "Rui Zheng",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17226v1",
                "updated": "2025-04-24T03:34:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    34,
                    37,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T03:34:37Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    34,
                    37,
                    3,
                    114,
                    0
                ],
                "title": "FLAG: Formal and LLM-assisted SVA Generation for Formal Specifications\n  of On-Chip Communication Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAG: Formal and LLM-assisted SVA Generation for Formal Specifications\n  of On-Chip Communication Protocols"
                },
                "summary": "Formal specifications of on-chip communication protocols are crucial for\nsystem-on-chip (SoC) design and verification. However, manually constructing\nthese formal specifications from informal documents remains a tedious and\nerror-prone task. Although recent efforts have used Large Language Models\n(LLMs) to generate SystemVerilog Assertion (SVA) properties from design\ndocuments for Register-Transfer Level (RTL) design verification, in our\nexperience these approaches have not shown promise in generating SVA properties\nfor communication protocols. Since protocol specification documents are\nunstructured and ambiguous in nature, LLMs often fail to extract the necessary\ninformation and end up generating irrelevant or even incorrect properties. We\npropose FLAG, a two-stage framework to help construct formal protocol\nspecifications from informal documents. In the first stage, a predefined\ntemplate set is used to generate candidate SVA properties. To avoid missing\nnecessary properties, we develop a grammar-based approach to generate\ncomprehensive template sets that capture critical signal behaviors for various\ncommunication protocols. In the second stage, we utilize unambiguous timing\ndiagrams in conjunction with textual descriptions from the specification\ndocuments to filter out incorrect properties. A formal approach is first\nimplemented to check the candidate properties and filter out those inconsistent\nwith the timing diagrams. An LLM is then consulted to further remove incorrect\nproperties with respect to the textual description, obtaining the final\nproperty set. Experiments on various open-source communication protocols\ndemonstrate the effectiveness of FLAG in generating SVA properties from\ninformal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal specifications of on-chip communication protocols are crucial for\nsystem-on-chip (SoC) design and verification. However, manually constructing\nthese formal specifications from informal documents remains a tedious and\nerror-prone task. Although recent efforts have used Large Language Models\n(LLMs) to generate SystemVerilog Assertion (SVA) properties from design\ndocuments for Register-Transfer Level (RTL) design verification, in our\nexperience these approaches have not shown promise in generating SVA properties\nfor communication protocols. Since protocol specification documents are\nunstructured and ambiguous in nature, LLMs often fail to extract the necessary\ninformation and end up generating irrelevant or even incorrect properties. We\npropose FLAG, a two-stage framework to help construct formal protocol\nspecifications from informal documents. In the first stage, a predefined\ntemplate set is used to generate candidate SVA properties. To avoid missing\nnecessary properties, we develop a grammar-based approach to generate\ncomprehensive template sets that capture critical signal behaviors for various\ncommunication protocols. In the second stage, we utilize unambiguous timing\ndiagrams in conjunction with textual descriptions from the specification\ndocuments to filter out incorrect properties. A formal approach is first\nimplemented to check the candidate properties and filter out those inconsistent\nwith the timing diagrams. An LLM is then consulted to further remove incorrect\nproperties with respect to the textual description, obtaining the final\nproperty set. Experiments on various open-source communication protocols\ndemonstrate the effectiveness of FLAG in generating SVA properties from\ninformal documents."
                },
                "authors": [
                    {
                        "name": "Yu-An Shih"
                    },
                    {
                        "name": "Annie Lin"
                    },
                    {
                        "name": "Aarti Gupta"
                    },
                    {
                        "name": "Sharad Malik"
                    }
                ],
                "author_detail": {
                    "name": "Sharad Malik"
                },
                "author": "Sharad Malik",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06601v3",
                "updated": "2025-04-24T03:29:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    29,
                    59,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-10T15:51:15Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    15,
                    51,
                    15,
                    1,
                    254,
                    0
                ],
                "title": "LaMsS: When Large Language Models Meet Self-Skepticism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMsS: When Large Language Models Meet Self-Skepticism"
                },
                "summary": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination is a major challenge for large language models (LLMs),\npreventing their further application in some fields. The skeptical thinking of\nhumankind could be useful for LLMs to self-cognition, self-reflection and\nalleviate their hallucinations. Inspired by this consideration, we propose a\nnovel approach called LaMsS, which combines the semantic understanding\ncapability of LLMs with self-skepticism. By introducing a series of skepticism\ntokens and augmenting them into the vocabulary, we conduct both pertaining and\nfinetuning, which allow the LLM to decode each normal token followed by a\nskeptical token, representing different skepticism levels. By calculating the\nresponse skepticism given a query, one can define a new self-aware LLM which is\nonly willing to answer with relative lower skepticism level than the threshold.\nBy examining the accuracy, AUC and AP of willingly answering questions, we\ndemonstrate that LaMsS achieves better performance than baselines on both\nmulti-choice questions and open-domain question-answering benchmarks, and can\ngeneralize to multi-task and out-of-domain settings. Our study sheds some\nlights on the self-skepticism modeling on further artificial intelligence.\nProject code and model checkpoints can be found in\nhttps://anonymous.4open.science/r/SM-1E76."
                },
                "authors": [
                    {
                        "name": "Yetao Wu"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Luo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Luo Ji"
                },
                "author": "Luo Ji",
                "arxiv_comment": "11 pages, 6 figures, Published at ICLR 2025 Workshop on Scaling\n  Self-Improving Foundation Models,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17220v1",
                "updated": "2025-04-24T03:18:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    18,
                    16,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T03:18:16Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    3,
                    18,
                    16,
                    3,
                    114,
                    0
                ],
                "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle\n  Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Knowledge Distillation Matter for Large Language Model based Bundle\n  Generation?"
                },
                "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation."
                },
                "authors": [
                    {
                        "name": "Kaidong Feng"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Hui Fang"
                    },
                    {
                        "name": "Xinghua Qu"
                    },
                    {
                        "name": "Wenyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Liu"
                },
                "author": "Wenyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16129v2",
                "updated": "2025-04-24T02:54:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    54,
                    2,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T07:03:54Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    7,
                    3,
                    54,
                    0,
                    111,
                    0
                ],
                "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARFT: Multi-Agent Reinforcement Fine-Tuning"
                },
                "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks requiring multifaceted reasoning and\ncollaboration, from generating high-quality presentation slides to conducting\nsophisticated scientific research. Meanwhile, RL has been widely recognized for\nits effectiveness in enhancing agent intelligence, but limited research has\ninvestigated the fine-tuning of LaMAS using foundational RL techniques.\nMoreover, the direct application of MARL methodologies to LaMAS introduces\nsignificant challenges, stemming from the unique characteristics and mechanisms\ninherent to LaMAS. To address these challenges, this article presents a\ncomprehensive study of LLM-based MARL and proposes a novel paradigm termed\nMulti-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal\nalgorithmic framework tailored for LaMAS, outlining the conceptual foundations,\nkey distinctions, and practical implementation strategies. We begin by\nreviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage\nfor a parallel analysis in the multi-agent domain. In the context of LaMAS, we\nelucidate critical differences between MARL and MARFT. These differences\nmotivate a transition toward a novel, LaMAS-oriented formulation of RFT.\nCentral to this work is the presentation of a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work aims to serve as a roadmap for researchers seeking to advance MARFT\ntoward resilient and adaptive solutions in agentic systems. Our implementation\nof the proposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks requiring multifaceted reasoning and\ncollaboration, from generating high-quality presentation slides to conducting\nsophisticated scientific research. Meanwhile, RL has been widely recognized for\nits effectiveness in enhancing agent intelligence, but limited research has\ninvestigated the fine-tuning of LaMAS using foundational RL techniques.\nMoreover, the direct application of MARL methodologies to LaMAS introduces\nsignificant challenges, stemming from the unique characteristics and mechanisms\ninherent to LaMAS. To address these challenges, this article presents a\ncomprehensive study of LLM-based MARL and proposes a novel paradigm termed\nMulti-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal\nalgorithmic framework tailored for LaMAS, outlining the conceptual foundations,\nkey distinctions, and practical implementation strategies. We begin by\nreviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage\nfor a parallel analysis in the multi-agent domain. In the context of LaMAS, we\nelucidate critical differences between MARL and MARFT. These differences\nmotivate a transition toward a novel, LaMAS-oriented formulation of RFT.\nCentral to this work is the presentation of a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work aims to serve as a roadmap for researchers seeking to advance MARFT\ntoward resilient and adaptive solutions in agentic systems. Our implementation\nof the proposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT."
                },
                "authors": [
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "36 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14128v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14128v4",
                "updated": "2025-04-24T02:50:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    50,
                    28,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-19T01:02:42Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    1,
                    2,
                    42,
                    5,
                    109,
                    0
                ],
                "title": "TALES: Text Adventure Learning Environment Suite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALES: Text Adventure Learning Environment Suite"
                },
                "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tale-suite."
                },
                "authors": [
                    {
                        "name": "Christopher Zhang Cui"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "name": "Marc-Alexandre Côté"
                    }
                ],
                "author_detail": {
                    "name": "Marc-Alexandre Côté"
                },
                "author": "Marc-Alexandre Côté",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14128v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14128v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13941v2",
                "updated": "2025-04-24T02:38:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    38,
                    53,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-15T21:37:13Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    21,
                    37,
                    13,
                    1,
                    105,
                    0
                ],
                "title": "Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced through Reinforcement Learning (RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we propose\nNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.\nNEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applying\nstructured templates (e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiable reward modeling\nbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,\nGPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,\nNEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --\nusing 28% fewer tokens for correct answers -- highlighting more focused and\neffective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that\nintegrating multi-domain, multi-format data in RL leads to more accurate,\nefficient, and generalizable LLMs."
                },
                "authors": [
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Eric Nyberg"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17203v1",
                "updated": "2025-04-24T02:27:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    27,
                    17,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T02:27:17Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    27,
                    17,
                    3,
                    114,
                    0
                ],
                "title": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code\n  Generation Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code\n  Generation Services"
                },
                "summary": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets."
                },
                "authors": [
                    {
                        "name": "Shivasankari Kannan"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Amita Gondi"
                    },
                    {
                        "name": "Tristan Swadell"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17200v1",
                "updated": "2025-04-24T02:25:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    25,
                    6,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T02:25:06Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    25,
                    6,
                    3,
                    114,
                    0
                ],
                "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and\n  Adaptation"
                },
                "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support."
                },
                "authors": [
                    {
                        "name": "Yangxinyu Xie"
                    },
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Tanwi Mallick"
                    },
                    {
                        "name": "Joshua David Bergerson"
                    },
                    {
                        "name": "John K. Hutchison"
                    },
                    {
                        "name": "Duane R. Verner"
                    },
                    {
                        "name": "Jordan Branham"
                    },
                    {
                        "name": "M. Ross Alexander"
                    },
                    {
                        "name": "Robert B. Ross"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Leslie-Anne Levy"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Camillo J. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Camillo J. Taylor"
                },
                "author": "Camillo J. Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13192v2",
                "updated": "2025-04-24T02:16:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    16,
                    4,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-13T05:31:37Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    5,
                    31,
                    37,
                    6,
                    103,
                    0
                ],
                "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent"
                },
                "summary": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)\nhave brought significant advances in personalized user experience and have\nattracted considerable attention. Despite the impressive progress, the research\nquestion regarding the safety vulnerability of LLM-empowered RecSys still\nremains largely under-investigated. Given the security and privacy concerns, it\nis more practical to focus on attacking the black-box RecSys, where attackers\ncan only observe the system's inputs and outputs. However, traditional attack\napproaches employing reinforcement learning (RL) agents are not effective for\nattacking LLM-empowered RecSys due to the limited capabilities in processing\ncomplex textual inputs, planning, and reasoning. On the other hand, LLMs\nprovide unprecedented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like decision-making\nprocesses. Therefore, in this paper, we propose a novel attack framework called\nCheatAgent by harnessing the human-like capabilities of LLMs, where an\nLLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our\nmethod first identifies the insertion position for maximum impact with minimal\ninput modification. After that, the LLM agent is designed to generate\nadversarial perturbations to insert at target positions. To further improve the\nquality of generated perturbations, we utilize the prompt tuning technique to\nimprove attacking strategies via feedback from the victim RecSys iteratively.\nExtensive experiments across three real-world datasets demonstrate the\neffectiveness of our proposed attacking method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)\nhave brought significant advances in personalized user experience and have\nattracted considerable attention. Despite the impressive progress, the research\nquestion regarding the safety vulnerability of LLM-empowered RecSys still\nremains largely under-investigated. Given the security and privacy concerns, it\nis more practical to focus on attacking the black-box RecSys, where attackers\ncan only observe the system's inputs and outputs. However, traditional attack\napproaches employing reinforcement learning (RL) agents are not effective for\nattacking LLM-empowered RecSys due to the limited capabilities in processing\ncomplex textual inputs, planning, and reasoning. On the other hand, LLMs\nprovide unprecedented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like decision-making\nprocesses. Therefore, in this paper, we propose a novel attack framework called\nCheatAgent by harnessing the human-like capabilities of LLMs, where an\nLLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our\nmethod first identifies the insertion position for maximum impact with minimal\ninput modification. After that, the LLM agent is designed to generate\nadversarial perturbations to insert at target positions. To further improve the\nquality of generated perturbations, we utilize the prompt tuning technique to\nimprove attacking strategies via feedback from the victim RecSys iteratively.\nExtensive experiments across three real-world datasets demonstrate the\neffectiveness of our proposed attacking method."
                },
                "authors": [
                    {
                        "name": "Liang-bo Ning"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Feiran Huang"
                    }
                ],
                "author_detail": {
                    "name": "Feiran Huang"
                },
                "author": "Feiran Huang",
                "arxiv_doi": "10.1145/3637528.3671837",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671837",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by KDD 2024;",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17198v1",
                "updated": "2025-04-24T02:15:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    15,
                    45,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T02:15:45Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    2,
                    15,
                    45,
                    3,
                    114,
                    0
                ],
                "title": "Automatically Generating Rules of Malicious Software Packages via Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Generating Rules of Malicious Software Packages via Large\n  Language Model"
                },
                "summary": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories."
                },
                "authors": [
                    {
                        "name": "XiangRui Zhang"
                    },
                    {
                        "name": "HaoYu Chen"
                    },
                    {
                        "name": "Yongzhong He"
                    },
                    {
                        "name": "Wenjia Niu"
                    },
                    {
                        "name": "Qiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Li"
                },
                "author": "Qiang Li",
                "arxiv_comment": "14 pages, 11 figures",
                "arxiv_journal_ref": "the 55th Annual IEEE/IFIP International Conference on Dependable\n  Systems and Networks(DSN), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17192v1",
                "updated": "2025-04-24T01:57:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    57,
                    1,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T01:57:01Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    57,
                    1,
                    3,
                    114,
                    0
                ],
                "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning"
                },
                "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins."
                },
                "authors": [
                    {
                        "name": "Minju Seo"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Seongyun Lee"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14809v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14809v3",
                "updated": "2025-04-24T01:48:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    48,
                    28,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T02:19:06Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    19,
                    6,
                    0,
                    111,
                    0
                ],
                "title": "vApps: Verifiable Applications at Internet Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vApps: Verifiable Applications at Internet Scale"
                },
                "summary": "Blockchain technology promises a decentralized, trustless, and interoperable\ninfrastructure. However, widespread adoption remains hindered by issues such as\nlimited scalability, high transaction costs, and the complexity of maintaining\ncoherent verification logic across different blockchain layers. This paper\nintroduces Verifiable Applications (vApps), a novel development framework\ndesigned to streamline the creation and deployment of verifiable blockchain\ncomputing applications. vApps offer a unified Rust-based Domain-Specific\nLanguage (DSL) within a comprehensive SDK, featuring modular abstractions for\nverification, proof generation, and inter-chain connectivity. This eases the\ndeveloper's burden in securing diverse software components, allowing them to\nfocus on application logic. The DSL also ensures that applications can\nautomatically take advantage of specialized precompiles and hardware\nacceleration to achieve consistently high performance with minimal developer\neffort, as demonstrated by benchmark results for zero-knowledge virtual\nmachines (zkVMs). Experiments show that native Rust execution eliminates\ninterpretation overhead, delivering up to an 832x cycle count improvement\ncompared to EVM-based approaches. Precompiled circuits can accelerate the proof\nby more than 95%, while GPU acceleration increases throughput by up to 30x and\nrecursion compresses the proof size by up to 230x, enabling succinct and\nefficient verification. The framework also supports seamless integration with\nthe Web2 and Web3 systems, enabling developers to focus solely on their\napplication logic. Through modular architecture, robust security guarantees,\nand composability, vApps pave the way toward a trust-minimized and verifiable\nInternet-scale application environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain technology promises a decentralized, trustless, and interoperable\ninfrastructure. However, widespread adoption remains hindered by issues such as\nlimited scalability, high transaction costs, and the complexity of maintaining\ncoherent verification logic across different blockchain layers. This paper\nintroduces Verifiable Applications (vApps), a novel development framework\ndesigned to streamline the creation and deployment of verifiable blockchain\ncomputing applications. vApps offer a unified Rust-based Domain-Specific\nLanguage (DSL) within a comprehensive SDK, featuring modular abstractions for\nverification, proof generation, and inter-chain connectivity. This eases the\ndeveloper's burden in securing diverse software components, allowing them to\nfocus on application logic. The DSL also ensures that applications can\nautomatically take advantage of specialized precompiles and hardware\nacceleration to achieve consistently high performance with minimal developer\neffort, as demonstrated by benchmark results for zero-knowledge virtual\nmachines (zkVMs). Experiments show that native Rust execution eliminates\ninterpretation overhead, delivering up to an 832x cycle count improvement\ncompared to EVM-based approaches. Precompiled circuits can accelerate the proof\nby more than 95%, while GPU acceleration increases throughput by up to 30x and\nrecursion compresses the proof size by up to 230x, enabling succinct and\nefficient verification. The framework also supports seamless integration with\nthe Web2 and Web3 systems, enabling developers to focus solely on their\napplication logic. Through modular architecture, robust security guarantees,\nand composability, vApps pave the way toward a trust-minimized and verifiable\nInternet-scale application environment."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Kshitij Kulkarni"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "John Guibas"
                    },
                    {
                        "name": "Uma Roy"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Ryan Zarick"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Zarick"
                },
                "author": "Ryan Zarick",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14809v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14809v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17173v1",
                "updated": "2025-04-24T01:16:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    16,
                    40,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T01:16:40Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    16,
                    40,
                    3,
                    114,
                    0
                ],
                "title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale\n  ISAC Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale\n  ISAC Platform"
                },
                "summary": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline."
                },
                "authors": [
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Dongheng Zhang"
                    },
                    {
                        "name": "Ruixu Geng"
                    },
                    {
                        "name": "Xuecheng Xie"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06425v2",
                "updated": "2025-04-24T00:37:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    0,
                    37,
                    48,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-10T13:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    13,
                    2,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Privacy-Preserving Personalized Advice with Zero-Knowledge\n  Proofs and LLMs"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in domains such as\nfinance, healthcare, and interpersonal relationships to provide advice tailored\nto user traits and contexts. However, this personalization often relies on\nsensitive data, raising critical privacy concerns and necessitating data\nminimization. To address these challenges, we propose a framework that\nintegrates zero-knowledge proof (ZKP) technology, specifically zkVM, with\nLLM-based chatbots. This integration enables privacy-preserving data sharing by\nverifying user traits without disclosing sensitive information. Our research\nintroduces both an architecture and a prompting strategy for this approach.\nThrough empirical evaluation, we clarify the current constraints and\nperformance limitations of both zkVM and the proposed prompting strategy,\nthereby demonstrating their practical feasibility in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Hiroki Watanabe"
                    },
                    {
                        "name": "Motonobu Uchikoshi"
                    }
                ],
                "author_detail": {
                    "name": "Motonobu Uchikoshi"
                },
                "author": "Motonobu Uchikoshi",
                "arxiv_doi": "10.1145/3701716.3715597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to The ACM Web Conference (WWW) 2025 Short Paper Track",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]